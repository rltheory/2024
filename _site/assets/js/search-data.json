{"0": {
    "doc": "About CMPUT 605",
    "title": "CMPUT 605: Theoretical Foundations of Reinforcement Learning W2023",
    "content": "The purpose of this course is to let students acquire a solid understanding of the theoretical foundations of reinforcement learning, as well as to give students a glimpse on what theoretical research looks like in the context of computer science. The topics will range from building up foundations (Markovian Decision Processes and the various special cases of it), to discussing solutions to two core problem settings: . | planning/simulation optimization, and | online reinforcement learning | . In each of these settings, we cover key algorithmic challenges and the core ideas to address these. Specific topics, ideas and algorithms covered include, for each topic: . | complexity of planning/simulation optimization; large scale planning with function approximation; | efficient online learning: the role (and limits) of optimism; scaling up with function approximation. | . While we will explore connections to (some) deep RL methods, mainly seeking an answer to the question of when we expect them to work well, the course will not focus on deep RL. ",
    "url": "/2024/pages/about/#cmput-605-theoretical-foundations-of-reinforcement-learning-w2023",
    
    "relUrl": "/pages/about/#cmput-605-theoretical-foundations-of-reinforcement-learning-w2023"
  },"1": {
    "doc": "About CMPUT 605",
    "title": "Pre-requisites",
    "content": "Students taking the course are expected to have an understanding of basic probability, basics of concentration inequalities, linear algebra and convex optimization. This background is covered in Chapters 2, 3, 5, 7, 26, and 38 of the Bandit Algorithms book. One very nice book that covers more, but is still highly recommended is A Second Course in Probability Theory. The book is available online and also in book format. Chapters 1, 3, 4, and 5 are most useful from here. It will also be useful to recall foundations of mathematical analysis, such as completeness, metric spaces and alike, as we will start off with results that will require Banach’s fixed point theorem. This is covered, for example, in Appendix A of Csaba’s “little” RL book. The wikipedia page on Banach’s fixed point theorem is not that bad either. ",
    "url": "/2024/pages/about/#pre-requisites",
    
    "relUrl": "/pages/about/#pre-requisites"
  },"2": {
    "doc": "About CMPUT 605",
    "title": "Instruction Team",
    "content": ". | David Janz (lead instructor) | Johannes Kirschner (lead instructor) | Csaba Szepesvári | Alex Ayoub (Teaching Assistant) | Vlad Tkachuk (Teaching Assistant) | . ",
    "url": "/2024/pages/about/#instruction-team",
    
    "relUrl": "/pages/about/#instruction-team"
  },"3": {
    "doc": "About CMPUT 605",
    "title": "Lecture Time",
    "content": "Monday and Wednesdays from 1:00 PM - 2:30 PM (MST) in CSC 3-33. Office Hours . Office hours are at 2:00pm - 4:00pm on the Friday before each assignment is due (Exceptions indicated below). Location: Breakout Room in CSC (Exact room to be announced around 1:55pm on the Friday). | Vlad Tkachuk: Jan 27 (Time changed to 10:00am - 12:00pm (noon)) and Feb 10 | Alex Ayoub: Mar 10 and Mar 24 | . ",
    "url": "/2024/pages/about/#lecture-time",
    
    "relUrl": "/pages/about/#lecture-time"
  },"4": {
    "doc": "About CMPUT 605",
    "title": "Slack Channel",
    "content": "We will use Slack for everything. We have a channel called #cmput-605-students on the Amii slack to discuss all topics related for this course. If you would like to join the channel please message Vlad Tkachuk (vtkachuk@ualberta.ca) for an invitation. All announcements will be made on #cmput-605-students. We strongly encourage all students to ask questions regarding course content on the Slack channel! . ",
    "url": "/2024/pages/about/#slack-channel",
    
    "relUrl": "/pages/about/#slack-channel"
  },"5": {
    "doc": "About CMPUT 605",
    "title": "Lectures Notes",
    "content": "The lecture notes for this year’s class are under the heading LECTURE NOTES. The lecture notes for this year serve as the required text for this course. Lecture notes for the last two years are available on this site under headings WINTER 2022 LECTURE NOTES and WINTER 2021 LECTURE NOTES. ",
    "url": "/2024/pages/about/#lectures-notes",
    
    "relUrl": "/pages/about/#lectures-notes"
  },"6": {
    "doc": "About CMPUT 605",
    "title": "Grading",
    "content": "The work you will be required to do for this course includes 4 (marked) assignments, 1 midterm, and 1 final project. For all course work submissions please send your completed work to Vlad Tkachuk via private message on Slack before the due date. More details can be found in The Work You Do page. Keywords: RL theory, Reinforcement Learning, Theoretical Reinforcement Learning . ",
    "url": "/2024/pages/about/#grading",
    
    "relUrl": "/pages/about/#grading"
  },"7": {
    "doc": "About CMPUT 605",
    "title": "About CMPUT 605",
    "content": " ",
    "url": "/2024/pages/about/",
    
    "relUrl": "/pages/about/"
  },"8": {
    "doc": "About CMPUT 653 (OLD)",
    "title": "CMPUT 653: Theoretical Foundations of Reinforcement Learning W2022",
    "content": "The purpose of this course is to let students acquire a solid understanding of the theoretical foundations of reinforcement learning, as well as to give students a glimpse on what theoretical research looks like in the context of computer science. The topics will range from building up foundations (Markovian Decision Processes and the various special cases of it), to discussing solutions to the three core problem settings: . | planning/simulation optimization | batch reinforcement learning, and | online reinforcement learning | . In each of these settings, we cover key algorithmic challenges and the core ideas to address these. Specific topics, ideas and algorithms covered include, for each topic: . | complexity of planning/simulation optimization; large scale planning with function approximation; | sample complexity of batch learning with and without function approximation; | efficient online learning: the role (and limits) of optimism; scaling up with function approximation. | . While we will explore connection to (some) deep RL methods, mainly seeking an answer to the question of when can we expect them to work well, the course will not focus on deep RL. ",
    "url": "/2024/pages/about_cmput653/#cmput-653-theoretical-foundations-of-reinforcement-learning-w2022",
    
    "relUrl": "/pages/about_cmput653/#cmput-653-theoretical-foundations-of-reinforcement-learning-w2022"
  },"9": {
    "doc": "About CMPUT 653 (OLD)",
    "title": "Pre-requisites",
    "content": "Students taking the course are expected to have an understanding of basic probability, basics of concentration inequalities, linear algebra and convex optimization. This background is covered in Chapters 2, 3, 5, 7, 26, and 38 of the Bandit Algorithms book. One very nice book that covers more, but is still highly recommended is A Second Course in Probability Theory. The book is available online and also in book format. Chapters 1, 3, 4, and 5 are most useful from here. It will also be useful to recall foundations of mathematical analysis, such as completeness, metric spaces and alike, as we will start off with results that will require Banach’s fixed point theorem. This is covered, for example, in Appendix A of Csaba’s “little” RL book. The wikipedia page on Banach’s fixed point theorem is not that bad either. ",
    "url": "/2024/pages/about_cmput653/#pre-requisites",
    
    "relUrl": "/pages/about_cmput653/#pre-requisites"
  },"10": {
    "doc": "About CMPUT 653 (OLD)",
    "title": "Instruction Team",
    "content": ". | Csaba Szepesvári | Alex Ayoub | Vlad Tkachuk | . ",
    "url": "/2024/pages/about_cmput653/#instruction-team",
    
    "relUrl": "/pages/about_cmput653/#instruction-team"
  },"11": {
    "doc": "About CMPUT 653 (OLD)",
    "title": "Lecture Time (recordings here)",
    "content": "Monday and Wednesdays from 2:00 PM - 3:20 PM (MST). The first three weeks will be online only and the format will be that of a flipped class. For the rest (Jan 25 and later), we aim for traditional, in-person lectures with chalk-board talks. The room is GSB 5-53 . ",
    "url": "/2024/pages/about_cmput653/#lecture-time-recordings-here",
    
    "relUrl": "/pages/about_cmput653/#lecture-time-recordings-here"
  },"12": {
    "doc": "About CMPUT 653 (OLD)",
    "title": "eClass",
    "content": "We will use eclass for assignment submissions. The link to join eClass can be found here. We will not use eClass for announcements and discussions. For these we will use Slack. ",
    "url": "/2024/pages/about_cmput653/#eclass",
    
    "relUrl": "/pages/about_cmput653/#eclass"
  },"13": {
    "doc": "About CMPUT 653 (OLD)",
    "title": "Slack Channel",
    "content": "We have a channel called #cmput653-discussion-w2022 on the Amii slack to discuss all topics related to this course. This channel is open to anyone who is on Amii slack. If you are already a part of the Amii slack, feel free to join this channel. For discussions related to marking, assignment schedule, etc. we have a second channel #cmput653-private-discussion-w2022, which is by invitation only. The TAs will add anyone who is taking the course for credit to these slack channels. All announcements will be made on #cmput653-discussion-w2022. ",
    "url": "/2024/pages/about_cmput653/#slack-channel",
    
    "relUrl": "/pages/about_cmput653/#slack-channel"
  },"14": {
    "doc": "About CMPUT 653 (OLD)",
    "title": "Google Meet Information",
    "content": "The google meet information will be posted on the slack channel and on eClass. This is relevant up to the point when teaching becomes in-person. ",
    "url": "/2024/pages/about_cmput653/#google-meet-information",
    
    "relUrl": "/pages/about_cmput653/#google-meet-information"
  },"15": {
    "doc": "About CMPUT 653 (OLD)",
    "title": "Grading Policies",
    "content": "Can be found on eClass. ",
    "url": "/2024/pages/about_cmput653/#grading-policies",
    
    "relUrl": "/pages/about_cmput653/#grading-policies"
  },"16": {
    "doc": "About CMPUT 653 (OLD)",
    "title": "Lectures Notes",
    "content": "Lecture notes of last year’s class is available on this site. The lecture notes for this year’s class starts from this, but may be modified. The lecture notes serve as the required text for this course. ",
    "url": "/2024/pages/about_cmput653/#lectures-notes",
    
    "relUrl": "/pages/about_cmput653/#lectures-notes"
  },"17": {
    "doc": "About CMPUT 653 (OLD)",
    "title": "Flipped Class",
    "content": "For the first three weeks, as mentioned above, we will follow a flipped class format: Students coming to class are required to . | read the associated lecture notes and/or watch the lecture recordings | prepare and vote on questions on the slack discussion channel | . In class time will be spent on a . | quick review of the material | discussing the most voted questions | small group discussions of various topics | . Keywords: RL theory, Reinforcement Learning, Theoretical Reinforcement Learning . ",
    "url": "/2024/pages/about_cmput653/#flipped-class",
    
    "relUrl": "/pages/about_cmput653/#flipped-class"
  },"18": {
    "doc": "About CMPUT 653 (OLD)",
    "title": "About CMPUT 653 (OLD)",
    "content": " ",
    "url": "/2024/pages/about_cmput653/",
    
    "relUrl": "/pages/about_cmput653/"
  },"19": {
    "doc": "The work you do",
    "title": "The Work You Do",
    "content": "| Component | Weight | Deadline | PDF and LaTex | . | Assignment 0 (Not Graded) | 0% | January 15, 2023 11:55pm | PDF and LaTex | . | Assignment 1 | 10% | January 29, 2023 11:55pm | PDF and LaTex | . | Assignment 2 | 10% | February 12, 2023 11:55pm | PDF and LaTex | . | Midterm | 20% | February 26, 2023 11:55pm | PDF and LaTex | . | Project (Proposal) | 10% | March 5, 2023 11:55pm |   | . | Assignment 3 | 10% | March 12, 2023 11:55pm | PDF and LaTex | . | Assignment 4 | 10% | March 26, 2023 11:55pm | PDF and LaTex | . | Project (Presentation) | 10% | April 11 and 12, 2023 (in class) |   | . | Project (Report) | 20% | April 21, 2023 11:55pm |   | . ",
    "url": "/2024/pages/assignments/#the-work-you-do",
    
    "relUrl": "/pages/assignments/#the-work-you-do"
  },"20": {
    "doc": "The work you do",
    "title": "Late Policy",
    "content": "Late submissions will not be graded. Please submit your work by the deadline. ",
    "url": "/2024/pages/assignments/#late-policy",
    
    "relUrl": "/pages/assignments/#late-policy"
  },"21": {
    "doc": "The work you do",
    "title": "Course Project",
    "content": " ",
    "url": "/2024/pages/assignments/#course-project",
    
    "relUrl": "/pages/assignments/#course-project"
  },"22": {
    "doc": "The work you do",
    "title": "Deadlines",
    "content": ". | Proposal: March 5, 2023 11:55pm | Presentations: April 11 and 12, 2023, in class | Report: April 18, 2023 11:55pm | . ",
    "url": "/2024/pages/assignments/#deadlines",
    
    "relUrl": "/pages/assignments/#deadlines"
  },"23": {
    "doc": "The work you do",
    "title": "Objective and evaluation",
    "content": "The project should be done individually or in a group of two (maximum group size of two). The goal is for everyone to get a taste of how it is to work on theoretical aspects of reinforcement learning. In the project, you do not actually need to produce research paper quality results (although if you do, no one will complain!). It is sufficient to demonstrate a thorough understanding of some aspect of the theory literature, such as: . | What are the interesting questions to ask (and what are less interesting questions?) | What is known about a given topic (and what is not known)? | Sorting out whether some assumption is critical for some result (or not). | . When evaluating the reports, we will not care that much about originality (new results) than coherence, soundness and the quality of writing. In fact, a typical report is expected to be a readable (and possibly entertaining) summary of a topic in the area. Reports that contain original results are also welcome, just to earn full grade, originality is absolutely not required. We strongly recommend to start small: Aim for writing a review of some results of interest. If time permits and as you feel fit, add new results. Having said this, if you score a new result early on, it is also OK to start on writing that result down. ",
    "url": "/2024/pages/assignments/#objective-and-evaluation",
    
    "relUrl": "/pages/assignments/#objective-and-evaluation"
  },"24": {
    "doc": "The work you do",
    "title": "How to choose a topic?",
    "content": ". | Choose a theory paper and rewrite it to make it better. Choose and pick of what you include in your report. It may be better proofs. It may be better exposition of the results. Be critical about assumptions (but not overly critical). It may be putting the results into a perspective. Aim for readable (but technically correct) writeups. | Choose a problem that you care about in the area. Ask what is known. Write a summary about it. Be specific about what problems you are writing about. If time permits and with some luck, add new results. Aim for small things, like, such and such is known in topic A but only under condition B. Do these results extend to condition C? What conditions are necessary? How about slightly changing the problem, for example switching from finite horizon to infinite horizon objectives? Multicriteria? | Choose an open question and try to answer it. Loads of open questions are mentioned in the class. When there is an upper bound, ask whether there is a matching lower bound. If not quite, try to reduce the gap. Ditto for lower bounds. Any time you see a bound you can ask: Is this tight? The endnotes of the lectures on this website list some of the open questions. | It is a bit more risky, but possibly more rewarding, is to choose a non-theory paper and look at it through the eyes of a theoretician. Are there any hard claims that could be formulated (and possibly proved) in the context of the paper? If the paper is proposing algorithms, are there any conditions when the algorithm proposed will work “well”? How well? Put the results into the context of what is known. Example: Is TRPO a sound algorithm? Say, in the tabular setting? | . ",
    "url": "/2024/pages/assignments/#how-to-choose-a-topic",
    
    "relUrl": "/pages/assignments/#how-to-choose-a-topic"
  },"25": {
    "doc": "The work you do",
    "title": "Formatting",
    "content": "The reports should be typeset in latex and sent as a pdf document. The template is available here. The report should be maximum 9 pages long, the proposal maximum 2 pages long. They should have the standard structure: . | Introduction (what is the problem studied, why do we study it) | Results (the “meat”) | Conclusions/summary (what did we learn? what is the short take-away from all of this? what’s next if anything?) | . ",
    "url": "/2024/pages/assignments/#formatting",
    
    "relUrl": "/pages/assignments/#formatting"
  },"26": {
    "doc": "The work you do",
    "title": "Examples of topics",
    "content": ". | Read “Is Plug-in Solver Sample-Efficient for Feature-based Reinforcement Learning?” (link) by Qiwen Cui and Lin F. Yang. Place the result in the framework of the class. Which problem are they solving? What are the pros and cons of what they do? What is the significance of the various assumptions? Are there assumptions we could be dropped? Relaxed? Would this extend to other model classes? What is the general lesson? . | Read “Variance-Aware Confidence Set: Variance-Dependent Bound for Linear Bandits and Horizon-Free Bound for Linear Mixture MDP” (link) by Zihan Zhang, Jiaqi Yang, Xiangyang Ji, Simon S. Du, which is a follow-up to “Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon” (link) by Zihan Zhang, Xiangyang Ji, Simon S. Du. Same questions as before. Do we expect the techniques to be practically useful? When? If not, why not? Can things be fixed up? Extensions to other settings, batch, planning? Infinite horizon problems would really test the limits. | We could go on and list all papers that appeared recently on RL theory (a long list, check out the RL Theory Seminar pages for some starting points). An alternative is to consider specific topics such as (1) how to deal with generalization (2) multiple criteria (3) robustness (4) long horizons beyond variance reduction (5) various model classes beyond linear and linear mixture MDPs (6) nonstationarity (7) value-aware model fitting – is it a good idea? (8) better ways of exploring? is Information Directed Sampling the way to go? (9) what are the limits of adaptive algorithms in RL? . | For further inspiration, visit the project page of the class that Nan Jiang taught recently, or this page by Wen Sun and Sam Kakade. | . ",
    "url": "/2024/pages/assignments/#examples-of-topics",
    
    "relUrl": "/pages/assignments/#examples-of-topics"
  },"27": {
    "doc": "The work you do",
    "title": "The work you do",
    "content": " ",
    "url": "/2024/pages/assignments/",
    
    "relUrl": "/pages/assignments/"
  },"28": {
    "doc": "Home",
    "title": "Welcome",
    "content": "This is the homepage of the course: Theoretical Foundations of Reinforcement Learning taught by Csaba Szepesvári at the University of Alberta. Additional information and resources can be accessed from the sidepane on the left. The main website pages are under the heading PAGES and the course notes for this year are organized under the heading LECTURE NOTES. The course notes from previous years are in the headings below. | This will be good! . | . ",
    "url": "/2024/#welcome",
    
    "relUrl": "/#welcome"
  },"29": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/2024/",
    
    "relUrl": "/"
  },"30": {
    "doc": "Lectures",
    "title": "Lectures",
    "content": "| # | Date | Lecture Videos | Whiteboard Notes/Slides | . | 1. | January 5, 2022 | Introduction Discussion | link link | . | 2. | January 10, 2022 | MDP, Fundamental Theorem Discussion | link link | . | 3. | January 12, 2022 | Value and Policy Iteration Discussion | link link | . | 4. | January 17, 2022 | Policy Iteration and Local Planning Discussion | link link | . | 5. | January 19, 2022 | Local Planning Discussion | link link | . | 6. | January 24, 2022 | Local Planning (contd.) Discussion | link link | . | 7. | January 26, 2022 | Function Approximation Discussion | link link | . | 8. | January 31, 2022 | Approximate Policy Iteration Discussion | link link | . | 9. | February 2, 2022 | Approximate Policy Iteration (contd.) Discussion | link link | . | 10. | February 7, 2022 | Planning Complexity Discussion | link link | . | 11. | February 9, 2022 | Planning Complexity: TensorPlan Discussion | link link | . | 12. | February 14, 2022 | Planning Complexity: TensorPlan (contd.) Discussion | link link | . | 13. | February 16, 2022 | Lower Bound for API and POLITEX Discussion | link link | . | 14. | February 28, 2022 | POLITEX (contd.) Discussion | link | . | 15. | March 2, 2022 | Policy Search Discussion | link | . | 16. | March 7, 2022 | Policy Search (contd.) Discussion | link | . | 17. | March 9, 2022 | Batch RL: Introduction Discussion | link | . | 18. | March 14, 2022 | Batch RL: Finite MDPs Discussion | link | . | 19. | March 16, 2022 | Batch RL: Function approximation Discussion | link | . | 20. | March 21, 2022 | Batch RL 4 Discussion | link | . | 21. | March 23, 2022 | Batch Excursion (contd.) Discussion | link | . | 22. | March 28, 2022 | Online RL Discussion | link | . | 23. | March 30, 2022 | Online RL 2 Discussion | link | . | 24. | April 4, 2022 | Online RL 3 Discussion | link | . ",
    "url": "/2024/pages/lectures/",
    
    "relUrl": "/pages/lectures/"
  },"31": {
    "doc": "22. Introduction",
    "title": "Sample complexity and regret: How good is the learner?",
    "content": "The goal of the learner is to collect as much reward as possible. We denote $V_k = \\sum_{h=0}^{H-1} r_{A_h^{(k)}}(S_h^{(k)})$ as the reward collected by the learner in episode $k$. The total reward is $\\sum_{k=1}^K V_k$. For the analysis it will be useful to introduce a normalization: Instead of directly arguing about the total reward, we compare the learner to the value \\(v_0^*(S_0^{(k)})\\) of the best policy in the MDP. This leads to the notation of regret defined as follows: . \\[R_K = \\sum_{k=1}^K \\big(v_0^*(S_0^{(k)}) - V_k\\big)\\] A learner has sublinear expected regret if $\\mathbb{E}[R_K/K]\\rightarrow 0$ as $K \\rightarrow \\infty$. Sublinear regret means that the average reward of the learner approaches the optimal value $v_0^*(\\mu)$ as the number of episodes increases. Certainly that is a desirable property! . Before we go on to construct learners with small regret, we briefly note that there are also other objectives. The most common alternative is PAC - which stands for probably approximately correct. A learner is said to be $(\\epsilon,\\delta)$-PAC if upon termination in episode $K$, it outputs a policy such that \\(v_0^{*}(s_0^{(k)}) - \\mathbb{E}[V_K] \\leq \\epsilon\\) with probability at least $1-\\delta$. We have discussed PAC bounds already in the context of planning. The difference to bounding regret is that in the first $K-1$ episodes, the learner does not ‘pay’ for choosing suboptimal actions. This is sometimes called a pure exploration problem. Note that a learner that achieves sublinear regret can be converted into a PAC learner (discussed in the notes). However, this may lead to a suboptimal (large) $K$ in the PAC framework. ",
    "url": "/2024/lecture-notes/online-rl/lec22/#sample-complexity-and-regret-how-good-is-the-learner",
    
    "relUrl": "/lecture-notes/online-rl/lec22/#sample-complexity-and-regret-how-good-is-the-learner"
  },"32": {
    "doc": "22. Introduction",
    "title": "$\\epsilon$-greedy",
    "content": "There exist many ideas on how to design algorithms with small regret. We first note that a “greedy” agent can easily fail: Following the best actions according to some empirical estimate can easily get you trapped in a supoptimal policy (think of some examples where this can happen!). A simple remedy is to add a small amount of “forced” exploration: With (small) probability $\\epsilon$, we choose an action uniformly at random. Thereby we eventually collect samples from all actions to improve our estimates. With probabilty $(1-\\epsilon)$ we follow the “greedy” choice, that is the action that appears best under our current estimates. This gives raise to the name $\\epsilon$-greedy. It is often possible to show that $\\epsilon$-greedy converges. By carefully choosing the exploration probability $\\epsilon$, we may show that in finite MDPs, the regret is at most $R_K \\leq \\mathcal{O}(K^{2/3})$. As we will discuss later, there are multiple algorithms that achieve a regret of only $\\mathcal{O}(K^{1/2})$. Thus, $\\epsilon$-greedy is not the best algorithm to minimize regret. Not unexpectedly, this type of exploration can be quite sub-optimal. It is easy to construct examples, where $\\epsilon$-greedy takes exponential time (in the number of states) to reach an optimal policy. Can you find an example (Hint: construct the MDP such that each time the agent explores a suboptimal action, the agent is reset to the starting state)? . On the upside, $\\epsilon$-greedy is very simple and can easily used in more complex scenarios. In fact, it is a popular choice when using neural network function approximations, where theoretically grounded exploration schemes are much harder to obtain. ",
    "url": "/2024/lecture-notes/online-rl/lec22/#epsilon-greedy",
    
    "relUrl": "/lecture-notes/online-rl/lec22/#epsilon-greedy"
  },"33": {
    "doc": "22. Introduction",
    "title": "Optimism Principle",
    "content": "A popular technique to construct regret minimizing algorithms is based on optimism in the face of uncertainty. To formally define the idea, let $\\mathcal{M}$ be the set of possible environments (e.g. finite MDPs). We make the realizability assumption that the true environment $M^* \\in \\mathcal{M}$ is in this set. After obtaining data in rounds $1,\\dots, k-1$, the learner uses the observations to compute a set of plausible models $\\mathcal{M}_k\\subset \\mathcal{M}$. The plausible model set is such that it contains the true model with high probabilty. Although this is not always required, it is useful to think of a decreasing sequence of sets $\\mathcal{M} \\supset\\mathcal{M}_1 \\supset\\mathcal{M}_2 \\supset \\cdots \\supset\\mathcal{M}_k$. This simply means that as more data arrives, the learner is able to exclude models that are statistically unlikely to produce the observation data. The optimism principle is to act according to the policy that achieves the highest reward among all plausible models, i.e. \\(\\begin{align} \\label{eq:opt-policy} \\pi_k = \\arg\\max_{\\pi} \\max_{M \\in \\mathcal{M}_k} v_M^\\pi \\end{align}\\) At this point it not be clear why this leads to an efficient learning algorithm (with small regret). The idea is that the learner systematically obtains data about the environment. For example, if data contradicts the optimistic model $\\tilde M_k = \\arg\\max_{M \\in \\mathcal{M}} \\max_\\pi v_M^\\pi$, then $\\tilde M_k \\notin \\mathcal{\\mathcal{M}}_{k+1}$ is excluded from the set of plausible models in the future. Consequently, the learner chooses a different policy in the next round. On the other hand, the learner ensures that \\(M^{*} \\in \\mathcal{M}_k\\) with high probability. In this case, it is often possible to show that the gap \\(v_{\\tilde M_k}^{\\pi_k} - v_{M^{*}}^{*} \\geq 0\\) is small (more specifically, behaves like a statistical estimation error of order $\\mathcal{O}(t^{-1/2})$ with a leading constant that depends on the “size” of $\\mathcal{M}$). One should also ask if the optimization problem \\(\\eqref{eq:opt-policy}\\) can be solved efficiently. This is far from always the case. Often one needs to rely on heuristics to implement the optimistic policy, or use other exploration techniques such as Thompson sampling (see below). How much regret the learner has of course depends on the concrete setting at hand. In the next lecture we will see how we can make use of optimism to design (and analyize) an online learning algorithm for finite MDPs. The literature has produced a large amount of papers with algorithms that use the optimism principle in many settings. This however does not mean that optimism is a universal tool. More recent literature has also pointed out limitations of the optimsm principle, and in lieu proposed other design ideas. ",
    "url": "/2024/lecture-notes/online-rl/lec22/#optimism-principle",
    
    "relUrl": "/lecture-notes/online-rl/lec22/#optimism-principle"
  },"34": {
    "doc": "22. Introduction",
    "title": "Notes",
    "content": "Other Exploration Techniques . Some other notable exploration strategies are: . | Phased-Elimination and Experimental Design | Thompson Sampling | Information-Directed Sampling (IDS) and Estimation-To-Decisions (E2D) | . ",
    "url": "/2024/lecture-notes/online-rl/lec22/#notes",
    
    "relUrl": "/lecture-notes/online-rl/lec22/#notes"
  },"35": {
    "doc": "22. Introduction",
    "title": "References",
    "content": "The paper showing the details behind how to convert between Regret and PAC bounds. | Dann, C., Lattimore, T., &amp; Brunskill, E. (2017). Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning. Advances in Neural Information Processing Systems, 30. [link] | . ",
    "url": "/2024/lecture-notes/online-rl/lec22/#references",
    
    "relUrl": "/lecture-notes/online-rl/lec22/#references"
  },"36": {
    "doc": "22. Introduction",
    "title": "22. Introduction",
    "content": "PDF Version . Online learning in reinforcement learning refers to the idea that a learner is placed in an (initially) unknown MDP. By interacting with the MDP, the learner collects data about the unknown transition and reward function. The learner’s goal is to collect as much reward as possible, or output a near-optimal policy. The difference to planning is that the learner does not have access to the true MDP. Unlike in batch RL, the learner gets to decide what actions to play. Importantly, this means the learner’s action affect the data that is available to the learner (sometimes refered to as “closed loop”). The fact that the learner needs to create its own data leads to an important decision: Should the learner sacrifice reward to collect more data that will improve decision making in the future? Or should it act according to what seems currently best? Clearly, too much exploration will be costly if the learner chooses actions with low reward too often. On the other hand, playing actions that appear optimal with limited data comes at the risk of missing out on even better rewards. In the literature, this is commonly known as exploration-exploitation dilemma. The exploration-exploitation dilemma is not specific to the MDP setting. It already arises in the simpler (multi-armed) bandit setting (i.e. an MDP with only one state and stochastic reward). In the following, we focus on finite-horizon episodic (undiscounted) MDPs $M = (\\mathcal{S},\\mathcal{A},P,r, \\mu)$. The learner interacts with the MDP for $K$ episodes of length $H &gt; 0$. At the beginning of each episode $k=1\\,\\dots,K$, an initial state is sampled from the initial distribution $S_0^{k} \\sim \\mu$. The data collected during the $k^{th}$ episode is . \\[S_0^{(k)}, A_0^{(k)}, R_1^{(k)}, S_1^{(k)} A_1^{(k)}, R_2^{(k)}, S_2^{(k)}, \\dots S_{H-1}^{(k)}, A_{H-1}^{(k)},R_H^{(k)}, S_H^{(k)}\\] where $A_h^{k}$ is the action chosen by the learner at step $h$, $S_{h+1}^{(k)} \\sim P_{A_h^{(k)}}(S_h^{(k)})$ is the next state and $R_{h+1}^{(k)} \\sim r_{A_h^{(k)}}(S_h^{(k)})$ is the (possibly stochastic) reward. This model contains some important settings as a special case. Most notably, . | $H=1$ recovers the contextual bandit setting, where the “context” $S_0^{(k)}$ is sampled from the distribution $\\mu$ | $H=1$ and $S=1$ is the finite multi-armed bandit setting. | . ",
    "url": "/2024/lecture-notes/online-rl/lec22/",
    
    "relUrl": "/lecture-notes/online-rl/lec22/"
  },"37": {
    "doc": "23. Tabular MDPs",
    "title": "UCRL: Upper Confidence Reinforcement Learning",
    "content": "The UCRL algorithm implements the optimism princple. For this we need to define a set of plausible models. First, we define the maximum likelihood estimates using data from rounds $1,\\dots, k-1$: . \\[P^{(k)}_a(s,s') = \\frac{N_k(s,a,s')}{1 \\vee N_k(s,a)}\\] The definition makes use of the notation $a \\vee b = \\max(a,b)$, and empirical counts: . \\[\\begin{align*} N_k(s,a) &amp;= \\sum_{k'&lt;k}\\sum_{h&lt;H} \\mathbb{I}(S_h^{(k)}=s,A_h^{(k)}=a)\\\\ N_k(s,a,s') &amp;= \\sum_{k'&lt;k}\\sum_{h&lt;H} \\mathbb{I}(S_h^{(k)}=s,A_h^{(k)}=a,S_{h+1}^{(k)}=s') \\end{align*}\\] Define the confidence set . \\[C_{k,\\delta} = \\{ P_a(s)\\,\\, :\\, \\,\\forall s,a\\,\\, \\|P_a^{(k)}(s) - P_a(s)\\|_1 \\leq \\beta_\\delta(N_k(s,a)) \\}\\] where $\\beta_\\delta : \\mathbb{N} \\rightarrow (0,\\infty)$ is a function that we will choose shortly. Our goal of choosing $\\beta_\\delta$ is to ensure that . | $P^* \\in C_{k,\\delta}$ for all $k=1,\\dots,K$ with probability at least $1-\\delta$ | $C_{k,\\delta}$ is “not too large” | . The second point will appear formually in the proof, however note that from a statistical perspective, we want the confidence set to be as efficient as possible. With the confidence set, we can now introduce the UCRL algorithm: . UCRL (Upper confidence reinforcement learning): . In episodes $k=1,\\dots,K$, . | Compute confidence set $C_{k,\\delta}$ | Use policy $\\tilde \\pi_k = \\arg\\max_\\pi \\max_{P \\in C_{k,\\delta}} v_P^\\pi$ | Observe episode data ${S_0^{(k)}, A_0^{(k)}, S_1^{(k)}, \\dots, S_{H-1}^{(k)}, S_{H-1}^{(k)}, S_H^{(k)}}$ | . Note that we omitted the rewards from the observation data. Since we made the assumption that the reward vector $r_a(s)$ is known, we can always recompute the rewards from the state and action sequence. For now we we also glance over the point of how to compute the optimistic policy $\\pi_k$ efficently, but we will get back to this point later. Step 1: Defining the confidence set . Lemma (L1-confidence set): Let $\\beta_\\delta(u) = 2\\sqrt{\\frac{S \\log(2) + \\log(u(u+1)SA/\\delta)}{2u}}$ and define the confidence sets . \\[C_{k,\\delta} = \\{ P_a(s)\\,\\, :\\, \\,\\forall s,a\\,\\, \\|P_a^{(k)}(s) - P_a(s)\\|_1 \\leq \\beta_\\delta(N_k(s,a)) \\}\\] Then, with probability at least $1-\\delta$, . \\[\\forall k \\geq 1, \\quad P^* \\in C_{k,\\delta}\\] . Proof: Let $s,a$ be fixed and denote by $X_v \\in \\mathcal{S}$ the next state observed upon visiting $(s,a)$ the $v^{\\text{th}}$ time. Assume that $(s,a)$ was visited in total $u$ times. Then \\(P_{u,a}(s,s') = \\frac{1}{u} \\sum_{v=1}^u \\mathbb{I}(X_v = s')\\). The Markov property implies that \\((X_v)_{v=1}^{u}\\) is i.i.d. Note that for any vector \\(p \\in \\mathbb{R}^{S}\\) we can write the 1-norm as \\(\\|p\\|_1 = \\sup_{\\|x\\|_\\infty \\leq 1} \\langle p,x\\rangle\\). Therefore . \\[\\|P_{u,a}(s) - P_a^*(s)\\|_1 = \\max_{x \\in \\{\\pm 1\\}^S} \\langle P_{u,a}(s) - P_a^*(s), x \\rangle\\] Fix some \\(x \\in \\{\\pm1\\}^S\\). \\[\\begin{align*} \\langle P_{u,a}(s) - P_a^*(s), x \\rangle &amp;= \\frac{1}{u} \\sum_{v=1}^u \\sum_{s'} x_{s'}\\big(\\mathbb{I}(X_v = s') - P_a^*(s,s')\\big)\\\\ &amp;=\\frac{1}{u} \\sum_{v=1}^u \\Delta_v \\end{align*}\\] where in the last line we defined $\\Delta_v = \\sum_{s’ \\in \\mathcal{S}} x_{s’}\\big(\\mathbb{I}(X_v = s’) - P_a^*(s,s’)\\big)$. Note that $\\mathbb{E}[\\Delta_v]=0$, $|\\Delta_v| \\leq 1$ and $(\\Delta_v)_{v=1}^u$ is an i.i.d. random variable. Therefore Hoeffding’s inequality implies that with probability at least $1-\\delta$, . \\[\\frac{1}{u} \\sum_{v=1}^u \\Delta_v \\leq 2\\sqrt{\\frac{\\log(1/\\delta)}{2u}}\\] Next note that \\(\\vert\\{\\pm1\\}^S\\vert = 2^S\\), therefore taking the union bound over all \\(x \\in \\{\\pm1\\}^S\\), we get that with probability at least $1-\\delta$, . \\[\\|P_{u,a}(s) - P_a^*(s)\\|_1 \\leq 2\\sqrt{\\frac{S \\log(2) + \\log(1/\\delta)}{2u}}\\] In a last step, we take a union bound over $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$ and $u \\geq 1$. For taking the union bound over the infinite set of natural numbers, we can use the following simple trick. Note that . \\[\\sum_{u=1}^\\infty \\frac{\\delta}{u(u+1)} = \\delta\\] This follows from the simple obseration that $\\frac{1}{u(u+1)} = \\frac{1}{u} - \\frac{1}{u+1}$ and using a telescoping sum argument. Therefore, with probability at least $1-\\delta$, for all $u \\geq 1$, $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$ . \\[\\|P_{u,a}(s) - P_a^*(s)\\|_1 \\leq 2\\sqrt{\\frac{S \\log(2) + \\log(u(u+1)SA/\\delta)}{2u}}\\] Lastly, the claim follows by noting that $P_a^{(k)}(s) = P_{N_k(s,a),a}(s)$. \\(\\qquad\\blacksquare\\) . Step 2: Bounding the regret . Theorem (UCRL Regret): The regret of UCRL defined with confidence sets $C_{k,\\delta}$ satisfies with probability at least $1-3\\delta$: . \\[R_K \\leq 4 c_\\delta H\\sqrt{SAHK} + 2 c_{\\delta} H^2 SA + 3H \\sqrt{\\frac{HK}{2} \\log(1/\\delta)}\\] where $c_{\\delta} = \\sqrt{2 S \\log(2) + \\log(HK(HK+1)SA/\\delta)}$. In particular, for large enough $K$, surpressing constants and logarithmic factors, we get . \\[R_K \\leq \\mathcal{\\tilde O}\\left( H^{3/2} S\\sqrt{AK \\log(1/\\delta)} \\right)\\] . Proof: Denote by $\\pi_k$ the UCRL policy defined as . \\[\\pi_k = \\arg\\max_{\\pi} \\max_{P \\in C_{k,\\delta}} v_{0,P}^\\pi(S_0^{(k)})\\] Further, let $\\tilde P^{(k)} = \\arg\\max_{P \\in C_{k,\\delta}} v_{0,P}^*(S_0^{(k)})$ be the optimistic model. In what follows we assume that we are on the event $\\mathcal{E} = \\cap_{k\\geq 1} C_{k,\\delta}$. By the previous lemma, $\\mathbb{P}(\\mathcal{E}) \\geq 1- \\delta$. Fix $k \\geq 1$ and decompose the (instantenous) regret in round $k$ as follows: . \\[\\begin{align*} v_0^*(S_0^{(k)}) - V_k =\\,\\, &amp; \\underbrace{v_{0,P^*}^*(S_0^{(k)}) - v_{0,\\tilde P_k}^*(S_0^{(k)})}_{\\text{(I)}}\\\\ &amp;+\\,\\, \\underbrace{v_{0,\\tilde P_k}^{\\pi_k}(S_0^{(k)}) - v_{0, P^*}^{\\pi_k}(S_0^{(k)})}_{\\text{(II)}}\\\\ &amp;+\\,\\, \\underbrace{v_{0, P^*}^{\\pi_k}(S_0^{(k)}) - V_k}_{\\text{(III)}} \\end{align*}\\] Note that we used that $v_{0,\\tilde P_k}^*(S_0^{(k)}) = v_{0,\\tilde P_k}^{\\pi_k}(S_0^{(k)})$ which holds because by definition $\\pi_k$ is an optimal policy for $\\tilde P_k$. The first term is easily bounded. This is the crucial step that makes use of the optimism principle. By \\(P^* \\in C_{k,\\delta}\\) and the choice of $\\tilde P_k$ it follows that $\\text{(I)} \\leq 0$. In particular, we already eliminated the dependence on the (unknown) optimal policy from the regret bound! . The last term is also relatively easy to control. Denote \\(\\xi_k = \\text{(III)}\\). Note that by the definition of the value function we have \\(\\mathbb{E}[ \\xi_k \\vert S_0^{(k)} ] = 0\\) and \\(\\vert\\xi_k\\vert \\leq H\\). Hence $\\xi_k$ behaves like noise! If $\\xi_k$ was an i.i.d variable we could directly apply Hoeffding’s inequality to bound \\(\\sum_{k=1}^K \\xi_k\\). The sequence $\\xi_k$ has a property that allows us to obtain a similar bound. Let . \\[\\mathcal{F}_k = \\{S_0^{(l)}, A_0^{(l)}, S_1^{(l)}, \\dots, S_{H-1}^{(l)}, S_{H-1}^{(l)}, S_H^{(l)}\\}_{l=1}^{k-1}\\] be the data available to the learner at the beginning of the episode $k$. Then by definition of the value function, $\\mathbb{E}[\\xi_k\\vert\\mathcal{F}_k, S_0^{(k)}] = 0$. A sequence of random variables $(\\xi_k)_{k\\geq 1}$ with this property is called a martingale difference sequence. Lucky for us, most properties that hold for (zero-mean) i.i.d. sequences can also be shown for martingale difference sequences. The analogue result to Hoeffding’s inequality is called the Azuma-Hoeffding’s inequalty. Applied to the sequence $\\xi_k$, Azuma-Hoeffdings inequality implies that . \\[\\sum_{k=1}^K \\xi_k \\leq H \\sqrt{\\frac{K}{2} \\log(1/\\delta)}\\] It remains to bound term (II) in the regret decomposition: . \\[\\text{(II)} = v_{0,P^*}^{\\pi_k}(S_0^{(k)}) - v_{0, \\tilde P^{(k)}}^{\\pi_k}(S_0^{(k)})\\] Using the Bellman equation, we can recursively compute the value function for any policy $\\pi$: . \\[\\begin{align*} v_{h,P}^{\\pi} &amp;= r^\\pi + M_\\pi Pv_{h+1,P}^\\pi\\,\\,,\\quad 0 \\leq h \\leq H-1\\\\ v_{H,P}^\\pi &amp;= 0 \\end{align*}\\] We introduce the following shorthand for the value difference of policy $\\pi_k$ under models $P^*$ and $\\tilde P^{(k)}$: . \\[\\delta_h^{(k)}= v_{h,\\tilde P^{(k)}}^{\\pi_k}(S_h^{(k)}) -v_{h,P^*}^{\\pi_k}(S_h^{(k)})\\] Let \\(\\mathcal{F}_{h,k}\\) contain all observation data up to episode $k$ and step \\(h\\) including \\(S_h^k\\). Using the Bellman equation, we can write . \\[\\begin{align*} \\delta_h^{(k)} &amp;= M_{\\pi_k} \\tilde P^{(k)} v_{h+1,\\tilde P^{(k)}}^{\\pi_k}(S_h^{(k)}) - M_{\\pi_k} P^* v_{h+1,P^*}^{\\pi_k}(S_h^{(k)}) \\pm M_{\\pi_k} P^*V_{h+1,\\tilde P^{(k)}}(S_h^{(k)})\\\\ &amp;= (M_{\\pi^k}(\\tilde P^{(k)} - P^*) v_{h+1, \\tilde P^{(k)}}^{\\pi_k})(S_h^{(k)}) + (M_{\\pi_k}P^*(v_{h+1,\\tilde P^{(k)}}^{\\pi_k} - v_{h+1,P^*}^{\\pi_k})(S_h^{(k)})\\\\ &amp;\\leq \\|P_{A_h^{(k)}}^*(S_h^{(k)}) - \\tilde P_{A_h^{(k)}}^{(k)}(S_h^{(k)}) \\|_1 H+ \\delta_{h+1}^{(k)} + \\underbrace{\\big(\\mathbb{E}[\\delta_{h+1}^{(k)}|\\mathcal{F}_{h,k}] - \\delta_{h+1}^{(k)}\\big)}_{=:\\eta_{h+1}^{(k)}}\\\\ &amp;\\leq 2 H \\beta_\\delta(N_k(S_h^{(k)}, A_h^{(k)})) + \\delta_{h+1}^{(k)} + \\eta_{h+1}^{(k)} \\end{align*}\\] The first inequality uses that for any two vectors $w,v$, we have \\(\\langle w,v\\rangle \\leq \\|w\\|_1 \\|v\\|_{\\infty}\\) and \\(\\|v_{h+1,\\tilde P^{(k)}}^{\\pi_k}\\|_\\infty \\leq H\\). Further we use that $\\pi_k$ is a deterministic policy, therefore \\(M_{\\pi_k} P(S_h^{(k)}) = P_{A_h^{(k)}}(S_h^{(k)})\\). The second follows from the definition of the confidence set in the previous lemma: . \\[\\begin{align*} &amp;\\|P_{A_h^{(k)}}^*(S_h^{(k)}) - \\tilde P_{A_h^{(k)}}^{(k)}(S_h^{(k)}) \\|_1 \\\\ &amp;\\leq \\|P_{A_h^{(k)}}^*(S_h^{(k)}) - P_{A_h^{(k)}}^{(k)}(S_h^{(k)}) \\|_1 + \\|P_{A_h^{(k)}}^{(k)}(S_h^{(k)}) - \\tilde P_{A_h^{(k)}}^{(k)}(S_h^{(k)}) \\|_1\\\\ &amp;\\leq 2 \\beta_\\delta(N_k(S_h^{(k)}, A_h^{(k)})) \\end{align*}\\] Telescoping and using that $\\delta_H^{(k)} = 0$ yields . \\[\\delta_0^{(k)} \\leq \\eta_1^{(k)} + \\cdots + \\eta_{H-1}^{(k)} + 2H \\underbrace{\\sum_{h=0}^{H-1}\\beta_\\delta(N_k(S_h^{(k)}, A_h^{(k)}))}_{\\text{(IV)}}\\] Note that $(\\eta_h^{(k)})_{h=1}^{H-1}$ is another martingale difference sequence (with \\(\\eta_h^{(k)}\\vert \\leq H\\)) that can be bounded by Azuma-Hoeffding: . \\[\\sum_{k=1}^K\\sum_{h=1}^{H-1} \\eta_h^{(k)} \\leq 2H \\sqrt{\\frac{HK}{2}\\log(1/\\delta)}\\] It remains to bound term $\\text{(IV)}$. For this we make use of the following algebraic lemma: . Lemma: . For any sequence \\(m_1, \\dots, m_k\\) that satisfies \\(m_1 + \\dots + m_k \\geq 0\\): . \\[\\sum_{k=1}^K \\frac{m_k}{\\sqrt{1 \\vee (m_1 + \\cdots + m_k)}} \\leq 2 \\sqrt{m_1 + \\cdots + m_k}\\] . Proof of Lemma: Let \\(f(x) = 1/\\sqrt{x}\\). \\(f(x)\\) is a concave function on \\((0,\\infty)\\). Therefore \\(f(A + x) \\leq f(A) + x f'(A)\\) for all $A, A + x, &gt;0$. This translates to: . \\[\\sqrt{A + x} \\leq \\sqrt{A} + \\frac{x}{2\\sqrt{A}}\\] The claim follows from telescoping. \\(\\qquad\\blacksquare\\) . Continuing the proof of the theorem where we need to bound $\\text{(IV)}$. Denote $c_{\\delta} = \\sqrt{2 S \\log(2) + \\log(HK(HK+1)SA/\\delta)}$. Further let \\(M_k(s,a) = \\sum_{h=1}^{H-1} \\mathbb{I}(S_h^{(k)}=s, A_h^{(k)} = a)\\) and note that \\(N_k(s,a) = M_1 + \\cdots + M_{k-1}\\). Then . \\[\\begin{align*} \\sum_{k=1}^K \\sum_{h=0}^{H-1}\\beta_\\delta(N_k(S_h^{(k)}, A_h^{(k)})) &amp;\\leq c_{\\delta} \\sum_{s,a} \\sum_{k=1}^K \\sum_{h=0}^{H-1}\\frac{\\mathbb{I}(S_h^{(k)}=s, A_h^{(k)} = a)}{\\sqrt{1 \\vee N_k(s,a)}}\\\\ &amp;=c_{\\delta} \\sum_{s,a} \\sum_{k=1}^K \\frac{M_k}{\\sqrt{1 \\vee (M_1 + \\dots + M_{k-1})}} \\end{align*}\\] Next, using the algebraic lemma above and the fact that $M_k(s,a) \\leq H$, we find . \\[\\begin{align*} \\sum_{k=1}^K \\sum_{h=0}^{H-1}\\beta_\\delta(N_k(S_h^{(k)}, A_h^{(k)})) &amp;\\leq c_{\\delta} \\sum_{s,a} \\sum_{k=1}^K \\frac{M_k(s,a)}{\\sqrt{1 \\vee( M_1(s,a) + \\dots + M_{k-1}(s,a))}}\\\\ &amp;\\leq c_{\\delta} \\sum_{s,a} \\sum_{k=1}^K \\frac{M_k(s,a)}{\\sqrt{1 \\vee (M_1(s,a) + \\dots + M_{k}(s,a) - H)}}\\\\ &amp;\\leq c_{\\delta} \\sum_{s,a} \\sum_{k=1}^K \\frac{M_k(s,a) \\mathbb{I}(M_1(s,a) + \\dots + M_{k}(s,a) &gt; H)}{\\sqrt{M_1(s,a) + \\dots + M_{k}(s,a) - H}} + c_{\\delta} HSA\\\\ &amp;\\leq 2 c_{\\delta}\\sum_{s,a} \\sqrt{N_k(s,a)} + c_{\\delta} HSA\\\\ &amp;\\leq 2 c_{\\delta} SA \\sqrt{\\sum_{s,a} N_k(s,a)/SA} + c_{\\delta} HSA\\\\ &amp;= 2 c_{\\delta}\\sqrt{SAHK} + c_{\\delta} HSA \\end{align*}\\] The last inequality uses Jensen’s inequality. Collecting all terms and taking the union bound over two applications of Azuma-Hoeffdings and the event $\\mathcal{E}$ completes the proof. \\(\\qquad\\blacksquare\\) . ",
    "url": "/2024/lecture-notes/online-rl/lec23/#ucrl-upper-confidence-reinforcement-learning",
    
    "relUrl": "/lecture-notes/online-rl/lec23/#ucrl-upper-confidence-reinforcement-learning"
  },"38": {
    "doc": "23. Tabular MDPs",
    "title": "Unknown reward functions",
    "content": "In our analysis of UCRL we assumed that the reward function is known. While this is quite a common assumption in the literature, it is mainly for simplicity. We also don’t expect the bounds to change by much: Estimating the rewards is not harder than estimating the transition kernels. To modify the analysis and account for unkown rewards, we first consider the case with deterministic reward function \\(r_a(s) \\in [0, R_{\\max}]\\), where $R_{\\max}$ is some known upper bound on the reward per step. Embracing the idea of optimism, we define reward estimates . \\[\\hat r_a^{(k)}(s) = \\begin{cases} r_{A_h^{(k')}}(S_h^{(k')}) &amp; \\text{(s,a) was visited in a round $k' &lt; k$ and step $h$}\\\\ R_{\\max} &amp; \\text{else.} \\end{cases}\\] Clearly this defines an optimistic estimate, \\(\\hat r_a^{(k)}(s) \\geq r_a(s)\\). Moreover, we have \\(\\hat r_{A_h^{(k)}}^{(k)}(S_h^{(k)}) \\neq r_{A_h^{(k)}}(S_h^{(k)})\\) at most $SA$ times. Therefore the regret in the previous analysis is increased by at most $R_{\\max}SA$. When the reward is stochastic, we can use a maximum likelihood estimate of the reward and construct confidence bounds around the estimate. This way we can define an optimistic reward. Still not much changes, as the reward estimates concentrate at the same rate as the estimates of $P$. ",
    "url": "/2024/lecture-notes/online-rl/lec23/#unknown-reward-functions",
    
    "relUrl": "/lecture-notes/online-rl/lec23/#unknown-reward-functions"
  },"39": {
    "doc": "23. Tabular MDPs",
    "title": "UCBVI: Upper Confidence Bound Value Iteration",
    "content": "Computing the UCRL policy can be quite challenging. However, we can relax the construction so that we can use backward induction. We define a time-inhomogenous relaxation of the confidence set: . \\[C_{k,\\delta}^H = \\underbrace{C_{k,\\delta} \\times \\cdots \\times C_{k,\\delta}}_{H \\text{ times}}\\] Let \\(\\tilde P_{1:H,k} := (\\tilde P_{1,k}, \\dots, \\tilde P_{H,k}) = \\arg\\max_{P \\in C_{k,\\delta}^H} v^*_P(s_0^{(k)})\\) be the optimistic (time-inhomogenous) transition matrices and \\(\\pi_k = \\arg\\max_{\\pi} v_{\\tilde P_{1:H,k}}^\\pi\\) the optimal policy for the optimistic model \\(\\tilde P_{1:H,k}\\). Then \\(v_{\\tilde P_{1:H,k}}^{\\pi^k} = v_{\\tilde P_{1:H,k}}^* = v^{(k)}\\) is defined by the following backwards induction: . \\[\\begin{align*} v^{(k)}_H(s) &amp;= 0 \\qquad\\forall s \\in [S]\\\\ Q_h^{(k)}(s,a) &amp;= r(s,a) + \\max_{P \\in C_{k,\\delta}} P_a(s) v_{h+1}^{(k)}\\\\ v^{(k)}_h(s) &amp;= \\max_{a} Q_h^{(k)}(s,a) \\end{align*}\\] Note that the maximum in the second line is a linear optimization with convex constraints that can be solved efficiently. Further, the proof of the UCRL regret still applies, because we used the same (step-wise) relaxation in the analysis. We can further relax the backward induction to avoid the optimization over $C_{k,\\delta}$ completely: . \\[\\begin{align*} \\max_{P \\in C_{k,\\delta}} P_a(s) v_{h+1}^{(k)}&amp;\\leq P_a^{(k)}(s) v_{h+1}^{(k)}+ \\max_{P \\in C_{k,\\delta}} (P_a(s) - P_a^{(k)}(s)) v_{h+1}^{(k)}\\\\ &amp;\\leq P_a^{(k)}(s) v_{h+1}^{(k)}+ \\max_{P \\in C_{k,\\delta}} \\|P_a(s) - P_a^{(k)}(s))\\|_1 \\| v_{h+1}^{(k)}\\|_\\infty\\\\ &amp;\\leq P_a^{(k)}(s) v_{h+1}^{(k)}+ \\beta_{\\delta}(N_k(s,a))H\\\\ \\end{align*}\\] This leads us to the the UCBVI (upper confidence bound value iteration) algorithm. In episode $k$, UCBVI uses value iteration for the estimated transition kernel $P_a^{(k)}(s)$ and optimistic reward function $r_a(s) + H \\beta_\\delta(N_k(s,a))$ to compute the policy. UCBVI (Upper confidence bound value iteration): . In episodes $k=1,\\dots,K$, . | Compute optimistic value function: | . \\[\\begin{align*} v^{(k)}_H(s) &amp;= 0 \\qquad\\forall s \\in [S]\\\\ b_k(s,a) &amp;= H\\beta_{\\delta}(N_k(s,a))\\\\ Q_h^{(k)}(s,a) &amp;= \\min\\left(r(s,a) + b_k(s,a) + P_a^{(k)}(s) v_{h+1}^{(k)}, H\\right)\\\\ v^{(k)}_h(s) &amp;= \\max_{a} Q_h^{(k)}(s,a) \\end{align*}\\] . | Follow greedy policy $A_{h}^{(k)} = \\arg\\max_{A} Q_h^{(k)}(S_h^{(k)}, A)$ | Observe episode data ${S_0^{(k)}, A_0^{(k)}, S_1^{(k)}, \\dots, S_{H-1}^{(k)}, S_{H-1}^{(k)}, S_H^{(k)}}$ | . Note that we truncate the $ Q_h^{(k)}$-function to be at most $H$, this avoids a blow up by a factor of $H$ in the regret bound. Carefully checking that the previous analysis still applies shows that UCBVI has regret at most $R_K \\leq \\mathcal{O}(H^{2}S \\sqrt{AK})$. By more carefully designing the reward bonuses for UCBVI, it is possible to achieve $R_K\\leq \\mathcal{\\tilde O}(H^{3/2}\\sqrt{SAK})$ which matches the lower bound up to logarithmic factors in the time in-homogeneous setting. ",
    "url": "/2024/lecture-notes/online-rl/lec23/#ucbvi-upper-confidence-bound-value-iteration",
    
    "relUrl": "/lecture-notes/online-rl/lec23/#ucbvi-upper-confidence-bound-value-iteration"
  },"40": {
    "doc": "23. Tabular MDPs",
    "title": "Notes",
    "content": " ",
    "url": "/2024/lecture-notes/online-rl/lec23/#notes",
    
    "relUrl": "/lecture-notes/online-rl/lec23/#notes"
  },"41": {
    "doc": "23. Tabular MDPs",
    "title": "References",
    "content": "The original UCRL paper. Notice that they consider the infinite horizon average reward setting, which is different from the episodic setting we present. Auer, P., &amp; Ortner, R. (2006). Logarithmic online regret bounds for undiscounted reinforcement learning. Advances in neural information processing systems, 19. [link] . The UCBVI paper. Notice that they consider the homogeneous setting, which is different from the in-homogeneous setting we present. Azar, M. G., Osband, I., &amp; Munos, R. (2017, July). Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning (pp. 263-272). PMLR. [link] . The paper that presents the lower bound. Notice the they consider the infinite horizon average reward setting. Thus, there results contains a diameter term $D$ instead of a horizon term of $H$. Auer, P., Jaksch, T., &amp; Ortner, R. (2008). Near-optimal regret bounds for reinforcement learning. Advances in neural information processing systems, 21. [link] . ",
    "url": "/2024/lecture-notes/online-rl/lec23/#references",
    
    "relUrl": "/lecture-notes/online-rl/lec23/#references"
  },"42": {
    "doc": "23. Tabular MDPs",
    "title": "23. Tabular MDPs",
    "content": "PDF Version . In this lecture we will analize an online learning algorithm for the finite-horizon episodic MDP setting. Let \\(M=(\\mathcal{S}, \\mathcal{A}, P^*, r, \\mu, H)\\) be an MDP with finite state and action spaces $\\mathcal{S}$ and $\\mathcal{A}$, unknown transition matrix \\(P^*\\), known reward function $r_a(s) \\in [0,1]$, an initial state distribution $\\mu$, and length of each episode $H \\ge 1$. The star-superscript in $P^*$ is used to distinquish the true environment from other (e.g. estimated) environments that occur in the algorithm and the analysis. The assumption that the reward function $r$ is known is for simplicity. In fact, most of the hardness (in terms of sample complexity and designing the algorithm) comes from unknown transition probabilities. We will focus on the finite-horizon setting where the learner interacts with the MDP over $k=1,\\dots, K$ episodes of length $H \\ge 1$. Most, but not all ideas translate to the infinite-horizon discounted or average reward settings. Recall that the regret is defined as follows: . \\[R_K = \\sum_{k=1}^K v_0^*(S_0^{(k)}) - V_k\\] where $V_k = \\sum_{h=0}^{H-1} r_{A_h^{(k)}}(S_h^{(k)})$. ",
    "url": "/2024/lecture-notes/online-rl/lec23/",
    
    "relUrl": "/lecture-notes/online-rl/lec23/"
  },"43": {
    "doc": "24. Featurized MDPs",
    "title": "Linear Mixture MDPs",
    "content": "We focus on the episodic, finite-horzion MDPs \\(M=(\\mathcal{S}, \\mathcal{A}, P_h, r_h, \\mu, H)\\) with time in-homogenous reward \\(r_h\\) and transition matrix \\(P_h\\). We let \\(\\mathcal{S}\\) be a finite but possibly very large state space, and \\(\\mathcal{A}\\) be a finite action space. With care, most of the analysis can be extended to infinite state and action spaces. As before, we assume that the reward function \\(r_h(s,a) \\in [0,1]\\) is known. We now impose additional (linear) structure on the transition kernel $P_h$. For this we assume the learner has access to features \\(\\phi(s,a,s') \\in \\mathbb{R}^d\\) that satisfy \\(\\|\\phi(s,a,s')\\|_2 \\leq 1\\). In time-inhomogeneous linear mixture MDPs, the transition kernel is of the form . \\[P_{h,a}(s,s') = \\langle \\phi(s,a,s'), \\theta_h^* \\rangle\\] for some unkown parameter \\(\\theta_h^* \\in \\mathbb{R}^d\\) with \\(\\|\\theta_h^*\\|_2 \\leq 1\\). We remark that tabular MDPs are recovered using $\\phi(s,a,s’) = e_{s,a,s’}$, where $e_{s,a,s’}$ are the unit vectors in \\(\\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}\\times \\mathcal{S}}\\). For any function \\(V : \\mathcal{S} \\rightarrow \\mathbb{R}\\), we define . \\[\\phi_V(s,a) = \\sum_{s'} \\phi(s,a,s')V(s') \\in \\mathbb{R}^d\\] Note that \\(\\langle\\phi_V(s,a), \\theta^*\\rangle\\) predicts the expected value of \\(V(s')\\) when \\(s'\\) is sampled from \\(P_{h,a}(s)\\): . \\[P_{h,a}(s)V = \\sum_{s'} P_{h,a}(s,s') V(s') = \\sum_{s'} \\langle \\phi(s,a,s'), \\theta_h^* \\rangle V(s') = \\langle \\phi_V(s,a), \\theta_h^* \\rangle\\] Value Targeted Regression (VTR) . Now that we have specified the parametrized model, the next step is to construct an estimator of the unknown paramter. An estimator of $\\theta^*$ allows us to predict the value of any policy. For the algorithm, we are particularly interested in constructing optimistic estimates of the value function. Hence we will also need a confidence set. Let \\((V_h^{(j)})^{j&lt;k}_{h\\leq H}\\) be a sequence of value functions constructed up to episode \\(k-1\\). Let \\(\\phi_{h,j} =\\phi_{V_{h+1}^{(j)}}(S_h^{(j)}, A_h^{(j)})\\) and \\(y_{h,j} = V_{h+1}^{(j)}(S_{h+1}^{(j)})\\). By constrution, we have that \\(\\mathbb{E}[y_{h,j}] = \\langle \\phi_{h,j}, \\theta^* \\rangle\\) and \\(\\vert y_{h,j}\\vert \\leq H\\). Define the regularized least-squares estimator . \\[\\hat \\theta_{h,k} = \\arg\\min_{\\theta} \\sum_{j=0}^{k-1} \\big(\\langle \\phi_{h,j},\\theta\\rangle - y_{h,j}\\big)^2 + \\lambda \\|\\theta\\|^2\\] Let $\\mathbf{I}_d \\in \\mathbb{R}^{d\\times d}$ be the idenity matrix. We have the following closed form for \\(\\hat \\theta_{h,k}\\): . \\[\\begin{align*} \\hat \\theta_{h,k} = \\Sigma_{h,k}^{-1} \\sum_{j=0}^{k-1}\\phi_{h,j} y_{h,j}\\qquad \\text{where} \\quad \\Sigma_{h,k} = \\sum_{j=0}^{k-1} \\phi_{h,j}\\phi_{h,j}^{\\top} + \\lambda \\mathbf{I}_d \\end{align*}\\] The next step is to quantify the uncertainy in the estimation. Mirroring the steps in the tabular setting, we construct a confidence set for \\(\\hat \\theta_{h,k}\\). For a positive (semi-)definite matrix \\(\\Sigma \\in \\mathbb{R}^{d\\times d}\\) and vector \\(v \\in \\mathbb{R}^d\\), define the (semi-)norm \\(\\|a\\|_\\Sigma = \\sqrt{\\langle v, \\Sigma v \\rangle}\\). We make use of the following elliptical confidence set for \\(\\hat \\theta_{h,k}\\) . \\[C_{h,\\delta}^{(k)} = \\{\\theta : \\|\\theta - \\hat \\theta_{h,k}\\|_{\\Sigma_{h,k}}^2 \\leq \\beta_{h,k,\\delta} \\}\\] where . \\[\\beta_{h,k,\\delta}^{1/2} = H\\sqrt{\\log \\det(\\Sigma_{h,k}) - \\log \\det(\\Sigma_{h,0}) + 2 \\log(1/\\delta)} + \\sqrt{\\lambda}\\] The log determinant of \\(\\Sigma_{h,k}\\) can be computed online by the algorithm. For the analysis, it is useful to further upper bound \\(\\beta_{h,k,\\delta}\\). It is possible to show the following upper bound on \\(\\beta_{h,k,\\delta}\\) that holds independent of the data sequence: . \\[\\beta_{h,k,\\delta}^{1/2} \\leq H \\sqrt{d\\log (1 + k/(d\\lambda)) + 2\\log(1/\\delta)} + \\sqrt{\\lambda}\\] For a derivation of the above inequality see Lemma 19.4 of the Bandit Book. The next lemma formally specifies the confidence probabilty. Lemma (Online Least-Squares Confidence) Fix some \\(0 \\leq h &lt; H\\). Then . \\[\\mathbb{P}[\\theta_h^* \\in \\cap_{k \\geq 1}C_{h,k,\\delta}] \\geq 1-\\delta\\] . Proof: The above result is presented as Theorem 2 in Abbasi-Yadkori et al (2011), where the proof can also be found. \\(\\qquad\\blacksquare\\) . The confidence set can be used to derive bounds on the estimation error with probability at least \\(1-\\delta\\) as follows: . \\[|\\langle \\phi_V(s,a), \\hat \\theta_{h,k} - \\theta^* \\rangle| \\leq \\| \\phi_V(s,a)\\|_{\\Sigma_{h,k}^{-1}} \\|\\hat \\theta_{h,k} - \\theta^*\\|_{\\Sigma_{h,k}} \\leq \\beta_{h,k,\\delta}^{1/2} \\| \\phi_V(s,a)\\|_{\\Sigma_{h,k}^{-1}}\\] The first inequality is by Cauchy-Schwarz and the second inequality uses the confidence bound from the previous lemma. UCRL-VTR . Similar to the tabular UCRL and UCBVI algorithms, UCRL-VTR uses the estimates \\(\\hat \\theta_{h,k}\\) to compute an optimistic policy. One way of obtaining an optimistic policy is from optimistic Q-estimates \\(Q_h^{(k)}(s,a)\\) defined via backwards induction. Then UCRL-VTR follows the greedy policy w.r.t. the optimistic Q-values. UCRL-VTR . In episodes \\(k=1,\\dots,K\\), . | Set \\(V^{(k)}_H(s) = 0\\). Compute \\(\\hat \\theta_{h,k}\\) and \\(\\Sigma_{h,k}\\). Recursively define optimistic value functions . For \\(h=H-1,\\dots,0\\): . \\[\\begin{align*} \\hat \\theta_{h,k} &amp;= \\arg\\min_{\\theta} \\sum_{j=1}^{k-1} \\big(\\langle\\phi_{h,j}, \\theta \\rangle - y_{h,j}\\big)^2 + \\lambda \\|\\theta\\|_2^2\\\\ \\Sigma_{h,k} &amp;= \\sum_{j=1}^k \\phi_{h,j}\\phi_{h,j}^\\top + \\lambda \\mathbf{I}_d\\\\ Q_h^{(k)}(s,a) &amp;= \\big(r_h(s,a) + \\langle \\phi_{V_{h+1}^{(k)}}(s,a), \\hat \\theta_{h,k} \\rangle + \\beta_{h,k,\\delta/H}^{1/2}\\|\\phi_{V_{h+1}^{(k)}}(s,a)\\|_{\\Sigma_{h,k}^{-1}} \\big)\\wedge H\\\\ V_h^{(k)}(s) &amp;= \\max_{a} Q_h^{(k)}(s,a) \\end{align*}\\] | Follow greedy policy w.r.t. $Q_h^{(k)}(s,a)$. For \\(h = 0, \\dots, H-1\\): . \\[\\begin{align*} A_{h}^{(k)} &amp;= \\arg\\max_{a \\in \\mathcal{A}} Q_h^{(k)}(S_h^{(k)}, a) \\end{align*}\\] Let \\(\\phi_{h,k} =\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\) and \\(y_{h,k} = V_{h+1}^{(k)}(S_{h+1}^{(k)})\\). | . We are now in the position to state a regret bound for UCRL-VTR. Theorem (UCRL-VTR Regret) The regret of UCRL-VTR satisfies with probability at least \\(1-2\\delta\\): . \\[R_K \\leq \\mathcal{O}\\big(d H^{2}\\log(K) \\sqrt{K \\log(KH/\\delta)} \\big)\\] . Note that the bound scales with the feature dimension \\(d\\), but not the size of the state space or action space. The lower bound for this setting is \\(R_K \\geq \\Omega(dH^{3/2} \\sqrt{K})\\), therefore our upper bound is tight except for a factor \\(\\sqrt{H}\\). Proof: . Our proof strategy follows the same steps as in the proof of UCRL. Step 1 (Optimism): . Taking the union bound over $h=0,\\dots, H-1$, the previous lemma implies that with probability at least \\(1-\\delta\\), for all $h \\in [H-1]$ and all $k \\geq 0$, \\(\\theta_h^* \\in C_{h,\\delta/H}^{(k)}\\). In the following, we condition on this event. Using induction over \\(h=H, H-1, \\dots, 0\\), we can show that . \\[V^*_0(S_h^{(k)}) \\leq V_{0}^{(k)}(S_h^{(k)})\\] Step 2 (Bellman recursion and estimation error): . For any $h =0, \\dots, H-1$, we find . \\[\\begin{align*} &amp;V_{h}^{(k)}(S_h^{(k)}) - V_h^{\\pi_k}(S_h^{(k)}) \\\\ &amp;\\leq \\langle \\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)}), \\hat \\theta_{h,k}\\rangle + \\beta_{h,k,\\delta/H}^{1/2} \\|\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\|_{\\Sigma_{h,k}^{-1}} - P_{h, A_{h}^{(k)}}^*(S_h^{(k)}) V_{h+1}^{\\pi_k}\\\\ &amp;= \\langle \\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)}), \\hat \\theta_{h,k} - \\theta^*\\rangle + \\beta_{h,k,\\delta/H}^{1/2} \\|\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\|_{\\Sigma_{h,k}^{-1}} + P_{h, A_{h}^{(k)}}^*(S_h^{(k)}) (V_{h+1}^{(k)}- V_{h+1}^{\\pi_k}) \\end{align*}\\] The inequality is by the definition of $V_h^{(k)}$ and dropping the truncation, and in the last line we add and subtract \\(P_{h, A_{h}^{(k)}}^*(S_h^{(k)}) V_{h+1}^{(k)} = \\langle \\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)}), \\theta^*\\rangle\\). Further, by Cauchy-Schwarz on the event \\(\\theta^* \\in C_{k,\\delta/H}\\) we get . \\[\\langle \\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)}), \\hat \\theta_{h,k} - \\theta^*\\rangle \\leq \\beta_{h,k,\\delta/H}^{1/2} \\|\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\|_{\\Sigma_{h,k}^{-1}}\\] Continuing the previous display, we find . \\[\\begin{align*} &amp;V_{h}^{(k)}(S_h^{(k)}) - V_h^{\\pi_k}(S_h^{(k)}) \\\\ &amp;\\leq 2 \\beta_{h,k,\\delta/H}^{1/2} \\|\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\|_{\\Sigma_{h,k}^{-1}} + P_{h, A_{h}^{(k)}}^*(S_h^{(k)}) (V_{h+1}^{(k)}- V_{h+1}^{\\pi_k})\\\\ &amp;= 2 \\beta_{h,k,\\delta/H}^{1/2} \\|\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\|_{\\Sigma_{h,k}^{-1}} + V_{h+1}^{(k)}(S_{h+1}^{(k)}) - V_{h+1}^{\\pi_k}(S_{h+1}^{(k)}) + \\xi_{h,k} \\end{align*}\\] where we defined . \\[\\xi_{h,k} = \\big(P_{h, A_{h}^{(k)}}^*(S_h^{(k)}) (V_{h+1}^{(k)}- V_{h+1}^{\\pi_k})\\big)- \\big(V_{h+1}^{(k)}(S_{h+1}^{(k)}) - V_{h+1}^{\\pi_k}(S_{h+1}^{(k)})\\big)\\] Recursively appliying the previous inequality and summing over all episodes yields . \\[\\sum_{k=1}^K V_{0}^{(k)}(S_0^{(k)}) - V_0^{\\pi_k}(S_0^{(k)}) \\leq \\sum_{k=1}^K \\sum_{h=0}^{H-1} 2 \\beta_{h,k,\\delta}^{1/2} \\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}} + \\xi_{h,k}\\] Note that $\\xi_{h,k}$ is a martingale difference sequence, hence by Azuma-Hoeffdings inequality we have with probability at least $1-\\delta$, . \\[\\sum_{k=1}^K \\sum_{h=0}^{H-1} \\xi_{h,k} \\leq H \\sqrt{\\frac{HK}{2} \\log(1/\\delta)}\\] Step 3 (Cauchy-Schwarz): . Note that \\(\\beta_{h,k,\\delta}\\) is non-decreasing in both \\(h\\) and \\(k\\). Very little is lost by bounding \\(\\beta_{h,k,\\delta} \\leq \\beta_{H,K,\\delta}\\). From the previous step, we are left to bound the sum over uncertainties \\(\\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}}\\). We start with an application of the Cauchy-Schwarz inequality. Applied to sequences \\((a_i)_{i=1}^n\\), \\((b_i)_{i=1}^n\\), we have that \\(\\vert\\sum_{i=1}^n a_i b_i \\vert \\leq \\sqrt{\\sum_{i=1}^n a_i^2 \\sum_{j=1}^n b_i^2}\\). Applied to the regret, we get: . \\[\\sum_{k=1}^K \\sum_{h=0}^{H-1} 2 \\beta_{h,k,\\delta}^{1/2} \\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}} \\leq \\sum_{h=0}^{H-1} 2 \\beta_{h,K,\\delta}^{1/2} \\sqrt{K \\sum_{k=1}^K \\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}}^2}\\] Step 4 (Elliptic potential lemma): . The penultima step is to control the sum over squared uncertainties \\(\\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}}^2\\). This classical result is sometimes refered to as the elliptic potential lemma: . \\[\\sum_{k=1}^K \\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}}^2 \\leq \\mathcal{O}(d \\log(K))\\] The proof, as mentioned earlier, can be found as Lemma 19.4 in the Bandit Book. Step 5 (Summing up): . It remains to chain the previous steps and take the union bound over the event where the confidence set contains the true parameter and the application of Azuma-Hoeffdings. \\[\\begin{align*} R_K &amp;= \\sum_{k=1}^K V_{0}^{(k)}(S_0^{(k)}) - V_0^{\\pi_k}(S_0^{(k)}) \\\\ &amp;\\leq \\sum_{k=1}^K \\sum_{h=0}^{H-1} \\big(2 \\beta_{h,k,\\delta}^{1/2} \\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}} + \\xi_{h,k}\\big)\\\\ &amp;\\leq C \\cdot H \\beta_{H,K,\\delta}^{1/2} \\sqrt{d \\log(K) K} + H^{3/2} \\sqrt{2K \\log(1/\\delta)} \\end{align*}\\] For some universal constant \\(C\\). This completes the proof. \\(\\qquad\\blacksquare\\) . ",
    "url": "/2024/lecture-notes/online-rl/lec24/#linear-mixture-mdps",
    
    "relUrl": "/lecture-notes/online-rl/lec24/#linear-mixture-mdps"
  },"44": {
    "doc": "24. Featurized MDPs",
    "title": "Linear MDPs",
    "content": "So far we have seen the linear mixture MDP model. This is not the only way one can parameterize the transition matrix. An alternative is the linear MDP model, defined as follows for features $\\phi(s,a) \\in \\mathbb{R}^d$ and parameters \\(\\psi_h^{*} \\in \\mathbb{R}^{d\\times S}\\) and \\(\\theta_h^* \\in \\mathbb{R}^d\\): . \\[\\begin{align*} P_h^*(s,s') &amp;= \\langle\\phi(s,a), \\psi_h^*(s')\\rangle\\\\ r_h(s,a) &amp;= \\langle \\phi(s,a), \\theta_h^* \\rangle \\end{align*}\\] Note that tabular MDPs are recovered using $\\phi(s,a) = e_{s,a}$, where $e_{s,a}$ are the unit vectors in \\(\\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}}\\). Compared to the linear mixture model, an immediate observation is that the dependence on the the next state $s’$ is pushed into the parameter \\(\\psi_h(s') \\in \\mathbb{R}^d\\). Consequently, the dimension of the parameter space scales with the number of states, and it is not immediately clear how we can avoid the $S$ dependence in the regret bounds. Another consequence of this model is that the $Q$-function for any policy is linear in the features $\\phi(s,a)$. Lemma: . Under the linear MDP assumption, for any policy $\\pi$ the Q-function $Q_h^\\pi(s,a)$ is linear in the features $\\phi(s,a)$. That is, there exist parameters $w_h^\\pi \\in \\mathbb{R}^d$ such that . \\[Q_h^\\pi(s,a) = \\langle \\phi(s,a), w_h^\\pi \\rangle\\] . Proof: The claim follows directly from the definition of \\(Q_h^\\pi\\) and the assumptions on \\(r_h(s,a)\\) and \\(P_{h,a}(s)\\). \\[\\begin{align*} Q_h^\\pi(s,a) &amp;= r_h(s,a) + P_{h,a}(s)V_{h+1}^\\pi\\\\ &amp;= \\langle \\phi(s,a), \\theta_h^* \\rangle + \\sum_{s'} V_{h+1}^\\pi(s') \\langle \\phi(s,a), \\psi_h^*(s') \\rangle\\\\ &amp;=\\langle \\phi(s,a), w_h^\\pi \\rangle \\end{align*}\\] where we defined \\(w_h^\\pi = \\theta_h^* + \\sum_{s'} \\psi_h^*(s') V_{h+1}^\\pi(s')\\) for the last equation. \\(\\qquad\\blacksquare\\) . In light of this lemma, our goal is to estimate \\(w_h^{\\pi^*}\\). This can be done using least-squares value iteration (LSVI). Let \\(\\{S_1^{(j)}, A_1^{(j)}, \\dots, S_{H-1}^{(j)}, A_{H-1}^{(j)}, S_{H}^{(j)}\\}_{j=1}^{k-1}\\) be the data available at the beginning of episode \\(k\\). Denote \\(\\phi_{h,j} = \\phi(S_h^{(j)}, A_h^{(j)})\\) and define targets \\(y_{h,j} = r_h(S_h^{(j)}, A_h^{(j)}) + \\max_{a \\in \\mathcal{A}} Q_{h+1}^{(j)}(S_h^{(j)},a)\\) based on \\(Q_{h+1}^{(j)}(s,a)\\) estimates obtained in episodes \\(j=1,\\dots,k-1\\). Least-squares value iteration solves the following problem: . \\[\\begin{align*} \\hat w_{h,k} &amp;= \\arg\\min_{w \\in \\mathbb{R}^d} \\sum_{j=1}^{k-1} \\big(\\langle\\phi_{j,h}, w\\rangle - y_{j,h}\\big)^2 + \\lambda \\|w\\|_2^2 \\end{align*}\\] The closed form solution is $w_{h,k} = \\Sigma_{h,k}^{-1}\\sum_{j=1}^{k-1} \\phi_{h,j}y_{h,j}$ where \\(\\Sigma_{h,k} = \\sum_{j=1}^{k-1} \\phi_{j,h}\\phi_{j,h}^\\top + \\lambda \\mathbf{I}_d\\). Based on the estimate \\(\\hat w_{h,k}\\), we can define optimistic \\(Q\\)- and \\(V\\)-estimates: . \\[\\begin{align*} Q_h^{(k)}(s,a) &amp;= (\\langle\\phi(s,a), \\hat w_{h,k}\\rangle + \\tilde \\beta_{k,\\delta}^{1/2} \\|\\phi(s,a)\\|_{\\Sigma_{h,k}^{-1}}) \\wedge H\\\\ V_h^{(k)}(s) &amp;= \\max_{a \\in \\mathcal{A}} Q_h^{(k)} \\end{align*}\\] Assuming that the features satisfy \\(\\|\\phi(s,a)\\|_2\\leq 1\\) and the true parameters satisfy \\(\\|\\theta_h^*\\|_2 \\leq 1\\) and \\(\\| \\psi_{h}^*v\\|_2 \\leq \\sqrt{d}\\) for all $v \\in \\mathbb{R}^S$ with \\(\\|v\\|_\\infty \\leq 1\\), one can choose the confidence parameter as follows: . \\[\\tilde \\beta_{k,h,\\delta} = \\mathcal{O}\\left(d^2 \\log(\\frac{HK}{\\delta})\\right)\\] This result is the key to unlock a regret bound that is independent of the size of the state space \\(S\\). The proof requires a delicate covering argument. For details refer to chapter 8 of the RL Theory Book . LSVI-UCB . Algorithm: LSVI-UCB . In episodes \\(k=1,\\dots,K\\), . | Initialize \\(V_H^{(j)}(s) = 0\\) for \\(j=1,\\dots, k-1\\). For \\(h=H-1,\\dots,0\\), compute optimistic $Q$ estimates: . \\[\\begin{align*} y_{h,j} &amp;= r_h(S_h^{(j)}, A_h^{(j)}) + V_{h+1}^{(j)}(S_h^{(j)})\\quad \\forall\\,j=1,\\dots,k-1\\\\ \\phi_{h,j} &amp;= \\phi(S_h^{(j)}, A_h^{(j)})\\quad \\forall\\,j=1,\\dots,k-1\\\\ \\hat w_{h,k} &amp;= \\arg\\min_{w \\in \\mathbb{R}^d} \\sum_{j=1}^{k-1} \\big(\\langle\\phi_{j,h}, w\\rangle - y_{j,h}\\big)^2 + \\lambda \\|w\\|_2^2\\\\ \\Sigma_{h,k} &amp;= \\sum_{j=1}^{k-1} \\phi_{j,h}\\phi_{j,h}^\\top + \\lambda \\mathbf{I}_d\\\\ Q_h^{(k)}(s,a) &amp;= (\\langle\\phi(s,a), \\hat w_{h,k}\\rangle + \\tilde \\beta_{k,\\delta}^{1/2} \\|\\phi(s,a)\\|_{\\Sigma_{h,k}^{-1}}) \\wedge H \\end{align*}\\] | For $h=0,\\dots, H-1$, follow greedy policy . \\[A_{h}^{(k)} = \\arg\\max_{a \\in \\mathcal{A}}Q_h^{(k)}(S_h^{(k)},a)\\] | . Note that computing the optimistic policy in episode \\(k\\) can be done in time \\(\\mathcal{O}(Hd^2 + HAd)\\) by incrementally updating the least-square estimates \\(\\hat w_{h,k}\\) using the Sherman-Morrison formula. Compared to UCRL-VTR, this avoids iteration over the state space \\(S\\), which is a big advantage! . Theorem (LSVI-UCB Regret) . The regret of LSVI-UCB is bounded up to logarihmic factors and with probability at least \\(1-\\delta\\) as follows: . \\[R_K \\leq \\mathcal{\\tilde O}(d^{3/2} H^2 \\sqrt{K})\\] . Proof: The proof idea follows a similar strategy as the proof we presented for UCRL-VTR. As mentioned before, the crux is to show a confidence bound for LSVI that is indepenent of the size of the state space. For details, we again refer you to chapter 8 of the RL Theory Book. \\(\\qquad\\blacksquare\\) . ",
    "url": "/2024/lecture-notes/online-rl/lec24/#linear-mdps",
    
    "relUrl": "/lecture-notes/online-rl/lec24/#linear-mdps"
  },"45": {
    "doc": "24. Featurized MDPs",
    "title": "Notes",
    "content": "Bernstein-type bounds for VTR (UCRL-VTR$^+$) . The UCRL-VTR$^+$ algorithm is computationally efficient and able to obtain a regret upper bound of \\(\\mathcal{O}(dH\\sqrt{K})\\), and \\(\\mathcal{O}(d\\sqrt{T}(1-\\gamma)^{-1.5})\\) in the episodic and discounted, infinite horizon setting respectively. These results rely on using bernstein-type bounds. Better regret bounds for Linear MDPs (Eleanor)? . A careful reader might have noticed that the regret bound for LSVI-UCB, \\(\\mathcal{\\tilde O}(d^{3/2} H^2 \\sqrt{K})\\), is not tight with the tabular lower bound, \\(\\Omega(d \\sqrt{K})\\). The difference is in a factor of \\(\\sqrt{d}\\). The Eleanor algorithm (Algorithm 1 in Zanette et al (2020)) is able to shave of the factor of \\(\\sqrt{d}\\), obtaining a regret upper bound of \\(\\mathcal{\\tilde O}(d H^2 \\sqrt{K})\\). However, it is not currently known if the alogrithm can be implemented in a computationally efficient way. The Eleanor algorithm operates under the assumption of low inherent Bellman error (Definition 1 in Zanette et al (2020)), which means the function class is approximately closed under the Bellman optimality operator. It is interesting to note that this assumption is more general than the Linear MDP, thus Eleanor is also able to operate under the Linear MDP assumption. ",
    "url": "/2024/lecture-notes/online-rl/lec24/#notes",
    
    "relUrl": "/lecture-notes/online-rl/lec24/#notes"
  },"46": {
    "doc": "24. Featurized MDPs",
    "title": "References",
    "content": "The UCRL-VTR paper. Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., &amp; Yang, L. (2020, November). Model-based reinforcement learning with value-targeted regression. In International Conference on Machine Learning (pp. 463-474). PMLR. [link] . The UCRL-VTR$^+$ paper. It also shows the regret lower bound for linear mixture MDPs \\(\\Omega(d H^{3/2} \\sqrt{K})\\). Zhou, D., Gu, Q., &amp; Szepesvari, C. (2021, July). Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In Conference on Learning Theory (pp. 4532-4576). PMLR. [link] . The LSVI-UCB paper. Jin, C., Yang, Z., Wang, Z., &amp; Jordan, M. I. (2020, July). Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory (pp. 2137-2143). PMLR. [link] . The Eleanor paper. Zanette, A., Lazaric, A., Kochenderfer, M., &amp; Brunskill, E. (2020, November). Learning near optimal policies with low inherent bellman error. In International Conference on Machine Learning (pp. 10978-10989). PMLR. Link . ",
    "url": "/2024/lecture-notes/online-rl/lec24/#references",
    
    "relUrl": "/lecture-notes/online-rl/lec24/#references"
  },"47": {
    "doc": "24. Featurized MDPs",
    "title": "24. Featurized MDPs",
    "content": "PDF Version . In tabular (finite-horizon) MDPs \\(M=(\\mathcal{S}, \\mathcal{A}, P, r, \\mu, H)\\), roughly speaking, the learner has to learn about reward and transition probabilities for all states and actions in the worst-case. This is reflected in lower bounds on the regret that scale with \\(R_K \\geq \\Omega(H^{3/2}\\sqrt{ASK})\\) (in the time in-homogeneous case). In many applications the state space can be huge, and reinforcement learning is often used together with function approximation. In such settings, we want to avoid bounds that scale directly with the number of states \\(S\\). The simplest parametric models often rely on state-action features and linearly parametrized transition and reward functions. The goal is to obtain bounds that scale with the complexity of the function class (e.g. the feature dimension in linear models), and are independent of \\(S\\) and \\(A\\). Historically, many ideas for online learning in linear MDP models are borrowed from the linear bandit model. Beyond what is written here, you may find it helpful to read about stochstic linear bandits and LinUCB (see chapters 19 and 20 of the Bandit Book). ",
    "url": "/2024/lecture-notes/online-rl/lec24/",
    
    "relUrl": "/lecture-notes/online-rl/lec24/"
  },"48": {
    "doc": "Online RL",
    "title": "Online RL",
    "content": "PDF Version . ",
    "url": "/2024/lecture-notes/online-rl",
    
    "relUrl": "/lecture-notes/online-rl"
  },"49": {
    "doc": "1. Introductions",
    "title": "Introduction",
    "content": "Hello everyone and welcome to CMPUT 605: Theoretical Foundations of Reinforcement Learning at the University of Alberta. We are very excited to be teaching this course and hope that you are excited to journey with us through reinforcement learning theory. The course will cover two sub-topics of RL theory: (1) Planning, and (2) Online RL: . | Planning refers to the problem of computing plans, or policies, or just action by interacting with some model. | Online RL refers to the problem of coming up with actions that maximize total reward while interacting with an environment. | . In all of these subproblems, we will use Markov Decision Processes, to describe how either the simulation models, or the environments work. Thus, we start by introducing the formal definition of a Markov Decision Process (MDP). ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec1/#introduction",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec1/#introduction"
  },"50": {
    "doc": "1. Introductions",
    "title": "Markov Decision Process",
    "content": "A Markov Decision Process is a mathematical model for modelling sequential decision making in an environment that undergoes stochastic transitions. An MDP consists of the following elements: states, actions, rules of stochastic transitions between states, rewards, and an objective, which we take for now to be the discounted total expected reward, or return. States are considered to be primitive thus we do not explicitly define what they are. The set of states will be denoted by \\(\\mathcal{S}\\). Actions are also primitive and their set is denoted by \\(\\mathcal{A}\\). For simplicity, we assume that both sets are finite. We will let the number of states be denoted by \\(\\mathrm{S}\\), and similarly, we let the number of actions be denoted by \\(\\mathrm{A}\\). Stochastic transitions between states \\(s\\) and \\(s'\\) are the result of choosing some action $a$ in a given state. For a fixed state \\(s\\) and action \\(a\\), the probabilities of landing in the various states \\(s'\\) is collected into a probability vector, which is denoted by \\(P_a(s)\\). To minimize clutter, by slightly abusing notation, we will write \\(P_a(s,s')\\) as the \\(s'\\in \\mathcal{S}\\) component of this probability vector. This is the probability that the process will transition into state \\(s'\\), when in state \\(s\\) it takes action \\(a\\). Rewards are scalars and the reward incurred as a result of taking action \\(a\\) in state \\(s\\) is denoted by \\(r_a(s)\\). Since the number of states and actions are finite, there is no loss in generality by assuming that all the rewards belong to the \\([0,1]\\) interval. Taking action \\(A_t\\) at time step \\(t\\) gives rise to an infinitely long trajectory of state-action pairs \\(S_0,A_0,S_1,A_1,...\\): here, \\(S_{t+1}\\) is the state that results from taking action \\(A_t\\) in time step \\(t\\ge 0\\) and the assumption is that as long as \\(A_t\\) is chosen based on the “past” only, the distribution of \\(S_{t+1}\\) given \\(S_0,A_0,\\dots,S_t,A_t\\) is solely determined by \\(P_{A_t}(S_t)\\), and, in particular, \\(\\mathbb{P}\\)-almost surely, . \\[\\begin{align} \\label{eq:markov} \\mathbb{P}(S_{t+1}=s|S_0,A_0,\\dots,S_t,A_t) = P_{A_t}(S_t,s)\\,. \\end{align}\\] The objective is to find a way of choosing the actions that result in the largest possible return along the trajectories that arise. The return along a trajectory is defined as . \\[R = r_{A_0}(S_0) + \\gamma r_{A_1}(S_1) + \\gamma^2 r_{A_2}(S_2) + \\dots + \\gamma^t r_{A_t}(S_t) + \\dots\\] where \\(\\gamma \\in [0,1)\\) is the discount factor. Formally, a (discounted) MDP will thus be described by the \\(5\\)-tuple \\(M = (\\mathcal{S},\\mathcal{A},P,r,\\gamma)\\), where \\(P=(P_a(s))_{s,a}\\) and \\(r=(r_a(s))_{s,a}\\) collect the transitions and the rewards, respectively. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec1/#markov-decision-process",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec1/#markov-decision-process"
  },"51": {
    "doc": "1. Introductions",
    "title": "On Discounting",
    "content": "Note that \\(\\gamma\\) makes it so that the future reward does not matter as much as the present reward. Also, if we truncate the above sum after \\(H\\ge 0\\) terms, by our assumption on the rewards, the difference between the return and the truncated return is between zero and . \\[\\gamma^H \\Big[r_{A_H}(S_H)+\\gamma r_{A_{H+1}}(S_{H+1})+\\dots \\Big]\\le \\gamma^H \\sum_{s\\ge 0} \\gamma^s = \\frac{\\gamma^H}{1-\\gamma}\\] by using the summation rule for geometric series. Solving for the largest \\(H\\) under which the above upper bound on the difference is below \\(\\varepsilon\\), we get that this bound on the difference holds as long as \\(H\\) satisfies . \\[H \\ge \\underbrace{\\frac{\\ln \\left( \\frac{1}{\\varepsilon(1-\\gamma)} \\right)}{\\ln(1/\\gamma)}}_{H_{\\gamma,\\varepsilon}^*} \\,.\\] For $H$ satisfying this, the return is maximized already when considering only the first \\(H\\) time steps. Notice that the critical value of \\(H\\) depends on not only \\(\\varepsilon\\) but also \\(\\gamma\\). For a fixed \\(\\varepsilon\\), this critical value is called the effective horizon. Oftentimes, for the sake of simplicity, we replace $H_{\\gamma,\\varepsilon}^*$ with the following quantity: . \\[H_{\\gamma,\\varepsilon}:=\\frac{\\ln \\left( \\frac{1}{\\varepsilon(1-\\gamma)} \\right)}{1-\\gamma}\\,.\\] (In fact, the literature often calls the latter the effective horizon). This quantity is an upper bound on \\(H_{\\gamma,\\varepsilon}^*\\). Furthermore, it is not hard to verify that the relative difference between these two quantities is of order $o(1-\\gamma)$ as $\\gamma\\to 1$. Thus, \\(H_{\\gamma,\\varepsilon}^*\\) behaves the same as $H_{\\gamma,\\varepsilon}$ up to a first-order approximation as $\\gamma\\to 1$. Since we are typically interested in this regime (large horizons), there is no loss in switching from \\(H_{\\gamma,\\varepsilon}^*\\) to $H_{\\gamma,\\varepsilon}$. The discounted setting may occasionally feel a bit cringey. Where is the discount factor coming from? One approach is to think about how many time steps in the future we think the optimization should look into for some level of desired accuracy and then work backwards to set \\(\\gamma\\) so that the resulting effective horizon matches our expectation. However, it is more honest to admit that the discounted objective may not faithfully capture the nature of a decision problem. Indeed, there are other objectives that one can consider, such as the finite horizon, undiscounted (or discounted) setting, the infinite horizon setting with no discounting (“total reward”), or the infinite horizon with the average reward. All these have their own pros and cons and we will consider some of these objectives and their relationships in future lectures. For now, we will stick to the discounted objective for pedagogical reasons: the math underlying the discounted objective is simple and elegant. Also, many results transfer to the other settings mentioned, perhaps with some extra conditions, or a little change. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec1/#on-discounting",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec1/#on-discounting"
  },"52": {
    "doc": "1. Introductions",
    "title": "Policies",
    "content": "A policy is a rule that describes how the actions should be taken in light of the past. Here, the past at time step \\(t\\ge 0\\) is defined as . \\[H_t = (S_0,A_0,\\dots,S_{t-1},A_{t-1},S_t)\\] which is the sequence of state-action pairs leading up to the state of the process at the current time step \\(t\\). We allow policies to randomize. As such, formally, a policy becomes an infinite sequence \\(\\pi = (\\pi_t)_{t\\ge 0}\\) of maps of histories to distributions over actions. For a (finite) set \\(X\\) let \\(\\mathcal{M}_1(X)\\) denote the set of probability distributions over \\(X\\). These probability distributions are uniquely determined by what probability they assign to the individual elements of \\(X\\). Hence, they will be identified with the probability vectors with \\(|X|\\) components, each component giving the probability of some $x\\in X$. If $p\\in \\mathcal{M}_1(X)$, we use both $p_x$ and $p(x)$ to denote this probability (whichever is more convenient). With this, we can write that the \\(t\\)th “rule” in \\(\\pi\\), which will be used in the \\(t\\)th time step to come up with the action for that time step, as . \\[\\pi_t: \\mathcal{H}_t \\to \\mathcal{M}_1(\\mathcal{A})\\,,\\] where . \\[\\mathcal{H}_t = (\\mathcal{S} \\times \\mathcal{A})^{t-1} \\times \\mathcal{S}\\,.\\] Note that \\(\\mathcal{H}_0 = \\mathcal{S}\\). Intuitively, following a policy \\(\\pi\\) means that in time step \\(t\\ge 0\\), the distribution of the action \\(A_t\\) to be chosen for that timestep is \\(\\pi_t(H_t)\\): the probability that \\(A_t=a\\) is \\(\\pi_t(H_t)(a)\\). Since writing \\(\\pi_t(H_t)(a)\\) is quite cumbersome, we abuse notation and will write $\\pi_t(a|H_t)$ instead. Thus, when following a policy \\(\\pi\\), in time step \\(t\\ge 0\\) we get that, \\(\\mathbb{P}\\)-almost surely, . \\[\\begin{align} \\label{eq:pol} \\mathbb{P}(A_t=a|H_t) = \\pi_t(a|H_t)\\,. \\end{align}\\] ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec1/#policies",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec1/#policies"
  },"53": {
    "doc": "1. Introductions",
    "title": "Initial State Distributions, Distributions over Trajectories",
    "content": "When a policy is interconnected with an MDP, the interconnection, together with an initial distribution \\(\\mu\\in \\mathcal{M}_1(\\mathcal{S})\\) over the states, uniquely determines a distribution over the infinite-long trajectories . \\[T = (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}\\] such that for every time step \\(t\\ge 0\\), both \\(\\eqref{eq:markov}\\) and \\(\\eqref{eq:pol}\\) hold, in addition to that . \\[\\begin{align} \\label{eq:init} \\mathbb{P}(S_0=s) = \\mu(s)\\,, \\qquad s\\in \\mathcal{S}\\,. \\end{align}\\] In fact, this distribution could be over some potentially bigger probability space, in which case uniqueness does not hold. When we want to be specific and take the distribution that is defined over the infinite-long state-action trajectories, we will say that this is the distribution over the canonical probability space induced by the interconnection of the policy and the MDP. To emphasize the dependence of the probability distribution \\(\\mathbb{P}\\) on \\(\\mu\\) and \\(\\pi\\), we will often use \\(\\mathbb{P}_\\mu^\\pi\\), but we will also take the liberty to drop any of these indices when its identity can be uniquely deduced from the context. When needed, the expectation operator corresponding to \\(\\mathbb{P}\\) (or \\(\\mathbb{P}_\\mu^\\pi\\)) will be denoted by \\(\\mathbb{E}\\) (respectively, \\(\\mathbb{E}_\\mu^\\pi\\)). What is the probability assigned to a trajectory \\(\\tau = (s_0,a_0,s_1,a_1,\\dots)\\in T\\)? Let \\(h_t = (s_0,a_0,\\dots,s_{t-1},a_{t-1},s_t)\\). Recall that \\(H_t = (S_0,A_0,\\dots,S_{t-1},A_{t-1},S_{t})\\). By a repeated application of the chain rule of probabilities, we get . \\[\\begin{align*} \\mathbb{P}(&amp;H_t=h_t)\\\\ &amp;= \\mathbb{P}(S_0=s_0,A_0=a_0,S_1=s_1,\\dots,S_t=s_t)\\\\ &amp;= \\mathbb{P}(S_t=s_t|H_{t-1}=h_{t-1},A_{t-1}=a_{t-1}) \\mathbb{P}(H_{t-1}=h_{t-1},A_{t-1}=a_{t-1}) \\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\mathbb{P}(H_{t-1}=h_{t-1},A_{t-1}=a_{t-1}) \\tag{by \\eqref{eq:markov}}\\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\mathbb{P}(A_{t-1}=a_{t-1}|H_{t-1}=h_{t-1}) \\mathbb{P}(H_{t-1}=h_{t-1})\\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\pi_{t-1}(a_{t-1}|h_{t-1}) \\mathbb{P}(H_{t-1}=h_{t-1}) \\tag{by \\eqref{eq:pol}}\\\\ &amp; \\;\\; \\vdots \\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\pi_{t-1}(a_{t-1}|h_{t-1}) \\times \\dots\\times P_{a_0}(s_0,s_1) \\pi_{0}(a_0|s_0) \\mathbb{P}(S_0=s_0) \\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\pi_{t-1}(a_{t-1}|h_{t-1}) \\times \\dots\\times P_{a_0}(s_0,s_1) \\pi_{0}(a_0|s_0) \\mu(s_0)\\,. \\tag{by \\eqref{eq:init}} \\end{align*}\\] Collecting the terms, . \\[\\mathbb{P}(H_t=h_t) = \\mu_0(s_0) \\left\\{ \\Pi_{i=0}^{t-1} \\pi_i(a_i|h_i)\\right\\} \\, \\left\\{ \\Pi_{i=0}^{t-1} P_{a_i}(s_i,s_{i+1})\\right\\}\\,.\\] Similarly, . \\[\\mathbb{P}(H_t=h_t,A_t=a_t) = \\mu_0(s_0) \\left\\{ \\Pi_{i=0}^{t} \\pi_i(a_i|h_i)\\right\\} \\, \\left\\{ \\Pi_{i=0}^{t-1} P_{a_i}(s_i,s_{i+1})\\right\\}\\,.\\] ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec1/#initial-state-distributions-distributions-over-trajectories",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec1/#initial-state-distributions-distributions-over-trajectories"
  },"54": {
    "doc": "1. Introductions",
    "title": "Value Functions, the Optimal Value Function and the Objective",
    "content": "The total expected discounted reward, or the expected return of policy \\(\\pi\\) in MDP \\(M\\) when the initial state is sampled from $\\mu$ is . \\[v^\\pi(\\mu) = \\mathbb{E}_\\mu^\\pi \\left[ R \\right] \\,.\\] When \\(\\mu=\\delta_s\\) where \\(\\delta_s\\) is the “Dirac” probability distribution that puts a point mass at \\(s\\), we use \\(v^\\pi(s)\\) to denote the resulting value. Since this assigns a value to every state, \\(v^\\pi\\) can be viewed as a function assigning a value to every state in \\(\\mathcal{S}\\). This function will be called the value function of policy \\(\\pi\\). When the dependence on the MDP is important, we may add “in MDP \\(M\\)” and denote the dependence by introducing an index: \\(v^\\pi_M\\). The best possible value in state \\(s\\in \\mathcal{S}\\) that can be obtained by optimizing over all possible policies is . \\[v^*(s) = \\sup_{\\pi} v^\\pi(s)\\,.\\] Then, \\(v^*: \\mathcal{S}\\to \\mathbb{R}\\), viewed as a function, is called the optimal value function. A policy is optimal in state \\(s\\) if \\(v^\\pi(s)=v^*(s)\\). A policy is uniformly optimal if it is optimal in every state. In what follows, we will drop uniformly as we will usually be interested in finding uniformly optimal policies. Given an MDP, we are interested in efficiently computing an optimal policy. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec1/#value-functions-the-optimal-value-function-and-the-objective",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec1/#value-functions-the-optimal-value-function-and-the-objective"
  },"55": {
    "doc": "1. Introductions",
    "title": "Planning=Computation",
    "content": "Computing an optimal policy can be seen as a planning problem: the optimal policy answers the question of how to take actions so that the expected return is maximized. This is also an algorithmic problem. The input, in the simplest case, is a big table (or a number of tables) that describes the transition probabilities and rewards. The interest is to develop algorithms that read in this table and then as output should return a description of an optimal policy. At this stage, it may seem unlikely that an efficient algorithm could do this: in the above unrestricted form, policies have an infinite description. As we shall find out soon though, we will be lucky with finite MDPs as in such MDPs one can always find optimal policies that have a short description. Then, the algorithmic question becomes interesting! . As for any algorithmic problem, the main question is how many elementary computational steps are necessary to solve an MDP? As can be suspected, the number of steps will need to scale with the number of states and actions. Indeed, even the size of the input scales with these. If computation indeed needs to scale with the number of state-action pairs, is there still any reason to consider this problem given that the number of states and actions in MDPs that one typically encounters in practical problems is astronomically large, if not infinite? Yes, there are: . | Not all MDPs are in fact large and it may be useful to know what it takes to “solve” a small MDP. Good solvers for “small” MDPs may serve as benchmarks for solvers developed for the “large MDP” case. | Even if a problem is large (or infinite), one may be able to approximate it well with a small MDP. Then, a solver for a small MDP may be useful. | Some ideas and tools developed for this problem also generalize (perhaps) with some twists to the “large” MDP setting. | . At this stage, the reader may be wondering about what is meant by “small” and “large”? As a rough guideline, by “small” we mean problems where the tables describing the MDP (and/or policy) comfortably fit in the memory of whatever computer one has access to. Large is everything else. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec1/#planningcomputation",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec1/#planningcomputation"
  },"56": {
    "doc": "1. Introductions",
    "title": "Miscellaneous Remarks",
    "content": "Probabilities of infinite long trajectories? . Based on the above calculations, one expects that the probability of a trajectory $\\tau = (s_0,a_0,s_1,a_1,\\dots)$ that never ends is . \\[\\begin{align*} \\mathbb{P}(S_0=s_0,A_0=a_0,S_1=s_1,A_1=a_1,\\dots) &amp;= \\mu(s_0) \\times \\pi_0(a_0|h_0) \\times P_{a_0}(s_0,s_1) \\\\ &amp; \\qquad\\quad\\;\\; \\times \\pi_1(a_1|h_1) \\times P_{a_1}(s_1,s_2) \\\\ &amp; \\qquad\\quad\\;\\; \\times \\cdots \\\\ &amp; \\qquad\\quad\\;\\; \\times \\pi_{t}(a_t|h_t) \\times P_{a_t}(s_t,s_{t+1}) \\\\ &amp; \\qquad\\quad\\;\\; \\times \\cdots \\end{align*}\\] where \\(h_t = (s_0,a_0,\\dots,s_{t-1},a_{t-1},s_t)\\) as before. However, this does not work: if in the trajectory, each action is taken with probability $1/2$ by the policy on the given history, the infinite product on the right-hand side is zero! This should make one pause at least for a moment: how is then \\(\\mathbb{P}\\) even defined? Does this distribution even exist? If yes, and it assigns zero probability to trajectories like above, could not it be that it assigns zero to all the trajectories of infinite length? In the world of infinite, one must tread carefully! The way out of this conundrum is that we must use measure theoretic probabilities, or we need to give up on objects like the return, \\(R= \\sum_{t\\ge 0}\\gamma^t r_{A_t}(S_t)\\), which is defined on trajectories of infinite length. The alternative to measure theoretical probability is to define everything through by taking limits (and always taking expectations over finite-length prefixes of the infinite long trajectories). As this would be quite cumbersome, we will take the measure-theoretic route, which will be explained in the next lecture. Why Markov? . Equation \\(\\eqref{eq:markov}\\) tells us that the only thing that matters from the history of the process as far as the prediction of the next state is concerned is the last action and the last state. This is known as the Markov property. More generally, Markov chains, which are specific stochastic processes, have a similar property. Bellman’s curse of dimensionality . Richard Bellman, who has made many foundational contributions to the early theory, coined the term the “curse of dimensionality”. By this, Bellman meant the following: oftentimes when MDPs are used to model a practical decision making problem, the state space oftentimes takes the product form \\(\\mathcal{S} = \\mathcal{S}_1 \\times \\dots \\times \\mathcal{S}_d\\) with some $d&gt;0$. If each set \\(\\mathcal{S}_i\\) here has at only two(!) elements, the state space will have at least \\(2^d\\) elements. This is an exponential growth as a function of \\(d\\), which is taken as the fundamental scaling quantity. Thus, any algorithm that needs to even just enumerate the states in the state space is “cursed” to perform a very lengthy calculation. While we start with considering the case when both the state and the action space are small (as described above), the main focus will be on the case when this is not true anymore. In this way, the problem will be to figure out ways of breaking the curse. But just to make things clear, in the worst-case, there is no cure to this curse, as we shall see it soon in a rigorous fashion. Any cure will come by changing the problem, either by changing the objective, or by changing the inputs available, or both. Actions shared across states? . We described MDPs as if the same set of actions was available in all the states. This may create the (false) impression that action $a_1$ in state $s_1$ has something to do with action $a_1$ in state $s_2$ (i.e., their rewards, or next state distributions are shared or are similar). Given the MDP definition though, clearly, no such assumptions are made. In a way, a better way of describing an MDP is using a set \\(Z\\) and an equivalence relation over \\(Z\\), or, equivalently, the partition induced by it over \\(Z\\). We should think of \\(Z\\) as the set of possible state-action pairs: The equivalence relation over \\(Z\\) then gives which of these share a common state. Alternatively, if \\(z_1\\) and \\(z_2\\) are in the same partition, they share a state, which we can identify with the partition. Then, for every \\(z\\in Z\\), the MDP would specify a distribution over the parts of the partition (the “next states”) and one should specify a reward. While this description is appealing from a mathematical perspective, it is nonstandard and would make it harder to relate everything to the literature. Furthermore, the description chosen, apart from the inconvenience that one need to forcefully remember that actions do not keep their identity across states, is quite intuitive and compact. A common variation in the literature, which avoids the “sharing issue” is to assume that every state is equipped with a set \\(\\mathcal{A}(s)\\) of actions admissible to the state and these sets are disjoint across the states. This description allows the number of actions to be varied across the states. While this has a minor advantage, our notation is simpler and tends not to lose much in comparison to these more sophisticated alternatives. Are states observed? . In many practical problems it is not a priori clear whether the problem has a good approximate description as an MDP. One critical aspect that is missing from the MDP description is that the states of the MDP may not be available for measurement and thus the control (the choice of the action) cannot use state information. For now, we push this problem aside, but we shall return to it time-to-time. The reason is that it is best to start with the simpler questions and, at least intuitively, the problem of finding a policy that can use state information feels easier than finding one that cannot even access the state information. First, at least, we should find out what can be done in this case (and how efficiently), hoping that the more complex cases will either be reducible to this case, or will share some common patterns. On the notation . Why use \\(r_a(s)\\) rather than, say, \\(r(s,a)\\)? Or \\(P_a(s)\\), or \\(P_a(s,s')\\) rather than \\(P(s'|s,a)\\)? All these notations have pros and cons. None of them is ideal for all purposes. One explanation for using this notation is that later we will replace $a$ with $\\pi$, where $\\pi$ will be a special policy (a memoryless, or stationary Markov policy). When doing so, the notation of $r_\\pi$ (suppressing $s$) and \\(P_\\pi\\) (a stochastic matrix!) will be tremendously useful. A bigger question is why use $s$ for states and $a$ for actions. Is not the answer in the words? Well, people working in control would disagree. They would prefer to use $x$ for state and $u$ for actions, and I am told by Maxim Raginsky, that these come from Russian abbreviations, so they make at least as much sense as the notation used here. That is, if one speaks Russian (and if not, why not learn it?). Dimitri Bertsekas likes using $i,j$ etc. for states, which seems fine if one has discrete (countable) state spaces. Stochastic rewards . Some authors (e.g., this author in some of their papers or even in his book) considers rewards which are stochastic. This may matter when the problem is to learn a good policy, or to find a good plan while interacting with a stochastic simulator. However, when it comes to defining the object of computation, we can safely ignore (well-behaved) stochastic rewards. Here, the well-behaved stochastic rewards are those whose conditional expectation given an arbitrary history up to a state $s$ and an action $a$ taken in that state depends only on $(s,a)$. Which is what we start here from. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec1/#miscellaneous-remarks",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec1/#miscellaneous-remarks"
  },"57": {
    "doc": "1. Introductions",
    "title": "References",
    "content": "“The” book about MDPs is: . Puterman, Martin L. 2005. Markov Decision Processes (Discrete Stochastic Dynamic Programming). Wiley-Interscience. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec1/#references",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec1/#references"
  },"58": {
    "doc": "1. Introductions",
    "title": "1. Introductions",
    "content": "PDF Version . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec1/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec1/"
  },"59": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Planning under \\(q^*\\) realizability",
    "content": "We consider fixed horizon online planning in large finite MDPs $(\\mathcal{S},\\mathcal{A},P,r)$. As usual, the horizon is denoted by $H&gt;0$ and we consider planning with a fixed initial state $s_0$, as in the previous lecture. Let us denote by \\(\\mathcal{S}_i\\) the states that are reachable from $s_0$ in $0\\le i \\le H$ steps. As before, we assume that \\(\\mathcal{S}_i\\cap \\mathcal{S}_j=\\emptyset\\) when $i\\ne j$. Recall that in this case the action-value functions depend on the number of steps left, of the current stage. For a fixed $0\\le h\\le H-1$, let \\(q^*_h:\\mathcal{S}_{h} \\times \\mathcal{A}\\to \\mathbb{R}\\) be the optimal action-value function with $h$ stages in the process, $H-h$ stages left. Since we do not need the values of $q^*_h$ outside of $\\mathcal{S}_h\\times \\mathcal{A}$, we abuse notation by redefining it restricted to this set. Important note: The indexing of $q^*_h$ used here is not consistent with the indexing used in the previous lecture, where it was more convenient to index value functions based on the number of stages left. The planner will be given a feature map $\\phi_h$ for every stage $0\\le h\\le H-1$ such that \\(\\phi_h:\\mathcal{S}_h \\times \\mathcal{A} \\to \\mathbb{R}^d\\). The realizability assumption means that . \\[\\begin{align} \\inf_{\\theta\\in \\mathbb{R}^d} \\max_{0\\le h \\le H-1}\\|\\Phi_h \\theta - q^*_{h} \\|_\\infty = 0\\,. \\label{eq:qsrealizability} \\end{align}\\] Note that we demand that the same parameter vector is shared between all stages. As it turns out, this makes our result stronger. Regardless, at the price of increasing the dimension from $d$ to $dH$, one can always assume that the parameter vector is shared. Since we will give a negative result concerning the query-efficiency of planners, we allow the planners access to the full feature-map: The negative result still applies even if the planner is allowed to perform any sort of computation with the feature-map during or before the planning process. For $\\delta&gt;0$, we call an online planner $\\delta$-sound for the $H$-step criterion if for any MDP $M$ and feature map $\\phi = (\\phi_h)_h$ pair such that the optimal action-value function of $M$ is realizable with the features $\\phi$ in the sense that \\eqref{eq:qsrealizability} holds, the planner induces a policy that is $\\delta$-suboptimal or better when evaluated with the $H$-horizon undiscounted total reward criterion from the designated start-state \\(s_0\\) in MDP $M$. Note that this is very much the same as the previous $(\\delta,\\varepsilon=0)$ soundness criterion, except that the definition of the approximation error is relaxed, while we demand $\\varepsilon=0$. The result below uses MDPs where the immediate reward (obtained from the simulator) can be random. The random reward is used to make the job of the planners harder and it allows us to consider MDPs with deterministic dynamics. (The result could also be proven for MDPs with deterministic rewards and random transitions.) . The usual definition of MDPs with random transitions and rewards is in a way even simpler: Such a (finite) MDP is given by the tuple \\(M=(\\mathcal{S},\\mathcal{A},Q)\\) where \\(Q = (Q_a(s))_{s,a}\\) is a collection of distributions over state-reward pairs. In particular, for all state-action pairs $(s,a)$, \\(Q_a(s)\\in \\mathcal{M}_1(\\mathcal{S}\\times\\mathbb{R})\\). Letting \\((S',R)\\sim Q_a(s)\\) (i.e., $(S’,R)$ is drawn from \\(Q_a(s)\\) at random), we can recover $P_a(s)$ as the distribution of $S’$ and $r_a(s)$ as the expected value of $R$. That the reward can be random forces a change to the notion of the canonical probability spaces, since histories now also show include rewards, $R_0,R_1,\\dots$ incurred in each time step $t=0,1,\\dots$. With appropriate modifications, we can nevertheless still introduce \\(\\mathbb{P}_\\mu^\\pi\\) and the corresponding expectation operator, \\(\\mathbb{E}_\\mu^\\pi\\), as well. The natural definition of the value of a policy $\\pi$ at state $s$, say, in the discounted setting is then \\(v^\\pi(s) = \\mathbb{E}_s^\\pi[ \\sum_{t=0}^\\infty \\gamma^t R_t]\\). However, it is easy to see that for any $t\\ge 0$, \\(\\mathbb{E}_\\mu^\\pi[R_t]=\\mathbb{E}_\\mu^\\pi[r_{A_t}(S_t)]\\), and, as such, nothing changes in the theoretical results derived so far. For $a,b$ reals, let $a\\wedge b = \\min(a,b)$. The main result of this lecture is as follows: . Theorem (worst-case query-cost is exponential under $q^*$-realizability): For any $d,H$ large enough and any online planner $\\mathcal{P}$ that is $9/128$-sound for the $H$-horizon planning problem, there exists a triplet $(M,s_0,\\phi)$ where $M$ is a finite MDP with random rewards taking values in $[0,1]$ and deterministic transitions, $s_0$ is a state of this MDP and $\\phi$ is a $d$-dimensional feature-map such that \\eqref{eq:qsrealizability} holds for the optimal action-value function \\(q^* = (q^*_h)_{0\\le h \\le H-1}\\) and the expected number of queries $q$ that $\\mathcal{P}$ uses when interconnected with $(M,s_0,\\phi)$ satisfies . \\[q = e^{\\Omega(d\\wedge H )}\\] . Note that with random rewards with no control on their tail behavior (e.g., unbounded variance) it would not be hard to make the job of any planner arbitrarily hard. As such, it is quite important that the MDPs that are constructed for the result, the rewards, while random, lie in a fixed interval. Note that the specific choice of this interval does not matter: If there is a hard example with some interval, that example can be translated into another by shifting and scaling, and at the price of introducing an extra dimension in the feature map to account for the shifts. A similar comment applies to $\\delta = 9/128$ (which, nevertheless, needs to be scaled to the range of the rewards). ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec10/#planning-under-q-realizability",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec10/#planning-under-q-realizability"
  },"60": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "The main ideas of the proof",
    "content": "Rather than giving the full proof, we will just explain the main ideas behind it. At a high-level, the proof merges the ideas behind the lower bound for the small action-set case and the lower bound of the large action-set case. That is, we will consider an action set that is exponentially large in $d$. In particular, we will consider action sets that have $k=e^{\\Theta(d)}$ elements. Note that because realizability holds, having a large action set but with a trivial dynamics (as in the lower bound in the last lecture) does not lead to the lower bound of the desired form. In particular, if the dynamics are trivial (i.e., $\\mathcal{S}_i={s_i}$, see the figure on the right) then the optimal action to be taken at $s_0$ does not depend on what actions are taken at later stages and can be efficiently found by just maximizing for the reward received in that stage, which can be done efficiently due to our realizability assumption, even in the presence of random rewards. Whether an example exists with only a few actions but with a more complicated dynamics remains open. With the construction provided here (which is based on tree dynamics and zero intermediate reward in the tree), this clearly fails, as we will make it clear below. In any case, since the “chain dynamics” does not work, the next simplest approach is to have a tree, but with exponentially many actions in every node. Since this creates many many states ($e^{\\Theta(dh)}$ states at stage $h$) the next question then is how to ensure realizability. There are two issues: We need to be able to keep the dimension fixed at $d$ at every stage and somehow we will need to have a way of controlling which action should be optimal at each state at each stage. Indeed, realizability means that we need to ensure that for all $0\\le h \\le H-1$ and $(s,a)\\in \\mathcal{S}_h \\times \\mathcal{A}$, . \\[\\begin{align} q_{h}^*(s,a) = r_a(s)+v_{h+1}^*(sa) \\label{eq:cons} \\end{align}\\] Here, $sa$ stands for the state that is reached by taking action $a$ in state $s$ (in the tree, every node, or state is uniquely indexed by the action sequence that reaches it). Now, in the definition of $v_{h}^*$, for all $h$, we also have \\(v_{h}^*(s) = \\max_{a\\in \\mathcal{A}} q_{h+1}^*(s,a)\\), which calls for the need to know the identity of the maximizing action. What is more, since the solution to the Bellman optimality equations is unique, if we guarantee that \\eqref{eq:cons} holds at all state-action pairs for \\(q_h(s,a) = \\langle \\phi_h(s,a), \\theta^* \\rangle\\) with some features and parameter vectors, it also follows that \\(q_h = q^*_h\\) for all \\(h\\ge 0\\), that is, \\(q^*\\) is realizable with the features. A simple approach to resolve all of these issues is to let a fixed action $a^*\\in \\mathcal{A}$ be the optimal action at all the states, together with using the JL features from the previous lecture (the identity of this action is of course hidden from the planner). In particular, the JL feature-matrix lemma from the previous lecture furnishes us with $k$ $d$-dimensional unit vectors $(u_a)_{a\\in \\mathcal{A}}$ such that for $a\\ne a’$, . \\[\\begin{align*} \\vert \\langle u_a, u_{a'} \\rangle \\vert \\le \\frac{1}{4}\\,. \\end{align*}\\] Fix these vectors. That $a^*$ should be optimal at all states $s$ is equivalent to that . \\[\\begin{align} q_h^*(s,a)\\le q_h^*(s,a^*) (=v_h^*(s)), \\qquad 0\\le h \\le H-1, s\\in \\mathcal{S}_h, a\\in \\mathcal{A}\\,. \\label{eq:aopt} \\end{align}\\] In our earlier proof we used \\(\\phi_h(s,a) = u_a\\) and \\(\\theta^* = u_{a^*}\\). Will this still work? Unfortunately, it does not. The first observation is that from this it follows that for any $h$, $s$, $a$, . \\[\\begin{align*} q_{h}^*(s,a) = \\langle u_{a^*}, u_a \\rangle\\,. \\end{align*}\\] As such, for almost all the actions $a$, we expect \\(|q_h^*(s,a)|\\) to be close to \\(1/4\\). Now, under this choice we also have that \\(v_h^*(s)=1\\) for all states and all stages $0\\le h \\le H-1$. This creates essentially the same problem as what we saw above with the trivial chain dynamics. In particular, from \\eqref{eq:cons} we get that \\(q_h^*(s,a) = r_a(s)+1\\). As such, we expect $r_a(s)$ to be close to either $-3/4$ or $-5/4$ (since \\(|q_h^*(s,a)|\\) is close to $1/4$). Putting aside the issue that we wanted the immediate reward be in $[0,1]$, we see that if the reward noise is not large, \\(\\theta^*\\) and thus the identity of $a^*$ can be obtained with just a few queries: The signal to noise ratio is just too good! . This problem replicates itself at the very last stage: Here, \\(v_H^*(s')=0\\) for any state $s’$, hence . \\[\\begin{align} q^*_{H-1}(s,a)=r_a(s) \\label{eq:laststage} \\end{align}\\] for any $(s,a)$ pair. Unless we choose \\(q^*_{H-1}(s,a)\\) to be small, say, \\(e^{-\\Theta(H)}\\), a planner will succeed with fewer queries than in our desired bound. This motivates us to introduce a scaling of the features (recall that the parameter vector is shared between the stages) with some scaling factors. For maximum generality, we allow for the scaling factor of the feature vector of \\((s,a)\\in \\mathcal{S}_h\\times \\mathcal{A}\\) to depend on \\((s,a)\\) itself (since states between stages are not shared, scaling can depend on the stage with this choice). Let \\((3/2)^{-h+1}\\sigma_{sa}\\) be the scaling factor we intend to use with \\((s,a)\\) where we intend to keep $\\sigma_{sa}$ in a constant range (so the scaling with the stage index works as intended) while we aim to use \\(\\phi_h(s,a) =(3/2)^{-h+1} \\sigma_{sa} u_a\\). Now, we can explain the need for many actions. By the Bellman optimality equation \\eqref{eq:cons} we have that for any suboptimal action, $a$, . \\[r_{a^*}(s)-r_a(s) =q_h^*(s,a^*)-q_h^*(s,a) \\approx (3/2)^{-h} \\langle u_{a^*}-u_a,u_{a^*} \\rangle \\ge (3/2)^{-h} (3/4),\\] where \\(\\approx\\) uses that \\(\\sigma_{sa}\\approx\\sigma_{sa^*}\\approx \\text{const}\\). From this we see that close to the initial state \\(s_0\\) the reward gaps are of constant order. In particular, if there were only a few actions per state, a planner could identify the optimal action by finding the action whose reward is significantly larger than that of the others. By choosing to have many actions, the planner faces a “needle-in-a-haystack” situation, which makes their job hopeless even with perfect signal (no noise). The next idea is to force “clever” planners to only experiment with actions in the last stage. Since here, the signal-to-noise ratio will be very poor, if we manage to achieve this, even clever planners will need to use a large number of queries. A simple way of forcing this is to choose all the rewards while transitioning in the tree and taking suboptimal actions to be identically zero except for stage $h=H-1$, where, in accordance to our earlier plan, the rewards are chosen at random to ensure consistency but the signal to noise ratio will be poor. Since the dynamics in the tree is known, and it is known that all rewards are zero with the possible exception of when using the optimal action (one of exponentially many actions and is thus hard to find), planners are either left with either solving the needle in a haystack problem of identifying the optimal action by randomly stumbling upon it, or they need to experiment with actions in the last stage. That the rewards are chosen to be identically zero is not critical: From the point of view of this argument, what is critical is that they are all the same. It remains to be seen that consistency can be achieved and also that the optimal action at $s_0$ has a large value compared to the values of suboptimal actions at the same state. Here, we still face some challenges with consistency. Since we want the immediate rewards to belong to the $[0,1]$ interval, all the action values have to be nonnegative. As such, it will be easier if we introduce an additional bias component $c_h$ in the feature vectors, which we allow to scale with the stage. To summarize, we let . \\[\\begin{align*} \\phi_h(s,a) = ( c_h, (3/2)^{-h+1} \\sigma_{sa} u_a^\\top )^\\top\\,. \\end{align*}\\] while we propose to use . \\[\\begin{align*} \\theta^* = \\frac{1}{3} (1, u_{a^*}^\\top)^\\top \\,. \\end{align*}\\] It remains to show that \\eqref{eq:aopt} and \\eqref{eq:cons} can be satisfied with \\(q_h(s,a):=\\langle \\phi_h(s,a), \\theta^* \\rangle\\), while also keeping the suboptimal gap of \\(a^*\\) at \\(s_0\\) large, and while the last stage rewards (\\eqref{eq:laststage}) are in $[0,1]$ and are of size \\(e^{-\\Theta(H)}\\) as planned. Assume for a moment that \\(a^*\\) is optimal in all states, i.e., that \\eqref{eq:aopt} holds. Then, \\(a^*\\) is also optimal in state $sa$, hence, under \\(q^*_h=q_h\\), \\eqref{eq:cons} for any \\(a\\ne a^*\\) is equivalent to . \\[\\begin{align*} q_h(s,a) = q_{h+1}(sa,a^*) \\end{align*}\\] where we also used that by assumption $r_a(s)=0$ because \\(a\\ne a^*\\). Plugging in the definitions, . \\[\\begin{align} \\sigma_{sa,a^*} = \\left(\\frac{3}{2}\\right)^h \\left(c_h-c_{h+1}\\right) + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a^*} \\rangle\\,. \\label{eq:sigmarec} \\end{align}\\] Define \\((c_h)_{0\\le h\\le H-1}\\) so that . \\[\\begin{align*} \\left(\\frac{3}{2}\\right)^h \\left(c_h-c_{h+1}\\right) =\\frac{5}{8}\\,. \\end{align*}\\] with \\(C_{H-1} = \\frac{1}{2}\\left(\\frac32\\right)^{-H}\\) (i.e., $c_h$ is a decreasing geometric sequence) This has two implications: \\eqref{eq:sigmarec} simplifies to . \\[\\begin{align} \\sigma_{sa,a^*} = \\frac{5}{8} + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a^*} \\rangle\\,, \\label{eq:sigmarec2} \\end{align}\\] and also for the last stage rewards, from \\eqref{eq:laststage} we get . \\[\\begin{align*} r_a(s) = \\frac{1}{3} \\left(\\frac32\\right)^{-H} \\left( \\frac{1}{2} + \\sigma_{sa} \\frac32 \\langle u_a,u_{a^*}\\rangle\\right)\\,. \\end{align*}\\] Clearly, if $\\sigma_{sa}\\in [-4/3,4/3]$, since for \\(a\\ne a^*\\), \\(\\vert \\langle u_a,u_{a^*}\\rangle \\vert \\le 1/4\\), \\(r_a(s)\\in [0,(3/2)^{-H}/3]\\) while also \\(r_{a^*}(s)\\in [0,1]\\). With this, to satisfy \\eqref{eq:cons}, on the one hand we choose to define $\\sigma_{sa}$ with the following “downward recursion” in the tree: For any $s$ in the tree and actions $a,a’$, . \\[\\begin{align} \\sigma_{sa,a'} = \\frac{5}{8} + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a'} \\rangle\\,. \\label{eq:sigmarec3} \\end{align}\\] Note that this is consistent with \\eqref{eq:sigmarec2}. The next challenge is to show that $\\sigma_{sa}$ stays within a constant range. In fact, with the above definition, this will not hold. In particular, when $a=a’$, the right-hand side can be as large as \\(5/8+3/2 \\sigma_{sa} \\ge 3/2 \\sigma_{sa}\\), which means that the scaling coefficients will exponentially increase with a base of $(3/2)$. Note, however, that if $a\\ne a’$, then provided that \\(\\sigma_{sa}\\in [1/4,1]\\) (which can be ensured at the root by choosing \\(\\sigma_{s_0,a}=1\\) for all actions \\(a\\)), . \\[\\frac{1}{4} = \\frac{5}{8} - \\frac{3}{8} \\le \\frac{5}{8} + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a'} \\rangle \\le \\frac{5}{8} + \\frac{3}{8} \\le 1\\,,\\] and thus \\(\\sigma_{sa,a'}\\in [1/4,1]\\) will also hold. Hence, we modify the construction so that the definition \\eqref{eq:sigmarec3} is never needed for $a=a’$. This is achieved by changing the dynamics: We introduce a special set of states, ${e_1,\\dots,e_H}$, the exit lane. Once, the process gets into this lane, there is now return and in fact all the remaining rewards up the end are zero. Specifically, all the actions in $e_h$ lead to state $e_{h+1}$ and we set the feature vector of all states in the exit-lane zero: . \\[\\phi_h(e_h,a) = \\boldsymbol{0}\\,.\\] This way, regardless the choice of the parameter vector, we ensure that the Bellman optimality equations hold at these state and the optimal values are correctly set to zero. The exit lane is introduced to remove the need to use \\eqref{eq:sigmarec3} with repeat actions. In particular, for any \\(s\\in \\mathcal{S}_h\\) with some $h\\ge 1$, say, \\(s=(a_1,\\dots,a_h)\\) (i.e., $s$ is obtained by following these actions) then if for \\(a\\in \\{a_1,\\dots,a_h\\}\\), the next state is $e_{h+1}$. Since the optimal value of $e_{h+1}$ is zero and we don’t intend to introduce an immediate reward, we set . \\[\\phi_h(s,a)=\\boldsymbol{0}\\,,\\] making the value of repeat actions zero. The next complication is that this ruins our plan to keep \\(a^*\\) optimal at all states: Indeed, \\(a^*\\) could be applied multiply times in a path from \\(s_0\\) to a leaf of the tree, and by the second application, the new rule forces the value of \\(a^*\\) to be zero. Hence, we need to modify this rule when the action is \\(a^*\\). Clearly, whether a suboptimal action, or \\(a^*\\) is repeated is problematic for the recursive definition of $\\sigma_{sa}$. Hence, it is better if \\(a^*\\) is also forced to use the exit lane. Thus, if \\(a^*\\) is used in \\(s\\in \\mathcal{S}_h\\) with \\(h\\ge 0\\), the next state is \\(e_{h+1}\\). However, we do not zero out \\(\\sigma_{sa^*}\\), but keep the recursive definition and we rather introduce an immediate reward to match \\(q_h(s,a^*) = \\langle \\phi_h(s,a^*), \\theta^* \\rangle\\). It is not hard to check that this reward is also in the \\([0,1]\\) range. Note that here if \\(s = (a_1,\\dots,a_h)\\) then by definition \\(a^*\\not\\in \\{a_1,\\dots,a_h\\}\\). This completes the description of the structure of the MDPs. That the action gap at \\(s_0\\) is large follows from the choice of the JL feature vectors. It remains to be seen that \\(a^*\\) is indeed the optimal action at any state. This boils down to checking that for \\(a'\\ne a^*\\), \\(q_{h+1}(sa,a^*)-q_{h+1}(sa,a')\\ge 0\\). When \\(a'\\) is a repeat action, this is trivial. When \\(a'\\) is not a repeat action, we have . \\[q_{h+1}(sa,a^*)-q_{h+1}(sa,a') = \\frac{1}{3}\\left(\\frac{3}{2}\\right)^{-h} \\left[ \\sigma_{sa,a^*}-\\sigma_{sa,a'}\\langle u_{a'},u_{a^*}\\rangle \\right] \\ge \\frac{1}{3}\\left(\\frac{3}{2}\\right)^{-h} \\left[ \\frac{1}{4}-\\frac{1}{4} \\right] = 0\\] where we used that \\(\\sigma_{sa,a^*}\\ge 1/4\\) and \\(1/4\\le \\sigma_{sa,a'}\\le 1\\) and thus \\(\\sigma_{sa,a'}\\langle u_{a'},u_{a^*}\\rangle\\ge -\\frac{1}{4}\\) by the choice of \\((u_a)_a\\) and since \\(a\\ne a'\\). Let \\(M_{a^*}\\) denote the MDP constructed this way when the optimal action is \\(a^*\\) (the feature maps, of course, are common between these MDPs). For a formal proof, one also needs to argue that planners that do not use many queries cannot distinguish between these MDPs. Intuitively, this is because such planners will receive, with high probability, identical observations under different MDPs in this class. As such, these planners can at best randomly choose an action (“needle in a haystack”) and since in MDP \\(M_{a}\\) only action \\(a\\) incurs high values, they cannot induce a policy with a near-optimal value. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec10/#the-main-ideas-of-the-proof",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec10/#the-main-ideas-of-the-proof"
  },"61": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Computation with many actions",
    "content": "In the construction given the number of actions was allowed to scale exponentially with the dimension. The above proof would show a separation between the query and computation complexity of planning, if one could demonstrate that there is a choice of the JL feature vectors when the optimization problems . \\[\\begin{align*} \\arg\\max_{a\\in \\mathcal{A}} \\langle \\phi(s,a), \\theta \\rangle \\end{align*}\\] admits a computationally efficient solver regardless of the choice of $\\theta\\in \\mathbb{R}^d$ and $s\\in \\mathcal{S}$ (for simplicity, we suppress dependence on $h$). Whether such a solver exist will depend on the choice of the feature-map and this is a fascinating question on its own. One approach to arrive at such a solver is to rewrite this problem as the problem of finding . \\[\\begin{align} \\arg\\max_{v\\in V_s} \\langle v, \\theta \\rangle \\label{eq:linopt} \\end{align}\\] where $V_s \\subset \\mathbb{R}^d$ is the convex hull of the feature vectors \\(\\{ \\phi(s,a) \\}_{a\\in \\mathcal{A}}\\). Provided that this problem admits an efficient solution and given any extreme point of $v\\in V_s$, we can efficiently recover an action $a\\in \\mathcal{A}$ such that $\\phi(s,a)=v$ (this amounts to “inverting” the feature map), the first problem can also be solved efficiently. Note that \\eqref{eq:linopt} is a linear optimization problem over a convex set $V_s$ and the question whether this problem admits an efficient solver lies at the heart of computer science. The general lesson is that the answer can be expected to be yes when $V_s$ has some “convenient” description other than the one that is used to define it. The second problem of inverting the feature map is known as the “decomposition problem” and the same conclusions hold for this problem. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec10/#computation-with-many-actions",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec10/#computation-with-many-actions"
  },"62": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Notes",
    "content": ". | It is possible to modify the construction to make it work in the discounted setting. The paper cited below shows how. | Back to the finite horizon setting, for an upper bound, one can employ the least-squares value iteration algorithm with $G$-optimal design (LSVI-G), which we have met in Homework 2. What results is that to get a $\\delta$-sound (global) planner with this approach, . | . \\[\\begin{align*} O\\left( \\frac{H^5(2d)^{H+1}}{\\delta^2}\\right) \\end{align*}\\] queries are sufficient (and the compute cost is also of similar order). We see that as far as the exponents in the lower and upper bounds are concerned, in the upper bound the exponent is \\(\\Theta(H \\log_2(d))\\) while in the lower bound it is \\(O(H\\wedge d)\\). Thus, there remains a logarithmic gap between them when $H\\ll d$, while the gap is unbounded when \\(H \\gg d\\), i.e., for long horizon problems. In particular, in the constant dimension and long-horizon featurized planning problem, the LSVI-G algorithm seems to be suboptimal because it calculates the optimal action-value function stage-wise. One conjectures that the upper bound for LSVI-G is tight, while the lower bound in this lecture is also essentially correct. This would means that there is an alternate algorithm that could perform much better than LSVI-G in large-horizon planning with constant feature-dimension. Clearly, for the specific construction used in this lecture, a planner that tries all actions, say at \\(s_0\\), will find the optimal action and the cost of this planner is independent of the horizon. Hence, at least in this case, the lower bound can be matched with an alternate algorithm. One may think that this problem is purely of theoretical interest. To counter this note that long-horizon planning is a really important practical question: Many applications require thousands of steps, if not millions, while perhaps the feature space dimension does not need to be very large. Whether there exist an algorithm that works better than LSVI-G thus remains to be a fascinating open problem with good potential for having a real impact on applications. | For infinite horizon undiscounted problems and \\(v^*\\) realizability, there is a simple example that shows that with \\(\\Theta(d)\\) actions and $d$-dimensional features, any query efficient planner that guarantees a constant suboptimality gap needs \\(\\Omega(2^d/d)\\) queries per state. This is based on a shortest path problem on a regular grid. Here, the obstruction is simply algebraic: There is no noise in either the transitions or the rewards. | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec10/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec10/#notes"
  },"63": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Bibliographical notes",
    "content": "This lecture is entirely based on the paper . | Weisz, Gellert, Philip Amortila, and Csaba Szepesvári. 2020. “Exponential Lower Bounds for Planning in MDPs With Linearly-Realizable Optimal Action-Value Functions.”, | . which is available on arXiv and which will also soon appear at ALT. The second lower for the undiscounted setting mentioned in the notes is from . | Weisz, Gellert, Philip Amortila, Barnabás Janzer, Yasin Abbasi-Yadkori, Nan Jiang, and Csaba Szepesvári. 2021. “On Query-Efficient Planning in MDPs under Linear Realizability of the Optimal State-Value Function.” | . available on arXiv. A beautiful book that is a very good source on reading about the linear optimization problem mentioned above is . | Grotschel, Martin, László Lovász, and Alexander Schrijver. 1993. Geometric Algorithms and Combinatorial Optimization. Vol. 2. Algorithms and Combinatorics. Berlin, Heidelberg: Springer Berlin Heidelberg. | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec10/#bibliographical-notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec10/#bibliographical-notes"
  },"64": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "10. Planning under $q^*$ realizability",
    "content": "PDF Version . The lesson from the last lecture is that efficient planners are limited to induce policies whose suboptimaly gap is polynomially larger than the misspecification error of the feature-map supplied to the planner. We have also seen) that if we accept this polynomial in the feature-space-dimension error amplification, a relatively straightforward adaptation of policy iteration gives rise to a computationally efficient (global) planner – at least, when the planner is furbished with the solution to an underlying optimal experimental design problem. In any case, the planner is query efficient. All this was shown in the context when the misspecification error is relative to the set of action value functions underlying all possible policies. In this lecture we look into whether this error metric could be changed so that the misspecification error is measured by how well the optimal action-value function, $q^*$, is approximated by the features, while still retaining the positive result. As the negative result already implies that there are no efficient planners unless the suboptimality gap of the induced policy is polynomially larger than the approximation error, we look into the case when the optimal action-value function is perfectly representable with the features supplied to the planner. This assumption is also known as “\\(q^*\\)-realizability”, or, “\\(q^*\\) linear realizability”, if we want to be more specific about the nature of the function approximation technique used. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec10/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec10/"
  },"65": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "TensorPlan: An optimistic planner",
    "content": "The planner that is referred to in the previous theorem is called TensorPlan. The reason for this name will become clear after we describe the algorithm. TensorPlan belongs to the class of optimistic algorithms. Since knowing \\(\\theta^*\\), the parameter vector that realizes \\(v^*\\), would be sufficient for acting near-optimally, the algorithm aims to find a good approximation to this vector. A suitable estimate is constructed in a two-step process: . | The algorithm maintains a non-empty “hypothesis” set \\(\\Theta\\subset \\mathbb{R}^d\\), which contains those parameter vectors that are consistent with the data that the algorithm has seen. The details of the construction of this set are at the heart of the algorithm and will come soon. | Given \\(\\Theta\\), an estimate $\\theta^+$ is produced by solving a maximization problem: | . \\[\\begin{align} \\theta^+=\\arg\\max_{\\theta\\in \\Theta} \\phi_0(s_0)^\\top \\theta\\,. \\label{eq:optplanning} \\end{align}\\] Here, \\(s_0\\) is the initial state of the episode, i.e., this is the state the planner is called when $h=0$. Recalling that \\(\\phi_0(s_0)^\\top \\theta^* = v_0^*(s_0)\\), we see that provided that \\(\\theta^*\\in \\Theta\\), . \\[v_0(s_0;\\theta^+)\\ge v_0^*(s_0)\\,,\\] where, for convenience, we introduce \\(v_h(s;\\theta) = \\phi_h(s)^\\top \\theta\\). When $\\theta^+$ is close enough to \\(\\theta^*\\), one hopes that the policy induced by \\(\\theta^+\\) will be near-optimal. Hence, the approach is to “roll out” with the induced policy (using the simulator) and verify whether during the rollout the data received is consistent with the Bellman equation, and as a result of this, also whether the episode return observed is close to \\(v_0(s_0;\\theta^+)\\). When a contradiction to any of these is detected, the data can be used to shrink the set \\(\\Theta\\) of consistent parameter vectors. The approach described leaves open the question of what we mean by a policy “induced” by \\(\\theta^+\\). The naive approach is to base this on the Bellman optimality equation, which states that . \\[\\begin{align} v_h^*(s) = \\max_a r_a(s)+ \\langle P_a(s), v_{h+1}^* \\rangle \\label{eq:fhboe} \\end{align}\\] holds for $h=0,1,\\dots,H-1$ with \\(v_H^* = \\boldsymbol{0}\\). If \\(\\theta^+= \\theta^*\\), \\(v_h(\\cdot;\\theta^+)\\) will also satisfy this equation and thus one might define the policy induced by \\(\\theta^+\\) that achieves the maximum above when \\(v_{h+1}^*\\) is replaced by \\(v_{h+1}(\\cdot;\\theta^+)\\). Consistency of \\(\\theta^+\\) would also mean checking whether \\eqref{eq:fhboe} holds (approximately) when \\(v^*_{\\cdot}(\\cdot)\\) is replaced in this equation by \\(v_{\\cdot}(\\cdot;\\theta^+)\\), which, one may imagine can be checked by generating data from the simulator. While this may approach work, it is not easy to see whether it does. (It is open problem whether this works!) TensorPlan defines induced policies and consistency slightly differently. The changed definition allows not only for proving that TensorPlan is query-efficient, but it even makes the guarantees for TensorPlan stronger than what was announced above in the theorem. What makes the analysis of the algorithm that is based on the Bellmean optimality equation difficult is the presence of the maximum in this equation. Hence, TensorPlan removes this maximum. Accordingly, the policy induced by $\\theta^+$ is defined as any policy $\\pi_{\\theta^+}$ which in state $s$ and stage $h$ chooses any action $a\\in \\mathcal{A}$ which ensures that . \\[\\begin{align} v_h(s;\\theta^+) = r_a(s)+ \\langle P_a(s), v_{h+1}(\\cdot;\\theta^+) \\rangle\\,. \\label{eq:tpcons} \\end{align}\\] If there is no such action, \\(\\pi_{\\theta}\\) is free to choose any action. We say that local consistency holds at $(s,h,\\theta^+)$ when there exists an action $a\\in \\mathcal{A}$ such that \\eqref{eq:tpcons} holds. If there are multiple actions that satisfy \\eqref{eq:tpcons}, any of them will do: Choosing the maximizing action is not enforced. However, when \\(v^*\\) is realizable and \\(\\theta^+=\\theta^*\\), any action that satisfies \\eqref{eq:tpcons} will be a maximizing action and the policy induced will be optimal. The advantage of the relaxed notion of induced policy is that with this choice, TensorPlan will also be able to compete with any deterministic policy whose value-function is realizable. This expands the scope of TensorPlan: Perhaps the optimal value function is not realizable with the features handed to TensorPlan, but if there is any deterministic policy whose value-function is realizable with them, then TensorPlan will be guaranteed to produce almost as much as reward as that policy. In fact, it will produce nearly as much reward as the policy that achieves the best value. TensorPlan . To summarize, after generating a hypothesis \\(\\theta^+\\), TensorPlan will run a number of rollouts using the simulator so that for each state $s$ encountered TensorPlan first finds an action $a$ satisfying \\eqref{eq:tpcons}. If this succeeds, the rollout continues by TensorPlan getting a next state from the simulator at $(s,a,h)$ and $h$ is incremented. This continues up to $h=H$, which ends a rollout. TensorPlan will run \\(m\\) rollouts of this type and if all of them succeeds, TensorPlan stops and will use the parameter vector \\(\\theta^+\\) in the rest of the episode and the same policy \\(\\pi_{\\theta^+}\\) as used during the rollouts. If during the rollouts an inconsistency is detected, TensorPlan will decrease the hypothesis set \\(\\Theta\\) and continue with a next experiment. It remains to be seen why TensorPlan (1) stops with a bounded number of queries and (2) why it is sound. Boundedness . We start with boundedness. This is where the change of how policies are induced by parameters is used in a critical manner. Introduce the discriminants: . \\[\\begin{align*} \\Delta(s,a,h,\\theta) = r_a(s) = \\langle P_a(s)\\phi_{h+1},\\theta \\rangle - \\phi_h(s)^\\top \\theta\\,. \\end{align*}\\] Note that \\(\\Delta(s,a,h,\\theta)\\) is just the difference between the right-hand and the left-hand side of \\eqref{eq:tpcons}, where we plugged in the definition $v_h$ and $v_{h+1}$ and we define . \\[P_a(s)\\phi_{h+1} = \\sum_{s'\\in \\mathcal{S}} P_a(s,s') \\phi_{h+1}(s')\\,;\\] thus \\(P_a(s)\\phi_{h+1}\\) is the “expected next feature vector” given $(s,a)$. Then, by definition, local consistency holds for $(s,h,\\theta)$ if and only if there exists some action $a\\in \\mathcal{A}$ such that $\\Delta(s,a,h,\\theta)=0$. Exploiting that the product of numbers is zero if and only if some of them is zero, we see that local consistency is equivalent to . \\[\\begin{align} \\prod_{a\\in \\mathcal{A}} \\Delta(s,a,h,\\theta) = 0\\,. \\label{eq:diprod} \\end{align}\\] The reason this purely algebraic reformulation of local consistency is helpful is because the product of the discriminants can be see as a linear function of the $A$-fold tensor product of \\((1,\\theta^\\top)^\\top\\). To see why this holds, it will be useful to introduce some extra notation: For a real $r$ and a finite-dimensional vector $u$, we will denote by \\(\\overline{ r u}\\) the vector \\((r,u^\\top)^\\top\\) (i.e., adding $r$ to the first position and shifting down all other entries in $u$). With this notation, we can write the discriminants as an inner product: . \\[\\begin{align*} \\Delta(s,a,h,\\theta) = \\langle \\overline{r_a(s)\\, (P_a(s)\\phi_{h+1}-\\phi_h(s))}, \\overline{1 \\, \\theta} \\rangle \\end{align*}\\] Now, recall that the tensor product \\(\\otimes\\) of vectors satisfies the following property: . \\[\\begin{align*} \\prod_a \\langle x_a, y_a \\rangle = \\langle \\otimes_a x_a, \\otimes_a y_a \\rangle\\,, \\end{align*}\\] where the inner product between two tensors is defined in the usual way, by overlaying them and then taking the sum of the products of the entries that are on the top of each other. Based on this identity, we see that \\eqref{eq:diprod}, and thus local consistency, is equivalent to . \\[\\begin{align*} \\langle \\underbrace{\\otimes_a \\overline{r_a(s)\\, (P_a(s)\\phi_{h+1}-\\phi_h(s))}}_{D(s,h)}, \\underbrace{\\otimes_a \\overline{1 \\, \\theta}}_{F(\\theta)} \\rangle = 0\\,. \\end{align*}\\] Note that while $F(\\theta)\\in \\mathbb{R}^{(d+1)^A}$ is a nonlinear function of \\(\\theta\\), the above equation is linear in \\(F(\\theta)\\). Imagine for a moment that the data \\(D(s,h)\\) above can be obtained with no errors and assume that \\(v^*\\) is realizable. Let $k = (d+1)^A$. We can think of both \\(D(s,h)\\) and \\(F(\\theta)\\) taking values in \\(\\mathbb{R}^k\\) (this corresponds to “flattening” these tensors). TensorPlan can be seen as an algorithm that generates a sequence $(\\theta_1,x_1), (\\theta_2,x_2), \\dots$ such that \\(\\theta_i\\in \\mathbb{R}^d\\) is the \\(i\\)th hypothesis that TensorPlan chooses, \\(x_i\\in \\mathbb{R}^k\\) is the \\(i\\)th data of the form \\(D(s,h)\\) with some \\((s,h)\\) where TensorPlan detects an inconsistency. When inconsistency is detected, the hypothesis set is shrunk: . \\[\\Theta_{i+1} = \\Theta_i \\cap \\{ \\theta\\,:\\, F(\\theta)^\\top x_i=0 \\},\\] and \\(\\theta_{i+1}\\) is chosen in \\(\\Theta_{i+1}\\) by \\eqref{eq:optplanning}. Together with \\(\\Theta_1 = B_2^d(B)\\) (the \\(\\ell^2\\) ball of radius \\(B\\) in \\(\\mathbb{R}^d\\)), we have that for $i&gt;1$, . \\[\\Theta_i = \\{ \\theta\\in B_2^d(B)\\,:\\, F(\\theta)^\\top x_1 = 0, \\dots, F(\\theta)^\\top x_{i-1}=0 \\}.\\] Let \\(f_i = F(\\theta_i)\\). By its construction, for any \\(i\\ge 1\\), \\(\\theta_i\\in \\Theta_i\\) and hence \\(f_i\\) is orthogonal to \\(x_1,\\dots,x_{i-1}\\). Also by its construction, \\(x_i\\) is not orthogonal to \\(f_i\\). Because of this, \\(x_i\\) cannot lie in the span of \\(x_1,\\dots,x_{i-1}\\) (if it did, it would be orthogonal to \\(f_i\\)). Hence, the vectors \\(x_1,x_2,\\dots\\) are linearly independent. As there are at most \\(k\\) linearly independent vectors in \\(\\mathbb{R}^k\\), Tensorplan will generate at most \\(k\\) of these data vectors (in fact, for TensorPlan, this is \\(k-1\\), can you explain why?). This means that after at most \\(k\\) “contradictions” to local consistency, TensorPlan will cease to detect more inconsistencies and thus it will stop. Soundness . It remains to be seen that TensorPlan is sound. Let \\(\\theta^+\\) be the parameter vector that TensorPlan generated when it stops. This means that during the \\(m\\) rollouts, TensorPlan did not detect any inconsistencies. Take a trajectory \\(S_0^{(i)},A_0^{(i)},\\dots,S_{H-1}^{(i)},A_{H-1}^{(i)},S_H^{(i)}\\) generated during the \\(i\\)th rollout of \\(m\\) rollouts. Since there is no inconsistency along it, for any \\(0\\le t \\le H-1\\) we have . \\[\\begin{align} r_{A_t^{(i)}}(S_t^{(i)}) = v_t(S_t^{(i)};\\theta^+)-\\langle P_{A_t^{(i)}}(S_t^{(i)}), v_{t+1}(\\cdot;\\theta^+) \\rangle\\,. \\label{eq:constr} \\end{align}\\] Hence, with probability \\(1-\\zeta\\), . \\[\\begin{align*} v_0^{\\pi_{\\theta^+}}(s_0) &amp; \\ge \\frac1m \\sum_{i=1}^m\\sum_{t=0}^{t-1} r_{A_t^{(i)}}(S_t^{(i)}) - H \\sqrt{ \\frac{\\log(1/\\zeta)}{2m}} \\\\ &amp; = \\frac1m \\sum_{i=1}^m\\sum_{t=0}^{t-1} v_t(S_t^{(i)};\\theta^+)-\\langle P_{A_t^{(i)}}(S_t^{(i)}), v_{t+1}(\\cdot;\\theta^+)\\rangle - H \\sqrt{ \\frac{\\log(2/\\zeta)}{2m}} \\\\ &amp; \\ge \\frac1m \\sum_{i=1}^m\\sum_{t=0}^{t-1} v_t(S_t^{(i)};\\theta^+)- v_{t+1}(S_{t+1}^{(i)};\\theta^+) - (H+2B) \\sqrt{ \\frac{\\log(2/\\zeta)}{2m}} \\\\ &amp; = v_0(s_0;\\theta^+) - (H+2B) \\sqrt{ \\frac{\\log(2/\\zeta)}{2m}}\\,, \\end{align*}\\] where the first inequality is by Hoeffding’s inequality and uses that rewards are bounded in \\([0,1]\\), the equality after it uses \\eqref{eq:constr}, the second inequality is again by Hoeffding’s inequality and uses that . \\[\\begin{align*} \\langle P_{A_t^{(i)}} (S_t^{(i)}), v_{t+1}(\\cdot;\\theta^+)\\rangle = \\mathbb{E} [ v_{t+1}(S_{t+1}^{(i)};\\theta^+) | S_t^{(i)},A_t^{(i)}] \\end{align*}\\] and that \\(v_t\\) is bounded between \\([-B,B]\\) (note that we could truncate \\(v_t\\) to \\([0,H]\\) to replace \\(H+2B\\) above by \\(2H\\)), while the last equality uses that \\(v_H(\\cdot;\\theta^+)=\\boldsymbol{0}\\) by definition and that \\(S_0^{(i)}=s_0\\) by definition. Setting \\(m\\) high enough (\\(m=\\tilde O((H+B)^2/\\delta^2)\\)) we can guarantee . \\[v_0^{\\pi_{\\theta^+}}(s_0) \\ge v_0(s_0;\\theta^+)-\\delta.\\] We now argue that this implies soundness. Letting \\(\\Theta^\\circ \\subset B_2^d(B)\\) be the set of \\(B\\)-bounded parameter vectors \\(\\theta\\) such that for some deterministic policy \\(\\pi\\), \\(v^\\pi = \\Phi \\theta\\). By the definition of \\(D\\) and \\(F\\), for any \\(i\\ge 1\\), \\(\\Theta^\\circ \\subset \\Theta_{i}\\) (no correct hypothesis is ever eliminated). It also follows that at any stage of the process, . \\[v_0(s_0;\\theta^+)\\ge \\max_{\\theta\\in \\Theta^\\circ} v^{\\pi_{\\theta}}_0(s_0).\\] Hence, when TensorPlan stops with parameter \\(\\theta^+\\), with high probability, . \\[v^{\\pi_{\\theta^+}}_0(s_0)\\ge v_0(s_0;\\theta^+)-\\delta \\ge \\max_{\\theta\\in \\Theta^\\circ} v^{\\pi_{\\theta}}_0(s_0)-\\delta\\,.\\] In particular, if \\(v^*\\) is \\(B\\)-realizable, \\(v^{\\pi_{\\theta^+}}_0(s_0) \\ge v^*_0(s_0)-\\delta\\). Thus, after stopping, for the rest of the episode, TensorPlan can safely use the policy induced by \\(\\theta^+\\). Summary . So far we have seen that if somehow TensorPlan would be able to get \\(\\Delta(s,a,h,\\theta)\\) with no errors, (1) it would stop after refining its hypothesis set at most \\(k\\) times and (2) when it stops, with high probability it would return with a parameter vector that induces a policy with high value. Regarding the number of queries used, if obtaining \\(\\Delta(s,a,h,\\theta)\\) is counted as a single query, TensorPlan would need at most \\(maH k =maH (d+1)^A\\) queries (\\(m\\) rollouts, for each of the \\(H\\) states in the rollout, \\(A\\) queries are needed). It remains to be seen how to adjust this argument to the case when \\(\\Delta(s,a,h,\\theta)\\) need to be estimated based on interactions with a stochastic simulator. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec11/#tensorplan-an-optimistic-planner",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec11/#tensorplan-an-optimistic-planner"
  },"66": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "Notes",
    "content": ". | It is not known whether TensorPlan can be computationally efficiently implemented. I suspect it cannot. This is because \\(\\Theta_i\\) is specified with a number of highly nonlinear constraints (in the parameter vector). | The essence of the construction here is lifting the problem into a higher-dimensional linear space. This is a standard technique in machine learning but in a very different context when data is mapped to a higher dimensional space to strengthen the power of linear predictors. The once popular RKHS methods take this to the extreme. Note that here, in contrast to this classic lifting procedure, the parameter vector is mapped through a nonlinear function to a higher dimensional space and the purpose is to simply have a clear grasp on why learning stops. | We call \\(\\Delta\\) here the discriminant function because what is important about it is that it discriminates between “good” and “bad” cases and it does it by using the special value of zero. Readers familiar with the RL literature will note, however, that \\(\\Delta\\) is nothing but, what is known as the “temporal difference error” (under some fixed action). | It is curious that the algorithm builds up a data-bank of critical data that it uses to restrain the set of parameter vectors and that it is quite selective in adding new data here. That is, TensorPlan may generate a lot more data then goes on the list $x_1,x_2,\\dots$. If we wanted to be philosophical and would not mind antropomorphising algorithms, we could say that TensorPlan remembers what it is “surprised by”. This is very much unlike other algorithms, like LSVI-$G$, which may generate a lot of redundant data. The other difference is that TensorPlan uses the data to generate a hypothesis set. The choice of the parameter vector from this set is dictated by the optimization (reward maximization) problem solved by TensorPlan. | There are quite a few examples of optimistic algorithms in planning; there is a considerable literature of using optimisim in tree search. However, classics, such as the \\(A^*\\) algorithm can also be seen as an optimistic algorithm (at least when used with an “admissible heuristic”, which is just a way of saying that \\(A^*\\) uses an optimistic estimate of the values). The \\(LAO^*\\) algorithm is another example. However, the real “homeland” of optimistic algorithms in online learning, a topic that will be covered later in the course. | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec11/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec11/#notes"
  },"67": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "Bibliographical notes",
    "content": "This lecture is entirely based on the paper . | Weisz, Gellert, Philip Amortila, Barnabás Janzer, Yasin Abbasi-Yadkori, Nan Jiang, and Csaba Szepesvári. 2021. “On Query-Efficient Planning in MDPs under Linear Realizability of the Optimal State-Value Function.” | . available on arXiv. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec11/#bibliographical-notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec11/#bibliographical-notes"
  },"68": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "content": "PDF Version . In the last lecture we saw that under \\(q^*\\) linear realizability, query-efficient fixed-horizon online planning with a constant suboptimality gap is intractable provided that there is no limit on the number of actions. In particular, the MDPs that were used to show intractability use $e^{\\Theta(d)}$ actions, where $d$ is the dimension of the feature-map that realizes the optimal action-value function. At the end of the lecture, we also noted that intractabality also holds for undiscounted infinite horizon problems under \\(v^*\\) linear realizability in the regime when the number of actions scales linearly with $d$. In this lecture we further dissect \\(v^*\\) realizability, but return to the fixed horizon setting and we will consider the case when the number of actions is fixed. As it turns out, in this case, query-efficient online planning is possible. Before giving the details of this result, we need to firm up some and refine other definitions. First, \\(v^*\\) realizability under a feature map \\(\\phi=(\\phi_h)_{0\\le h \\le H-1}\\) in the $H$-horizon setting means that . \\[\\begin{align} \\inf_{\\theta\\in \\mathbb{R}^d} \\max_{0\\le h \\le H-1}\\|\\Phi_h \\theta - v^*_{h} \\|_\\infty = 0\\,, \\label{eq:vsrealizability} \\end{align}\\] where \\(v^*_h\\) is the optimal-value function when \\(H-h\\) steps are left (in particular, \\(v^*_H=\\boldsymbol{0}\\)). Again, this uses the indexing introduced in the previous lecture. In what follows, without the loss of generality we assume that the feature map is such that all the feature-vectors lie within the a ($2$-norm) ball of radius one. When realizability holds with a parameter vector bounded in $2$-norm by $B$, we say that \\(v^*\\) is $B$-realizable under the feature map $\\phi$. We also slightly modify the interaction protocol between the planner and the simulator, as shown on the figure below. The main new features are introducing stages, and restricting the planners to access states and features only through local calls to the simulator. Illustration of the interaction protocol between the planner and the simulator. Because in fixed-horizon problems the stage index influences what actions should be taken, the planner is called with an initial state \\(s_0\\) and a stage index \\(h\\). For defining the policy induced the planner, it is assumed that the planner is first called with \\(h=0\\) at some state, then it is called with \\(h=1\\) with a state obtained following a transition by taking the action returned by the planner, etc. While interacting with the simulator, the planner is restricted to use only states that it has encountered before. Also, the planner can feed a stage index to the simulator, to get the features of the next state corresponding to the incremented input stage index. There is no other access to the features. Note also that just like in the previous lecture, we allow the MDPs to generate random rewards. In this setting a \\(\\delta\\)-sound planner is one which, under the above protocol, induces a policy of the MDP whose simulator it interacts with which is at most \\(\\delta\\)-suboptimal. Theorem (query-efficient planning under \\(v^*\\)-realizability): For any integers $A,H&gt;0$ and reals $B,\\delta&gt;0$, there exists an online planner $\\mathcal{P}$ with the following properties: . | The planner $\\mathcal{P}$ is \\(\\delta\\)-sound for the $H$-horizon planning problem and the class of MDP-feature-map pairs $(M,\\phi)$ such that $v^*$ is $B$-realizable under $\\phi$ and $M$ has at most $A$ actions and its rewards are bounded in $[0,1]$; | The number of queries used by the planner in each of its call is at most | . \\[\\begin{align*} \\text{poly}\\left( \\left(\\frac{dH}{\\delta}\\right)^A, B \\right) \\end{align*}\\] . Note that for $A&gt;0$ fixed the query-cost is polynomial in $d,H,1/\\delta$ and $B$. It remains to be seen whether this bound can be improved. However, this is somewhat of a theoretical question as under \\(v^*\\)-realizability, even if the coefficients $\\theta\\in \\mathbb{R}^d$ that realize \\(v^*\\) are known, in the lack of extra information, one needs to perform \\(\\Theta(A)\\) simulation calls to be able to get good approximations to the action-value function \\(q^*\\), which seems necessary for inducing a good policy. Hence, the query cost must scale at least linearly with \\(A\\), hence, no algorithm is expected to be even query-efficient when the number of actions is large. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec11/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec11/"
  },"69": {
    "doc": "12. TensorPlan and eluder sequences",
    "title": "12. TensorPlan and eluder sequences",
    "content": "PDF Version . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec12/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec12/"
  },"70": {
    "doc": "13. From API to Politex",
    "title": "Politex",
    "content": "Politex comes from Policy Iteration with Expert Advice. Assume that one is given a featurized MDP \\((M,\\phi)\\) with state-action feature-map \\(\\phi\\) and access to a simulator, and a $G$-optimal design \\(\\mathcal{C}\\subset \\mathcal{S}\\times\\mathcal{A}\\) for \\(\\phi\\). Politex generates a sequence of policies \\(\\pi_0,\\pi_1,\\dots\\) such that for \\(k\\ge 1\\), . \\[\\pi_k(a|s) \\propto \\exp\\left( \\eta \\bar q_{k-1}(s,a)\\right)\\,,\\] where . \\[\\bar q_{k} = \\hat q_0 + \\dots + \\hat q_j,\\] with . \\[\\hat q_j = \\Pi \\Phi \\hat \\theta_j,\\] where for \\(j\\ge 0\\), \\(\\hat\\theta_j\\) is the parameter vector obtained by running the least-squares policy evaluation algorithm based on G-optimal design (LSPE-G) to evaluate policy \\(\\pi_j\\) (see this lecture). In particular, recall that this algorithm rolls out policy \\(\\pi_j\\) from the points of a G-optimal design to produce \\(m\\) independent trajectories of length \\(H\\) each, calculates the average return for each of these design points and then solves the (weighted) least-squares regression problem where the features are used to regress on the obtained values. Above, \\(\\Pi : \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}}\\) truncates its argument to the \\([0,1/(1-\\gamma)]\\) interval: . \\[(\\Pi q)(s,a) = \\max(\\min( q(s,a), 1/(1-\\gamma)), 0), \\qquad (s,a) \\in \\mathcal{S}\\times \\mathcal{A}\\,.\\] Note that to calculate \\(\\pi_k(a\\vert s)\\), one does need to calculate \\(E_k(s,a)=\\exp\\left( \\eta \\Pi [ \\phi(s,a)^\\top \\bar \\theta_{k-1} ] \\right)\\) and then compute \\(\\pi_k(a\\vert s) = E_k(s,a)/\\sum_{a'} E_k(s,a')\\). Unlike in policy iteration, the policy returned by Politex after $k$ iterations is either the “mixture policy” . \\[\\bar \\pi_k = \\frac{1}{k} (\\pi_0+\\dots+\\pi_{k-1})\\,,\\] or the policy which gives the best value with respect to the start state, or start distribution. For simplicity, let us just consider the case when $\\bar \\pi_k$ is used as the output. The meaning of a mixture policy is simply that one of the $k$ policies is selected uniformly at random and then the selected policy is followed for the rest of time. Homework 3 gives precise definitions and asks you to prove that the value function of $\\bar \\pi_k$ is just the mean of the value functions of the constituent policies: . \\[\\begin{align} v^{\\bar \\pi_k} = \\frac{1}{k} \\left(v^{\\pi_0}+\\dots+v^{\\pi_{k-1}}\\right)\\,. \\label{eq:avgpol} \\end{align}\\] We now argue that the dependence on the approximation error of the suboptimality gap of $\\bar \\pi_k$ only scales with $1/(1-\\gamma)$, unlike the case of approximate policy iteration. For this, recall that by the value difference identity . \\[v^{\\pi^*} - v^{\\pi_j} = (I-\\gamma P_{\\pi^*})^{-1} \\left[T_{\\pi^*} v^{\\pi_j} - v^{\\pi_j} \\right]\\,.\\] Summing up, dividing by $k$, and using \\eqref{eq:avgpol} gives . \\[v^{\\pi^*} - v^{\\bar \\pi_k} = \\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\sum_{j=0}^{k-1} T_{\\pi^*} v^{\\pi_j} - v^{\\pi_j}\\,.\\] Now, \\(T_{\\pi^*} v^{\\pi_j} = M_{\\pi^*} (r+\\gamma P v^{\\pi_j}) = M_{\\pi^*} q^{\\pi_j}\\). Also, \\(v^{\\pi_j} = M_{\\pi_j} q^{\\pi_j}\\). Let \\(\\hat q_j = \\Pi \\Phi \\hat \\theta_j\\). Elementary algebra then gives . \\[\\begin{align*} v^{\\pi^*} - v^{\\bar \\pi_k} &amp; = \\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\sum_{j=0}^{k-1} M_{\\pi^*} q^{\\pi_j} - M_{\\pi_j} q^{\\pi_j}\\\\ &amp; = \\frac1k(I-\\gamma P_{\\pi^*})^{-1} \\underbrace{ \\sum_{j=0}^{k-1} M_{\\pi^*} \\hat q_j - M_{\\pi_j} \\hat q_j}_{T_1} + \\underbrace{\\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\sum_{j=0}^{k-1} ( M_{\\pi^*} - M_{\\pi_j} )( q^{\\pi_j}-\\hat q_j)}_{T_2} \\,. \\end{align*}\\] We see that the approximation errors $\\varepsilon_j = q^{\\pi_j}-\\hat q_j$ appear only in term $T_2$. In particular, taking pointwise absolute values, using the triangle inequality, we get that . \\[\\|T_2\\|_\\infty \\le \\frac{2}{1-\\gamma} \\max_{0\\le j \\le k-1}\\| \\varepsilon_j\\|_\\infty\\,,\\] which shows the promised dependence. It remains to show that \\(\\|T_1\\|_\\infty\\) above is also under control. However, this is left to the next lecture. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec13/#politex",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec13/#politex"
  },"71": {
    "doc": "13. From API to Politex",
    "title": "Notes",
    "content": "State aggregation and extrapolation friendliness . The $\\sqrt{d}$ in our results comes from controlling the extrapolation errors of linear prediction. In the case of state-aggregretion, however, this extra \\(\\sqrt{d}\\) error amplification is completely avoided: Clearly, if we measure a function with a precision \\(\\varepsilon\\) and there is at least one measurement per part, then by using the value measured at each part (at an arbitrary state there) over the whole part, the worst-case error is bounded by \\(\\varepsilon\\). Weighted least-squares in this context just takes the weighted average of the responses over each part and uses this as the prediction, so it also avoids amplifying approximation errors. In this case, our analysis of extrapolation errors is clearly conservative. The extrapolation error was controlled in two steps: In our first lemma, for \\(\\rho\\) weighted least-squares we reduced this problem to that of controlling \\(g(\\rho)=\\max_{z\\in \\mathcal{Z}} \\| \\phi(z) \\|_{G_{\\rho}^{-1}}\\) where \\(G_{\\rho}\\) is the moment matrix for \\(\\rho\\). In fact, the proof of this lemma is the culprit: By carefully inspecting the proof, we can see that the application of Jensen’s inequality introduces an unnecessary term: For the case of state aggregation (orthonormed feature matrix), . \\[\\sum_{z' \\in C} \\varrho(z') |\\phi(z')^\\top G_\\varrho^{-1} \\phi(z')| = 1\\,\\] as long as the design \\(\\rho\\) is such that it chooses any group exactly once. Thus, the case of state-aggregation shows that some feature-maps are more extrapolation friendly than others. Also, note that the Kiefer-Wolfowitz theorem, of course, still gives that \\(\\sqrt{d}\\) is the smallest value that we can get for \\(g\\) when optimizing for \\(\\rho\\). It is a fascinating question of how extrapolation errors behave for various feature-maps. Least-squares value iteration (LSVI) . In homework 2, Question 3 was concerned with least-squares value iteration. The algorithm concerned (call it LSVI-G) uses a random approximation of the Bellman operator, based on a G-optimal design (and action-value functions). The problem was to show a result similar to what holds for LSPI-G holds for LSVI-G, as well. That is, for any MDP feature-map pair $(M,\\phi)$ and any $\\varepsilon’&gt;0$ excess suboptimality target, with a total runtime of . \\[\\text{poly}\\left( d, \\frac{1}{1-\\gamma}, A, \\frac{1}{\\varepsilon'} \\right)\\,,\\] least-squares policy iteration with $G$-optimal design (LSPI-G) can produce a policy $\\pi$ such that the suboptimality gap $\\delta$ of $\\pi$ satisfies . \\[\\begin{align} \\delta \\le \\frac{4(1+\\sqrt{d})}{(1-\\gamma)^{\\color{red} 2}} \\varepsilon_\\text{BOO} + \\varepsilon'\\,. \\label{eq:lsviup} \\end{align}\\] Thus, the dependence on the horizon of the approximation error is similar to the one that was obtained for LSPI. Note that the definition of \\(\\varepsilon_\\text{BOO}\\) is different from what we have used in analyzing LSPI: . \\[\\varepsilon_{\\text{BOO}} := \\sup_{\\theta}\\inf_{\\theta'} \\| \\Phi \\theta' - T \\Pi \\Phi \\theta \\|_\\infty\\,.\\] Above, \\(T\\) is the Bellman optimality oerator for action-value functions and $\\Pi$ is defined so that for \\(f:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}\\), \\(\\Pi f\\) is also a $\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$ function which is obtained from $f$ by truncating for each input $(s,a)$ the value $f(s,a)$ to $[0,1/(1-\\gamma)]$: $(\\Pi(f))(s,a) = \\max(\\min( f(s,a), 1/(1-\\gamma) ), 0)$. In $\\varepsilon_{\\text{BOO}}$, “BOO” stands for “Bellman-optimality operator” in reference to the appearance of $T$ in the definition. In general, the error measures \\(\\varepsilon\\) used in LSPI and \\(\\varepsilon_{\\text{BOO}}\\) are incomparable. The latter quantity measures a “one-step error”, while \\(\\varepsilon\\) is concerned with approximating functions defined over an infinite-horizon. Linear MDPs . Call an MDP linear if both the reward function and the next state distributions for each state lie in the span of the features: \\(r = \\Phi \\theta_r\\) with some $\\theta_r\\in \\mathbb{R}^d$ and $P$, as an $\\mathrm{S}\\mathrm{A}\\times \\mathrm{S}$ matrix takes the form \\(P = \\Phi W\\) with some \\(W\\in \\mathbb{R}^{d\\times \\mathrm{S}}\\). Clearly, this is a notion that captures how well the “dynamics” (including the reward) of the MDP can be “compressed”. When an MDP is linear, \\(\\varepsilon_{\\text{BOO}}=0\\). We also have in this case that $\\varepsilon=0$. More generally, defining \\(\\zeta_r = \\inf_{\\theta}\\| \\Phi \\theta_r - r \\|_\\infty\\) and \\(\\zeta_P=\\inf_W \\|\\Phi W - P \\|_\\infty\\), it is not hard to see that \\(\\varepsilon_{\\text{BOO}}\\le \\zeta_r + \\gamma \\zeta_P/(1-\\gamma)\\) and \\(\\varepsilon\\le \\zeta_r + \\gamma \\zeta_P/(1-\\gamma)\\), which shows that both policy iteration (and its soft versions) and value iteration are “valid” approaches, though, by ignoring the fact that we are comparing upper bounds, this also shows that value iteration may have an edge over policy iteration when the MDP itself is compressible. This should not be too surprising given that value-iteration is “more direct” in aiming to calculate \\(q^*\\). Yet, they may exist cases when the action-value functions are compressible, while the dynamics is not. Stationary points of a policy search objective . Let \\(J(\\pi) = \\mu v^\\pi\\). A stationary point of \\(J\\) with respect to some set of memoryless policies \\(\\Pi\\) is any \\(\\pi\\in \\Pi\\) such that . \\[\\langle \\nabla J(\\pi), \\pi'- \\pi \\rangle \\le 0\\,.\\] It is known that if $\\phi$ are state-aggregation features then any stationary point \\(\\pi\\) of \\(J\\) satisfies . \\[\\mu v^\\pi \\ge \\mu v^* - \\frac{4\\varepsilon_{\\text{apx}}}{1-\\gamma}\\,,\\] where $\\varepsilon_{\\text{apx}}$ is defines as the worst-case error of approximation action-value functions of $\\phi$-measurable policies with the features (the same constant as used in the analysis of approximate policy iteration). Soft-policy iteration with Averaging . Politex can be seen as a “soft” version of policy iteration with averaging. The softness is controlled by $\\eta$: When $\\eta\\to \\infty$, Politex uses a greedy policy w.r.t. to an average of all previous \\(Q\\)-functions. Notice that in this case if Politex were to use a greedy policy w.r.t. the last \\(Q\\)-function, then it would reduce exactly to LSPI-G. As we have seen, in LSPI-G the approximation error can get quadratically amplified with the horizon $1/(1-\\gamma)$. Thus, one way to avoid this quadratic amplification is to stay soft with averaging. As we shall see in the next lecture, the price of this is a relatively slower convergence to a target suboptimality excess value. Nevertheless, the promise is that the algorithm will still stay polynomial in all the relevant quantities. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec13/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec13/#notes"
  },"72": {
    "doc": "13. From API to Politex",
    "title": "References",
    "content": "The LSPI lower bound idea came from this paper . | Russo D. Approximation benefits of policy gradient methods with aggregated states. arXiv preprint arXiv:2007.11684. 2020 Jul 22. pdf | . Here we have presented a simpler version of the construction which still obtains the same lower bound gaurantees. The simpler construction was inspired by Tadashi Kozuno when he taugh this lecture in Winter 2022. Politex was introduced in the paper . | POLITEX: Regret Bounds for Policy Iteration using Expert Prediction. Abbasi-Yadkori, Y.; Bartlett, P.; Bhatia, K.; Lazic, N.; Szepesvári, C.; and Weisz, G. In ICML, pages 3692–3702, May 2019. pdf | . However, as this paper also notes, the basic idea goes back to the MDP-E algorithm by Even-Dar et al: . | Even-Dar, E., Kakade, S. M., and Mansour, Y. Online Markov decision processes. Mathematics of Operations Research, 34(3):726–736, 2009. | . This algorithm considered a tabular MDP with nonstationary rewards – a completely different setting. Nevertheless, this paper introduces the basic argument presented above. The Politex paper notices that the argument can be extended to the case of function approximation. In particular, it also notes the nature of the function approximator is irrelevant as long as the approximation and estimation errors can be tightly controlled. The Politex paper presented an analysis for online RL and average reward MDPs. Both add significant complications. The argument shown here is therefore a simpler version. Connecting Politex to LSPE-G in the discounted setting is trivial, but has not been presented before in the literature. The first paper to use the error decomposition shown here together with function approximation is . | Abbasi-Yadkori, Y., Lazic, N., and Szepesvári, C. Modelfree linear quadratic control via reduction to expert prediction. In AISTATS, 2019. | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec13/#references",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec13/#references"
  },"73": {
    "doc": "13. From API to Politex",
    "title": "13. From API to Politex",
    "content": "PDF Version . In the lecture on approximate policy iteration, we proved that for any MDP feature-map pair $(M,\\phi)$ and any $\\varepsilon’&gt;0$ excess suboptimality target, with a total runtime of . \\[\\text{poly}\\left( d, \\frac{1}{1-\\gamma}, A, \\frac{1}{\\varepsilon'} \\right)\\,,\\] least-squares policy iteration with $G$-optimal design (LSPI-G) can produce a policy $\\pi$ such that the suboptimality gap $\\delta$ of $\\pi$ satisfies . \\[\\begin{align} \\delta \\le \\frac{2(1+\\sqrt{d})}{(1-\\gamma)^{\\color{red} 2}} \\varepsilon + \\varepsilon'\\,, \\label{eq:lspiup} \\end{align}\\] where \\(\\varepsilon\\) is the worst-case error with which the $d$-dimensional features can approximate the action-value functions of memoryless policies of the MDP $M$. In fact, the result continues to hold if we restrict the memoryless policies to those that are $\\phi$-measurable in the sense that the probability assigned by such a policy to taking some action $a$ in some state $s$ depends only on \\(\\phi(s,\\cdot)\\). Denote the set of such policies by \\(\\Pi_\\phi\\). Then, for an MDP $M$ and associated feature-map $\\phi$, let . \\[\\tilde\\varepsilon(M,\\phi) = \\sup_{\\pi \\in \\Pi_\\phi}\\inf_{\\theta} \\|\\Phi \\theta - q^\\pi\\|_\\infty\\,.\\] Checking the proof, noticing that LSPI produces \\(\\phi\\)-measurable policies only, it follows that provided the first policy it uses is also \\(\\phi\\)-measurable, $\\varepsilon$ in \\eqref{eq:lspiup} can be replaced by \\(\\tilde \\varepsilon(M,\\phi)\\). Earlier, we also proved that the amplification of $\\varepsilon$ by the $\\sqrt{d}$-factor is unavoidable by any efficient planner. However, this leaves open the question of whether the amplification by a polynomial power of $1/(1-\\gamma)$ is necessary, and whether in particular, the quadratic dependence is necessary? Our first result shows that in the case of LSPI this amplification is real and the quadratic dependence cannot be improved. The result is stated for the “limiting” version of LSPI that uses infinitely many rollouts of infinite length each. As the sample size grows, one expects that the behavior of the finite-sample version of LSPI will track closely the behavior of this “idealized” LSPI and since a good algorithm should have the property that larger sample sizes give rise to better performance, that the idealized LSPI performs poorly (as we shall see soon there are methods that significantly reduce the error amplification) is an indication that LSPI is not a very good algorithm. Theorem (LSPI error amplification lower bound): The quadratic dependence in \\eqref{eq:lspiup} is tight: for every \\(1/2&lt; \\gamma&lt;1\\) and every \\(\\varepsilon&gt;0\\) there exists a featurized MDP \\((M,\\phi)\\), a policy \\(\\pi\\) of the MDP, a distribution \\(\\mu\\) over the states such that LSPI, when it is allowed infinitely many rollouts of infinite length from a $G$-optimal design, produces a sequence of policies \\(\\pi_0=\\pi,\\pi_1,\\dots\\) such that . \\[\\inf_{k\\ge 1} \\mu (v^*-v^{\\pi_k}) \\ge \\frac{\\tilde\\varepsilon(M,\\phi)}{2(1-\\gamma)^2}\\,,\\] while the optimal policy of $M$ can be obtained as a greedy policy with respect to the action-value function $(s,a)\\mapsto \\phi(s,a)^\\top \\theta$ with an appropriate vector $\\theta\\in \\mathbb{R}^d$. As we shall see in the proof, the result of the theorem holds even when LSPI is used with state-aggregation. Intuitively, state-aggregation means that states are groups into a number of groups and states belonging to the same group are treated identically when it comes to representing value functions. This, value-functions based on state-aggregation are constant over any group. When we are concerned with state-value functions, aggregating the states based on a partitioning of the states \\(\\mathcal{S}\\) into the groups \\(\\{\\mathcal{S}_i\\}_{1\\le i \\le d}\\) (i.e., \\(\\mathcal{S}_i\\subset \\mathcal{S}\\) and all the subsets are disjoint from each other), a feature-map that allows to represent these piecewise constant functions is . \\[\\phi_i(s) = \\mathbb{I}(s\\in \\mathcal{S}_i)\\,, \\qquad i\\in [d]\\,,\\] where $\\mathbb{I}$ is the indicator function that takes the value of one when its argument (a logical expression) is true, and is zero otherwise. In other words, \\(\\phi: \\mathcal{S} \\to \\{ e_1,\\dots,e_d\\}\\). Any feature map of this form defines a partitioning of the state-space and thus corresponds to the state-aggregation. Note that the piecewise constant functions can also be represented if we rotate all the features by the same rotation. The only important aspect here is that the features of different states are either identical, or orthogonal to each other, making the rows of the feature matrix an orthonormal system. For approximating action-value functions, state-aggregation uses the same partitioning of states regardless of the identity of the actions: In effect, for each action, one uses the feature map from above, but with a private parameter vector. This effectively amounts to stacking $\\phi(s)$ \\(\\mathrm{A}\\)-times, to get one copy of it for each action $a\\in \\mathcal{A}$. Note that for state-aggregation, there is no $\\sqrt{d}$ amplification of the approximation errors: State-aggregation is extrapolation friendly, as will be explained at the end of the lecture. Proof: . We define the MDP to be as in figure to the right. The state and action spaces are \\(\\mathcal{S} = \\{u_1, l_1, u_2, l_2\\}\\) and \\(\\mathcal{A} = \\{a_1, a_2\\}\\). We define \\(\\mathcal{S}_i = \\{u_i, l_i\\}\\), \\(\\mathbf{U} = \\{u_1, u_2\\}\\), and \\(\\mathbf{L} = \\{l_1, l_2\\}\\). In words, for $i\\in {1,2}$, action $a_1$ makes the next state to be $l_i$ whenever the initial state is either $l_i$ or $u_i$. On the other hand, action $a_2$ makes the next state $l_i$ when the inital state is either $l_{2-i}$ or $u_{2-i}$. Thus, both actions lead to a state in $\\mathbf{L}$, but $a_1$ stays in $\\mathcal{S}_i$, while $a_2$ switches from $\\mathcal{S}_1$ to $\\mathcal{S}_2$ and vice versa. The reward function is specified in terms of a positive scalar \\(r^*&gt;0\\) whose value will be chosen later. In particular, we let . \\[\\begin{align*} r (s, a_1) = \\begin{cases} - r^* + 2 \\varepsilon, &amp; \\text{if $s \\in \\mathbf{U}$}; \\\\ - r^*, &amp; \\text{if $s \\in \\mathbf{L}$}; \\end{cases} \\quad\\text{and}\\quad r (s, a_2) = 0\\,. \\end{align*}\\] Importantly, the reward is larger at $u_i$ than at $l_i$, which will cause LSPI (with appropriate features) to overestimate the values of states in $\\mathbf{L}$. Provided that $r(s,a_1)&lt;0$, equivalently, . \\[\\begin{align} 2 \\varepsilon&lt; r^*\\,, \\label{eq:c1} \\end{align}\\] the optimal policy is unique and takes $a_2$ everywhere: Switching incurs no cost, while not switching does. For any memoryless policy $\\pi$, from the Bellman equations for the policy we get that the action-value function of $\\pi$ satisfies the following equations: . \\[\\begin{align*} q^{\\pi} (s, a_1) = \\begin{cases} - r^* + 2 \\varepsilon \\mathbb{I} \\{s \\in \\mathbf{U}\\} + \\gamma v^{\\pi} (l_1)\\,, &amp; \\text{if $s \\in \\mathcal{S}_1$}\\,; \\\\ - r^* + 2 \\varepsilon \\mathbb{I} \\{s \\in \\mathbf{U}\\} + \\gamma v^{\\pi} (l_2)\\,, &amp; \\text{if $s \\in \\mathcal{S}_2$}\\,, \\end{cases} \\quad q^{\\pi} (s, a_2) = \\begin{cases} \\gamma v^{\\pi} (l_2)\\,, &amp; \\text{if $s \\in \\mathcal{S}_1$}\\,; \\\\ \\gamma v^{\\pi} (l_1)\\,, &amp; \\text{if $s \\in \\mathcal{S}_2$}\\,. \\end{cases} \\end{align*}\\] The feature map we consider is a state-aggregation feature map, where states in $\\mathcal{S}_i$ receive the same action-values: . \\[\\begin{align*} \\phi (s, a) = \\begin{pmatrix} \\mathbb{I} \\{s \\in \\mathcal{S}_1\\} \\mathbb{I} \\{a = a_1\\} \\\\ \\mathbb{I} \\{s \\in \\mathcal{S}_1\\} \\mathbb{I} \\{a = a_2\\} \\\\ \\mathbb{I} \\{s \\in \\mathcal{S}_2\\} \\mathbb{I} \\{a = a_1\\} \\\\ \\mathbb{I} \\{s \\in \\mathcal{S}_2\\} \\mathbb{I} \\{a = a_2\\} \\end{pmatrix}\\,. \\end{align*}\\] Therefore, the class of greedy policies w.r.t. $\\theta^\\top \\phi$ contains the optimal policy, provided that we keep the rewards of using $a_1$ negative. The action-value function is estimated by . \\[\\begin{align*} \\theta_\\pi = \\arg\\min_{\\theta \\in \\mathbb{R}^d} \\sum_{(s, a) \\in \\mathcal{S} \\times \\mathcal{A}} \\left( \\theta^\\top \\phi (s, a) - q^{\\pi} (s, a) \\right)^2\\,, \\end{align*}\\] which is the same as minimizing the weighted squared error where the weighting is given by the uniform distribution over $\\mathcal{S} \\times \\mathcal{A}$. Note that the uniform distribution is a $G$-optimal design distribution for this feature map. By the choice of features, for \\(i,j\\in \\{1,2\\}\\), $\\Phi \\theta$ assigns the same value to members of \\(\\mathcal{S}_i\\times \\{a_j\\}\\). Hence, it follows that . \\[\\begin{align*} \\theta_\\pi = \\frac12 \\begin{pmatrix} q^{\\pi} (u_1, a_1) + q^{\\pi} (l_1, a_1) \\\\ q^{\\pi} (u_1, a_2) + q^{\\pi} (l_1, a_2) \\\\ q^{\\pi} (u_2, a_1) + q^{\\pi} (l_2, a_1) \\\\ q^{\\pi} (u_2, a_2) + q^{\\pi} (l_2, a_2) \\end{pmatrix}\\,. \\end{align*}\\] Note that $\\theta_\\pi$ also minimizes the maximum-error: \\(\\theta_\\pi \\in \\arg\\min_{\\theta \\in \\mathbb{R}^d} \\|\\theta^\\top \\Phi - q^{\\pi}\\|_\\infty\\). It follows then that . \\[\\inf_{\\theta \\in \\mathbb{R}^d} \\|\\theta^\\top \\Phi - q^{\\pi}\\|_\\infty = \\frac12 \\max_{1\\le i,j\\le 2} |q^{\\pi}(u_i,a_j)-q^{\\pi}(l_i,a_j)| = \\varepsilon\\,,\\] and hence . \\[\\tilde \\varepsilon (M, \\phi) = \\sup_{\\pi \\in \\Pi_\\phi} \\inf_{\\theta \\in \\mathbb{R}^d} \\|\\theta^\\top \\Phi - q^{\\pi}\\|_\\infty = \\varepsilon.\\] Consider the policy \\(\\pi \\in \\Pi_\\phi\\) that takes \\(a_1\\) at states in \\(\\mathcal{S}_1\\) and takes $a_2$ at states in \\(\\mathcal{S}_2\\). By symmetry, the policy $\\pi’ \\in \\Pi_\\phi$ that takes $a_2$ at states in \\(\\mathcal{S}_1\\) and takes $a_1$ in states at \\(\\mathcal{S}_2\\) will show a similar behavior. Then, . \\[\\begin{align*} q^{\\pi} (s, a_1) = \\begin{cases} - \\dfrac{r^*}{1-\\gamma} + 2 \\varepsilon \\mathbb{I} \\{s \\in \\mathbf{U}\\}\\,, &amp; \\text{if $s \\in \\mathcal{S}_1$}\\,; \\\\ - \\dfrac{r^*}{1-\\gamma} + 2 \\varepsilon \\mathbb{I} \\{s \\in \\mathbf{U}\\} + \\gamma r^*\\,, &amp; \\text{if $s \\in \\mathcal{S}_2$}\\,, \\end{cases} \\quad q^{\\pi} (s, a_2) = \\begin{cases} - \\dfrac{\\gamma^2 r^*}{1-\\gamma}\\,, &amp; \\text{if $s \\in \\mathcal{S}_1$}\\,; \\\\ - \\dfrac{\\gamma r^*}{1-\\gamma}\\,, &amp; \\text{if $s \\in \\mathcal{S}_2$}\\,. \\end{cases} \\end{align*}\\] Therefore, . \\[\\begin{align*} \\theta_\\pi^\\top = \\begin{pmatrix} - \\dfrac{r^*}{1-\\gamma} + \\varepsilon\\,, - \\dfrac{\\gamma^2 r^*}{1-\\gamma}\\,, - \\dfrac{r^*}{1-\\gamma} + \\varepsilon + \\gamma r^*\\,, - \\dfrac{\\gamma r^*}{1-\\gamma} \\end{pmatrix}\\,, \\end{align*}\\] and thus, for \\(\\hat{q}_\\pi = \\Phi \\theta_\\pi\\) we have . \\[\\begin{align*} \\hat{q}_\\pi (s, a_1) - \\hat{q}_\\pi (s, a_2) = \\begin{cases} - (1+\\gamma) r^* + \\varepsilon\\,, &amp; \\text{if $s \\in \\mathcal{S}_1$}\\,; \\\\ - (1-\\gamma) r^* + \\varepsilon\\,, &amp; \\text{if $s \\in \\mathcal{S}_2$}\\,. \\end{cases} \\end{align*}\\] Hence, if we choose $r^*$ such that . \\[\\begin{align} -(1+\\gamma) r^*+\\varepsilon&lt;0 \\,, \\label{eq:c2} \\\\ -(1-\\gamma) r^*+\\varepsilon&gt;0 \\,, \\label{eq:c3} \\end{align}\\] then the policy that is greedy with respect to \\(\\hat{q}_\\pi\\) will be $\\pi’$. Vice versa, the policy that will be greedy with respect to $\\hat{q}_{\\pi’}$ will be $\\pi$. Hence, the idealized version of LSPI switches between $\\pi$ and $\\pi’$ indefinitely. Conditions \\eqref{eq:c2} and \\eqref{eq:c3}, together with \\eqref{eq:c1}, mean that \\(r^*\\) should take on a value in the interval $(2\\varepsilon,\\varepsilon/(1-\\gamma))$, which is non-empty since by assumption $\\gamma&gt;1/2$. Let us choose the midpoint of this interval for \\(r^*\\): . \\[r^* = \\varepsilon+ \\frac{\\varepsilon}{2(1-\\gamma)}\\,.\\] Now, noting that $v^* = 0$, we have . \\[\\begin{align*} v^{*} (l_1) - v^{\\pi} (l_1) = v^{*} (l_2) - v^{\\pi'} (l_2) = \\dfrac{r^*}{1-\\gamma} \\ge \\frac{\\varepsilon}{2(1-\\gamma)^2} \\,, \\end{align*}\\] and the proof is concluded by noting that $v^* \\ge v^{\\pi},v^{\\pi’}$. \\(\\qquad \\blacksquare\\) . Thus, the proof reveals that in the MDP constructed in the proof LSPI leads to a sequence of policies that alternate between each other. “Convergence” is fast, yet, the guarantee is far from satisfactory. In particular, in the same example, an alternate algorithm, which we will cover next can reduce the quadratic dependence on the horizon to a linear dependence. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec13/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec13/"
  },"74": {
    "doc": "14. Politex",
    "title": "Online linear optimization",
    "content": "As it happens, the problem of controlling terms of this type is the central problem studied in a subfield of learning theory, online learning. In particular, in online linear optimization, the following problem is studied: . An adversary and a learner are playing a zero-sum minimax game in $k$ discrete rounds, taking actions in an alternating manner. In round $j$ ($0\\le j \\le k-1$), first, the learner needs to choose a vector \\(x_j\\in \\mathcal{X}\\subset \\mathbb{R}^d\\). Then, the adversary chooses a vector, \\(y_j \\in \\mathcal{Y}\\subset \\mathbb{R}^d\\). Before its choice, the adversary learns about all previous choices of the learner, and the learner also learns about all previous choices of the adversary. They also remember their own choices. For simplicity, let us constraint the adversary and the learner to be deterministic. The payoff to the adversary at the end of the $k$ rounds is . \\[\\begin{align} R_k = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} \\langle x, y_j \\rangle - \\langle x_j,y_j \\rangle\\,. \\label{eq:regretdefolo} \\end{align}\\] In particular, the adversary’s goal is maximize this, while the learner’s goal is to minimize this (the game is zero-sum). Both the adversary and the learner are given $k$ and the sets $\\mathcal{X},\\mathcal{Y}$. Letting $L$ to denote the learner’s strategy (a sequence of maps of histories to $\\mathcal{X}$) and $A$ to denote the adversary’s strategy (a sequence of maps of histories to $\\mathcal{Y}$), the above quantity depends on $L$ and $A$: $R_k = R_K(A,L)$. Taking the perspective of the learner, the quantity defined in \\eqref{eq:regretdefolo} is called the learner’s regret. Denote the minimax value of the game by \\(R_k^*\\): \\(R_k^* = \\inf_L \\sup_A R_k(A,L)\\). Thus, this only depends on \\(k\\), \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). The dependence is suppressed when it is clear from the context. The central question then is how $R_k^*$ depends on $k$ and also on $\\mathcal{X}$ and $\\mathcal{Y}$. In online linear optimization both sets $\\mathcal{X}$ and $\\mathcal{Y}$ are convex. Connecting these games to our problem, we can see that \\(T_1(s)\\) in \\eqref{eq:t1s} matches the regret definition in \\eqref{eq:regretdefolo} if we let \\(d=\\mathrm{A}\\), \\(\\mathcal{X} = \\mathcal{M}_1(\\mathrm{A}) = \\{ p\\in [0,1]^{\\mathrm{A}} \\,:\\, \\sum_a p_a = 1 \\}\\) be the \\(\\mathrm{A}-1\\) simplex of \\(\\mathbb{R}^{\\mathrm{A}}\\) and \\(\\mathcal{Y} = [0,1/(1-\\gamma)]^{\\mathrm{A}}\\). Furthermore, \\(\\pi_j(s,\\cdot)\\) needs to be chosen first, which is followed by the choice of \\(\\hat q_j(s,\\cdot)\\). While \\(\\hat q_j(s,\\cdot)\\) will not be chosen in an adversarial fashion, a bound $B$ on the regret against arbitrary choices will also serve as a bound for the specific choice we will need to make for \\(\\hat q_j(s,\\cdot)\\). ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec14/#online-linear-optimization",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec14/#online-linear-optimization"
  },"75": {
    "doc": "14. Politex",
    "title": "Mirror descent",
    "content": "Mirror descent (MD) is an algorithm that originates in optimization theory. In the context of online linear optimization, MD is a strategy for the learner which is known to guarantee near minimax regret for the learner under a wide range of circumstances. To align with the large body of literature on online linear optimization, it will be beneficial to switch signs. Thus, in what follows we assume that the learner will aim at minimizing \\(\\langle x,y \\rangle\\) by its choice \\(x\\in \\mathcal{X}\\) and the adversary will aim at maximizing the same expression over its choice \\(y\\in \\mathcal{Y}\\). This means that we also redefine the regret to . \\[\\begin{align} R_k &amp; = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} \\langle x_j, y_j \\rangle - \\langle x,y_j \\rangle \\nonumber \\\\ &amp; = \\sum_{j=0}^{k-1} \\langle x_j, y_j \\rangle - \\min_{x\\in \\mathcal{X}} \\sum_{j=0}^{k-1}\\langle x,y_j \\rangle\\,. \\label{eq:regretdefololosses} \\end{align}\\] Everything else remains the same: The game is zero-sum, minimax, the regret is the payoff for the adversary and the negative regret is the payoff of the learner. This version is called a loss-game. The reason to prefer the loss game is because most of optimization theory is written for minimizing convex functions rather than for maximizing concave functions. However, clearly, this is an arbitrary choice. The second form of the regret shows that the player’s goal is to compete with the best single decision from \\(\\mathcal{X}\\) but chosen given the hindsight of knowing all the choices of the adversary. That is, the learner’s goal is to keep its cumulative loss \\(\\sum_{j=0}^{k-1} \\langle x_j, y_j \\rangle\\) close to, or even below the best cumulative loss in hindsight, \\(\\min_{x\\in \\mathcal{X}} \\sum_{j=0}^{k-1}\\langle x,y_j \\rangle\\). (With this, $T_1(s)$ matches \\(R_k\\) when we change \\(\\mathcal{Y} = [-1/(1-\\gamma),0]^{\\mathrm{A}}\\).) . MD is recursively defined and in its simplest form it has two design parameters. The first is an extended real-valued convex function \\(F: \\mathbb{R}^d \\to \\bar {\\mathbb{R}}\\), called the “regularizer”, while the second is a stepsize, or learning rate parameter $\\eta&gt;0$. (The extended reals is just \\(\\mathbb{R}\\) together with \\(+\\infty,-\\infty\\) and an appropriate extension of basic arithmetic. By allowing convex functions to take the value \\(+\\infty\\) allows to merge “constraints” with objectives in a seamless fashion. The value $-\\infty$ is added because sometimes we have to work with negated extended real-valued convex functions.) . The specification of MD is as follows: In round $0$, $x_0\\in \\mathcal{X}$ is picked to minimize \\(F\\): . \\[x_0 = \\arg\\min_{x\\in \\mathcal{X}} F(x)\\,.\\] In what follows, we assume that all the minimizers that we need in the definition of MD do exist. In the specific case that we need, \\(\\mathcal{X}\\) is the \\(d-1\\) simplex, which is a closed convex set, and since convex functions are also continuous, the minimizers that we will need are guaranteed to exist. Then, in round \\(j&gt;0\\), MD chooses \\(x_j\\) as follows: . \\[\\begin{equation} \\begin{split} x_j &amp; = \\arg\\min_{x\\in \\mathcal{X}}\\,\\,\\eta \\langle x, y_{j-1} \\rangle + D_F(x,x_{j-1}) \\\\ \\end{split} \\label{eq:mddef} \\end{equation}\\] Here, . \\[D_F(x,x') = F(x)-(F(x')+\\langle \\nabla F(x'), x-x'\\rangle)\\] is the remainder term in the first-order Taylor-series expansion of the value of $F$ at $x$ when the expansion is carried out at \\(x'\\) and, for simplicity, we assume that \\(F\\) is differentiable on the interior of its domain \\(\\text{dom}(F) = \\{ x\\in \\mathbb{R}\\,:\\, F(x)&lt;+\\infty \\}\\). Since for any convex function and any linear approximation of it stays below the graph of the convex function, we immediately get that \\(D_F\\) is nonnegative valued. For an illustration see the figure on the right, which shows a convex function, the first-order Taylor approximation of the function at some point. One should think of \\(F\\) is a “nonlinear distance inducing function”; above \\(D_F(x,x')\\) can be thought of penalty imposed on deviating from \\(x'\\). However, \\(D_F\\) is more often than not is not a distance, i.e., often it is not even symmetric. Because of this, we can’t really call $D_F$ a distance. Hence, it is called a divergence. In particular, \\(D_F(x,x')\\) is called the Bregman divergence of \\(x\\) from \\(x'\\). In the definition of the MD update rule, we tacitly assumed that \\(D_F(x,x_{j-1})\\) is well-defined. This requires that \\(F\\) should be differentiable at \\(x_{j-1}\\), which one needs to check when applying MD. In our specific case, this will hold, again. The idea of the MD update rule is to (1) allow the learner to react to the last loss \\(y_{j-1}\\) vector chosen by the adversary, while also (2) limiting how much \\(x_j\\) can depart from \\(x_{j-1}\\), thus, effectively stabilizing the algorithm, the tradeoff governed by the choice of $\\eta&gt;0$. (Separating \\(\\eta\\) from \\(F\\) only makes sense because there are some standard choices for \\(F\\), but \\(\\eta\\) is really just a scale parameter for \\(F\\)). In particular, the larger the value of \\(\\eta\\) is, the less “data-sensitive” MD will be (here, \\(y_0,\\dots,y_{k-1}\\) constitute the data), and vice versa, the smaller \\(\\eta\\) is, the more data-sensitive MD will be. Where is the mirror? . Under some technical conditions on \\(F\\), the update rule \\eqref{eq:mddef} has a two step-implementation: . \\[\\begin{align} \\tilde x_j &amp; = (\\nabla F)^{-1} ( \\nabla F (x_{j-1}) - \\eta y_{j-1} )\\,,\\label{eq:mds1}\\\\ x_j &amp;= \\arg\\min_{x\\in \\mathcal{X}} D_F(x,\\tilde x_j)\\,.\\label{eq:mds2} \\end{align}\\] The first equation above explains the name: To obtain \\(\\tilde x_j\\), one first transforms \\(x_{j-1}\\) using \\(\\nabla F: \\text{dom}(\\nabla F ) \\to \\mathbb{R}^d\\) to the “mirror” (dual) space where “gradients”/”slopes live”, where one then adds to the result \\(-\\eta y_{j-1}\\), which can be seen as a “gradient step” (interpreting \\(y_{j-1}\\) as the gradient of some loss). Finally, the result is then mapped back to the original (primal) space using the inverse of \\(\\nabla F\\). The second step of the update takes the resulting point \\(\\tilde x_j\\) and “projects” it to \\(\\mathcal{X}\\) in a way that respects the “geometry induced by \\(F\\)” on the space \\(\\mathbb{R}^d\\). The use of complex terminology, like “primal” and “dual” spaces, which happen to be the same old Euclidean space, \\(\\mathbb{R}^d\\), probably sounds like an overkill. Indeed, in the simple case we consider when these spaces are identical it is. The distinction would become important when working with infinite dimensional spaces, which we leave to others for now. Besides helping with understanding the terminology, the two-step update shown can also be useful for computation. In fact, this will be the case in the special case that we need. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec14/#mirror-descent",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec14/#mirror-descent"
  },"76": {
    "doc": "14. Politex",
    "title": "Mirror descent on the simplex",
    "content": "We have seen that in the special case we need, . \\[\\begin{align*} \\mathcal{X} &amp;= \\mathcal{P}_{d-1}:=\\{ p\\in [0,1]^{d}\\,:\\, \\sum_a p_a = 1 \\}\\,, \\\\ \\mathcal{Y} &amp;= [-1/(1-\\gamma),0]^d\\,, \\text{and} \\\\ d &amp;= \\mathrm{A}\\,. \\end{align*}\\] To use MD we need to specify the regularizer \\(F\\) and the learning rate. For the former, we choose . \\[F(x) = \\sum_i x_i \\log(x_i) - x_i\\,,\\] which is known as the unnormalized negentropy function. Note that \\(F\\) takes on finite values when \\(x\\in [0,\\infty]^d\\) (since \\(\\lim_{x\\to 0+} x \\log(x)=0\\), we set \\(x_i \\log(x_i)=0\\) whenever $x_i=0$). Outside of this quadrant, we define the value of \\(F\\) to be \\(+\\infty\\). The plot of \\(x\\log(x)-x\\) for \\(x\\ge 0\\) is shown on the right. It is not hard to verify that \\(F\\) is convex: First, \\(\\text{dom}(F) = [0,\\infty]^d\\) is convex. Taking the first derivative, we find that for any \\(x\\in (0,\\infty)^d\\), . \\[\\nabla F(x) = \\log(x)\\,,\\] where \\(\\log\\) is applied componentwise. Taking the derivative again, we find that for \\(x\\in (0,\\infty)^d\\), . \\[\\nabla^2 F(x) = \\text{diag}(1/x)\\,,\\] i.e., the matrix whose $(i,i)$th diagonal entry is \\(1/x_i\\). Clearly, this is a positive definite matrix, which suffices to verify that \\(F\\) is a convex function. The Bregman divergence induced by \\(F\\) is . \\[\\begin{align*} D_F(x,x') &amp; = \\langle \\boldsymbol{1}, x \\log(x) - x - x' \\log(x')+x'\\rangle - \\langle \\log(x'), x-x'\\rangle \\\\ &amp; = \\langle \\boldsymbol{1}, x \\log(x/x') - x +x'\\rangle \\,, \\end{align*}\\] where again we use an “intuitive” notation when operations are first applied componentwise (i.e., $x \\log(x)$ denotes a vector whose $i$th component is $x_i \\log(x_i)$). Note that the domain of $D_F$ is \\([0,\\infty)^d \\times (0,\\infty)^d\\). If both $x$ and $x’$ lie in the $d-1$-simplex, $D_F$ becomes the well-known relative entropy, or Kullback-Leibler (KL) divergence. It is not hard to verify that $x_j$ can be obtained as shown in \\eqref{eq:mds1}-\\eqref{eq:mds2} and in particular this two-step update takes the form . \\[\\begin{align*} \\tilde x_{j,i} &amp;= x_{j-1,i} \\exp(-\\eta y_{j-1,i})\\,, \\qquad x_{j,i} = \\frac{\\tilde x_{j,i}}{ \\sum_{i'} \\tilde x_{j,i'}}\\,, \\quad i\\in [d]\\,. \\end{align*}\\] Unrolling the recursion, we can also that this is the same as . \\[\\begin{equation} \\tilde x_{j,i} = \\exp(-\\eta (y_{0,i}+\\dots + y_{j-1,i}))\\,, \\qquad x_{j,i} = \\frac{\\tilde x_{j,i}}{ \\sum_{i'} \\tilde x_{j,i'}}\\,, \\quad i\\in [d]\\,. \\label{eq:mdunrolled} \\end{equation}\\] Based on this, it is obvious that MD can be efficiently implemented with this choice of $F$. As far as the regret is concerned, the following theorem holds: . Theorem (MD with negentropy on the simplex): Let \\(\\mathcal{X}= \\mathcal{P}_{d-1}\\) amd \\(\\mathcal{Y} = [0,1]^d\\). Then, no matter the adversary, a learner using MD with . \\[\\eta = \\sqrt{ \\frac{2\\log(d)}{k}}\\] is guaranteed that its regret \\(R_k\\) in \\(k\\) rounds is at most . \\[R_k \\le \\sqrt{2k \\log(d)}\\,.\\] . When the adversary plays in \\(\\mathcal{Y} = [a,b]^d\\) with \\(a&lt;b\\), we can use MD on the transformed sequence \\(\\tilde y_j = (y_j-a \\boldsymbol{1})/(b-a) \\in [0,1]^d\\). Then, for any $x\\in \\mathcal{X}$, . \\[\\begin{align*} R_k(x) &amp; := \\sum_{j=0}^{k-1} \\langle x_j-x, y_j \\rangle \\\\ &amp; = \\sum_{j=0}^{k-1} \\langle x_j-x, (b-a)\\tilde y_j+a \\boldsymbol{1} \\rangle \\\\ &amp; = (b-a)\\sum_{j=0}^{k-1} \\langle x_j-x, \\tilde y_j \\rangle \\\\ &amp; \\le (b-a) \\sqrt{2k \\log(d)}\\,, \\end{align*}\\] where the third equality used that $\\langle x_j,\\boldsymbol{1}\\rangle = \\langle x, \\boldsymbol{1} \\rangle = 1$. Taking the maximum over $x\\in \\mathcal{X}$ gives that . \\[\\begin{align} R_k \\le (b-a) \\sqrt{2k \\log(d)}\\,. \\label{eq:mdrbscaled} \\end{align}\\] By the update rule in \\eqref{eq:mdunrolled}, . \\[\\begin{align*} \\tilde x_{j,i} = \\exp(-\\eta (\\tilde y_{0,i}+\\dots + \\tilde y_{j-1,i})) = \\exp(-\\eta/(b-a) (y_{0,i}+\\dots + y_{j-1,i}-j b) )\\,, \\qquad i\\in [d]\\,. \\end{align*}\\] Note that the “shift” by $-jb$ cancels out in the normalization step. Hence, MD in this case takes the form . \\[\\begin{equation} \\begin{split} \\tilde x_{j,i} &amp;= \\exp(-\\eta/(b-a) (y_{0,i}+\\dots + y_{j-1,i}))\\,, \\qquad x_{j,i} = \\frac{\\tilde x_{j,i}}{ \\sum_{i'} \\tilde x_{j,i'}}\\,, \\quad i\\in [d]\\,, \\label{eq:mdunrolledscaled} \\end{split} \\end{equation}\\] which is the same as before, except that the learning rate is scaled by $1/(b-a)$. In particular, in this case one can set . \\[\\begin{align} \\eta = \\frac{1}{b-a} \\sqrt{\\frac{2\\log(d)}{k}}\\,. \\label{eq:etascaled} \\end{align}\\] and use update rule \\eqref{eq:mdunrolled}. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec14/#mirror-descent-on-the-simplex",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec14/#mirror-descent-on-the-simplex"
  },"77": {
    "doc": "14. Politex",
    "title": "MD applied to MDP planning",
    "content": "As agreed, \\(T_1(s)\\) from \\eqref{eq:t1s} takes the form of a $k$-round regret against \\(\\pi^*(s,\\cdot)\\) in online linear optimization on the simplex with losses in \\([-1/(1-\\gamma),0]^{\\mathrm{A}}\\). This suggest to use MD in a state-by-state manner to control \\(T_1(s)\\). Using \\eqref{eq:mdunrolled} and \\eqref{eq:etascaled} gives . \\[E_j(s,a) = \\exp(\\eta (\\hat q_0(s,a) +\\dots + \\hat q_{j-1}(s,a)))\\,, \\qquad \\pi_j(a|s) = \\frac{E_j(s,a)}{ \\sum_{a'} E_j(s,a')}\\,, \\quad a\\in \\mathcal{A}\\] to be used with . \\[\\eta = (1-\\gamma) \\sqrt{\\frac{2\\log(\\mathrm{A})}{k}}\\,.\\] Note that this is the update used by Politex. Then, \\eqref{eq:mdrbscaled} gives that simultaneously for all $s\\in \\mathcal{S}$, . \\[\\begin{align} |T_1(s)| \\le \\frac{1}{1-\\gamma} \\sqrt{2k \\log(\\mathrm{A})}\\,. \\label{eq:t1sbound} \\end{align}\\] Putting things together, we get the following result: . Theorem (Politex suboptimality gap bound): Pick a featurized MDP $(M,\\phi)$ with a full rank feature-map $\\varphi: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}^d$ and let $K,m,H\\ge 1$. Assume that B2\\(_{\\varepsilon}\\) holds for $(M,\\phi)$ and the rewards in $M$ are in the $[0,1]$ interval. For $0\\le \\zeta&lt;1$, define . \\[\\kappa(\\zeta) = \\varepsilon (1 + \\sqrt{d}) + \\sqrt{d} \\left(\\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log( d(d+1)K / \\zeta)}{2m}}\\right)\\,,\\] Then, in $K$ iterations, Politex produces a mixed policy $\\bar \\pi_K$ such that with probability $1-\\zeta$, the suboptimality gap $\\delta$ of $\\bar \\pi_K$ satisfies . \\[\\begin{align*} \\delta \\le \\frac{1}{(1-\\gamma)^2}\\sqrt{\\frac{2\\log(\\mathrm{A})}{K}}+\\frac{2 \\kappa(\\zeta) }{1-\\gamma} \\,. \\end{align*}\\] In particular, for any $\\varepsilon’&gt;0$, choosing $K,H,m$ so that . \\[\\begin{align*} K &amp; \\ge \\frac{32 \\log(A)}{ (1-\\gamma)^4 (\\varepsilon')^2}\\,, \\\\ H &amp; \\ge H_{\\gamma,(1-\\gamma)\\varepsilon'/(8\\sqrt{d})} \\qquad \\text{and} \\\\ m &amp; \\ge \\frac{32 d}{(1-\\gamma)^4 (\\varepsilon')^2} \\log( (d+1)^2 K /\\zeta )\\,, \\end{align*}\\] policy $\\pi_K$ is $\\delta$-optimal with . \\[\\begin{align*} \\delta \\le \\frac{2(1 + \\sqrt{d})}{1-\\gamma}\\, \\varepsilon + \\varepsilon'\\,, \\end{align*}\\] while the total computation cost is $\\text{poly}(\\frac{1}{1-\\gamma},d,\\mathrm{A},\\frac{1}{(\\varepsilon’)^2},\\log(1/\\zeta))$. Note that as compared to the result of LSPI with G-optimal design, the amplification of the approximation error $\\varepsilon$ is reduced by a factor of $1/(1-\\gamma)$, as it was promised. The price is that now the number of iterations $K$, is a polynomial of $\\frac{1}{(1-\\gamma)\\varepsilon’}$, whereas before it was logarithmic. This suggest that perhaps a higher learning rate can help initially to speed up convergence to get the best of both words. Proof: As in the proof of the suboptimality gap for LSPI, we get that for any $0\\le \\zeta \\le 1$, with probability at least $1-\\zeta$, for any $0 \\le k \\le K-1$, . \\[\\begin{align*} \\| q^{\\pi_k} - \\hat q_k \\|_\\infty =\\| q^{\\pi_k} - \\Pi \\Phi \\hat \\theta_k \\|_\\infty \\le \\| q^{\\pi_k} - \\Phi \\hat \\theta_k \\|_\\infty &amp;\\leq \\kappa(\\zeta)\\,, \\end{align*}\\] where the first inequality uses that \\(q_{\\pi_k}\\) takes values in $[0,1]$. On the event when the above inequalities hold, by \\eqref{eq:polsubgapgen} and \\eqref{eq:t1sbound}, . \\[\\begin{align*} \\delta \\le \\frac{1}{(1-\\gamma)^2}\\sqrt{\\frac{2\\log(\\mathrm{A})}{K}}+\\frac{2 \\kappa(\\zeta) }{1-\\gamma} \\,. \\end{align*}\\] The details of this calculation are left to the reader. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec14/#md-applied-to-mdp-planning",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec14/#md-applied-to-mdp-planning"
  },"78": {
    "doc": "14. Politex",
    "title": "Notes",
    "content": "Optimality of the Final Policy . Notice that we said the policy returned by Politex after $k$ iterations should be a mixture policy $\\bar \\pi_k = \\frac{1}{k} (\\pi_0 + \\dots + \\pi_{k-1})$. A more natural policy to return is the final policy \\(\\pi_{k-1}\\). The question then is: can one ensure similar optimality gaurantees for the final policy $\\pi_{k-1}$ as we have seen for $\\bar \\pi_k$? The answer turns out to be yes, if we use the unnormalized negentropy regularizer for mirror descent (as we have already been using in this lecture note). To see this, we aim to bound \\(\\| v^{\\pi^*} - v^{\\pi_{k-1}} \\|_\\infty\\). We begin by writing. \\[\\begin{align*} v^{\\pi^*} - v^{\\pi_{k-1}} &amp; = v^{\\pi^*} - v^{\\bar \\pi_k} + v^{\\bar \\pi_k} - v^{\\pi_{k-1}} \\\\ &amp; = \\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\sum_{j=0}^{k-1} M_{\\pi^*} q^{\\pi_j} - M_{\\pi_j} q^{\\pi_j} \\\\ &amp; \\quad + \\frac1k (I-\\gamma P_{\\pi_{k-1}})^{-1} \\sum_{j=0}^{k-1} M_{\\pi_j} q^{\\pi_j} - M_{\\pi_{k-1}} q^{\\pi_j}\\\\ &amp; = \\frac1k(I-\\gamma P_{\\pi^*})^{-1} \\underbrace{ \\sum_{j=0}^{k-1} M_{\\pi^*} \\hat q_j - M_{\\pi_j} \\hat q_j}_{T_1} \\\\ &amp; \\quad + \\underbrace{\\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\sum_{j=0}^{k-1} ( M_{\\pi^*} - M_{\\pi_j} )( q^{\\pi_j}-\\hat q_j)}_{T_2} \\\\ &amp; \\quad + \\frac1k(I-\\gamma P_{\\pi_{k-1}})^{-1} \\underbrace{ \\sum_{j=0}^{k-1} M_{\\pi_j} \\hat q_j - M_{\\pi_{k-1}} \\hat q_j}_{T_3} \\\\ &amp; \\quad + \\underbrace{\\frac1k (I-\\gamma P_{\\pi_{k-1}})^{-1} \\sum_{j=0}^{k-1} ( M_{\\pi_j} - M_{\\pi_{k-1}} )( q^{\\pi_j}-\\hat q_j)}_{T_4} \\,. \\end{align*}\\] Notice how $T_1$ and $T_2$ are defined as before, and we already have bounds for both of them. It is also easy to see that that $T_4$ takes a very similar form to $T_2$ and can also be bounded in the same way as $T_2$. If we can show that $T_3(s) \\le 0$ for all $s \\in \\mathcal{S}$ then we would get the result that . \\[\\begin{align*} \\|v^{\\pi^*} - v^{\\pi_{k-1}}\\|_\\infty \\le \\delta \\le \\frac{1}{(1-\\gamma)^2}\\sqrt{\\frac{2\\log(\\mathrm{A})}{K}}+\\frac{\\textcolor{blue}{4} \\kappa(\\zeta) }{1-\\gamma} \\,. \\end{align*}\\] Which is identical to the result of the main theorem in the lecture note above, except with the constant $2$ scaling replaced with a constant $\\textcolor{blue}{4}$ scaling infront of the approximation error (since $T_4$ used the same bound as $T_2$). We are left to show that indeed $T_3(s) \\le 0$. To do this we first write out $T_3(s)$ in vector notation to help us aline with the math syntax to come. Fix a state $s \\in \\mathcal{S}$, then . \\[T_3(s) = \\sum_{j=0}^{k-1} \\langle \\pi_j(\\cdot|s), \\hat q_j(s, \\cdot) \\rangle - \\langle \\pi_{k-1}(\\cdot|s), \\hat q_j(s, \\cdot) \\rangle\\] Since we will hold $s$ fixed for all the following steps we slightly abuse notation in favor of avoiding clutter and write the above equation as follows where it is assumed that all functions were first evaluated at $s$. \\[T_3 = \\sum_{j=0}^{k-1} \\langle \\pi_j, \\hat q_j \\rangle - \\langle \\pi_{k-1}, \\hat q_j \\rangle\\] Next recall that the policy selected by MD at iteration $k$ is defined as . \\[\\begin{align*} \\pi_k &amp; = \\arg\\min_{\\pi \\in \\mathcal{M}_1(A)}\\,\\,\\eta \\langle \\pi, -\\hat q_{k-1} \\rangle + D_F(\\pi,\\pi_{k-1}) \\\\ &amp; = \\arg\\max_{\\pi \\in \\mathcal{M}_1(A)}\\,\\,\\eta \\langle \\pi, \\hat q_{k-1} \\rangle - D_F(\\pi,\\pi_{k-1}) \\end{align*}\\] where we have negated $\\hat q_{k-1}$ to formulate our problem as a minimization problem as was needed for the MD analysis. If we set $F$ to the unnormalized negentropy regularizer (as was done in the notes above) . \\[F(x) = \\sum_i x_i \\log(x_i) - x_i\\,,\\] we have that . \\[\\begin{align*} \\pi_k &amp; = \\arg\\max_{\\pi \\in \\mathcal{M}_1(A)}\\,\\,\\eta \\langle \\pi, \\hat q_{k-1} \\rangle - KL(\\pi || \\pi_{k-1}) \\end{align*}\\] which turns out to be equivilant to . \\[\\begin{align} \\pi_k &amp; = \\arg\\max_{\\pi \\in \\mathcal{M}_1(A)}\\,\\,\\eta \\langle \\pi, \\sum_{j=0}^{k-1} \\hat q_{j} \\rangle - F(\\pi). \\label{eq:ftrlpolicy} \\end{align}\\] The above equation is the policy selection made by the Follow The Regularized Leader (FTRL) algorithm. For further details of the equavilance between MD and FTRL when $F$ is the unnormalized negentropy one can refer to chapter 28 of the Bandit Book. Importantly, the above equation will be useful for our proof. We will now show that $T_3 \\le 0$ by showing that . \\[\\sum_{j=0}^{k-1} \\langle \\pi_{k-1}, \\hat q_j \\rangle \\ge \\sum_{j=0}^{k-1} \\langle \\pi_{j}, \\hat q_j \\rangle\\] To do this notice that . \\[\\begin{align*} \\eta \\sum_{j=0}^{k-1} \\langle \\pi_{k-1}, \\hat q_j \\rangle &amp; = \\eta \\langle \\pi_{k-1}, \\hat q_{k-1} \\rangle + \\eta \\langle \\pi_{k-1}, \\sum_{j=0}^{k-2} \\hat q_j \\rangle - F(\\pi_{k-1}) + F(\\pi_{k-1}) \\\\ &amp; \\ge \\eta \\langle \\pi_{k-1}, \\hat q_{k-1} \\rangle + \\eta \\langle \\pi_{k-2}, \\sum_{j=0}^{k-2} \\hat q_j \\rangle - F(\\pi_{k-2}) + F(\\pi_{k-1}) \\\\ &amp; \\ge \\eta \\sum_{j=0}^{k-1} \\langle \\pi_{j}, \\hat q_j \\rangle - F(\\pi_0) + F(\\pi_{k-1}) \\\\ &amp; \\ge \\eta \\sum_{j=0}^{k-1} \\langle \\pi_{j}, \\hat q_j \\rangle \\\\ \\end{align*}\\] where the first inequality holds since by \\eqref{eq:ftrlpolicy} we know that . \\[\\pi_{k-1} = \\arg\\max_{\\pi \\in \\mathcal{M}_1(A)}\\,\\,\\eta \\langle \\pi, \\sum_{j=0}^{k-2} \\hat q_{j} \\rangle - F(\\pi).\\] The second inequality holds by repeatadly apply the first two steps. The third inequality holds since $\\pi_0$ was initialized as \\(\\pi_0 = \\arg\\min_{\\pi \\in \\mathcal{M}_1(A)} F(\\pi)\\) so we have that $F(\\pi_{k-1}) - F(\\pi_0) \\ge 0$. Which concludes the argument. Online convex optimization, online learning . Online linear optimization is a special case of online convex/concave optimization, where the learner chooses elements of some nonempty convex set \\(\\mathcal{X}\\subset \\mathbb{R}^d\\) and the adversary needs to choose an element of a nonempty set \\(\\mathcal{Y}\\) of concave functions over \\(\\mathcal{X}\\): \\(\\mathcal{Y} \\subset \\{ f: \\mathcal{X} \\to \\mathbb{R}\\,:\\, f \\text{ is concave} \\}\\). Then, the definition of regret is changed to . \\[\\begin{align} R_k = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} y_j(x) - y_j(x_j) \\,, \\label{eq:regretdefoco1} \\end{align}\\] where as before \\(x_j\\in \\mathcal{X}\\) is the choice of the learner for round \\(j\\) and \\(y_j\\in \\mathcal{Y}\\) is the choice of the adversary for the same round. Identifying any vector \\(u\\) of \\(\\mathbb{R}^d\\) with the linear map \\(x \\mapsto \\langle x, u \\rangle\\), we see that online linear optimization is a special case of this problem. Of course, by negating all functions in \\(\\mathcal{Y}\\) (i.e., letting \\(\\tilde {\\mathcal{Y}} = \\{ - y \\,:\\, y\\in \\mathcal{Y} \\}\\)) and redefining the regret to . \\[\\begin{align} R_k = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} \\tilde y_j(x_j)- \\tilde y_j(x) \\, \\label{eq:regretdefoco} \\end{align}\\] we get a definition that is used in the literature, which prefers the convex case to the concave. Here, the interpretation is that \\(\\tilde y_j\\in \\tilde {\\mathcal{Y}}\\) is a “loss function” chosen by the adversary in round \\(j\\). The standard function notation (\\(y_j\\) is applied to \\(x\\)) injects unwarranted asymmetry in the notation. After all, from the perspective of the learner, they need to choose a value in \\(\\mathcal{X}\\) that works for the various functions in \\(\\mathcal{Y}\\). Thus, we can consider any element of \\(\\mathcal{X}\\) as a function that maps elements of \\(\\mathcal{Y}\\) to reals through \\(y \\mapsto y(x)\\). Whether \\(\\mathcal{Y}\\) has functions in them or \\(\\mathcal{X}\\) has functions in them does not matter that much; it is the interconnection between \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) that matters more. For this reason, one can study online learning when \\(y(x)\\) above is replaced by \\(b(x,y)\\), where \\(b: \\mathcal{X}\\times \\mathcal{Y} \\to \\mathbb{R}\\) is a specific map that assigns payoffs to every pair of points in \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). When the map is fixed, one can spare an extra symbol by just using \\([x,y]\\) in place of \\(b(x,y)\\), which makes things almost a full circle given that we started with the linear case when \\([x,y] = \\langle x,y \\rangle\\). Truncation or no truncation? . We introduced truncation to simplify the analysis. The proof can be made to go through even without it, with a mild increase of the suboptimality gap (or runtime). The advantage of removing the projection is that without projection, \\(\\hat q_0 + \\dots + \\hat q_{j-1} = \\Phi (\\hat \\theta_0 + \\dots + \\hat \\theta_{j-1})\\), which leads to a practically significant reduction of the runtime. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec14/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec14/#notes"
  },"79": {
    "doc": "14. Politex",
    "title": "References",
    "content": "The optimality of the final policy presented in the Notes was shown by Tadashi Kozuno when he taugh this lecture in Winter 2022. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec14/#references",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec14/#references"
  },"80": {
    "doc": "14. Politex",
    "title": "14. Politex",
    "content": "PDF Version . The following lemma can be extracted from the calculations found at the end of the last lecture: . Lemma (Mixture policy suboptimality): Fix an MDP $M$. For any sequence \\(\\pi_0,\\dots,\\pi_{k-1}\\) of policies, any sequence $\\hat q_0,\\dots,\\hat q_{k-1}: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}$ of functions, and any policy \\(\\pi^*\\), the mixture policy $\\bar \\pi_k = 1/k(\\pi_0+\\dots+\\pi_{k-1})$ satisfies . \\[\\begin{align} v^{\\pi^*} - v^{\\bar \\pi_k} &amp; \\le \\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\underbrace{ \\sum_{j=0}^{k-1} M_{\\pi^*} \\hat q_j - M_{\\pi_j} \\hat q_j}_{T_1} + \\frac{2 \\max_{0\\le j \\le k-1}\\| q^{\\pi_j}-\\hat q_j\\|_\\infty }{1-\\gamma} \\,. \\label{eq:polsubgapgen} \\end{align}\\] . In particular, the only restriction is on policy $\\pi^*$ so far and that is that it has to be a memoryless policy. To control the suboptimality of the mixture policy, one just needs to control the action-value approximation errors \\(\\| q^{\\pi_j}-\\hat q_j\\|_\\infty\\) and the term $T_1$ and for this we are free to choose the policies \\(\\pi_0,\\dots,\\pi_{k-1}\\) in any way we want them to be chosen. To help with this choice, let us now inspect \\(T_1(s)\\) for a fixed state $s$: . \\[\\begin{align} T_1(s) = \\sum_{j=0}^{k-1} \\langle \\pi^*(s,\\cdot),\\hat q_j(s,\\cdot)\\rangle - \\langle \\pi_j(s,\\cdot),\\hat q_j(s,\\cdot)\\rangle \\,, \\label{eq:t1s} \\end{align}\\] where, abusing notation, we use \\(\\pi(s,a)\\) for \\(\\pi(a|s)\\). Now, recall that \\(\\hat q_j\\) will be computed based on \\(\\pi_j\\) while \\(\\pi^*\\) is unknown. One must thus wonder whether it is possible to control this term? . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec14/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec14/"
  },"81": {
    "doc": "15. From policy search to policy gradients",
    "title": "Policy search",
    "content": "A reasonable goal then is to ask for a planner that competes with the best policy within the parameterized family, or the $\\varepsilon$-best policy for some positive $\\varepsilon$. Since there may not be a parameter $\\theta$ such that $v^{\\pi_\\theta}\\ge v^{\\pi_{\\theta’}}-\\varepsilon\\boldsymbol{1}$ for any $\\theta’\\in \\mathbb{R}^d$, we simplify the problem by requiring that the policy computed is nearly best when started from some initial distribution $\\mu \\in \\mathcal{M}_1(\\mathcal{S})$. Defining $J: \\text{ML} \\to \\mathbb{R}$ as . \\[J(\\pi) = \\mu v^{\\pi} (=\\sum_{s\\in \\mathcal{S}}\\mu(s)v^{\\pi}(s)),\\] the policy search problem is to find a parameter $\\theta\\in \\mathbb{R}^d$ such that . \\[\\begin{align*} J(\\pi_{\\theta}) = \\max_{\\theta'} J(\\pi_{\\theta'})\\,. \\end{align*}\\] The approximation version of the problem asks for finding $\\theta’\\in \\mathbb{R}^d$ such that . \\[\\begin{align*} J(\\pi_{\\theta}) \\ge \\max_{\\theta'} J(\\pi_{\\theta'}) - \\varepsilon\\,. \\end{align*}\\] The formal problem definition then is as follows: a planning algorithm is given the MDP $M$ and a policy parameterization $(\\pi_\\theta)_{\\theta}$ and we are asking for an algorithm that returns the solution to the policy search problem in time polynomial in the number of actions $\\mathrm{A}$ and the number of parameters $d$ that describes the policy. An even simpler problem is when the MDP has finitely many states, and the algorithm needs to run in polynomial time in $\\mathrm{S}$, $\\mathrm{A}$ and $d$. In this case, it is clearly advantageous for the algorithm if it is given the exact description of the MDP (as described in Lecture 3) Sadly, even this mild version of policy search is intractable. Theorem (Policy search hardness): Unless $\\text{P}=\\text{NP}$, there is no polynomial time algorithm for the finite policy search problem even when the policy space is restricted to the constant policies and the MDPs are restricted to be deterministic with binary rewards. The constant policies are those that assign the same probability distribution to each state. This is a special case of state aggregation when all the states are aggregated into a single class. As the policy does not depend on the state, the problem is also known as the blind policy search problem. Note that the result holds regardless of the representation used to express the set of constant policies. Proof: Let $\\mathcal{S} = \\mathcal{A}=[n]$. The dynamics is deterministic: The next state is $a$ if action $a\\in \\mathcal{A}$ is taken regardless of the state. A policy is simply a probability distribution \\(\\pi \\in \\mathcal{M}_1([n])\\) over the action space, which we shall view as a column vector taking values in $[0,1]^n$. The transition matrix of $\\pi$ is $P_{\\pi}(s,s’) = \\pi(s’)$, or, in matrix form, $P_\\pi = \\boldsymbol{1} \\pi^\\top$. Clearly, $P_\\pi^2 = \\boldsymbol{1} \\pi^\\top \\boldsymbol{1} \\pi^\\top = P_\\pi$ (i.e., $P_\\pi$ is idempotent). Thus, $P_\\pi^t = \\boldsymbol{1}\\pi^\\top$ for any $t&gt;0$ and hence . \\[\\begin{align*} J(\\pi) &amp; = \\mu (r_\\pi + \\sum_{t\\ge 1} \\gamma^t P_\\pi^t r_\\pi) = \\mu \\left(I + \\frac{\\gamma}{1-\\gamma} \\boldsymbol{1} \\pi^\\top \\right)r_\\pi\\,. \\end{align*}\\] Defining $R_{s,a} = r_a(s)$ so that $R\\in [0,1]^{n\\times n}$, we have $r_\\pi = R\\pi$. Plugging this in into the previous displayed equation and using that $\\mu \\boldsymbol{1}=1$, we get . \\[\\begin{align*} J(\\pi) &amp; = \\mu R \\pi + \\frac{\\gamma}{1-\\gamma} \\pi^\\top R \\pi\\,. \\end{align*}\\] Thus we see that the policy search problem is equivalent to maximizing the quadratic expression in the previous display over the probability simplex. Since there is no restriction on $R$, one may at this point conjecture that this will be hard to do. That this is indeed the case can be shown by a reduction to the maximum independent set problem, which asks for checking whether the independence number of a graph is above a threshold and which is known to be NP-hard even for $3$-regular graphs (i.e., graphs where every vertex has exactly three neighbours). Here, the independence number of a graph is defined as follows: We are given a simple graph $G=(V,E)$ (i.e., there are no self-loops, no double edges, and the graph is undirected). An independent set in $G$ is a neighbour-free subset of vertices. The independence number of $G$ is defined as . \\[\\begin{align*} \\alpha(G) = \\max \\{ |V'| \\,:\\, V'\\subset \\text{ independent in } G \\}\\,. \\end{align*}\\] Quadratic optimization has close ties to the maximum independent set problem: . Lemma (Motzkin-Strauss ‘65): Let \\(G\\in \\{0,1\\}^n\\) be the vertex-vertex adjacency matrix of simple graph (i.e., $G_{ij}=1$ if and only if $(i,j)$ is an edge of the graph). Then, for \\(I\\in \\{0,1\\}^{n\\times n}\\) the $n\\times n$ identity matrix, . \\[\\begin{align*} \\frac{1}{\\alpha(G)} = \\min_{y\\in \\mathcal{M}_1([n])} y^\\top (G+I) y\\,. \\end{align*}\\] . We now show that if there is an algorithm that solves policy search in polynomial time then it can also be used to solve the maximum independent set problem for simple, $3$-regular graphs. For this pick a $3$-regular graph $G$ with $n$ vertices. Define the MDP as above with $n$ states and actions and the rewards chosen so that $R = E-(I+G)$ where $G$ is the vertex-vertex adjacency matrix of the graph and $E$ is the all-ones matrix: $E = \\boldsymbol{1} \\boldsymbol{1}^\\top$. We add $E$ so that the rewards are in the $[0,1]$ interval and in fact are binary as required. Choose $\\mu$ as the uniform distribution over the states. Note that $\\boldsymbol{1}^\\top (I+G) = 4 \\boldsymbol{1}^\\top$ because the graph is $3$-regular. Then, for $\\pi \\in \\mathcal{M}_1(\\mathcal{A})$, . \\[\\begin{align*} J(\\pi) &amp; = \\frac{1}{1-\\gamma}- \\mu(E+I+G) \\pi - \\frac{\\gamma}{1-\\gamma} \\pi^\\top (E+I+G) \\pi \\\\ &amp; = \\frac{1}{1-\\gamma}- \\frac{1}{n} \\boldsymbol{1}^\\top (I+G) \\pi - \\frac{\\gamma}{1-\\gamma} \\pi^\\top (I+G) \\pi \\\\ &amp; = \\frac{1}{1-\\gamma}- \\frac{4}{n} - \\frac{\\gamma}{1-\\gamma} \\pi^\\top (I+G) \\pi\\,. \\end{align*}\\] Hence, \\(\\begin{align*} \\max_{\\pi \\in \\mathcal{M}_1([n]} J(\\pi) &amp; = \\frac{1}{1-\\gamma}- \\frac{4}{n} - \\frac{\\gamma}{1-\\gamma} \\frac{1}{\\alpha(G)} \\ge \\frac{1}{1-\\gamma}- \\frac{4}{n} - \\frac{\\gamma}{1-\\gamma} \\frac{1}{m} \\end{align*}\\) holds if and only if $\\alpha(G)\\ge m$. Thus, the decision problem of deciding that $J(\\pi)\\ge a$ is at least as hard as the maximum independent set problem. As noted, this is an NP-hard problem, hence the result follows. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec15/#policy-search",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec15/#policy-search"
  },"82": {
    "doc": "15. From policy search to policy gradients",
    "title": "Potential remedy: Local search",
    "content": "Based on the theorem just proved it is not very likely that we can find computationally efficient planners to compete with the best policy in a restricted policy class, even if the class looks quite benign. This motivates aiming at some more modest goal, one possibility of which is to compute local maxima of the map $J:\\pi \\mapsto \\mu v^{\\pi}$. Let \\(\\Pi = \\{ \\pi_\\theta \\,:\\, \\theta\\in \\mathbb{R}^d \\} \\subset [0,1]^{\\mathcal{S}\\times\\mathcal{A}}\\) be the set of policies that can represented; we view these now as “large vectors”. Then, in this approach we aim to identify \\(\\pi^*\\in \\Pi\\) (and its parameters) so that for any $\\pi’\\in \\Pi$ and small enough $\\delta&gt;0$ so that \\(\\pi^*+\\delta (\\pi'-\\pi^*)\\in \\Pi\\), \\(J(\\pi^*+\\delta (\\pi'-\\pi^*))\\le J(\\pi^*)\\). For $\\delta$ small, \\(J(\\pi^*+\\delta (\\pi'-\\pi^*))\\approx J(\\pi^*) + \\delta \\langle J'(\\pi^*), \\pi'- \\pi^* \\rangle\\). Plugging this in into the previous inequality, reordering and dividing by $\\delta&gt;0$ gives . \\[\\begin{align} \\langle J'(\\pi^*), \\pi'- \\pi^* \\rangle \\le 0\\,, \\qquad \\pi' \\in \\Pi\\,. \\label{eq:stp} \\end{align}\\] Here, $J’(\\pi)$ denotes the derivative of $J$. What remains to be seen is whether (1) relaxing the goal to computing \\(\\pi^*\\) helps with the computation (and when) and (2) whether we can get some guarantees for how well $\\pi^*$ satisfying \\eqref{eq:stp} will do compared to \\(J^* = \\max_{\\pi\\in \\Pi} J(\\pi)\\), that is obtaining some approximation guarantees. For the latter we seek for some function $\\varepsilon$ of the MDP $M$ and $\\Pi$ (or $\\varphi$, when $\\Pi$ is based on some featuremap) so that . \\[\\begin{align*} J(\\pi^*) \\ge J^* - \\varepsilon(M,\\Pi) \\end{align*}\\] As to the computational approaches, we will consider a simple approach based on (approximately) following the gradient of $\\theta \\mapsto J(\\pi_\\theta)$. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec15/#potential-remedy-local-search",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec15/#potential-remedy-local-search"
  },"83": {
    "doc": "15. From policy search to policy gradients",
    "title": "Notes",
    "content": "Access models . The reader may be wondering about what is the appropriate “access model” when $\\pi_\\theta$ is not restricted to the form given in \\eqref{eq:boltzmannpp}. There are many possibilities. One is to develop planners for specific parametric forms. A more general approach is to let the planner access \\(\\pi_{\\theta}(\\cdot\\vert s)\\) and $\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(\\cdot \\vert s)$ for any $s$ it has encountered and any value of $\\theta\\in \\mathbb{R}^d$ it chooses. This is akin to the first-order black-box oracle model familiar from optimization theory. From function approximation to POMDPs . The hardness result for policy search is taken from a paper of Vlassis, Littman and Barber, who actually were interested in the computational complexity of planning in partially observable Markov Decision Problems (POMDPs). It is in fact an important observation that with function approximation, planning in MDPs becomes a special case of planning in POMDPs: In particular, if policies are restricted to depend on the states through a feature-map $\\varphi:\\mathcal{S}\\to \\mathbb{R}^d$ (any two states with identical features will get the same action distribution assigned to them), then planning to achieve high reward with this restricted class is almost the same as planning to achieve high reward in a partially observable MDP where the observation function is $\\varphi$. Planners for the former problem could still have some advantage though if they can also access the states: In particular, an online planner which is given a feature-map to help its search but is also given access to the states is in fact not restricted to return actions whose distribution follows a policy from the feature-restricted class of policies. In machine learning, in the analogue problem of competing with a best predictor within a class but using predictors that do not respect the restrictions put on the competitors are called improper and it is known that improper learning is often more powerful than proper learning. However, when it comes to learning online or in a batch fashion then feature-restricted learning and learning in POMDPs become exact analogs. Finally, we note in passing that Vlassis et al. (2012) also add an argument that shows that it is not likely that policy search is in NP. Open problem: Hardness of approximate policy search . Provided that from an approximate solution to the Motzkin-Straus problem one can efficiently extract an approximate solution to the maximum independent set problem, it follows that the approximate version of policy search is also NP-hard. In particular, it is not hard to see with the same construction that if one has an efficient method to find a policy with $J(\\pi) \\ge \\max_\\pi J_\\pi - \\varepsilon$ then this gives an efficient method to find an independent set of size $c\\alpha(G)$ for the said $3$-regular graphs where . \\[c = \\frac{1}{1 + \\frac{1-\\gamma}{\\gamma} \\varepsilon \\alpha(G)} \\ge \\frac{1}{1+ \\frac{1-\\gamma}{\\gamma} \\varepsilon n} \\ge 94/95 \\,,\\] where the last inequality follows if $\\varepsilon\\le 0.5$, $\\gamma\\ge 0.5$ and $ H:=\\frac{1}{1-\\gamma} \\ge \\frac{n}{95/94-1} = 94 n$ holds. Now, it is known that, unless P=NP, there is no polynomial time approximation algorithm for the maximal independent set problem with approximation factor $c=94/95$ or better. Hence, we get that, unless P=NP, there is no polynomial time approximation algorithm for the policy search problem for any fixed $0\\le \\epsilon\\le 0.5$ provided the planning horizon is scaled with $n$ so that $H = \\mathrm{const} n$. (This is somewhat unsatisfactory given that the range of the optimal values is $1/(1-\\gamma)$: It would be more natural to scale $\\epsilon$ with $1/(1-\\gamma)$, i.e., consider relative errors as in complexity theory.) Also, it remains an open problem to get a hardness result for a “constant” $\\gamma$ (independent of $n$). The above is still dependent on whether an approximate solution to the maximum independent set problem can be extracted from an approximate solution to the Motzkin-Straus optimization problem. Dealing with large action spaces . A common reason to consider policy search is because working with a restricted parametric family of policies holds the promise of decoupling the computational cost of learning and planning from the cardinality of the action-space. Indeed, with action-value functions, one usually needs an efficient way of computing greedy actions (with respect to some fixed action-value function). Computing $\\arg\\max_{a\\in \\mathcal{A}} q(s,a)$ in the lack of extra structure of the action-space and the function $q(s,\\cdot)$ takes linear time in the size of $\\mathcal{A}$, which is highly problematic unless $\\mathcal{A}$ has a small cardinality. In many applications of practical interest this is not the case: The action space can be “combinatorially sized”, or even a subset of some (potentially multidimensional) continuous space. If sampling from $\\pi_{\\theta}(\\cdot\\vert s)$ can be done efficiently, one may then potentially avoid the above expensive calculation. Thus, policy search is often proposed as a remedy to extend algorithms to work with large action spaces. Of course, this only applies if the sampling problem can indeed be efficiently implemented, which adds an extra restriction on the policy representation. Nevertheless, there are a number of options to achieve this: One can use for example an implicit representation (perhaps in conjunction with a direct one that uses probabilities/densities) for the policy. For example, the policy may be “represented” as a map $f_\\theta: \\mathcal{S} \\times \\mathcal{R} \\to \\mathcal{A}$ so that sampling from $\\pi_\\theta(\\cdot\\vert s)$ is accomplished by drawing a sample $R\\sim P$ from a fixed distribution over the set $\\mathcal{R}$ and then returning $f(s,R)\\in \\mathcal{A}$. Clearly, this is efficient as long as $f_\\theta$ can be efficiently evaluated at any of its inputs and the random value $R$ can be efficiently produced. If $f_\\theta$ is sufficiently flexible, one can in fact choose a very simple distribution for $P$, such as the standard normal distribution, or the uniform distribution. Note that when $\\mathcal{A}$ is continuous and the policies are deterministic is a special case: The key is still to be able to efficiently produce a sample from $\\pi_\\theta(\\cdot\\vert s)$, just in this case this means a deterministic computation. The catch is that one may also still need the derivatives of $\\pi_{\\theta}(\\cdot\\vert s)$ with respect to the parameter $\\theta$ and with an implicit representation as described above, it is unclear whether these derivatives can be efficiently obtained. As it turns out, this can be arranged if $f_{\\theta}(\\cdot\\vert s)$ is made of composition of elementary (invertible, differentiable) transformations with this property (by the chain rule). This observation is the basis of various approaches to “neural” density estimation (e.g., Tabak and Vanden-Eijnden, 2010, Rezende, Mohamed, 2015, or Jaini et al. 2019). ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec15/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec15/#notes"
  },"84": {
    "doc": "15. From policy search to policy gradients",
    "title": "References",
    "content": ". | Vlassis, Nikos, Michael L. Littman, and David Barber. 2012. “On the Computational Complexity of Stochastic Controller Optimization in POMDPs.” ACM Trans. Comput. Theory, 12, 4 (4): 1–8. | Esteban G. Tabak. Eric Vanden-Eijnden. “Density estimation by dual ascent of the log-likelihood.” Commun. Math. Sci. 8 (1) 217 - 233, March 2010. | Rezende, Danilo Jimenez, and Shakir Mohamed. 2015. “Variational Inference with Normalizing Flows” link. | Rezende, D. J., and S. Mohamed. 2014. “Stochastic Backpropagation and Approximate Inference in Deep Generative Models.” ICML. link. | Jaini, Priyank, Kira A. Selby, and Yaoliang Yu. 2019. “Sum-of-Squares Polynomial Flow.” In Proceedings of the 36th International Conference on Machine Learning, edited by Kamalika Chaudhuri and Ruslan Salakhutdinov, 97:3009–18. Proceedings of Machine Learning Research. PMLR. | Arora, Sanjeev, and Boaz Barak. 2009. Computational Complexity. A Modern Approach. Cambridge: Cambridge University Press. | . The hardness of the maximum independent set problem is a classic result; see, e.g., Theorem 2.15 in the book of Arora and Barak (2009) above, though this proof does not show that the hardness also applies to the case of 3-regular graphs. Below is the paper that shows that approximating the maximum independent set size within a factor of $94/95=0.9894\\dots$ is NP-hard even for $3$-regular graphs. The precise statement is in the main theorem statement on page 29 (this is the first, unnumbered and unnamed theorem on pdf page 3). In particular, the 2nd bullet point has this bound, specifically the hardness kicks in for approximation factors at least as large as $94/95$. I am very grateful for Zachary Friggstad who pointed me to this paper. | Miroslav Chlebík, Janka Chlebíková: Inapproximability Results for Bounded Variants of Optimization Problems. FCT 2003: 27-38 DBLP page | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec15/#references",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec15/#references"
  },"85": {
    "doc": "15. From policy search to policy gradients",
    "title": "15. From policy search to policy gradients",
    "content": "PDF Version . In the previous lectures we attempted to reduce the complexity of planning by assuming that value functions over the large state-action spaces can be compactly represented with a few parameters. While value-functions are an indispensable component of poly-time MDP planners (see Lectures 3 and 4), it is far from clear whether they should also be given priority when working with larger MDPs. Indeed, perhaps it is more natural to consider sets of policies with a compact description. Formally, in this problem setting the planner will be given a black-box simulation access to a (say, $\\gamma$-discounted) MDP $M=(\\mathcal{S},\\mathcal{A},P,r)$ as before, but the interface also provides access to a parameterized family of policies over $(\\mathcal{S},\\mathcal{A})$, \\(\\pi = (\\pi_\\theta)_{\\theta\\in \\mathbb{R}^d}\\), where for any fixed parameter $\\theta\\in \\mathbb{R}^d$, $\\pi_\\theta$ is a memoryless stochastic policy: $\\pi_\\theta:\\mathcal{S} \\to \\mathcal{M}_1(\\mathcal{A})$. For example, $\\pi_\\theta$ could be such that for some feature-map $\\varphi: \\mathcal{S}\\times \\mathcal{A} \\to \\mathcal{R}^d$, . \\[\\begin{align} \\pi_\\theta(a|s) = \\frac{\\exp( \\theta^\\top \\varphi(s,a))}{\\sum_{a'} \\exp(\\theta^\\top \\varphi(s,a'))}\\,, \\qquad (s,a)\\in \\mathcal{S}\\times \\mathcal{A}\\,. \\label{eq:boltzmannpp} \\end{align}\\] In this case “access” to $\\pi_\\theta$ means access to $\\varphi$, which can be either global (i.e., the planner is given the “whole” of $\\varphi$ and can run any preprocessing on it), or local (i.e., $\\varphi(s’,a)$ is returned by the simulator for the “next states” $s’\\in \\mathcal{S}$ and for all actions $a$). Of course, the exponential function can be replaced with other functions, or, one can just use a neural network to output “scores”, which are turned into probabilities in some way. Dispensing with stochastic policies, a narrower class is the class of policies that are greedy with respect to action-value functions that belong to some parametric class. One special case that is worthy of attention due to its simplicity is the case when $\\mathcal{S}$ is partitioned into $m$ (disjoint) subsets $\\mathcal{S}_1,\\dots,\\mathcal{S}_m$ and for $i\\in [m]$, we have $\\mathrm{A}$ basis functions defined as follows: . \\[\\begin{align} \\varphi_{i,a'}(s,a) = \\mathbb{I}( s\\in \\mathcal{S}_i, a= a' )\\,, \\qquad s\\in \\mathcal{S}, a,a'\\in \\mathcal{A}, i\\in [m]\\,. \\label{eq:stateagg} \\end{align}\\] Here, to minimize clutter, we allow the basis functions to be indexed by pairs and identified $\\mathcal{A}$ with ${ 1,\\dots,\\mathrm{A}}$, as usual. Then, the policies are given by $\\theta = (\\theta_1,\\dots,\\theta_m)$, the collection of $m$ probability vectors $\\theta_1,\\dots,\\theta_m\\in \\mathcal{M}_1(\\mathcal{A})$: . \\[\\begin{align} \\pi_\\theta(a|s) = \\sum_{i=1}^m \\sum_{a'} \\varphi_{i,a'}\\theta_{i,a'}\\,. \\label{eq:directpp} \\end{align}\\] Note that because of the special choice of $\\varphi$, $\\pi_{\\theta}(a|s) = \\theta_{i,a}$ for the unique index $i\\in [m]$ such that $s\\in \\mathcal{S}_i$. This is known as state-aggregretion: States belonging to the same group give rise to the same probability distribution over the actions. We say that the featuremap $\\varphi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ is of the state-aggregation type if it takes the form \\eqref{eq:stateagg} with an appropriate reindexing of the basis functions. Fix now a state-aggregation type featuremap. We can consider both the direct parameterization of policies given in \\eqref{eq:directpp}, or the “Boltzmann” parameterization given in \\eqref{eq:boltzmannpp}. As it is easy to see the set of possible policies that can be expressed with the two parameterizations are nearly identical. Letting $\\Pi_{\\text{direct}}$ be the set of policies that can be expressed using $\\varphi$ and the direct parameterization and letting $\\Pi_{\\text{Boltzmann}}$ be the set of policies that can be expressed using $\\varphi$ but with the Boltzmann parameterization, first note that \\(\\Pi_{\\text{direct}},\\Pi_{\\text{Boltzmann}} \\subset \\mathcal{M}_1(\\mathcal{A})^{\\mathcal{S}} \\subset ([0,1]^{\\mathrm{A}})^{\\mathrm{S}}\\), and if we take the closure, $\\text{clo}(\\Pi_{\\text{Boltzmann}})$ of $\\Pi_{\\text{Boltzmann}}$ then we can notice that . \\[\\text{clo}(\\Pi_{\\text{Boltzmann}}) = \\Pi_{\\text{direct}}\\,.\\] In particular, the Boltzmann policies cannot express point-mass distributions with finite parameters, but letting the parameter vectors grow without bound, any policy that can be expressed with the direct parameterization can also be expressed by the Boltzmann parameterization. There are many other possible parameterizations, as also mentioned earlier. The important point to notice is that while the parameterization is necessary so that the algorithms can work with a compressed representation, different representations may describe an identical set of policies. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec15/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec15/"
  },"86": {
    "doc": "16. Policy gradients",
    "title": "The policy gradient theorem",
    "content": "Fix an MDP $M=(\\mathcal{S},\\mathcal{A},P,r)$ and a discount factor $0\\le \\gamma &lt;1$. Continuing from the last lecture for $\\theta\\in \\mathbb{R}^d$ let $\\pi_\\theta$ be a stochastic policy: $\\pi_\\theta:\\mathcal{S}\\to \\mathcal{M}_1(\\mathcal{A})$. Further, fix a distribution $\\mu \\in \\mathcal{M}_1(\\mathcal{S})$ over the states and for a policy $\\pi:\\mathcal{S}\\to \\mathcal{M}_1(\\mathcal{A})$ let . \\[J(\\pi) = \\mu v^\\pi\\] denote the expected value of using policy $\\pi$ in $M$ from an initial state randomly chosen from $\\mu$. The policy gradient theorem gives sufficient conditions under which the map $\\theta \\mapsto J(\\pi_\\theta)$ is differentiable at some parameter $\\theta=\\theta_0$ and gives a “simple” expression for the gradient as a function of $\\frac{d}{dx} M_{\\pi_x} q^{\\pi_{\\theta_0}}$. Just to demistify this, for finite (or discrete) action spaces, for a memoryless policy $\\pi$ and function $q:\\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}} \\to \\mathbb{R}$, $M_{\\pi} q$ is a function mapping states to reals defined via . \\[(M_{\\pi} q)(s) = \\sum_a \\pi(a\\vert s) q(s,a)\\,.\\] Hence, the derivative, $\\frac{d}{dx} M_{\\pi_x} q$ is actually quite simple. It is a function mapping states to $d$ dimensional vectors which satisfies . \\[\\frac{d}{dx} (M_{\\pi_x} q)(s) = \\sum_a \\frac{d}{dx}\\pi_x(a\\vert s) q(s,a)\\,.\\] The theorem we give though is not limited to this case and also applies to when the action space is infinite and even when the policy is deterministic. For the theorem statement, recall that for a policy $\\pi$ we used $\\tilde \\nu_\\mu^\\pi$ to denote its discounted state occupancy measure. Also, for a function $f$, we use $f’$ to denote its derivative. Theorem (Policy Gradient Theorem): Fix an MDP $(\\mathcal{S},\\mathcal{A},P,r)$. For $x\\in \\mathbb{R}^d$, define the maps $f_\\pi: x\\mapsto \\tilde\\nu_\\mu^\\pi M_{\\pi_x} q^\\pi$ and $g_\\pi: x \\mapsto \\tilde \\nu_\\mu^{\\pi_x} v^\\pi$. Fix $\\theta_0\\in \\mathbb{R}^d$. Assume that at least one of the following two conditions is met: . | $\\theta \\mapsto f_{\\pi_\\theta}’(\\theta_0)$ exists and is continuous in a neighborhood of $\\theta_0$ and $g_{\\pi_{\\theta_0}}’(\\theta_0)$ exists; | $\\theta \\mapsto g_{\\pi_\\theta}’(\\theta_0)$ exists and is continuous in a neighborhood of $\\theta_0$ and $f_{\\pi_{\\theta_0}}’(\\theta_0)$ exists; | . Then, $x\\mapsto J(\\pi_x)$ is differentiable at $x=\\theta_0$ and . \\[\\begin{align} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} &amp;= \\frac{d}{dx} \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} M_{\\pi_x} q^{\\pi_{\\theta_0}}|_{x=\\theta_0} = \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} \\frac{d}{dx} M_{\\pi_x} q^{\\pi_{\\theta_0}}|_{x=\\theta_0}\\,, \\label{eq:pgt} \\end{align}\\] where the last equality holds if $\\mathcal{S}$ is finite. For the second expression, we treat $\\frac{d}{dx} M_{\\pi_x} q^{\\pi_{\\theta_0}}$ as an $\\mathrm{S}\\times d$ matrix. Note that this fits well with our convention of treating functions as “column vectors” (hence $M_{\\pi_x}) q^{\\pi_{\\theta_0}}$ is a vector of dimension $\\mathrm{S}$) and with the standard convention that a “vector derivative” creates “row vectors”. Above, the second expression where we moved the derivative with respect to the parameter inside the expression will only be valid in infinite state spaces when some additional regularity assumption is met. One such assumption is that \\(s\\mapsto\\|\\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s)|_{x=\\theta_0}\\|\\) is $\\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}}$-integrable. In words, the theorem shows that the derivative of the performance of a policy can be obtained by integrating a simple derivative that involves the action-value function of the policy. Of the two conditions of the theorem, the first condition is the one that is generally easier to verify. In particular, the condition on the continuous differentiability of $x\\mapsto f_{\\pi_x}$ at $x=\\theta_0$ is usually easy to verify. To show the differentiability of $x\\mapsto g_{\\pi_x}$ at $x=\\theta_0$ just recall that if the partial derivatives of a function exist and are continuous the function is differentiable. Then recall that $\\tilde \\nu_\\mu^{\\pi_x} v = \\sum_{t=0}^\\infty \\gamma^t \\nu P_{\\pi_x}^t v$ and hence its differentiability with respect to (say) $x_1$ follows if $x\\mapsto \\nu M_{\\pi_x} P v$ is continuously differentiable at $x=\\theta_0$. In effect, for finite state-action spaces, differentiability at $\\theta_0$ follows (and the conditions of the theorem are satisfied) as long as for any $(s,a)$ state-action pair, the maps $x\\mapsto \\pi_x(a|s)$ have continuous partial derivatives at $x=\\theta_0$. Proof: The proof is based on a short calculation that starts with writing the value difference identity for $v^{\\pi_x}-v^{\\pi_{\\theta_0}}$, multiplied from the right by $\\mu$, taking derivatives and then letting $x=\\theta_0$. The details are as follows: Recall from Calculus 101 the following result: Assume that $f = f(u,v)$ satisfies at least one of the two conditions: . | \\(z\\mapsto \\frac{\\partial}{\\partial v} f(z,x)\\) exists and is continuous in a neighborhood of $z=x$ and \\(\\frac{\\partial}{\\partial u} f(u,x)\\vert_{u=x}\\) exists; | \\(z\\mapsto \\frac{\\partial}{\\partial u} f(x,z)\\) exists and is continuous in a neighborhood of $z=x$ and \\(\\frac{\\partial}{\\partial v} f(x,v)\\vert_{v=x}\\) exists. | . Then $z \\mapsto f(z,z)$ is differentiable at $z=x$ and . \\[\\begin{align} \\frac{d}{dx} f(x,x) = \\frac{\\partial}{\\partial u} f(x,x) + \\frac{\\partial}{\\partial v} f(x,x)\\,. \\label{eq:c101} \\end{align}\\] Let $\\pi’,\\pi$ be two memoryless policies. By the value difference identity, . \\[\\begin{align*} v^{\\pi'}- v^{\\pi} &amp; = (I-\\gamma P_{\\pi'})^{-1} [ T_{\\pi'} v^\\pi - v^\\pi] \\\\ &amp; = (I-\\gamma P_{\\pi'})^{-1} [ M_{\\pi'} q^\\pi - v^\\pi]\\,, \\end{align*}\\] where the last equality just used that that $T_{\\pi’} v^\\pi = M_{\\pi’} (r+\\gamma P v^\\pi) = M_{\\pi’} q^{\\pi}$. Now let $\\pi’ = \\pi_x$ and $\\pi = \\pi_{\\theta_0}$ and multiply the value difference identity from the left by $\\mu$ to get . \\[\\begin{align} \\mu( v^{\\pi_x}- v^{\\pi_{\\theta_0}}) = \\tilde \\nu_\\mu^{\\pi_x} [ M_{\\pi_x}q^{\\pi_{\\theta_0}} - v^{\\pi_{\\theta_0}}]\\,. \\label{eq:vdi2} \\end{align}\\] Now, focusing on the first term on the right-hand-side, let . \\[\\begin{align} f(u,v) = \\tilde \\nu_\\mu^{\\pi_u} M_{\\pi_v}q^{\\pi_{\\theta_0}}\\,. \\label{eq:fdef} \\end{align}\\] Provided that $f$ is sufficiently regular in a neighborhood of $(x,x)$ (to be discussed later), \\eqref{eq:c101} gives that . \\[\\begin{align*} \\frac{d}{dx} f(x,x) = \\frac{d}{du} \\tilde \\nu_\\mu^{\\pi_u} M_{\\pi_x}q^{\\pi_{\\theta_0}}|_{u=x} + \\frac{d}{dv} \\tilde \\nu_\\mu^{\\pi_x} M_{\\pi_v}q^{\\pi_{\\theta_0}}|_{v=x} \\end{align*}\\] Taking the derivative of both sides of \\eqref{eq:vdi2} with respect to $x$ and using the above display we get . \\[\\begin{align*} \\frac{d}{dx} J(x) = \\frac{d}{dx} \\mu( v^{\\pi_x}- v^{\\pi_{\\theta_0}}) = \\frac{d}{du} \\tilde \\nu_\\mu^{\\pi_u} M_{\\pi_x}q^{\\pi_{\\theta_0}}|_{u=x} + \\frac{d}{dv} \\tilde \\nu_\\mu^{\\pi_x} M_{\\pi_v}q^{\\pi_{\\theta_0}}|_{v=x} + \\frac{d}{dx} \\tilde \\nu_\\mu^{\\pi_x} v^{\\pi_{\\theta_0}}\\,. \\end{align*}\\] Now let $x = \\theta_0$. Then, $M_{\\pi_x}q^{\\pi_{\\theta_0}} = M_{\\pi_{\\theta_0}}q^{\\pi_{\\theta_0}} = v^{\\pi_{\\theta_0}}$. Hence, the first and the third term of the above display cancel each other and we get . \\[\\begin{align*} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} = \\frac{d}{dv} \\tilde \\nu_\\mu^{\\pi_{\\theta_0}} M_{\\pi_v} q^{\\pi_{\\theta_0}}|_{v=\\theta_0}\\,. \\end{align*}\\] Finally, the conditions to apply \\eqref{eq:c101} to our $f$ in \\eqref{eq:fdef} are met by our assumption on $f_\\pi$ and $g_\\pi$, finishing the proof. \\(\\qquad \\blacksquare\\) . When the action-space is discrete and $\\pi_\\theta$ are stochastic policies, we can further manipulate the expression we obtained. In particular, in this case . \\[\\begin{align*} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\pi_x(a\\vert s) q^{\\pi_{\\theta_0}}(s,a) \\end{align*}\\] and thus, for finite $\\mathcal{A}$, . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\frac{d}{dx} \\pi_x(a\\vert s) q^{\\pi_{\\theta_0}}(s,a)\\,. \\label{eq:basicpg} \\end{align}\\] While this can be used as the basis of evaluating (or approximating) gradient, it may be worthwhile to point out an alternate form which is available when $\\pi_x(a\\vert s)&gt;0$. In this case, using the chain rule we get . \\[\\begin{align*} \\frac{d}{dx} \\log \\pi_x(a\\vert s) = \\frac{\\frac{d}{dx} \\pi_x(a\\vert s)}{\\pi_x(a\\vert s)}\\,. \\end{align*}\\] Using this in \\eqref{eq:basicpg} we get . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) q^{\\pi_{\\theta_0}}(s,a)\\,, \\label{eq:basicpga} \\end{align}\\] which has the pleasant property that it takes the form of an expected value over the actions of the score function of the policy map correlated with the action-value function. Before moving on it is worth pointing out that an equivalent expression is obtained if $q^{\\pi_{\\theta_0}}(s,a)$ above is shifted by an arbitrary constant which may depend on $\\theta_0$ or $s$ but not $a$. Indeed, since $\\sum_a \\pi_x(a\\vert s) b(s,\\theta_0) = b(s,\\theta_0)$, differentiating both sides with respect to $x$ gives $\\sum_a \\frac{d}{dx}\\pi_x(a\\vert a) b(s,\\theta_0)=0$. Hence, we also have . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) (q^{\\pi_{\\theta_0}}(s,a)-b(s,\\theta_0))\\,. \\label{eq:basicpga2} \\end{align}\\] This may have significance when using simulation to evaluate derivatives: One may attempt to use an appropriate “bias” term to reduce the variance of the estimate of the gradient. Before discussing simulation any further, it may be also worthwhile to discuss what happens when the action-space is infinite. For countable infinite action spaces, the only difference is that \\eqref{eq:basicpg} may not always hold. An easy sufficient condition for this to hold is that \\(\\sum_{a} \\|\\frac{d}{dx} \\pi_x(a\\vert s)\\|\\, |q^{\\pi_{\\theta_0}}(s,a)|\\) is summable, or equivalently, \\(\\|\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\|\\, |q^{\\pi_{\\theta_0}}(s,a)|\\) is $\\pi_x(\\cdot\\vert s)$-summable/integrable. For uncountably infinite action spaces, this argument works with the minimal necessary changes. In the most general case, $\\pi_\\theta(\\cdot\\vert s)$ is a probability measure over $\\mathcal{A}$ and its derivative is a vector-valued measure. The formulae derived above (e.g., \\eqref{eq:basicpga2}) remain valid if we replace the sum with an integral when $\\pi_\\theta(\\cdot\\vert s)$ is given in the form of a density with respect to some fixed measure $\\lambda$ over $\\mathcal{A}$: . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\int_{\\mathcal{A}} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) (q^{\\pi_{\\theta_0}}(s,a)-b(s,\\theta_0)) \\lambda(da)\\,. \\label{eq:basicpga3} \\end{align}\\] In fact, this is a strictly more general form: \\eqref{eq:basicpga2} is a special case of \\eqref{eq:basicpga3} when $\\lambda$ is set to the counting measure over $\\mathcal{A}$. In the special case when \\(\\pi_{\\theta}(\\cdot\\vert s) = \\delta_{f_{\\theta}(s)}(\\cdot)\\) (a Dirac at $f_{\\theta}(s)$), in words, when we have a deterministic policy map and $f$ is differentiable with respect to $\\theta$, it is better to start from the formula given in the theorem. Indeed, in this case, . \\[\\begin{align*} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = q^{\\pi_{\\theta_0}}(s,f_\\theta(s)) \\end{align*}\\] and hence . \\[\\begin{align*} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\frac{d}{dx} q^{\\pi_{\\theta_0}}(s,f_x(s)) \\end{align*}\\] and thus, if either $\\mathcal{S}$ is finite or an appropriate regularity condition holds, . \\[\\begin{align*} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} &amp;= \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} \\frac{d}{dx} q^{\\pi_{\\theta_0}}(\\cdot,f_x(\\cdot))|_{x=\\theta_0}\\,. \\end{align*}\\] If $a\\mapsto q^{\\pi_{\\theta_0}}(s,a)$ is differentiable and $x\\mapsto f_x(s)$ is also differentiable at $x=\\theta_0$ for every $s$ then . \\[\\begin{align*} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} &amp;= \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} \\frac{\\partial}{\\partial a} q^{\\pi_{\\theta_0}}(\\cdot,f_{\\theta_0}(\\cdot)) \\frac{d}{dx} f_x(\\cdot)|_{x=\\theta_0}\\,, \\end{align*}\\] which is known as the “deterministic policy gradient formula”. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec16/#the-policy-gradient-theorem",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec16/#the-policy-gradient-theorem"
  },"87": {
    "doc": "16. Policy gradients",
    "title": "Gradient methods",
    "content": "The idea of gradient methods is to make small steps in the parameter space in the direction of the gradient of an objective function that is to be maximized. In the context of policy search, this works as follows: If $x_i\\in \\mathbb{R}^d$ denotes the parameter vector in round $i$, . \\[x_{i+1} = x_i + \\alpha_i \\nabla_x J(\\pi_x)|_{x=x_i}\\,,\\] where for $f$ differentiable, $\\nabla_x f = (\\frac{d}{dx} f)^\\top$ is the “gradient” (transpose of derivative). Above, $\\alpha_i$ is a positive tuning parameter, called the “stepsize” of the update. The idea is that the “gradient” points in the direction where the function is expected to grow. Indeed, since by definition, . \\[f(x') =f(x) + f'(x) (x'-x) + o(\\|x'-x\\|)\\] if $x’ = x+ \\delta (f’(x))^\\top$, . \\[f( x+ \\delta (f'(x))^\\top ) = f(x)+\\delta \\| f'(x)\\|_2^2 + o(|\\delta|)\\,,\\] or . \\[\\frac{f( x+ \\delta (f'(x))^\\top ) - f(x)}{\\delta} = \\| f'(x)\\|_2^2 + o(1)\\,,\\] For any $\\delta$ sufficiently small so that the $o(1)$ term (in absolute value) is below \\(\\| f'(x)\\|_2^2\\), we see that the right-hand side is positive, hence so is the left-hand side, as claimed. This simple observation is the basis of a huge number of algorithmic variants. In the lack of extra structure the best we can hope from a gradient method is that it will end up in the vicinity of a stationary point. In the presence of extra structure (.e.g, concave function to be maximized), convergence to a global maximum can be guaranteed. In all cases the key to the success of gradient methods is the appropriate choice of the stepsizes; these choices are based on a refinement of the above simple argument that shows that moving towards the direction of the gradient helps. There are also ways of “speeding up” convergence; these “acceleration methods” use a refined iteration (two iterates updated simultaneously) and can greatly speed up convergence. As there are many excellent texts that describe various aspects of gradient methods which cover these ideas, we will not delve into them any further, but I will rather give some pointers to this literature in the endnotes. The elephant in the room here is that the gradient of $J$ is not readily available. The next best thing then is to attempt to build an estimate $G$ of $\\nabla_x J(\\pi_x)$. In the planning setting, the question is whether one can get reasonable estimates of this gradient using a simulator. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec16/#gradient-methods",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec16/#gradient-methods"
  },"88": {
    "doc": "16. Policy gradients",
    "title": "Gradient estimation",
    "content": "Generally speaking there are two types of errors when construction an estimate of the gradient: The one that is purely random, and the one that is not. Defining $g(x) = \\mathbb{E}[G]$, $b(x)=\\nabla_x J(\\pi_x) - g(x)$ measures the “bias” of the gradient estimate, while $G-g(x)$ is the noise. Gradient methods with decreasing (or small) stepsizes naturally “average out” the noise. The version of gradient methods that are able to do this are called stochastic gradient methods. Naturally, these methods are slower when the noise is larger and in general cannot converge faster than how fast the noise averages out. In particular, in persistent noise (i.e., noise with nonvanishing variance), the best rate available for stochastic gradient methods is $O(1/\\sqrt{t})$. While this can be slower than what can be achieved without noise, if the iteration cost is polynomial in the relevant quantities, the total cost of achieving an $\\varepsilon&gt;0$ stationary point can be bounded by a polynomial in these quantities and $1/\\varepsilon^2$. When the gradient estimates are biased, the bias will in general put a limit on how close a gradient method can get to a stationary point. While generally a zero bias is preferred to a nonzero bias, a nonzero bias which is positively aligned with the gradient ($\\langle b(x),\\nabla_x J(\\pi_x) \\rangle\\ge 0$) does not hurt (again, for small stepsizes). When there is no way to guarantee that the bias is positively aligned with the gradient, one may get back into control by making sure that the magnitude of the bias is small relative to the magnitude of the gradient. The next question is of course, how to estimate the gradient. For this many approaches have been proposed in the literature. When a simulator is available, as in our case, a straightforward approach is to start from the policy gradient theorem. Indeed, under mild regularity conditions (e.g., if there are finitely many states) \\eqref{eq:pgt} together with \\eqref{eq:basicpga3} gives . \\[\\begin{align} \\frac{d}{dx} J(\\pi_x) = \\int_{\\mathcal{S}} \\tilde \\nu_\\mu^{\\pi_x}(ds) \\int_{\\mathcal{A}} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) (q^{\\pi_{x}}(s,a)-b(s,x)) \\lambda(da)\\,. \\end{align}\\] Now note that $(1-\\gamma)\\tilde \\nu_\\mu^{\\pi_x}$ is a probability measure over $\\mathcal{S}$. Let $S_0,A_0,S_1,A_1,\\dots$ be an infinite sequence of state-action pairs obtained by simulating policy $\\pi_x$ starting from $S_0\\sim \\mu$. In particular, $A_t \\sim \\pi_x(\\cdot|S_t)$ and $S_{t+1}\\sim P_{A_t}(S_t)$ for any $t\\ge 0$. In addition, define $T_1,T_2$ to be independent of each other and from the trajectory $S_0,A_0,S_1,A_1,\\dots$ and have a geometric distribution with parameter $1-\\gamma$. Then, . \\[G = \\frac{1}{1-\\gamma} \\frac{d}{dx} \\log \\pi_x(A_{T_1}\\vert S_{T_1}) \\left(\\sum_{t=0}^{T_2-1} r_{A_{T_1+t}}(S_{T_1+t}) -b(S_{T_1},x)\\right)\\] is an unbiased estimate of $\\frac{d}{dx} J(\\pi_x)$: . \\[\\mathbb{E}[G] = \\frac{d}{dx} J(\\pi_x)\\,.\\] The argument to show this has partially be given earlier in Lecture 8. One can also show that $G$ has a finite covariance matrix, as well as that the expected effort to obtain $G$ is $O(\\frac{1}{1-\\gamma})$. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec16/#gradient-estimation",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec16/#gradient-estimation"
  },"89": {
    "doc": "16. Policy gradients",
    "title": "Vanilla policy gradients (PG) with some special policy classes",
    "content": "Given the hardness result presented in the previous lecture, there is no hope that gradient methods or any other method will find the global optima of the objective function in policy search in a policy-class agnostic manner. To guarantee computational efficiency, one then . | either needs to give up on convergence to a global optima, or | give up on generality, i.e., give up on that the method should work for any policy class and/or policy parameterization. | . Gradient ascent to find a good policy (“vanilla policy gradients”) is one possible approach to take even if it faces these restrictions. In fact, gradient ascent in some cases will find a globally optimal policy. In particular, it has been long known that with small enough stepsizes gradient ascent converges at a reasonable speed to a global optimum provided that two conditions hold: . | The objective function $f$ is smooth (its derivative is Lipschitz continuous); | The objective function is gradient dominated, i.e., with some constants $c&gt;0$, $p \\ge 1$, $f$ satisfies \\(\\sup_x f(x)-f(x')\\le c \\| f'(x') \\|_2^p\\) for any $x’\\in \\mathbb{R}^d$. | . An example when both of these conditions are met is the direct policy parameterization, which does not allow any compression and is thus not helpful per se, but can serve as a test-case to see how far policy gradient (PG) methods can be pushed. In this case, the parameter vector $\\theta$ is $\\mathrm{S}\\mathrm{A}$ dimensional. By allowing “two-dimensional index”, $\\pi_{\\theta}(a\\vert s)=\\theta_{s,a}$, that is, the parameters encode the action selection probabilities in a direct manner. In this case, since the components of $\\theta$ represent probabilities, they need to be nonnegative and the appropriate components needs to sum to one. Hence, $\\theta\\in \\Theta$ for an appropriate set $\\Theta \\subset [0,1]^{\\mathrm{S}\\mathrm{A}}$. Accordingly, one needs to change gradient ascent. This is done as follows: When a proposed update moves the parameter vector outside of $\\Theta$, the proposed updated parameter vector is “back-projected” to $\\Theta$. For the projection there are a number of reasonable options, such as choosing the point within $\\Theta$ which is closest to the proposed point in the standard Euclidean distance. With this modification, gradient ascent can be shown to converge at a reasonable speed in this case. This parallels the methods that were developed for the tabular case (policy iteration, value iteration). In fact, the algorithm can be seen as a “smoother”, incremental version of policy iteration, which gradually adjusts the probabilities assigned to the individual actions. Using $\\pi_i$ to denote the $i$th policy, from the policy gradient theorem one gets . \\[\\tilde \\pi_{i+1}(a\\vert s ) = \\pi_i(a\\vert s) + \\alpha_i \\tilde \\nu_\\mu^{\\pi_i}(s) q^{\\pi_i}(s,a) \\,,\\] and . \\[\\pi_{i+1}(\\cdot\\vert s) = \\arg\\min_{p\\in \\mathcal{M}_1(\\mathcal{A})} \\| p - \\pi_{i+1}(\\cdot\\vert s) \\|_2, \\qquad s\\in \\mathcal{S}\\,.\\] Thus, the probability of an action in a state is increased in proportion to the value of that state. That the action-value of action $a$ at state $s$ is multiplied with the discounted occupancy at $s$ induced by using policy $\\pi_i$ started from $\\mu$ is a bit of a surprise. In particular, if a state is inaccessible under policy $\\pi_i$, the corresponding probabilities will not be updated. In fact, because this, the above iteration may get stuck at a suboptimal policy. The reader is invited to construct an example when this happens. To prevent this, it turns out to be sufficient if there is a constant $C&gt;0$ such that it holds that . \\[\\begin{align} \\tilde \\nu_\\mu^{\\pi^*}(s)\\ge C \\mu(s)\\,, \\qquad \\text{for all } s\\in \\mathcal{S}\\,, \\label{eq:exploinit} \\end{align}\\] where \\(\\pi^*\\) is an optimal policy. Since $\\mu$ appears on both sides and \\(\\pi^*\\) is unknown, this condition does not look to helpful. However, if one chooses $\\mu$ to be positive everywhere, the condition is clearly met. In any case, when \\eqref{eq:exploinit} holds, gradient dominance and smoothness can be both verified, which in turn implies that the above update will converge at a geometric speed, the geometric speed involves an instance dependent constant which has no polynomial bound in terms of $H_\\gamma = 1/(1-\\gamma)$ and the size of the state-action space. Needless to say this is quite unattractive. Policy gradient methods can be sensitive to how policies are parameterized. For illustration, consider still the “tabular case”, just now change the way the memoryless policies are represented. One possibility is to use the Boltzmann, also known as the softmax representation. In this case $\\theta\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ and . \\[\\begin{align*} \\pi_{\\theta}(a\\vert s) = \\frac{\\exp( \\theta_{s,a})}{ \\sum_{a'} \\exp( \\theta_{s,a'})}\\,, \\qquad (s,a)\\in \\mathcal{S}\\times\\mathcal{A}\\,. \\end{align*}\\] A straightforward calculation gives . \\[\\begin{align*} \\frac{\\partial}{\\partial \\theta_{s,a}} \\log \\pi_\\theta( a'\\vert s' ) = \\mathbb{I}(s=s',a=a') - \\pi_{\\theta}(a\\vert s) \\mathbb{I}(s=s') \\end{align*}\\] and hence . \\[\\begin{align*} \\frac{\\partial}{\\partial\\theta_{(s,a)}} J(\\pi_{\\theta}) &amp; = \\sum_{s'} \\tilde \\nu_\\mu^{\\pi_\\theta}(s') \\sum_{a'}\\pi_\\theta(a'\\vert s) \\frac{\\partial}{\\partial \\theta_{s,a}} \\log \\pi_\\theta( a'\\vert s' ) q^{\\pi_\\theta}(s',a')\\\\ &amp; = \\nu_\\mu^{\\pi_\\theta}(s,a) \\left( q^{\\pi_\\theta}(s,a)-v^{\\pi_\\theta}(s) \\right)\\,, \\end{align*}\\] where recall that $\\nu_\\mu^{\\pi}$ is the discounted state-occupancy measure over the state-action pairs of policy $\\pi$ when the initial state distribution is $\\mu$. The difference in the bracket on the right-hand side is known as the advantage of action $a$ and, accordingly, the function . \\[\\mathfrak{a}^{\\pi} = q^\\pi-v^\\pi\\,,\\] which is a function mapping state-action pairs to reals, is called the advantage function underlying policy $\\pi$. To justify the terminology, note that policy iteration can be seen as choosing in each state the action that maximizes the “advantage”. Thus, we expect that we get a better policy if the “probability mass” in the action distribution is shifted towards actions with a larger advantage. Note though that advantages (as defined above) can also be negative and in fact if $\\pi$ is optimal, all actions have nonnegative advantages only. The gradient ascent rule prescribes that . \\[\\theta_{i+1} = \\theta_i + \\alpha_i \\nu_\\mu^{\\pi_{\\theta_i}} \\circ \\mathfrak{a}^{\\pi_{\\theta_i}}\\,,\\] where $\\circ$ denotes componentwise product. While this is similar to the previous update, now the meaning of parameters is quite different. In fact, just because a parameter is increased does not necessarily mean that the probability of the corresponding action is increased: This will only happen if the increase of this parameter exceeds that of the other parameters “at the same state”. By slightly abusing notation with defining $\\pi_i = \\pi_{\\theta_i}$, we have . \\[\\begin{align} \\pi_{i+1}(a\\vert s) \\propto \\pi_i(a\\vert s) \\exp( \\alpha_i \\nu_\\mu^{\\pi_i}(s,a) \\mathfrak{a}^{\\pi_i}(s,a))\\,. \\label{eq:softmaxtab} \\end{align}\\] Just like in the previous update rule, we also see the occupancy measure “weighting” the update. This is again not necessarily helpful and if anything, again, speaks to the arbitrariness of gradient methods. And while this does not entirely stop policy gradient to find an optimal policy, and again, one can even show that the speed is geometric, though, as before, the algorithm altogether fails to run in polynomial time in the relevant quantities. For this theorem which we give without proof recall that $H_\\gamma = 1/(1-\\gamma)$. Theorem (PG is slow with Boltzmann policies): There exists universal constants $\\gamma_0,c,C&gt;0$ such that for any $\\gamma_0&lt;\\gamma&lt;1$, if $\\mathrm{S}&gt;C H_\\gamma^6$ then one can find a discounted MDP with $\\mathrm{S}$ states and $3$ actions, setting $\\mu$ to be the uniform distribution and initializing the parameters so that $\\pi_0$ is the uniform random policy, softmax PG with a constant stepsize of $\\alpha&gt;0$ takes at least . \\[\\frac{c}{\\alpha} \\mathrm{S}^{2^{\\Omega({H_\\gamma})}}\\] iterations. As one expects that without any compression, the chosen planner should behave reasonably, this rules out the “vanilla” version of policy gradient. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec16/#vanilla-policy-gradients-pg-with-some-special-policy-classes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec16/#vanilla-policy-gradients-pg-with-some-special-policy-classes"
  },"90": {
    "doc": "16. Policy gradients",
    "title": "Natural policy gradient (NPG) methods",
    "content": "In fact, a quite unsatisfactory property of gradient ascent that the speed at which it converges can greatly depend on the parameterization used. Thus, for the same policy class, there are many possible “gradient directions”, depending on the parameterization chosen. What is a gradient direction for one parameterization is not necessarily a gradient direction for another one. But what is common about these directions that an infinitesimal step along them is guaranteed increase the objective. One can in fact take a direction obtained with a parameterization and look at what direction it gives with another parameterizations. To get some order, consider transforming all these directions into the space that corresponds to the direct parameterization. It is not hard to see that all possible directions that are within 90 degrees of the gradient direction with this parameterization can be obtained by considering an appropriate parameterization. More generally, regardless of parameterization, all directions within 90 degrees of the gradient direction are ascent directions. This motivates changing the stepsize $\\alpha_i$ from a scalar to a matrix $A_i$. Clearly, to keep the angle between the original gradient direction $g$ and the transformed direction $A_i g$ below 90 degrees, $g^\\top A_i g\\ge 0$ has to hold. For $A_i$ symmetric, this restricts the set of matrix “stepsizes” to the set of positive definite matrices (still, a large set). There are many ways to choose a matrix stepsize. Newton’s method is to choose it so that the direction is the “best” if the function is replaced by its local quadratic approximation. This provably helps to reduce the number of iterations when the objective function is “ill-conditioned”, though all matrix stepsize methods incur additional cost per each iteration, which will often offset the gains. Another idea, which comes from statistical problems where one often works with distributions is to find the direction of update which coincides with the direction one would obtain if one used the steepest descent direction directly in the space of distributions where distances are measured with respect to relative entropy. In some cases, this approach, which was coined the “natural gradient” approach, has been shown to give better results, though the evidence is purely empirical. As it turns out, the matrix stepsize to be used with this approach is the (pseudo)inverse of the so-called Fisher information matrix. In our context, for every state, we have distributions over the actions. Fixing a state $s$, the Fisher information matrix becomes . \\[F_x(s) = \\frac{d}{dx} \\log \\pi_x(\\cdot\\vert s)\\, \\frac{d}{dx} \\log \\pi_x(\\cdot\\vert s)^\\top\\,.\\] To get the “information rate” over the states, one can sum these matrices up, weighted by the discounted state occupancy measure underlying $\\mu$ and $\\pi_x$ to get . \\[F(x) := \\nu_\\mu^{\\pi_x} F_x \\,.\\] The update rule then takes the form . \\[x_{i+1} = x_i + \\alpha_i F(x_i)^{\\dagger} \\nabla_x J(\\pi_x)\\,,\\] where for a square matrix $A$, $A^{\\dagger}$ denotes the pseudoinverse of $A$. Interestingly, the update direction can be obtained without calculating $F$ and inverting it: . Proposition: We have . \\[\\begin{align*} (1-\\gamma) F(x)^{\\dagger} \\nabla_x J(\\pi_x) = \\arg\\min_{w\\in \\mathbb{R}^d} \\nu_\\mu^{\\pi_x} \\left( w^\\top \\nabla_x \\log \\pi_x(\\cdot\\vert \\cdot)- \\mathfrak{a}^{\\pi_x}\\right)^2\\,, \\end{align*}\\] where $\\mathfrak{a}^{\\pi_x} =q^{\\pi_x}-v^{\\pi_x}$ and $\\arg\\min$ chooses the minimum \\(\\|\\cdot\\|_2\\)-norm solution if multiple minimizers exist. Proof: Just recall the formula that gives the solution to a least-squares problem. The details are left to the reader. \\(\\qquad \\blacksquare\\) . As an example of how things look like consider the case when $\\pi_x$ takes the form of a Boltzmann policy: . \\[\\pi_x(a\\vert s) \\propto \\exp(x^\\top \\phi(s,a))\\,,\\] where $\\phi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ is a feature-map. Then, assuming that there are finitely many actions, . \\[\\nabla_x \\log \\pi_x(a\\vert s) = \\underbrace{\\phi(s,a)- \\sum_{a'} \\pi_x(a'\\vert s) \\phi(s,a')}_{\\psi_x(s,a)}\\,.\\] Then, the natural policy gradient update takes the form . \\[x_{i+1} = x_i + \\alpha_i w_i\\,,\\] where . \\[w_i = \\arg\\min_{w\\in \\mathbb{R}^d} \\nu_\\mu^{\\pi_x} \\left( w^\\top \\psi_x - \\mathfrak{a}^{\\pi_{x_i}}\\right)^2\\] In the tabular case $(d=\\mathrm{S}\\mathrm{A}$, no compression), . \\[w_i(s,a) = \\mathfrak{a}^{\\pi_{x_i}}(s,a)\\] and thus . \\[\\pi_{i+1}(a\\vert s) \\propto \\pi_i(a\\vert s) \\exp( \\alpha_i \\mathfrak{a}^{\\pi_i}(s,a) ) = \\pi_i(a\\vert s) \\exp( \\alpha_i q^{\\pi_i}(s,a) )\\,.\\] Note that this update rule eliminates the term $\\nu_\\mu^{\\pi_i}(s,a)$ term that we have previously seen (cf. \\eqref{eq:softmaxtab}). NPG is known to enjoy a reasonable speed of convergence, which gives altogether polynomial planning time. This is promising. No similar results are available for the nontabular case. Note that if we (arbitrarily) change the definition of $w_i$ by replacing $\\psi_x$ above with $\\phi$ and $a^{\\pi_x}$ with $q^{\\pi_x}$, we get what has been called in the literature Q-NPG: . \\[w_i = \\arg\\min_{w\\in \\mathbb{R}^d} \\nu_\\mu^{\\pi_x} \\left( w^\\top \\phi - q^{\\pi_x}\\right)^2\\,.\\] Note that the only difference between Q-NPG and Politex is that in Politex one uses . \\[w_i = \\arg\\min_{w\\in \\mathbb{R}^d} \\hat \\nu \\left( w^\\top \\phi - q^{\\pi_x}\\right)^2\\,,\\] where $ \\hat \\nu$ is the measure obtained from solving the G-optimal design problem. The price of not using $\\hat \\nu$ but using $\\nu_\\mu^{\\pi_x}$ in Q-NPG is that the approximation error in Q-NPG becomes . \\[\\frac{C\\varepsilon}{(1-\\gamma)^{1.5}}\\] where . \\[C = \\left\\| \\frac{d\\tilde\\nu_\\mu^{\\pi^*} }{d\\mu} \\right\\|_\\infty\\] gives a bound on how much the distribution $\\mu$ differs from that of obtained when the optimal policy $\\pi^*$ is followed from $\\mu$. As was argued before, it is necessary that $C$ is finite for policy gradient methods not to “get stuck” at local optima. However, $C$ can be arbitrarily large even for finite state-action MDPs; an in fact it is the presence of $C$ that makes the policy gradient with the direct parameterization a slow algorithm. In contrast, the same quantity in Politex is . \\[\\frac{\\sqrt{d}\\varepsilon}{1-\\gamma}\\,.\\] Not only the uncontrolled constant $C$ is removed, but the dependence on the planning horizon is also improved. Other than these differences, the results available for Q-NPG are similar to that of Politex and in fact the proof technique to obtain the results is also the same. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec16/#natural-policy-gradient-npg-methods",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec16/#natural-policy-gradient-npg-methods"
  },"91": {
    "doc": "16. Policy gradients",
    "title": "The proof of the Calculus 101 result",
    "content": "For completeness, here is the proof of \\eqref{eq:c101}. For the proof recall that for a function $g:\\mathbb{R}^d \\to \\mathbb{R}$, $\\frac{d}{dx} g(x_0)$ is the unique linear operator (row vector, in the Euclidean case) that satisfies . \\[\\begin{align*} g(x)=g(x_0)+\\frac{d}{dx} g(x_0) (x-x_0) + o( \\|x-x_0\\|) \\text{ as } x\\to x_0\\,. \\end{align*}\\] Hence, it suffices to show that . \\[\\begin{align*} f(x',x') = f(x,x) + \\left( \\frac{\\partial}{\\partial u} f(u,x)\\vert_{u=x} + \\frac{\\partial}{\\partial v} f(x,v)\\vert_{v=x} \\right) (x'-x) + o( \\|x'-x\\|)\\,. \\end{align*}\\] To minimize clutter we will write $\\frac{\\partial}{\\partial u} f(x’,x)$ for $\\frac{\\partial}{\\partial u} f(u,x)\\vert_{u=x’}$ (and similarly we write $\\frac{\\partial}{\\partial v} f(x,x’)$ for $\\frac{\\partial}{\\partial v} f(x,v)\\vert_{v=x’}$). By definition we have . \\[\\begin{align*} f(x',x') = f(x',x) + \\frac{\\partial}{\\partial v} f(x',x) (x'-x) + o( \\| x'-x\\| ) \\end{align*}\\] and . \\[\\begin{align*} f(x',x) = f(x,x) + \\frac{\\partial}{\\partial u} f(x,x) (x'-x) + o( \\| x'-x\\| )\\,. \\end{align*}\\] Putting these together we get . \\[\\begin{align*} f(x',x') &amp; = f(x,x) + \\left( \\frac{\\partial}{\\partial v} f(x',x) + \\frac{\\partial}{\\partial u} f(x,x) \\right) (x'-x) + o( \\|x'-x\\|) \\\\ &amp; = f(x,x) + \\left( \\frac{\\partial}{\\partial v} f(x,x) + \\frac{\\partial}{\\partial u} f(x,x) \\right) (x'-x) \\\\ &amp; \\qquad\\qquad\\,\\,+ \\left( \\frac{\\partial}{\\partial v} f(x',x) - \\frac{\\partial}{\\partial v} f(x,x)\\right) (x'-x) + o( \\|x'-x\\|) \\\\ &amp; = f(x,x) + \\left( \\frac{\\partial}{\\partial v} f(x,x) + \\frac{\\partial}{\\partial u} f(x,x) \\right) (x'-x) + o( \\|x'-x\\|) \\,. \\end{align*}\\] where the last equality follows if $\\frac{\\partial}{\\partial v} f(x’,x) - \\frac{\\partial}{\\partial v} f(x,x) = o(1)$ as $x’\\to x$, i.e., if $x’\\mapsto \\frac{\\partial}{\\partial v} f(x’,x)$ is continuous at $x’=x$. That the result also holds under the assumption that $x’\\mapsto \\frac{\\partial}{\\partial u} f(x,x’)$ is continuous at $x’=x$ follows from a symmetric argument. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec16/#the-proof-of-the-calculus-101-result",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec16/#the-proof-of-the-calculus-101-result"
  },"92": {
    "doc": "16. Policy gradients",
    "title": "Summary",
    "content": "While policy gradient methods remain extremely popular and the idea of directly searching in the set of policies is attractive, at the moment it appears that they not only lack theoretical support, but the theoretical results suggest that it is hard to find any setting where policy gradient methods would be provably competitive with alternatives. At minimum, they need careful choices of policy parameterizations and even in that case the update rule may need to be changed to guarantee efficiency and effectiveness, as we have seen above. As an approach to algorithm design their main advantage is their generality and a strong support through various software libraries. Compared to vanilla “dynamic programming” methods they make generally smaller, more incremental changes to the policies, which seems useful. However, this is also achieved by methods like Politex, which is derived using a “bound minimization” approach. While this may seem more ad hoc than following gradients, in fact, one may argue that following gradients is more ad hoc as it fails to guarantee good performance. However, perhaps the most important point here is that one should not care too much about how a method is derived, or what “interpretation” it may have (is Politex a gradient algorithm? does this matter?). What matters is the outcome: In this case how the methods perform. It is thus wise to learn about all possible ways of designing algorithms, especially since there is much room for improving the performance of current algorithms. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec16/#summary",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec16/#summary"
  },"93": {
    "doc": "16. Policy gradients",
    "title": "Notes",
    "content": "Philip Thomas (2014, see citation below) takes a careful look at the claims surrounding natural gradient descent. One claim that is often heard is that natural gradient descent will speed up convergence. This is usually back up by giving a demonstration (e.g., Kakade, 2002, or Amari, 1998). However, it is far from clear whether this speedup will necessarily happen. As it turns out, this is far from being true. In fact, natural policy gradient can cause divergence even where following the normal gradient is guaranteed to converge to a global optimum. An example of this is given in Section 6.5 of the paper of Thomas (2014). ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec16/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec16/#notes"
  },"94": {
    "doc": "16. Policy gradients",
    "title": "References",
    "content": ". | Amari, S. Natural gradient works efficiently in learning.Neural Computation, 10:251–276, 1998. | Kakade, S. A natural policy gradient. In Advances in Neural Information Processing Systems, volume 14, pp.1531–1538, 2002. | Bagnell, J. A. and Schneider, J. Covariant policy search. In Proceedings of the International Joint Conference on Artificial Intelligence, pp. 1019–1024, 2003. | Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (1999). Policy gradient methods for reinforce-ment learning with function approximation. In Neural Information Processing Systems 12, pages 1057–1063. | Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. “Deterministic Policy Gradient Algorithms.” In ICML. http://hal.inria.fr/hal-00938992/. | Bhandari, Jalaj, and Daniel Russo. 2019. “Global Optimality Guarantees For Policy Gradient Methods,” June. https://arxiv.org/abs/1906.01786v1. | Agarwal, Alekh, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. 2019. “On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1908.00261. | Mei, Jincheng, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. 2020. “On the Global Convergence Rates of Softmax Policy Gradient Methods.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2005.06392. | Zhang, Junyu, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. 2020. “Variational Policy Gradient Method for Reinforcement Learning with General Utilities.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2007.02151. | Bhandari, Jalaj, and Daniel Russo. 2020. “A Note on the Linear Convergence of Policy Gradient Methods.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2007.11120. | Chung, Wesley, Valentin Thomas, Marlos C. Machado, and Nicolas Le Roux. 2020. “Beyond Variance Reduction: Understanding the True Impact of Baselines on Policy Optimization.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2008.13773. | Li, Gen, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. 2021. “Softmax Policy Gradient Methods Can Take Exponential Time to Converge.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2102.11270. | Thomas, Philip S. “GeNGA: A Generalization of Natural Gradient Ascent with Positive and Negative Convergence Results.” ICML 2014. http://proceedings.mlr.press/v32/thomasb14.pdf. | . The paper to read about natural gradient methods: . | Martens, James. 2014. “New Insights and Perspectives on the Natural Gradient Method,” December. https://arxiv.org/abs/1412.1193v9. Last update: September, 2020. | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec16/#references",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec16/#references"
  },"95": {
    "doc": "16. Policy gradients",
    "title": "16. Policy gradients",
    "content": "PDF Version . In this last lecture on planning, we look at policy search through the lens of applying gradient ascent. We start by proving the so-called policy gradient theorem which is then shown to give rise to an efficient way of constructing noisy, but unbiased gradient estimates in the presence of a simulator. We discuss at a high level the ideas underlying gradient ascent and stochastic gradient ascent methods (as opposed to more common case in machine learning where the goal is to minimize a loss, or objective function, we are maximizing rewards, hence ascending on the objective rather than descending). We then find out about the limitations of policy gradient even in the presence of “perfect representation” (unrestricted policy classes, tabular case) and perfect gradient information, which motivates the introduction of a variant known as “natural policy gradients” (NPG). We then uncover a close relationship between this method and Politex. The lecture concludes with comparing results for NPG and Politex. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec16/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec16/"
  },"96": {
    "doc": "2. The Fundamental Theorem",
    "title": "Introduction",
    "content": "A Markov decision Process (MDP) is a 5-tuple $M = (\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$, where $\\mathcal{S}$ represents the state space, $\\mathcal{A}$ represents the action space, $P = (P_a(s))_{s,a}$ collects the next state distributions for each state-action pair (to represent the transition dynamics), \\(r= (r_a(s))_{s,a}\\) gives the immediate rewards incurred for taking a given action in a given state, and $0 \\leq \\gamma &lt; 1$ is the discount factor. As said before, for simplicity, the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$ are finite. A policy \\(\\pi = (\\pi_t)_{t \\geq 0}\\) is an infinite long sequence where for each $t\\ge 0$, \\(\\pi_t: (\\mathcal{S} \\times \\mathcal{A})^{t-1} \\times \\mathcal{S} \\rightarrow \\mathcal{M}_1(\\mathcal{A})\\) assigns a probability distribution to histories of length $t$. (For $\\rho\\ge 0$ we use $\\mathcal{M}_\\rho(X)$ to denote the set of nonnegative measures $\\mu$ over $X$ that satisfy $\\mu(X)=\\rho$.) Following a policy in an MDP means that the distribution of the actions in each time step $t\\ge 0$ will follow what is prescribed by the policy for whatever the history is at that time. When a policy is used in an MDP, the interconnection of the policy and the MDP, together with a start-state distribution, results in a distribution $\\mathbb{P}$ such that for the infinite long sequence of state-action pairs $S_0,A_0,S_1,A_1,\\dots$, $S_0 \\sim \\mu(\\cdot), A_t \\sim \\pi_t(\\cdot | H_t)$, and $S_{t+1} \\sim P_{A_t}(S_t)$ for all $t \\geq 0$ where $H_t = (S_0,A_0,\\dots,S_{t-1},A_{t-1},S_t)$ is the history at time step $t$. This closed loop interaction (or interconnection) of the policy and the MDP is shown in the figure below. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec2/#introduction",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec2/#introduction"
  },"97": {
    "doc": "2. The Fundamental Theorem",
    "title": "Probabilities over Trajectories",
    "content": "One loose end from the previous lecture was the existence of the probability measures \\(\\mathbb{P}_\\mu^\\pi\\). For this, we have the following result: . Theorem (existence theorem): Fix a finite MDP $M$ with state space $\\mathcal{S}$ and action space $\\mathcal{A}$. Then there exists a measurable space $(\\Omega,\\mathcal{F})$ and a sequence of random elements $S_0, A_0, S_1, A_1, \\ldots$ over this space, $S_t\\in \\mathcal{S}$, $A_t\\in \\mathcal{A}$ for $t\\ge 0$, such that for any policy \\(\\pi = (\\pi_t)_{t \\geq 0}\\) of the MDP $M$ and any probability measure \\(\\mu \\in \\mathcal{M}_1(\\mathcal{S})\\) over $\\mathcal{S}$, there exists a probability measure $\\mathbb{P}(=\\mathbb{P}_{\\mu}^{\\pi})$ over $(\\Omega,\\mathcal{F})$ satisfying the following properties: . | $\\mathbb{P}(S_0 = s) = \\mu(s)$ for all $s \\in \\mathcal{S}$, | $\\mathbb{P}(A_t = a | H_t) = \\pi_t(a | H_t)$ for all $a \\in \\mathcal{A}, t \\geq 0$, and | $\\mathbb{P}(S_{t+1} = s’ | H_t, A_t) = P_{A_t}(S_t, s’)$ for all $s’ \\in \\mathcal{S}$. | . Furthermore, uniqueness holds in the following sense: if \\((\\tilde{\\Omega},\\tilde{\\mathcal{F}})\\) together with \\(\\tilde S_0,\\tilde A_0,\\tilde S_1,\\tilde A_1,\\dots\\) also satisfy the conditions of the definition with \\({\\tilde{\\mathbb{P}}}_{\\mu}^{\\pi}\\) denoting the associated probability measures for specific choices of $(\\pi,\\mu)$ then for any $\\pi$, $\\mu$, the joint distribution of $S_0,A_0,S_1,A_1,\\dots$ under \\(\\mathbb{P}_\\mu^{\\pi}\\) and that of \\(\\tilde S_0,\\tilde A_0,\\tilde S_1,\\tilde A_1,\\dots\\) under \\(\\tilde{\\mathbb{P}}_{\\mu}^{\\pi}\\) are identical. Proof: Use the Ionescu-Tulcea theorem (Theorem 3.3 in the “bandit book”, though the theorem statement there is weaker in that the uniqueness property is left out). \\(\\qquad\\blacksquare\\) . Property 3 above is known as the Markov property and is how MDPs derive their name. Note that implicit in the statement of this result is that $\\mathcal{S}$ and $\\mathcal{A}$ are endowed with the discrete $\\sigma$-algebra. This is because we want both ${S_t = s}$ and ${A_t = a}$ to be events for any $s\\in \\mathcal{S}$ and $a\\in \\mathcal{A}$ (these appear in the conditions underlying properties 1-3). Note that the result does not point to any singular measurable space. Indeed, there are many ways to choose $(\\Omega,\\mathcal{F})$. However, as long as we are only concerned with properties of the distributions of state-action trajectories, thanks to the uniqueness part of the theorem, no ambiguity will arise from this. As a result, in general, we will not care about the choice of $(\\Omega,\\mathcal{F})$: Any choice as given in the theorem will work. However, for some proofs, it will be convenient to choose $(\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}$, the set of infinite long trajectories as $\\Omega$, while setting $S_t((s_0,a_0,s_1,a_1,\\dots)) = s_t$, $A_t((s_0,a_0,s_1,a_1,\\dots)) = a_t$ ($t\\ge 0$) and choosing $\\mathcal{F}=(2^{\\mathcal{S}\\times\\mathcal{A}})^{\\otimes \\mathbb{N}}$, which the smallest $\\sigma$ algebra that makes $(S_t,A_t)$ measurable for any $t\\ge 0$. We will call the resulting probability space the canonical probability space underlying the MDP. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec2/#probabilities-over-trajectories",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec2/#probabilities-over-trajectories"
  },"98": {
    "doc": "2. The Fundamental Theorem",
    "title": "Optimality and Some Notation",
    "content": "As usual, we use $\\mathbb{E}$ to denote the expectation operator underlying a probability measure $\\mathbb{P}$. When the dependence on $\\mu$ or $\\pi$ is important, we use $\\mathbb{E}_\\mu^\\pi$. We may drop any of these, when the dropped quantity is clear from the context. We will pay special attention to start state distributions concentrated on a single state. When this is state $s$, the distribution is denoted by $\\delta_s$: this is the well-known Dirac distribution with an atom at $s$. The reason we pay special attention to these is because these in a way form the basis of all start state distributions (and in fact quantities that depend linearly on start state distributions). We will use the shorthand \\(\\mathbb{P}_{s}^{\\pi}\\) for \\(\\mathbb{P}_{\\delta_s}^{\\pi}\\). Similarly, we use \\(\\mathbb{E}_{s}^{\\pi}\\) for \\(\\mathbb{E}_{\\delta_s}^{\\pi}\\). Define the return over a trajectory $\\tau = (S_0, A_0, S_1, A_1, \\ldots)$ as . \\[R = \\sum_{t=0}^{\\infty} \\gamma^t r_{A_t}(S_t).\\] The value function $v^\\pi$ of policy $\\pi$ maps states to values and in particular for a state $s\\in\\mathcal{S}$, $v^\\pi(s)$ is defined via $v^\\pi(s) = \\mathbb{E}_{s}^{\\pi}[R]$: This is the expected return under the distribution induced by the interconnection of policy $\\pi$ and the MDP when the start state is $s$. Note that $v^\\pi(s)$ is well-defined. This is because it is the expectation of a quantity that is a function of the trajectory $\\tau$; for an explanation see the end-notes. The standard goal in an MDP is to identify a policy that maximizes this value in every state. A policy achieving this is known as an optimal policy. Whether an optimal policy exists at all is not clear at this stage. In any case, if it exist, an optimal policy must satisfy \\(v^\\pi = v^*\\) where $v^*:\\mathcal{S} \\to \\mathbb{R}$ is defined by . \\[v^{*}(s) = \\sup_{\\pi} v^{\\pi}(s)\\,, \\qquad s\\in \\mathcal{S}\\,.\\] By the definition of the optimal value function, we have \\(v^\\pi(s) \\leq v^{*}(s)\\) for all $s \\in \\mathcal{S}$ and any policy $\\pi$. We also use $v^\\pi \\le v^*$ to express this. In general, $f \\le g$ for two functions $f,g$ that are defined over the same domain and take values (say) in the reals, if $f(z)\\le g(z)$ holds for all the possible elements $z$ of their common domain. We similarly define $f\\ge g$. We will also identify functions with vectors and allow vector-space operations on them. All vectors, unless otherwise stated, are column vectors. The symbol $\\boldsymbol{1}$ is defined as a vector of ones. The length of this vector can change depending on the context. In this lecture, it will be $\\mathrm{S}$-dimensional. This symbol will be very useful in a number of calculations. We start with a definition that uses it. Approximately optimal policies . Let $\\varepsilon&gt;0$. A policy $\\pi$ is said to be $\\varepsilon$-optimal if . \\[v^\\pi \\ge v^* - \\varepsilon \\boldsymbol{1}\\,.\\] Finding an $\\varepsilon$-optimal policy with a positive $\\varepsilon$ should intuitively be easier than finding an optimal policy. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec2/#optimality-and-some-notation",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec2/#optimality-and-some-notation"
  },"99": {
    "doc": "2. The Fundamental Theorem",
    "title": "Memoryless Policies (ML)",
    "content": "If optimal policies would need to remember the past of arbitrary length, it would be hopeless to search for efficient algorithms that can compute them as even describing them could take infinite time. Luckily, this is not the case. In finite MDPs, it will turn out to be sufficient to consider policies that use only the most recent state without losing optimality: this is the subject of the fundamental theorem of MDPs, which we will give shortly. We call the policies that take only the most recent state into account memoryless. Formally, a memoryless policy can be identified with a map from the states to probability distributions over the actions: \\(m: \\mathcal{S}\\to \\mathcal{M}_1(\\mathcal{A})\\). Given $m$, the memoryless policy, using our previous policy notation, is $\\pi_t(a|s_0,a_0,\\dots,s_{t-1},a_{t-1},s_t) = m(a|s_t)$, where we abuse notation by using $m(a|s_t)$ in place of $m(s_t)(a)$. Thus, as expected, the policy itself “forgets” the past and just uses the most recent state in assigning probabilities to the individual actions. Under a distribution induced by interconnecting a memoryless policy with an MDP, the sequence of state-action pairs forms a Markov chain. In what follows, by abusing notation further, when it comes to a memoryless policy, we will identify $\\pi$ with $m$ and will just write $\\pi: \\mathcal{S} \\to \\mathcal{M}_1(\\mathcal{A})$. For building up to the proof of the fundamental theorem, we start with the concept of discounted occupancy measures. (Discounted) Occupancy Measure . Given a start state distribution \\(\\mu \\in \\mathcal{M}_1(\\mathcal{S})\\) and a policy \\(\\pi\\), the (discounted) occupancy measure \\(\\nu_\\mu^\\pi \\in \\mathcal{M}_{1/(1-\\gamma)}(\\mathcal{S} \\times \\mathcal{A})\\) induced by \\(\\mu\\) and \\(\\pi\\) and the underlying MDP \\(M\\) is defined as . \\[\\nu_\\mu^\\pi(s, a) = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{P}_\\mu^\\pi (S_t = s, A_t = a).\\] Interestingly, the value function can be represented as an inner product between the immediate reward function $r$ and the occupancy measure $\\nu_\\mu^\\pi$: . \\[\\begin{align*} v^\\pi(\\mu) &amp;= \\mathbb{E}_\\mu^\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r_{A_t}(S_t) \\right] \\\\ &amp;= \\sum_{s, a} \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_\\mu^\\pi \\left[ r_{A_t}(S_t) \\mathbb{I}(S_t = s, A_t = a) \\right] \\\\ &amp;= \\sum_{s, a} r_{a}(s) \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_\\mu^\\pi \\left[ \\mathbb{I}(S_t = s, A_t = a) \\right] \\\\ &amp;= \\sum_{s, a} r_{a}(s) \\sum_{t=0}^\\infty \\gamma^t \\mathbb{P}_\\mu^\\pi(S_t = s, A_t = a) \\\\ &amp;= \\sum_{s, a} r_a(s) \\nu_\\mu^\\pi(s, a) \\\\ &amp;=: \\langle \\nu_\\mu^\\pi, r \\rangle, \\end{align*}\\] where $\\mathbb{I}(S_t = s, A_t = a)$ is the indicator of the event ${S_t=s,A_t=a}$, which gives the value of one when the event holds (i.e., $S_t = s$ and $A_t = a$), and gives zero otherwise. That the summation over $(s,a)$ can be moved outside of the expectation in the first equality follows because expectations are linear. That the infinite sum can be moved outside is more subtle: this follows from Lebesgue’s dominated convergence theorem. See, for example, Chapter 2 of Lattimore &amp; Szepesvári (2020). With the above equation, we see that the problem of maximizing the expected reward for a given initial distribution is the same as choosing a policy that “stirs” the occupancy measure to maximally align with the reward vector $r$. A better alignment will result in a higher value for the policy. This is depicted in the figure below. A key step in proving the sufficiency of memoryless policies for optimal control is the following result: . Theorem: For any policy $\\pi$ and a start state distribution $\\mu \\in \\mathcal{M}_1(\\mathcal{S})$, there exists a memoryless policy $\\pi’$ such that . \\[\\nu_\\mu^{\\pi'} = \\nu_\\mu^{\\pi}.\\] . Proof (hint): First define the occupancy measure over the state space \\(\\tilde{\\nu}_\\mu^\\pi(s) := \\sum_a \\nu_\\mu^\\pi(s, a)\\). Then show that the theorem statement holds for the policy $\\pi’$ defined as follows: . \\[\\pi'(a | s) = \\begin{cases} \\frac{\\nu_\\mu^\\pi(s, a)}{\\tilde{\\nu}_\\mu^\\pi(s)} &amp; \\text{if } \\tilde{\\nu}_\\mu^\\pi(s) \\neq 0 \\\\ \\pi_0(a) &amp; \\text{otherwise,} \\end{cases}\\] where \\(\\pi_0(a) \\in \\mathcal{M}_1(\\mathcal{A})\\) is an arbitrary distribution. To do this, expand $\\tilde \\nu_\\mu^\\pi$ using the definition of discounted occupancy measures and use algebra. \\[\\tag*{$\\blacksquare$}\\] Note that it is crucial that the memoryless policy obtained depends on the start state distribution: The reader should try to convince themselves that there are non-memoryless policies whose value function cannot be reproduced by the same memoryless policy at every state. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec2/#memoryless-policies-ml",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec2/#memoryless-policies-ml"
  },"100": {
    "doc": "2. The Fundamental Theorem",
    "title": "Bellman Operators, Contractions",
    "content": "The last definitions and results that we need before stating the fundamental theorem concern what are known as Bellman operators. Fix a memoryless policy $\\pi$. Recall that $\\mathrm{S}$ is the cardinality (size) of $\\mathcal{S}$. First, define $r_\\pi(s) = \\sum_a \\pi(a|s) r_a(s)$ to be the expected reward under policy $\\pi$ for a given state $s$. Again, we overload the notation and let $r_\\pi \\in \\mathbb{R}^{\\mathrm{S}}$ denote a vector whose $s$th element \\((r_\\pi)_s = r_\\pi(s)\\). Similarly, we define $P_\\pi(s, s’) := \\sum_a \\pi(a|s) P_a(s, s’)$ and let $P_\\pi \\in [0, 1]^{\\mathrm{S} \\times \\mathrm{S}}$ denote the stochastic transition matrix where the element in the \\(s\\)th row and \\(s'\\)th column \\((P_\\pi)_{s, s'} = P_\\pi(s, s')\\). Note that each row of $P_\\pi$ sums to one: . \\[P_\\pi \\mathbf{1} = \\mathbf{1}\\,.\\] The Bellman/policy evaluation operator underlying $\\pi$, $T_\\pi: \\mathbb{R}^{\\mathrm{S}} \\rightarrow \\mathbb{R}^{\\mathrm{S}}$, is defined as . \\[\\begin{align*} T_\\pi v(s) &amp;= \\sum_a \\pi(a|s) \\left \\{r_a(s) + \\gamma \\sum_{s'} P_a(s, s') v(s') \\right \\} \\\\ &amp;= \\sum_a \\pi(a|s) \\left \\{r_a(s) + \\gamma \\langle P_a(s), v \\rangle \\right \\} \\end{align*}\\] or, in short, . \\[T_\\pi v = r_\\pi + \\gamma P_\\pi v,\\] where $v \\in \\mathbb{R}^{\\mathrm{S}}$. The Bellman operator performs a one-step lookahead (also called a Bellman lookahead) on the value function. We will use the notations $(T_\\pi(v))(s)$, $T_\\pi v(s)$, and \\((T_\\pi v)_s\\) interchangeably. $T_\\pi$ is also known as the policy evaluation operator for the policy $\\pi$. The Bellman optimality operator $T: \\mathbb{R}^{\\mathrm{S}} \\rightarrow \\mathbb{R}^{\\mathrm{S}}$ is defined as . \\[T v(s) = \\max_a \\{ r_a(s) + \\gamma \\langle P_a(s), v \\rangle \\}.\\] We use \\(\\|\\cdot\\|_\\infty\\) to denote the maximum-norm: \\(\\| v \\|_{\\infty} = \\max_i |v_i|\\). The maximum-norm is a “good friend” of the operators we just defined. This is because stochastic matrices, viewed as operators and “maximizing” are “good friends” of this norm. All this results in the following proposition: . Proposition ($\\gamma$-contraction of the Bellman Operators): Given any two vectors $u, v \\in \\mathbb{R}^{\\mathrm{S}}$ and any memoryless policy \\(\\pi\\), . | \\(\\|T_\\pi u - T_\\pi v\\|_\\infty \\leq \\gamma \\|u - v\\|_\\infty\\), and | \\(\\|T u - T v\\|_\\infty \\leq \\gamma \\|u - v\\|_\\infty\\). | . The proposition can be proved by elementary algebra and the complete proof can be found in Appendix A.2 of Szepesvári (2010). For action $a\\in \\mathcal{A}$, we will find it useful to also define the operator $T_a: \\mathbb{R}^{\\mathrm{S}} \\to \\mathbb{R}^{\\mathrm{S}}$ which matches $T_\\pi$ with the memoryless policy which in every state chooses action $a$. Of course, this operator, being a special case, satisfies the above contraction property as well. This can be seen as performing a one-step lookahead with a fixed action. From Banach’s fixed point theorem, we get the following corollary: . Proposition (Fixed-point iteration): Given any $u \\in \\mathbb{R}^{\\mathrm{S}}$ and any memoryless policy $\\pi$, . | \\(v^\\pi = \\lim_{k\\to\\infty} T_\\pi^k u\\) and in particular for any $k\\ge 0$, \\(\\| v^\\pi - T_\\pi^k u \\|_\\infty \\le \\gamma^k \\| u - v^\\pi \\|_\\infty\\) where \\(v^\\pi\\) is the unique vector/function that satisfies \\(T_\\pi v^\\pi = v^\\pi\\); | \\(v_\\infty=\\lim_{k\\to\\infty} T^k u\\) is well-defined and in particular for any $k\\ge 0$, \\(\\| v_\\infty - T^k u \\|_\\infty \\le \\gamma^k \\| u - v_\\infty \\|_\\infty\\). Furthermore, \\(v_\\infty\\) is the unique vector/function that satisfies \\(Tv_\\infty = v_\\infty\\). | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec2/#bellman-operators-contractions",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec2/#bellman-operators-contractions"
  },"101": {
    "doc": "2. The Fundamental Theorem",
    "title": "The Fundamental Theorem",
    "content": "Definition: A memoryless policy $\\pi$ is greedy w.r.t. to a value function $v: \\mathcal{S} \\rightarrow \\mathbb{R}$ if in every state $s\\in \\mathcal{S}$, with probability one $\\pi$ chooses actions that maximize $(T_a v)(s)=r_a(s) + \\gamma \\langle P_a(s), v \\rangle$. Note that there can be more than one action that maximizes the (one-step) Bellman lookahead $(T_a v)(s)$ at any given state (in case there are ties). In fact, ties can be extremely common: Just imagine “duplicating an action” in every state (i.e., the new action has the same associated transitions and rewards as the copied one). If the copied one was maximizing the Bellman lookahead at some state, the new action will do the same. Because we have finitely many actions, a maximizing action always exist. Thus, we can always “take” a greedy policy w.r.t. any $v\\in \\mathbb{R}^{\\mathrm{S}}$. Proposition (Characterizing greedyness): A memoryless policy $\\pi$ is greedy w.r.t. $v\\in \\mathbb{R}^{\\mathrm{S}}$ if and only if . \\[T_\\pi v = T v\\,.\\] . With this, we are ready to state what I call the Fundamental Theorem of MDPs: . Theorem (Fundamental Theorem of MDPs): The following hold true in any finite MDP: . | Any policy $\\pi$ that is greedy with respect to $v^*$ is optimal: \\(v^\\pi = v^*\\); | It holds that \\(v^* = T v^*\\). | . The equation $v=Tv$ is known as the Bellman optimality equation and the second part of the result can be stated in words by saying that the optimal value function satisfies the Bellman optimality equation. Also, our previous proposition on fixed-point iteration, where we already came across the Bellman optimality equation, foreshadows a way of approximately computing \\(v^*\\) that we will get back to after the proof. Proof: The proof would be easy if we only considered memoryless policies when defining $v^*$. In particular, letting $\\text{ML}$ stand for the set of memoryless policies of the given MDP, define . \\[\\tilde{v}^*(s) = \\sup_{\\pi \\in \\text{ML}} v^\\pi(s) \\quad \\text{for all } s \\in \\mathcal{S}\\,.\\] As we shall see soon, it is not hard to show the theorem just with \\(v^*\\) replaced everywhere with \\(\\tilde{v}^*\\). That is: . | Any policy $\\pi$ that is greedy with respect to $\\tilde{v}^*$ satisfies \\(v^\\pi = \\tilde{v}^*\\); | It holds that \\(\\tilde{v}^* = T \\tilde{v}^*\\). | . This is what we will show in Part 1 of the proof, while in Part 2 we will show that \\(\\tilde{v}^*=v^*\\). Clearly, the two parts together establish the desired result. Part 1: The idea of the proof is to first show that \\(\\begin{align} \\tilde{v}^*\\le T \\tilde{v}^* \\label{eq:suph} \\end{align}\\) and then show that for any greedy policy $\\pi$, \\(v^\\pi \\ge \\tilde{v}^*\\). The displayed equation follows by noticing that \\(v^\\pi \\le \\tilde{v}^*\\) holds for all memoryless policies $\\pi$ by definition. Applying $T_\\pi$ on both sides, using $v^\\pi = T_\\pi v^\\pi$, we get $v^\\pi \\le T_\\pi \\tilde{v}^*$. Taking the supremum of both sides over $\\pi$ and noticing that $T v = \\sup_{\\pi \\in \\text{ML}} T_\\pi v$ for any $v$, together with the definition of \\(\\tilde{v}^*\\) gives \\(\\eqref{eq:suph}\\). Now, take any memoryless policy $\\pi$ that is greedy w.r.t. \\(\\tilde{v}^*\\). Thus, \\(T_\\pi \\tilde{v}^* = T \\tilde{v}^*\\). Combined with \\(\\eqref{eq:suph}\\), we get . \\[\\begin{align} \\label{eq:start} T_\\pi \\tilde{v}^* \\ge \\tilde{v}^*\\,. \\end{align}\\] Applying $T_\\pi$ on both sides and noticing that $T_\\pi$ keeps the inequality intact (i.e., for any $u,v$ such that $u\\le v$ we get $T_\\pi u \\le T_\\pi v$), we get . \\[T_\\pi^2 \\tilde{v}^* \\ge T_\\pi \\tilde{v}^* \\ge \\tilde{v}^*\\,,\\] where the last inequality follows from \\(\\eqref{eq:start}\\). With the same reasoning we get that for any $k\\ge 0$, . \\[T_\\pi^k \\tilde{v}^* \\ge T_\\pi^{k-1} \\tilde{v}^* \\ge \\dots \\ge \\tilde{v}^*\\,,\\] Now, by our proposition, the fixed-point iteration $T_\\pi^k \\tilde{v}^*$ converges to $v^\\pi$. Hence, taking the limit above, we get . \\[v^\\pi \\ge \\tilde{v}^*.\\] This, together with \\(v^\\pi \\le \\tilde{v}^*\\) shows that \\(v^\\pi = \\tilde{v}^*\\). Finally, $T \\tilde{v}^* = T_\\pi \\tilde{v}^* = T_\\pi v^\\pi = v^\\pi = \\tilde{v}^*$. Part 2: It remains to be shown that \\(\\tilde{v}^* = v^*\\). Let $\\Pi$ be the set of all policies. Because $\\text{ML}\\subset \\Pi$, \\(\\tilde{v}^*\\le v^*\\). Thus, it remains to show that . \\[\\begin{align} \\label{eq:mlbigger} v^* \\le \\tilde{v}^*\\,. \\end{align}\\] To show this, we will use the theorem that guaranteed that for any state-distribution $\\mu$ and policy $\\pi$ (memoryless or not) we can find a memoryless policy, which we will call for now $\\text{ML}(\\pi)$, such that $\\nu_\\mu^\\pi = \\nu_\\mu^{\\text{ML}}$. Fix a state $s\\in \\mathcal{S}$. Applying this result with $\\mu = \\delta_s$, we get . \\[\\begin{align*} v^\\pi(s) &amp; = \\langle \\nu_s^\\pi, r \\rangle \\\\ &amp; = \\langle \\nu_s^{\\text{ML}(\\pi)}, r \\rangle \\\\ &amp; \\le \\sup_{\\pi'\\in \\text{ML}} \\langle \\nu_s^{\\pi'}, r \\rangle \\\\ &amp; = \\sup_{\\pi'\\in \\text{ML}} v^{\\pi'}(s) = \\tilde{v}^*(s)\\,. \\end{align*}\\] Taking the supremum of both sides over $\\pi$, we get \\(v^*(s)= \\sup_{\\pi\\in \\Pi} v^\\pi(s) \\le \\tilde{v}^*(s)\\). Since $s\\in \\mathcal{S}$ was arbitrary, we get \\(v^*\\le \\tilde{v}^*\\), finishing the proof. \\(\\qquad\\blacksquare\\) . A property that came up during the proof that we will repeatedly use is that $T_\\pi$ is monotone as an operator. The same holds for $T$. For the record, we state these as a proposition: . Proposition (monotonicity of Bellman operators): For any memoryless policy $\\pi$, $T_\\pi u \\le T_\\pi v$ holds for any $u,v\\in \\mathbb{R}^{\\mathrm{S}}$ such that $u\\le v$. The same also holds for $T$, the Bellman optimality operator. According to the Fundamental Theorem of MDPs, if we have access to the optimal value function \\(v^*\\), then we can find the optimal policy in an efficient and effective way. We just have to greedify it w.r.t. to the value function: (abusing the policy notation) \\(\\pi(s) = \\arg\\max_{a \\in \\mathcal{A}} \\{r_a(s) + \\gamma \\langle P_a(s), v^* \\rangle \\} \\quad \\forall s \\in \\mathcal{S}\\). Such a greedy policy can be found in $O(\\mathrm{S}^2 \\mathrm{A})$ time. Hence, if we can efficiently find the optimal value function, we will get an efficient way of computing an optimal policy. This is to be contrasted with the naive approach to finding an optimal policy, which is to enlist all the policies and compare their value functions to find a policy whose value function dominates the value functions of all the other policies. However, even if we restrict ourselves to just the set of deterministic policies, there are $\\Theta(\\mathrm{A}^{\\mathrm{S}})$ such policies and thus this can be a costly procedure. As it turns out, for finite MDPs, there is a way to calculate optimal policies in time that is polynomial in $\\mathrm{S}$, $\\mathrm{A}$, and $1/(1-\\gamma)$, avoiding the exponential growth of the naive approach with the size of the state space. Algorithms that can do this belong to the family of dynamic programming algorithms. For our purposes, we call any algorithm a dynamic programming algorithm that uses the idea of keeping track of value of states (that is, uses value functions) while doing its calculations. The Fundamental Theorem is somewhat surprising: how come that we can find policies whose value function dominates that of all other policies? In a way, the Fundamental Theorem tells us that the set of value functions of all policies in some MDP (as a set in $\\mathbb{R}^{\\mathrm{S}}$) is very special: It has a “vertex” which dominates all the other value functions. This is quite fascinating. Of course, the key was the Markov property as this gave us the tool to show the result that allowed us to switch from arbitrary policies to memoryless ones. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec2/#the-fundamental-theorem",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec2/#the-fundamental-theorem"
  },"102": {
    "doc": "2. The Fundamental Theorem",
    "title": "Value Iteration",
    "content": "By the Fundamental Theorem, \\(v^*\\) is the fixed point of $T$. By our earlier proposition, which built on the Banach’s fixed point theorem, the sequence \\(\\{T^k v\\}_{k\\ge 0}\\) converges to \\(v^*\\) at a geometric rate. In the context of MDPs, the process of repeatedly applying $T$ to some function is called value iteration. The initial function is usually taken to be the all-zero function, which we denote by $\\mathbf{0}$, but, of course, if there is a better initial guess on \\(v^*\\), that guess can also be used at initialization. The next result gives a bound on the number of iterations required to reach an $\\varepsilon$-neighborhood (in the max-norm sense) of $v^*$: . Theorem (Value Iteration): Consider an MDP with immediate rewards in the $[0,1]$ interval. Pick an arbitrary positive number $\\varepsilon&gt;0$. Let $v_0 = \\boldsymbol{0}$ and set . \\[v_{k+1} = T v_k \\quad \\text{for } k = 0, 1, 2, \\ldots\\] Then, for $k\\ge \\ln(1/(\\varepsilon(1-\\gamma))/\\ln(1/\\gamma)$, \\(\\|v_k -v^*\\|_\\infty \\le \\varepsilon\\). Before the proof recall that . \\[H_{\\gamma,\\varepsilon}:= \\frac{\\ln(1/(\\varepsilon(1-\\gamma)))}{1-\\gamma} \\ge \\frac{\\ln(1/(\\varepsilon(1-\\gamma)))}{\\ln(1/\\gamma)}\\,.\\] Thus, the effective horizon, $H_{\\gamma,\\varepsilon}$, whom we met in the first lecture, appeared again. Of course, this is no coincidence. Proof: By our assumptions on the rewards, $\\mathbf{0} \\le v^\\pi \\le \\frac{1}{1-\\gamma} \\mathbf{1}$ holds for any policy $\\pi$. Hence, \\(\\|v^*\\|_\\infty \\le \\frac{1}{1-\\gamma}\\) also holds. By our fixed-point iteration proposition, we get . \\[\\begin{align*} \\|v_k - v^*\\|_\\infty &amp;\\leq \\gamma^k \\|v^* - \\mathbf{0}\\|_\\infty = \\gamma^k \\|v^*\\|_\\infty \\leq \\frac{\\gamma^k}{1 - \\gamma} \\,. \\end{align*}\\] Solving for the smallest $k$ such that \\(\\gamma^k/(1-\\gamma)\\le \\varepsilon\\) gives the result. \\[\\tag*{$\\blacksquare$}\\] For fixed $\\gamma&lt;1$, note the mild dependence of the iteration complexity on the target accuracy $\\varepsilon$: we can expect with only a handful iterations to get in a small vicinity of $v^*$. Note also that the total computation cost is $O(\\mathrm{S}^2 \\mathrm{A}k)$ and the space required is at most $O(\\mathrm{S})$, all assuming each value takes up $O(1)$ memory and arithmetic and logic operations also require $O(1)$ time. Note that accuracy requirement was set up in the form of additive errors. If the value function \\(v^*\\) is of order $1/(1-\\gamma)$ (the maximum possible order), a relative accuracy of order $2$ means setting $\\epsilon=0.5/(1-\\gamma)$, making the iteration complexity to be $\\ln(2)/(1-\\gamma)$. However, for controlling the relative error, the more interesting case is when \\(v^*\\) takes on small values. Here, we see that the complexity may grow unbounded. Later, we will see that in a way this lack of fine-grained error control of value iteration will mean that value iteration is not ideal for calculating exactly optimal policies. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec2/#value-iteration",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec2/#value-iteration"
  },"103": {
    "doc": "2. The Fundamental Theorem",
    "title": "Notes",
    "content": "Value functions are well-defined . As noted in the text, value functions are well-defined despite that the probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ is not uniquely defined. In fact, for any $f: (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}} \\to \\mathbb{R}$ (measurable) function and for any $(\\Omega,\\mathcal{F},\\mathbb{P})$ and $(\\Omega’,\\mathcal{F}’,\\mathbb{P}’)$ probability spaces, as long as both $\\mathbb{P}$ and $\\mathbb{P}’$ satisfy the requirements postulated in the existence theorem, \\(\\int f(\\tau(\\omega)) \\mathbb{P}(d\\omega)=\\int f(\\tau(\\omega)) \\mathbb{P}'(d\\omega)\\), or, introducing $\\mathbb{E}$ ($\\mathbb{E}’$) to denote the expectation operator underlying $\\mathbb{P}$ (respectively, $\\mathbb{P}’$), \\(\\mathbb{E}[f(\\tau)]=\\mathbb{E}'[f(\\tau)]\\). It also follows that if we only need probabilities and expectations over trajectories, it suffices to choose $(\\Omega,\\mathcal{F},\\mathbb{P})$ as the canonical probability space induced by the state-action space of the MDP at hand. Other types of MDPs . The obvious question is what survives of all this in other types of MDPs, such as finite-horizon homogenous or inhomogeneous, with or without discounting, total cost (i.e. negative rewards only), or of course the average cost setting? The story is that the arguments can be usually made to work, but this is not entirely automatic. The subject is well-studied and we will give some references and hints later, perhaps even answer some of these questions. Infinite spaces anyone? . The first thing that changes when we switch to infinite spaces is that we cannot take the assumption that the immediate rewards are bounded for granted. This can cause quite a bit of trouble: $v^\\pi$ for some policies can be unbounded, and the same holds for $v^*$. Negative infinite values could be especially “hurtful”. (LQR control is the simplest example where this comes up.) . Another issue is that we cannot take the existence of greedy policies for granted. This happens already when the number of actions is infinite (what is the action that maximizes the reward $r_a(s)=1-1/a$ where $a&gt;0$?). Oftentimes compactness of the action space and continuity assumptions help with this, though, as much of what we will do will be approximate, approximate greedification should be sufficient for most of the time. From this perspective, that greedy actions may not exist is just annoyance. Finally, when either the state or action space is uncountably infinite, one has to be careful even with the definition of policies. Using a technical term from probability theory, a choice that makes thing work is to restrict policies to be probability kernels. Using this definition means that we need to put measurability structures over both the state and action spaces (this is only crucial when either respective set has a larger than countable cardinality). The main change here is that with policies defined this way, for any $U$ measurable subset of $\\mathcal{A}$, $h_t \\mapsto \\pi_t(U|h_t)$ must be measurable. This allows us then the use of the Ionescu-Tulcea theorem and at least the definitions can be made to work. The next difficulty in this case is that “greedification” may lead to outside of the set of these “measurable policies”, which could prevent the existence of optimal policies (again, if we are contend with approximate optimality, this difficulty disappears). There is a large literature concerned with these issues. From infinite trajectories to their finite prefixes . Since trajectories are allowed to be infinitely long, we have a nonconstructive result only for the existence of the probability measures induced by the interconnection of policies and MDPs. Oftentimes we need to check whether two probability measures over these infinitely long trajectories coincide. How can this be done? A general result from measure theory says that two measures agree, if they agree on a generator of the underlying $\\sigma$-algebra. A convenient generator system for the $\\sigma$-algebra over the trajectories (for the canonical probability space) is the cylinder sets that take the form . \\[\\{s_0\\} \\times \\{a_0\\} \\times \\dots\\times \\{ s_t \\} \\times \\mathcal{A} \\times (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}\\] and . \\[\\{s_0\\} \\times \\{a_0\\} \\times \\dots\\times \\{ s_t \\} \\times \\{a_t\\} \\times (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}\\] for some $s_0,a_0,\\dots,s_t,a_t,\\dots$. That is, if $\\mathbb{P}$ and $\\mathbb{P}’$ agree on the probabilities assigned to these sets, they agree everywehere. This makes things a full circle: what this result says is that we only need to check the probabilities assigned to finite prefixes of the infinitely long trajectories. Phew. Since the probabilities assigned to these finite prefixes are a function of $\\mu$, $P$ and $\\pi$ alone, it follows that there is a unique probability measure over the trajectory space $ (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}$ that satisfies the requirements postulated in the existence theorem. That is, the canonical probability space is uniquely defined. Optimization with (Discounted) Occupancy Measures . We learned that the value function can be represented as $v^\\pi(\\mu) = \\sum_{s, a} r_a(s) \\nu_\\mu^\\pi(s, a) = \\langle \\nu_\\mu^\\pi, r \\rangle$. Thus, maximizing the value function for a given initial distribution $\\mu$ is equivalent to maximizing the dot product between $\\nu_\\mu^\\pi$ and $r$. Next, we present a concrete example and point out some interesting results. To keep this example as simple as possible, we introduce some new notation. Let $\\mathcal{A}(s)$ represent the set of actions admissable to the state $s \\in \\mathcal{S}$. We now define the MDP. Let $\\mathcal{S} = \\{s_1, s_2 \\}$, $\\mathcal{A}(s_1) = \\{a_1, a_2 \\}$ and $\\mathcal{A}(s_2) = \\{a_3 \\}$. Also, let . \\[\\begin{align*} P_{a_1}(s_1, s_1) = 1, &amp;\\quad r_{a_1}(s_1) = 1 \\\\ P_{a_2}(s_1, s_2) = 1, &amp;\\quad r_{a_2}(s_1) = 1/2 \\\\ P_{a_3}(s_2, s_2) = 1, &amp;\\quad r_{a_3}(s_2) = 1/2. \\end{align*}\\] Our policy $\\pi$ can be parametrized by one parameter $p$ as . \\[\\begin{align*} \\pi(a_1|s_1) &amp;= p \\\\ \\pi(a_2|s_1) &amp;= 1 - p \\\\ \\pi(a_3|s_2) &amp;= 1. \\end{align*}\\] Finally, we assume $\\mu(s_1) = 1$. We explicitly write out $\\nu_\\mu^\\pi(s, a) = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{P}_\\mu^\\pi (S_t = s, A_t = a)$ for all state-action pairs. \\[\\begin{align*} \\nu_\\mu^\\pi(s_1, a_1) &amp;= \\sum_{t=0}^\\infty \\gamma^t p^{t+1} \\\\ &amp;= p\\sum_{t=0}^\\infty (\\gamma p)^t \\\\ &amp;= \\frac{p}{1-\\gamma p} \\\\ \\nu_\\mu^\\pi(s_1, a_2) &amp;= \\sum_{t=0}^\\infty \\gamma^t p^t (1-p) \\\\ &amp;= (1-p)\\sum_{t=0}^\\infty (\\gamma p)^t \\\\ &amp;= \\frac{1-p}{1-\\gamma p} \\\\ \\nu_\\mu^\\pi(s_2, a_3) &amp;= \\frac{1}{1-\\gamma} - \\frac{p}{1-\\gamma p} - \\frac{1-p}{1-\\gamma p} \\end{align*}\\] Recall, our goal is to maximize $\\sum_{s, a} r_a(s) \\nu_\\mu^\\pi(s, a)$. To do this we plug in the above quantities for $r_a(s)$ and $\\nu_\\mu^\\pi(s, a)$ . \\[\\begin{align*} \\sum_{s, a} r_a(s) \\nu_\\mu^\\pi(s, a) &amp;= \\frac{1-p}{1-\\gamma p} + \\frac{1}{2} \\left( \\frac{p}{1-\\gamma p} \\right) + \\frac{1}{2} \\left( \\frac{1}{1-\\gamma} - \\frac{p}{1-\\gamma p} - \\frac{1-p}{1-\\gamma p} \\right) \\\\ &amp;= \\frac{1}{2} \\left( \\frac{p}{1-\\gamma p} \\right) + \\frac{1}{2} \\left( \\frac{1}{1-\\gamma} \\right). \\end{align*}\\] Noting that the function on the right hand side is monotone increasing for $p \\in [0, 1]$, so we get that the above quantity is maximized for $p = 1$. Thus, the optimal policy is . \\[\\begin{align*} \\pi(a_1|s_1) &amp;= 1 \\\\ \\pi(a_2|s_1) &amp;= 0 \\\\ \\pi(a_3|s_2) &amp;= 1. \\end{align*}\\] Which, aligns with our intuition that action $a_1$ should always be selected in state $s_1$ since it produces larger reward. Notice how the set of occupancy measures . \\[\\left\\{ (t, (1-\\gamma t-t), 1/(1-\\gamma)-t-(1-\\gamma t-t)) : t \\in [0, 1/(1-\\gamma)] \\right\\}\\] is a convex set. This examples shows that optimizing in the space of occupancy measures could be a linear optimization while optimizing with a policy parametrization could be a non-linear optimization. Fundamental Theorem . I think I have seen Bertsekas and Shreve call the theorem I call fundamental also by the same name. However, this is not quite a standard name. Nevertheless, the result is important and many other things follow from it. In a way, this is the result that is at the heart of all the theory. I think it deserves this name. I have probably read the proof presented here somewhere, but this was a while ago and the source escapes me. In the RL literature people often start with memoryless policies and work with \\(\\tilde{v}^*\\) rather than with \\(v^*\\). The question whether \\(\\tilde{v}^*=v^*\\) is well-studied and understood, mostly in the control and operations research literature. The geometry of the space of value functions . An alternative way of seeing the fundamental theorem is as a result concerning the geometry of the space of value functions. Indeed, fix an MDP $M$ and let \\(\\mathcal{V} = \\{ v^\\pi \\,:\\, \\pi \\text{ is a policy of } M \\}\\), while let \\(\\mathcal{V}^{\\mathrm{DET}} = \\{ v^\\pi \\,:\\, \\pi \\text{ is a deterministic memoryless policy of } M \\}\\). The set $\\mathcal{V}$ is the set of all value functions of $M$. Both sets are subsets of $\\mathbb{R}^{\\mathcal{S}}$. Using terminology from multicriteria optimization, the optimal value function, \\(v^*\\), is the ideal point of $\\mathcal{V}$: \\(v^*(s) = \\sup \\{ v(s)\\,:\\, v\\in \\mathcal{V} \\}\\) for all $s\\in \\mathcal{S}$. Then, the fundamental theorem states that the ideal point of $\\mathcal{V}$ belongs to $\\mathcal{V}$: \\(v^* \\in \\mathcal{V}\\) and in fact \\(v^* \\in \\mathcal{V}^{\\mathrm{DET}}\\). However, more is known about $\\mathcal{V}$: . Theorem (existence theorem): Fix a finite MDP $M$. Then $\\mathcal{V} \\subset \\mathbb{R}^{\\mathcal{S}}$ is convex. Furthermore, any extreme point of $\\mathcal{V}$ belongs to $\\mathcal{V}^{\\mathrm{DET}}$. This result is due to Dadashi et al. (2019). Banach’s fixed point theorem . This theorem can be found in Appendix A.1 of my short RL book (Szepesvári, 2010). However, of course, it can be found in many places (the Wikipedia article is also OK). It is worthwhile to spend some time with this theorem to understand its conditions, going back to concepts like Cauchy-sequences (which should perhaps be called sequences with vanishing oscillations) and completeness of the set of real numbers. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec2/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec2/#notes"
  },"104": {
    "doc": "2. The Fundamental Theorem",
    "title": "References",
    "content": "The references mentioned before: . | Lattimore, T., &amp; Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press. | Szepesvári, C. (2010). Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence and machine learning, 4(1), 1-103. | . The next work (a book chpater) gives a concise yet relatively thorough introduction. The chapter also gives a proof of the fundamental theorem; through the sufficiency of Markov policies. This is done for the discounted and also for a number of alternate criteria. | Garcia, Frédérick, and Emmanuel Rachelson. 2013. “Markov Decision Processes.” In Markov Decision Processes in Artificial Intelligence, 1–38. Hoboken, NJ USA: John Wiley &amp; Sons, Inc. | . A summary of basic results for countable and Borel state-space, and Borel action spaces, with potentially unbounded (from below) reward functions can be found in the next (excellent) paper, which also gives a concise overview of the history of these results: . | Feinberg, Eugene A. 2011. Total Expected Discounted Reward MDPS: Existence of Optimal Policies. In Wiley Encyclopedia of Operations Research and Management Science. Hoboken, NJ, USA: John Wiley &amp; Sons, Inc. | . An argument showing the fundamental theorem for the finite-horizon case derived from a general result of David Blackwell can be found in a blog-post of Maxim Raginsky, who gives further pointers, most notable this. David Blackwell has contributed in numerous ways to the foundations of statistics, decision theory, probability theory, and many many other subjects and the importance of his work cannot be overstated. | Robert Dadashi, Adrien Ali Taïga, Nicolas Le Roux, Dale Schuurmans, Marc G. Bellemare. 2019. The Value Function Polytope in Reinforcement Learning. ICML. arXiv | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec2/#references",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec2/#references"
  },"105": {
    "doc": "2. The Fundamental Theorem",
    "title": "2. The Fundamental Theorem",
    "content": "PDF Version . We start by recapping the definition of MDPs and then firm up the loose ends from the previous lecture: why do the probability distributions \\(\\mathbb{P}_\\mu^\\pi\\) exist and how are they defined? We then continue with the introduction of what we call the Fundamental Theorem of Dynamic Programming and end with the discussion of value iteration. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec2/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec2/"
  },"106": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "Finding a Near-Optimal Policy using Value Iteration",
    "content": "In the previous lecture we found that the iterative computation that starts with some $v_0\\in \\mathbb{R}^{\\mathrm{S}}$ and then obtains $v_{k+1}$ using the “Bellman update” . \\[\\begin{align} v_{k+1} = T v_k \\label{eq:vi} \\end{align}\\] leads to a sequence \\(\\{v_k\\}_{k\\ge 0}\\) whose \\(k\\)th term approaches \\(v^*\\), the optimal value function, at a geometric rate: . \\[\\begin{align} \\| v_k - v^* \\|_\\infty \\le \\gamma^k \\| v_0 - v^* \\|_\\infty\\,. \\label{eq:vierror} \\end{align}\\] While this is reassuring, our primary goal is to obtain an optimal, or at least a near-optimal policy. Since any policy that is greedy with respect to (w.r.t) \\(v^*\\) is optimal, a natural idea is to stop the value iteration after some finite number of iteration steps and return a policy that is greedy w.r.t. the approximation of \\(v^*\\) that was just obtained. If we stop the process after the \\(k\\)th step, this defines a policy \\(\\pi_k\\) such that \\(\\pi_k\\) is greedy w.r.t. \\(v_k\\): \\(T_{\\pi_k} v_k = T v_k\\). The hope is that as \\(v_k\\) approaches \\(v^*\\), the policies \\(\\{\\pi_k\\}\\) will also get better in the sense that \\(\\|v^*-v^{\\pi_k}\\|_\\infty\\) decreases. The next theorem guarantees that this will indeed be the case. Theorem (Policy Error Bound): Let \\(v: \\mathcal{S} \\to \\mathbb{R}\\) be arbitrary and \\(\\pi\\) be the greedy policy w.r.t. \\(v\\): \\(T_\\pi v = T v\\). Then, . \\[v^\\pi \\geq v^* - \\frac{2 \\gamma \\|v^*-v\\|_\\infty}{1 - \\gamma} \\boldsymbol{1}.\\] . In words, the theorem states that the policy error (\\(\\|v^*-v^{\\pi}\\|_\\infty\\)) of a policy that is greedy with respect to a function \\(v\\) is controlled by the distance of $v$ to \\(v^*\\). This can also be seen as stating that the “greedy operator” $\\Gamma$, which maps functions $v\\in \\mathbb{R}^{\\mathcal{S}}$ to a policy that is greedy w.r.t. $v$, is continuous at $v=v^*$ when the “distance” $d(\\pi,\\pi’)$ between policies $\\pi,\\pi’$ is defined as the maximum norm distance between their value functions: \\(d(\\pi,\\pi') = \\| v^{\\pi}- v^{\\pi'}\\|_\\infty\\). Indeed, with the help of this notation, an alternative form of the theorem statement is that for any $v\\in \\mathbb{R}^{\\mathcal{S}}$, . \\[d( \\Gamma(v^*), \\Gamma(v) ) \\le \\frac{2\\gamma \\|v^*-v\\|_\\infty}{1-\\gamma}\\,.\\] In words, this can be described as that $v\\mapsto \\Gamma(v)$ is is “\\(2\\gamma/(1-\\gamma)\\)-smooth” at $v=v^*$ when the input space is equipped with the maximum norm distance and the output space is equipped with $d$. One can also show that this result is sharp in that the constant \\(2\\gamma/(1-\\gamma)\\) cannot be improved. The proof is an archetypical example of proofs of using contraction and monotonicity arguments to prove error bounds. We will see variations of this proof many times. Before the proof, let us introduce the notation $|x|$ for a vector $\\mathbb{R}^d$ to mean the componentwise absolute value of the vector: \\(|x|_i = |x_i|\\), \\(i\\in [d]\\). As a way of using this notation, note that for any memoryless policy $\\pi$, \\(\\begin{align} |P_\\pi x |\\le P_\\pi |x| \\le \\|x\\|_\\infty P_\\pi \\boldsymbol{1} = \\|x\\|_\\infty \\boldsymbol{1}\\,, \\label{eq:ppineb} \\end{align}\\) and hence \\(\\begin{align} \\|P_\\pi x \\|_\\infty \\le \\|x\\|_\\infty\\,. \\label{eq:stochmxne} \\end{align}\\) In Eq. \\(\\eqref{eq:ppineb}\\) the first inequality follows because $P_\\pi$ is monotone and \\(x\\le |x| \\le \\|x\\|_\\infty \\boldsymbol{1}\\). For the proof it will also be useful to recall that we also have . \\[\\begin{align} T_\\pi (v+c \\boldsymbol{1}) &amp;= T_\\pi v \\,\\, + c \\gamma \\boldsymbol{1}\\,, \\label{eq:tpiadd1} \\\\ T (v+c \\boldsymbol{1}) &amp;= T v \\,\\, + c \\gamma \\boldsymbol{1}\\,, \\label{eq:tadd1} \\end{align}\\] for any \\(v\\in \\mathbb{R}^{\\mathrm{S}}\\), \\(c\\in \\mathbb{R}\\) and memoryless policy \\(\\pi\\). These two identities follow just by the definitions of $T$ and $T_\\pi$, as the reader can easily verify them. Proof: Let \\(v,v^*,\\pi\\) be as in the theorem statement and let \\(\\varepsilon = \\|v^*-v\\|_\\infty\\). Let \\(\\delta = v^*-v^\\pi\\). The result follows by algebra once we prove that \\(\\|\\delta\\|_\\infty \\le \\gamma \\|\\delta\\|_\\infty + 2\\gamma \\varepsilon\\). Hence, we only need to prove this inequality. By our assumptions on \\(v\\) and \\(v^*\\), \\(-\\varepsilon\\boldsymbol{1}\\le v^*-v \\le \\varepsilon\\boldsymbol{1}\\). Now, . \\[\\begin{align*} \\delta &amp; = v^*-v^\\pi \\\\ &amp; = \\textcolor{red}{T} v^* - \\textcolor{red}{T_\\pi} v^\\pi &amp; \\text{(Fundamental Theorem, $T_\\pi v^\\pi = v^\\pi$)}\\\\ &amp; \\le T(v+\\textcolor{red}{\\varepsilon\\boldsymbol{1}})-T_\\pi v^\\pi &amp; \\text{($T$ monotone)}\\\\ &amp; = Tv-T_\\pi v^\\pi +\\textcolor{red}{\\gamma\\varepsilon\\boldsymbol{1}} &amp; \\text{(Eq. \\eqref{eq:tadd1})}\\\\ &amp; = \\textcolor{red}{T_\\pi} v-T_\\pi v^\\pi +\\gamma\\varepsilon\\boldsymbol{1} &amp; \\text{($\\pi$ def.)}\\\\ &amp; \\le T_\\pi(v^*+\\textcolor{red}{\\varepsilon\\boldsymbol{1}})-T_\\pi v^\\pi + \\gamma \\varepsilon \\boldsymbol{1} &amp; \\text{($T_\\pi$ monotone)}\\\\ &amp; = T_\\pi v^* - T_\\pi v^\\pi + \\textcolor{red}{2}\\gamma \\varepsilon\\boldsymbol{1} &amp; \\text{(Eq. \\eqref{eq:tpiadd1})}\\\\ &amp; = \\textcolor{red}{\\gamma P_\\pi}(v^*-v^\\pi)+2\\gamma \\varepsilon\\boldsymbol{1} &amp; \\text{($T_\\pi$ def.)}\\\\ &amp; = \\gamma P_\\pi \\textcolor{red}{\\delta}+2\\gamma \\varepsilon\\boldsymbol{1}\\,. &amp; \\text{($\\delta$ def.)} \\end{align*}\\] Taking the (pointwise) absolute value of both sides and using the triangle inequality, and then Eq. \\(\\eqref{eq:stochmxne}\\) we find that \\(\\begin{align*} |\\delta| \\le \\gamma \\|\\delta\\|_\\infty \\boldsymbol{1} + 2\\gamma \\varepsilon\\boldsymbol{1}\\,. \\end{align*}\\) The proof is finished by taking the maximum over the components, noting that \\(\\max_s |\\delta|_s = \\|\\delta\\|_\\infty\\). \\(\\qquad \\blacksquare\\) . An alternative way of finishing the proof is to note that from $\\delta = \\gamma P_\\pi \\delta + 2\\gamma \\varepsilon \\boldsymbol{1}$, by reordering and using that $(I-\\gamma P_\\pi)^{-1} = \\sum_{i\\ge 0} \\gamma^i P_\\pi^i$ is a monotone operator, $\\delta \\le 2\\gamma \\varepsilon \\sum_{i\\ge 0} \\gamma^i P_\\pi \\boldsymbol{1} = 2\\gamma \\varepsilon/(1-\\gamma) \\boldsymbol{1}$. Taking the max-norm of both sides, we get \\(\\|\\delta\\|_\\infty \\le 2\\gamma \\varepsilon/(1-\\gamma)\\). ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec3/#finding-a-near-optimal-policy-using-value-iteration",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec3/#finding-a-near-optimal-policy-using-value-iteration"
  },"107": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "Value Iteration as an Approximate Planning Algorithm",
    "content": ". From Eq. \\(\\eqref{eq:vierror}\\) we see for \\(k \\geq H_{\\gamma, \\varepsilon} = \\frac{\\ln(1 / (\\varepsilon (1 - \\gamma)))}{1 - \\gamma}\\), started with \\(v_0 =0\\), value iteration yields \\(v_k\\) such that \\(\\|v_k - v^*\\|_\\infty \\leq \\varepsilon\\) and consequently, for a policy \\(\\pi_k\\) that is greedy w.r.t. \\(v_k\\), \\(v^{\\pi_k} \\geq v^* - \\frac{2 \\gamma \\varepsilon}{1 - \\gamma} \\boldsymbol{1}\\). Now, for a fixed $\\delta&gt;0$ setting $\\varepsilon$ so that \\(\\delta = \\frac{2 \\gamma \\varepsilon}{1 - \\gamma}\\) holds, we see that after \\(k \\geq H_{\\gamma, \\frac{\\delta(1 - \\gamma)}{2\\gamma}}\\) iterations, we get a \\(\\delta\\)-optimal policy \\(\\pi_k\\): \\(v^{\\pi_k} \\geq v^* - \\delta \\boldsymbol{1}\\). Computing \\(v_{k+1}\\) using \\(\\eqref{eq:vi}\\) takes \\(O(\\mathrm{S}^2 \\mathrm{A})\\) elementary arithmetic (and logic) operations. Putting things together we get the following result: . Theorem (Runtime of Approximate Planning with Value Iteration): Fix a finite discounted MDP and a target accuracy $\\delta&gt;0$. Then, after . \\[O \\left(\\mathrm{S}^2 \\mathrm{A} H_{\\gamma, \\frac{\\delta(1 - \\gamma)}{2\\gamma}} \\right) = \\tilde O\\left( \\frac{\\mathrm{S}^2 \\mathrm{A} }{1 - \\gamma}\\, \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\\] elementary arithmetic operations, value iteration produces a policy $\\pi$ that is $\\delta$-optimal: $v^\\pi \\ge v^* - \\delta \\boldsymbol{1}$, where the $\\tilde{O}(\\cdot)$ result holds when $\\delta \\le 1/e$ is fixed and $\\tilde{O}(\\cdot)$ hides a $\\log(1/(1-\\gamma))$ term. Note that the number of operations needed depends very mildly on the target accuracy. However, accuracy here means an additive error. While the optimal value could be as high as \\(1/(1-\\gamma)\\), it can easily happen that the best value that can be achieved, \\(\\|v^*\\|_\\infty\\), is significantly smaller than \\(1/(1-\\gamma)\\). It may be for example that \\(\\|v^*\\|_\\infty = 0.01\\), in which case a guarantee with \\(\\delta = 0.5\\) is vacuous. By a careful inspection of \\(\\eqref{eq:vierror}\\) we can improve the previous result so that this problem is avoided: . Theorem (Runtime when Controlling for the Relative Error): Fix a finite discounted MDP and a target accuracy \\(\\delta_{\\text{rel}}&gt;0\\). Then, stopping value iteration after $k \\ge H_{\\gamma,\\frac{\\delta_{\\text{rel}}}{2\\gamma}}$ iterations, the policy $\\pi$ produced satisfies the relative error bound . \\[v^\\pi \\ge v^* - \\delta_{\\text{rel}} \\|v^*\\|_\\infty \\boldsymbol{1}\\,,\\] while the total number of elementary arithmetic operations is . \\[O \\left(\\mathrm{S}^2 \\mathrm{A} H_{\\gamma, \\frac{\\delta_{\\text{rel}}}{2\\gamma}} \\right) = \\tilde O\\left( \\frac{\\mathrm{S}^2 \\mathrm{A} }{1 - \\gamma}\\, \\ln\\left(\\frac{1}{\\delta_{\\text{rel}}}\\right)\\right)\\] where $\\tilde{O}(\\cdot)$ hides $\\log(1/(1-\\gamma))$. Notice that the runtime required to achieve a fixed relative accuracy appears to be the same as the runtime required to achieve the same level of absolute accuracy. In fact, the runtime slightly decreases. This should make sense: The worst-case for the fixed absolute accuracy is when \\(\\|v^*\\|_\\infty=1/(1-\\gamma)\\), and in this case the relative accuracy is significantly less demanding: With \\(\\delta_{\\text{rel}}=0.5\\), value iteration can stop after guaranteeing values of \\(0.5/(1-\\gamma)\\), which, as a value, is much smaller than \\(1/(1-\\gamma)-0.5\\), the target with the absolute accuracy level of \\(\\delta = 0.5\\). Note that the relative error bound is not without problems either: It is possible that for some states $s$, \\(v^*(s)-\\delta_{\\text{rel}} \\|v^*\\|_\\infty\\) is negative, a vacuous guarantee. A reasonable stopping criteria would be to stop when the policy that we read out satisfies . \\[v^{\\pi_k} \\ge (1-\\delta_{\\text{rel}}) v^*\\,.\\] Since \\(v^*\\) is not available, to arrive at a stopping condition that can be verified and which implies the above inequality, one can replace $v^*$ above with an upper bound on it, such as \\(v_k +\\gamma^k \\|v_k\\|_\\infty/(1-\\gamma^k) \\boldsymbol{1}\\). In this imagined procedure, in each iteration, one also needs to compute the value function of policy $\\pi_k$ to verify whether the stopping condition is met. If we do this much computation, we may as well replace $v_k$ with $v^{\\pi_k}$ in the update equation \\(\\eqref{eq:vi}\\) hoping that this will further speed up convergence. This results in what is known as policy iteration, which is the subject of the next lecture. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec3/#value-iteration-as-an-approximate-planning-algorithm",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec3/#value-iteration-as-an-approximate-planning-algorithm"
  },"108": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "The Computational Complexity of Planning in MDPs",
    "content": "Now that we have our first results for the computation of approximately optimal policies, it is time to ask whether the algorithm we discovered is doing unnecessary work. That is, what is the minimax computational cost of calculating an optimal, or approximately optimal policy? . To precisely formulate this problem, we need to specify the inputs and the outputs of the algorithms considered. The simplest setting is when the inputs to the algorithms are arrays, describing the transition probabilities and the rewards for each state action pair with some ordering of state-action pairs (and next states in the case of transition probabilities). The output, by the Fundamental Theorem, can be a memoryless policy, either deterministic or stochastic. To describe such a policy, the algorithm could write a table. Clearly, the runtime of the algorithm will be at least the size of the table that needs to be written, so the shorter the output, the better the runtime can be. To be nice with the algorithms, we should allow them to output deterministic policies. After all, the Fundamental Theorem also guarantees that we can always find a deterministic memoryless policy which is optimal. Further, greedy policies can also be chosen to be deterministic, so the value-iteration algorithm would also satisfy this requirement. The shortest specification for a deterministic policy is an array of the size of the state space that has \\(\\mathrm{S}\\) entries. Thus, the runtime of any algorithm that needs to “produce” a fully specified policy is at least \\(\\Omega(\\mathrm{S})\\). This is quite bad! As was noted before, \\(\\mathrm{S}\\), the number of states, in typical problems is expected to be gigantic. But by this easy argument we see that if we demand algorithms to produce fully specified policies then without any further help, they have to do as much work as the number of states. However, things are a bit even worse. In Homework 0, we have seen that no algorithm can find a given value in an array without looking at all entries of the array (curiously, we saw that if we allow randomized computation, that on expectation it is enough to check half of the entries). Based on this, it is not hard to show the following result: . Theorem (Computation Complexity of Planning in MDPs): . Let $0\\le \\delta &lt; \\gamma/(1-\\gamma)$. Any algorithm that is guaranteed to produce $\\delta$-optimal policies in any finite MDP described with tables, with a fixed discount factor $0\\le \\gamma &lt;1$ and rewards in the $[0,1]$ interval needs at least \\(\\Omega(\\mathrm{S}^2\\mathrm{A})\\) elementary arithmetic operations on some MDP with the above properties and whose state space is of size $\\mathrm{S}$ and action space is of size $\\mathrm{A}$. Proof sketch: We construct a family of MDPs such that no matter the algorithm, the algorithm will need to perform the said number of operations in at least one of the MDPs. One-third of the states is reserved for “heaven”, one-third is reserved for “hell” states. The remaining one-third set of states, call them $R$, is where the algorithms will need to make some nontrivial amount of work. The MDPs are going to be deterministic. In the tables given to the algorithms as input, we (conveniently for the algorithms) order the states so that the “hell” states come first, followed by the “heaven” states, followed by the states in $R$. In the “heaven” class, all states self-loop under all actions and give a reward of one. The optimal value of any of these states is $1/(1-\\gamma)$. In the “hell” class, states also self-loops under all actions but give a reward of zero. The optimal value of these states is $0$. For the remaining states, all actions except one lead to some hell state, while the chosen special action leads to some state in the heaven class. The optimal value of all states in set $R$ have a value of $\\gamma/(1-\\gamma)$ and the value of a policy that in a state in $R$ does not choose the special optimal action gets the value of $0$ in that state. It follows that any algorithm that is guaranteed to be $\\delta$ optimal needs to identify the unique optimal action at every state in $R$. In particular, for every state $s\\in R$ and action $a\\in \\mathcal{A}$, the algorithm needs to read $\\Omega(\\mathrm{S})$ entries of the transition probability vector $P_a(s)$ or it can’t find out whether $a$ leads to a state in the heaven class or the hell class: The probability vector $P_a(s)$ will have a single one at such an entry, either among the $\\mathrm{S}/3$ entries representing the hell, or the $\\mathrm{S}/3$ entries representing the heaven states. By the aforementioned homework problem, any algorithm that needs to find this “needle” requires to check $\\Omega(\\mathrm{S})$ entries. Since the number of states in $R$ is also $\\Omega(\\mathrm{S})$, we get that the algorithm needs to do $\\Omega( \\mathrm{S}\\times \\mathrm{A}) \\mathrm{S}) = \\Omega( \\mathrm{S}^2 \\mathrm{A})$ work. \\(\\qquad \\blacksquare\\) . We immediately see two differences between the lower bound and our previous upper bound(s): In the lower bound there is no dependence on $1/(1-\\gamma)$ (the effective horizon at a constant precision). Furthermore, there is no dependence on $1/\\delta$, the inverse accuracy. As it turns out, the dependence on $1/\\delta$ of value-iteration is superfluous and can be removed. The algorithm that achieves this is policy iteration, which was mentioned earlier. However, this result is saved for the next lecture. After this, the only remaining gap will be the order of the polynomials and the dependence on $1/(1-\\gamma)$, which is closely related to the said polynomial order. And of course, we save for later the most pressing issue that we need to somehow be able to avoid the situation when the runtime depends on the size of the state space (forgetting about the action space for a moment). By the lower bound just presented we already know that this will require changing the problem setting. Just how to do this will be the core question that we will keep returning to in the class. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec3/#the-computational-complexity-of-planning-in-mdps",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec3/#the-computational-complexity-of-planning-in-mdps"
  },"109": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "Notes",
    "content": "Value iteration . The idea of value iteration is probably due to Richard Bellman. Error bound for greedification . This theorem is due to Singh &amp; Yee, 1994. The example that shows that the result stated in the theorem is tight. Consider an MDP with two states, call them $A$ and $B$, two actions, and deterministic dynamics. Call the two actions $a$ and $b$. Regardless the state where it is used, action $a$ makes the next state transit to state $A$, while giving a reward of $2\\gamma \\epsilon$. Analogously, action $b$ makes the next state transit to state $B$, while giving a reward of $0$. The optimal values in both states are $2\\gamma \\varepsilon/(1-\\gamma)$. Let $v$ be so that \\(v(A) = v^*(A)-\\epsilon\\), while \\(v(B)=v^*(B)+\\epsilon\\). Thus, $v$ underestimates the value of $A$, while it overestimates the value of state $B$. It is not hard to see that the policy $\\pi$ that uses action $b$ regardless the state is greedy with respect to $v$ (actually, the action-values of the two actions tie at both states). The value function of this policy assigns the value of $0$ to both states, showing that the result stated in the theorem is indeed tight. Computational complexity lower bound . The last theorem is due to Chen and Wang (2017), but the construction is also (unsurprisingly) similar to one that appeared in an earlier paper that studied query complexity in the setting when the access to the MDP is provided by a simulation model. In fact, we will present this lower bound later in a lecture where we study batch RL. According to this result, the query-complexity (also known as sample-complexity) of finding a $\\delta$-optimal policy with constant probability in discounted MDPs accessible through a random access simulator, apart from logarithmic factors, is $SA H^3/\\delta^2$, where $H=1/(1-\\gamma)$. Representations matter . We already saw that in order to just clearly define the computational problems (which is necessary for being able to talk about lower bounds), we need to be clear about the inputs (and the outputs). The table representation of MDPs is far from being the only possibility. We just mentioned the “simulation model”. Here the algorithm “learns” about the MDP by issuing next state and reward queries to the simulator at some state-action pair $(s,a)$ of its choice to which the simulator responds with a random next state (drawn fresh) and the $r_a(s)$. Interestingly, this can provably reduce the number of queries compared to the table representation. Another alternative, which still keeps tables, is to give the algorithm a cumulative probability representation. In this representation, the states are identified with ${1,\\dots,\\mathrm{S}}$ as before but instead of giving the algorithm the tables $[P_a(s,1), \\dots, P_a(s,\\mathrm{S})]$ for fixed $(s,a)$, the algorithm is given . \\[[P_a(s,1), P_a(s,1)+P_a(s,2), \\dots, 1]\\] (the last entry could be saved, because it is always equal to one, but in the grand scheme of things, of course, this does not matter). Now, it is not hard to see that if the original probability vector had a single one and zeroes everywhere else, the “needle in the haystack problem” used in the lower bound, with the integral representation above, a clever algorithm can find the entry with the one with at most $O(\\log( \\mathrm{S})) $ queries. As it turns out, with this representation, the query complexity (number of queries required) of producing a good policy can indeed be reduced from the quadratic dependence on the size of the state-space to a log-linear dependence. Hence, we see that the input representation crucially matters. Chen and Wang (2017) also make this point and they discuss yet another, “tree” representation, which leads to a similar speedup. MDPs with short descriptions . The simulator model assumption addresses the problem that just reading the input may be the bottleneck. This is not the only possibility. One can imagine various classes of MDPs that have a short description, which may raise the hope that one can find out a good policy in them without touching each state-action pair. There are many examples of classes of MDPs that belong to this category. These include . | factored MDPs: The transition dynamics have a short, structured (factored) representation, and the same applies to the reward | parametric MDPs: The transition dynamics and the rewards have a short, parametric representation. Examples include linear-quadratic regulation (linear dynamics, quadratic reward, Euclidean state and action spaces, Gaussian noise in the transition dynamics), robotic systems, various operations research problems. | . For factored MDPs one is out of luck: In these, planning is provably “very hard” (computationally). For linear-quadratic regulation, on the other hand, planning is “easy”; once the data is read, all one has to do is to solve some algebraic equations, for which efficient solution methods have been worked out. Query vs. computational complexity . The key idea of the lower bound crucially hinges upon that good algorithms need to “learn” about their inputs: The number of arithmetic and logic operations of any algorithm is at least as large as the number of “read” operations it issues. The minimum number of required read operations to produce an input of some desired property is often called the problems query complexity and by the above reasoning we see that the computational complexity is lower bounded by the query complexity. As it happens, query complexity is much easier to bound than computational complexity in the sense that it is rare to see computational complexity lower bounds strictly larger than the query complexity (the exceptions to this come when a “compact” representation of the MDP is available, such as in the case of factored MDPs). At the heart of query complexity lower bounds is often the needle in the haystack problem. This seems to be generally true when the inputs are “deterministic”. When querying results in stochastic (random) outcomes, multiple queries may be necessary to “reject”, “reduce”, or “filter out” the noise and then new considerations appear. In any case, query complexity is a question about quickly determining the information crucial to arrive at a good decision early and is in a way about “learning”: Before a table is read, the algorithm does not know which MDP it faces. Hence, query complexity is essentially an “information” question and is also sometimes called information complexity and we can think of query complexity as the most basic information theory question. This is a bit different though than mainstream information theory, which is somehow tied up in dealing with reducing the effect of random responses (random “corruptions” of the clean information). Query complexity everywhere . Query complexity is widely studied in a number of communities which, sadly, are almost entirely disjoint. Information-theory, mentioned above is one of them, though as was noted, here the problems are often tied to studying the speed of gaining information in the presence of noise. Besides information theory, there is the whole field of information-based complexity, which has its own journal, multiple books and more. Also notable is the theory community that studies the complexity of evolutionary algorithms. Besides these, of course, query complexity made appearances in the optimization literature (with or without noise), operations research, and of course in the machine learning and statistics community. In particular, in the machine learning and statistics community, when the algorithm is just handed over noisy data, “the sample”, one can ask how large this sample needs to be to achieve some good outcome (e.g., good predictions on unseen data). This leads to the notion of sample complexity, which is the same as our query complexity except that the queries are of the “dull”, “passive” nature of “give me the next datapoint”. As opposed to this, “active learning” refers to the case when the algorithms themselves control some aspects of how the data is collected. Free lunches, needles and a bit of philosophy . Everyone after going to a few machine learning conferences or reading their first book, or blog posts would have heard about David Wolpert’s “no-free lunch theorems”. Yet, I find that to most people the exact nature (or significance) of these theorems remain elusive. Everyone heard that these theorem essentially state that “in the lack of bias, all algorithms are equal” (and therefore there is no free lunch), from which we should conclude that the only way to choose between algorithms is by introducing bias. But what does bias means? If one reads these results carefully (and the theory community of evolutionary computation made a good job of making them accessible) one finds that the results are nothing more that describing some corollaries that to find a needle in a haystack (the special entry in a long array), one needs to search the whole haystack (query almost all entries of the array). Believers of the power of data like to dismiss the significance of the no-free lunch result by claiming that it is ridiculous in that it assumes no structure at all. I find these arguments weak. The main problem is that they are evasive. The evasiveness comes from the reluctance to be clear about what we expect the algorithms to achieve. The claim is that once we are clear about this, that is, clear about the goals, or just the problem specification, we can always hunt for the “needle in the haystack” subproblems within the problem class. This is about figuring out the symmetries (as symmetry equals no structure) that sneakily appear in pretty much any reasonable problem we think of worth studying. The only problems that do not have “needle in the haystack” situations embedded into them are the ones that are not specified at all. What is the upshot of all this? In a way, the real problem is to be clear about what the problem we want to solve is. This is the problem that most theoreticians in my field struggle with every day. Just because this is hard, we cannot give up on this before even starting, or this will just lead to chaos. As we shall see in this class, how to specify the problem is also at the very heart of reinforcement learning theory research. We constantly experiment with various problem definitions, tweaking them in various ways, trying to separate hopelessly hard problems from the easy, but reasonably general ones. Theoreticians like to build a library of various problem settings that they can classify in various ways, including relating the problem settings to each other. While algorithm design is the constructive side of RL (and computer science, more generally), understanding the relationship between the various problem settings is just as equally important. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec3/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec3/#notes"
  },"110": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "References",
    "content": ". | Chen, Y., &amp; Wang, M. (2017). Lower bound on the computational complexity of discounted markov decision problems. arXiv preprint arXiv:1705.07312. [link] | Singh, S. P., &amp; Yee, R. C. (1994). An upper bound on the loss from approximate optimal-value functions. Machine Learning, 16(3), 227-233. [link] | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec3/#references",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec3/#references"
  },"111": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "3. Value Iteration and Our First Lower Bound",
    "content": "PDF Version . Last time, we discussed the Fundamental Theorem of Dynamic Programming, which then led to the efficient “value iteration” algorithm for finding the optimal value function. And then we could find the optimal policy by greedifying w.r.t. the optimal value function. In this lecture we will do two things: . | Elaborate more on the the properties of value iteration as a way of obtaining near-optimal policies; | Discuss the computational complexity of planning in finite MDPs. | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec3/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec3/"
  },"112": {
    "doc": "4. Policy Iteration",
    "title": "Policy Iteration",
    "content": "Policy iteration starts with an arbitrary deterministic (memoryless) policy \\(\\pi_0\\). Then, in step $k=0,1,2,\\dots$, the following computations are done: . | calculate \\(v^{\\pi_k}\\), and | obtain \\(\\pi_{k+1}\\), another deterministic memoryless policy, by “greedifying” w.r.t. \\(v^{\\pi_k}\\). | . How do we calculate $v^{\\pi_k}$? Recall that $v^{\\pi}$, for an arbitrary memoryless policy $\\pi$, is the fixed-point of the operator $T_\\pi$: $v^\\pi = T_\\pi v^\\pi$. Also, recall that $T_\\pi v = r_\\pi + \\gamma P_\\pi v$ for any $v\\in \\mathbb{R}^{\\mathcal{S}}$. Thus, $v^\\pi = T_\\pi v^\\pi$ is just a linear equation in $v^\\pi$, which we can solve explicitly. In the context of policy iteration from this we get . \\[\\begin{align} v^{\\pi_k} = (I - \\gamma P_{\\pi_k})^{-1} r_{\\pi_k}\\,. \\label{eq:vpiinv} \\end{align}\\] The careful reader will think of why the inverse of the matrix $I-\\gamma P_{\\pi_k}$ exist. There are many tools we have at this stage to argue that the above is well-defined. One approach is to note that $(I-A)^{-1} = \\sum_{i\\ge 0} A^i$ holds whenever all eigenvalues of the square matrix $A$ lie strictly within the unit circle on the complex plain (see homework 0). This is known as the von Neumann series expansion of $I-A$, but these big words just hide that at the heart of this is the elementary geometric series formula, $1/(1-x) = \\sum_{i\\ge 0} x^i$, which holds for all $|x|&lt;1$, as we have all learned in high school. Based on Eq. \\(\\eqref{eq:vpiinv}\\) we see that \\(v^{\\pi_k}\\) can be obtained with at most \\(O( \\mathrm{S}^3 )\\) (and in fact with at most \\(O( \\mathrm{S}^{2.373\\dots})\\) ) arithmetic and logic operations. In particular, the cost of computing $r_{\\pi_k}$ is $O(\\mathrm{S})$ (since $\\pi_k$ is deterministic), the cost of computing $P_{\\pi_k}$, with the table representation of the MDP and “random access” to the tables, is $O(\\mathrm{S}^2)$. Note that all these are independent of the number of actions. Computationally, the “greedification step” above just means to compute for each state $s\\in \\mathcal{S}$ an action that maximizes the one-step Bellman lookahead values w.r.t. $v^{\\pi_k}$. Writing this out, we see that we need to solve the maximization problem . \\[\\max_{a\\in \\mathcal{A}} r_a(s) + \\gamma \\langle P_a(s),v^{\\pi_k} \\rangle\\] and store the result as the action that will be selected by $\\pi_{k+1}$. Since we agreed that all these policies will be deterministic, we may remove a bit of the storage redundancy, if we allow the algorithm just to store the action chosen by $\\pi_{k+1}$ at every state (and eventually produce the output in this form), rather than requiring it to produce a probability vector for each state, which would have a lot of redundant zero entries in it. Correspondingly, we will further abuse notation and will allow deterministic memoryless policies to be identified with $\\mathcal{S} \\to \\mathcal{A}$ maps. Thus, $\\pi_{k+1}: \\mathcal{S} \\to \\mathcal{A}$. Given $v^{\\pi_k}$, a vector of length $\\mathrm{S}$, the cost of evaluating the argument of the maximum is $O(\\mathrm{S})$. Thus, the cost of computing the maximum is $O(\\mathrm{S}\\mathrm{A})$: This is where the number of actions appears (in these steps) in the runtime. Our main result will be a theorem that states that after $\\tilde O( \\mathrm{SA}/(1-\\gamma))$ iterations, the policy computed by policy iteration is necessarily optimal (and not only approximately optimal!). The proof of this result hinges up on two key observations: . | Policy iteration converges geometrically | After every $H_{\\gamma,1}$ iterations, it eliminates at least one suboptimal action at some state. | . The first result follows from comparing policy iteration with value iteration. We know that value iteration converges at a geometric rate regardless of its initialization. Hence, if we can prove that \\(\\| v^{\\pi_k}-v^* \\|_\\infty \\le \\| T^k v^{\\pi_0}-v^* \\|_\\infty\\) then we will be done. In the so-called “policy improvement lemma”, we will in fact prove a result that implies . \\[\\begin{align} T^k v^{\\pi_0} \\le v^{\\pi_{k}}\\,, \\qquad k=0,1,2,\\dots\\, \\label{eq:pilk} \\end{align}\\] which is stronger than the geometric convergence result. Lemma (Geometric Progress Lemma): Let $\\pi,\\pi’$ be memoryless policies such that $\\pi’$ is greedy w.r.t. $v^\\pi$. Then, . \\[\\begin{align*} v^\\pi \\le T v^{\\pi} \\le v^{\\pi'}\\,. \\end{align*}\\] . Proof: By definition, $T v^\\pi = T_{\\pi’} v^\\pi$. We also have $v^\\pi = T_\\pi v^\\pi \\le T v^\\pi$. Chaining these, we get . \\[\\begin{align} v^\\pi \\le T v^{\\pi} = T_{\\pi'} v^{\\pi}\\,. \\label{eq:pilemmabase} \\end{align}\\] We prove by induction on $i\\ge 1$ that . \\[\\begin{align} v^\\pi \\le T v^{\\pi} \\le T_{\\pi'}^i v^{\\pi}\\,. \\label{eq:pilemmainduction} \\end{align}\\] From this, the result will follow by taking $i\\to \\infty$ of both sides. The base case of induction $i=1$ has just been established. For the general case, assume that the required inequality holds for $i\\ge 1$. We show that it also holds for $i+1$. For this, apply $T_{\\pi’}$ on both sides of Eq. \\(\\eqref{eq:pilemmainduction}\\). Since $T_{\\pi’}$ is monotone, we get . \\[\\begin{align*} T_{\\pi'} v^\\pi \\le T_{\\pi'}^{i+1} v^{\\pi}\\,. \\end{align*}\\] Chaining this with Eq. \\(\\eqref{eq:pilemmabase}\\), we get . \\[\\begin{align*} v^\\pi \\le T v^\\pi = T_{\\pi'} v^\\pi \\le T_{\\pi'}^{i+1} v^{\\pi}\\,, \\end{align*}\\] finishing the inductive step, and hence the proof. \\(\\qquad \\blacksquare\\) . The lemma shows that the value functions are monotonically increasing. Applying this lemma $k$ times starting with $\\pi = \\pi_0$ gives Eq. \\(\\eqref{eq:pilk}\\) and this implies the promised result: . Corollary (Geometric convergence): Let \\(\\{\\pi_k\\}_{k\\ge 0}\\) be the sequence of policies produced by policy iteration. Then, for any \\(k\\ge 0\\), . \\[\\begin{align} \\|v^{\\pi_k} - v^*\\|_\\infty \\leq \\gamma^k \\|v^{\\pi_0} - v^*\\|_\\infty\\,. \\label{eq:pig} \\end{align}\\] . Proof: By \\(\\eqref{eq:pilk}\\), . \\[T^k v^{\\pi_0} \\le v^{\\pi_k} \\le v^*\\,, \\qquad k=0,1,2,\\dots\\,.\\] Hence, . \\[v^* - v^{\\pi_k} \\le v^* - T^k v^{\\pi_0}\\,, \\qquad k=0,1,2,\\dots\\,.\\] Taking componentwise absolute values and then the maximum over the states, we get that . \\[\\|v^* - v^{\\pi_k}\\|_\\infty \\le \\|v^* - T^k v^{\\pi_0}\\|_\\infty = \\|T^k v^* - T^k v^{\\pi_0}\\|_\\infty \\le \\gamma^k \\|v^* - v^{\\pi_0}\\|_\\infty\\,,\\] which is the desired statement. In the equality above we used the Fundamental Theorem and in the last inequality we used that $T$ is a $\\gamma$-contraction. \\(\\qquad\\blacksquare\\) . We now set out to finish by showing the “strict progress lemma”. The lemma uses the corollary we just obtained, but it will also require some truly novel ideas. Lemma (Strict progress lemma): Fix an arbitrary suboptimal memoryless policy $\\pi_0$ and let \\(\\{\\pi_k\\}_{k\\ge 0}\\) be the sequence of policies produced by policy iteration. Then, there exists a state $s_0\\in \\mathcal{S}$ such that for any $k\\ge k^*:= \\lceil H_{\\gamma,1} \\rceil +1$, . \\[\\pi_k(s_0)\\ne \\pi_0(s_0)\\,.\\] . The lemma shows that after every \\(k^* = \\tilde O \\left( \\frac{1}{1-\\gamma}\\right)\\) iterations, policy iteration eliminates one action-choice at one state until there remains no suboptimal action to be eliminated. This can only be continued for at most $SA - S$ times: In every state, at least one action must be optimal. As an immediate corollary of the progress lemma, we get the main result of this lecture: . Theorem (Runtime Bound for Policy Iteration): Consider a finite, discounted MDP with rewards in $[0,1]$. Let \\(k^*\\) be as in the progress lemma, \\(\\{\\pi_k\\}_{k\\ge 0}\\) the sequence of policies obtained by policy iteration starting from an arbitrary initial policy $\\pi_0$. Then, after at most \\(k= k^* (\\mathrm{S}\\mathrm{A}-\\mathrm{S}) = \\tilde O\\left( \\frac{\\mathrm{S}\\mathrm{A}-\\mathrm{S} }{1-\\gamma } \\right)\\) iterations, the policy $\\pi_k$ produced by policy iteration is optimal: $v^{\\pi_k}=v^*$. In particular, policy iteration computes an optimal policy with at most \\(\\tilde O\\left( \\frac{ \\mathrm{S}^4 \\mathrm{A} +\\mathrm{S}^3{\\mathrm{A}^2} }{1-\\gamma} \\right)\\) arithmetic and logic operations. It remains to prove the progress lemma. We start with an identity which will be useful beyond the proof of this lemma. The identity is called the value difference identity and it gives us an alternate form of the difference of values functions of two memoryless policies. Let $\\pi,\\pi’$ be two memoryless policies. Recalling that $v^{\\pi’} = (I-\\gamma P_{\\pi’})^{-1} r_{\\pi’}$, by algebra, we find that . \\[\\begin{align*} v^{\\pi'} - v^{\\pi} &amp; = (I-\\gamma P_{\\pi'})^{-1} [ r_{\\pi'} - (I-\\gamma P_{\\pi'}) v^\\pi] \\\\ &amp; = (I-\\gamma P_{\\pi'})^{-1} [ T_{\\pi'} v^\\pi - v^\\pi]\\,. \\end{align*}\\] Introducing . \\[g(\\pi',\\pi) = T_{\\pi'} v^\\pi - v^\\pi\\,,\\] which we can think of the “advantage” of $\\pi’$ relative to $\\pi$, we get the following lemma: . Lemma (Value Difference Identity): For all memoryless policies \\(\\pi, \\pi'\\), . \\[v^{\\pi'} - v^\\pi = (I - \\gamma P_{\\pi'})^{-1} g(\\pi',\\pi)\\,.\\] . Of course, a symmetric relationship also holds. With this, we are now ready to prove the progress lemma. Note that if \\(\\pi^*\\) is an optimal memoryless policy then for any other memoryless policy $\\pi$, \\(g(\\pi,\\pi^*)\\le 0\\). In fact, the reverse statement also holds: if the above holds for any $\\pi$, $\\pi^*$ must be optimal. This makes it \\(-g(\\pi_k,\\pi^*)\\) an ideal target to track the progress that policy iteration makes. We expect this to start at a high value and decrease as $k$ increases. Note, in particular, that if . \\[\\begin{align} -g(\\pi_k,\\pi^*)(s_0)&lt;-g(\\pi_0,\\pi^*)(s_0) \\label{eq:strictprogress} \\end{align}\\] for some state $s_0\\in \\mathcal{S}$ then, by algebra, . \\[r_{\\pi_k(s_0)}(s_0) + \\gamma \\langle P_{\\pi_k(s_0)} , v^* \\rangle &gt; r_{\\pi_0(s_0)}(s_0) + \\gamma \\langle P_{\\pi_0(s_0)} , v^* \\rangle\\] which means that $\\pi_k(s_0)\\ne \\pi_0(s_0)$. Hence, the idea of the proof is to show that Eq. \\(\\eqref{eq:strictprogress}\\) holds for any $k\\ge k^*$. Proof (of the progress lemma): Fix $k\\ge 0$ and \\(\\pi_0\\) such that \\(\\pi_0\\) is not optimal. Let \\(\\pi^*\\) be an arbitrary memoryless optimal policy. Then, for policy \\(\\pi_k\\), by the value difference identity and since \\(\\pi^*\\) is optimal, . \\[- g(\\pi_k,\\pi^*) = (I - \\gamma P_{\\pi_k}) (v^* - v^{\\pi_k}) = (v^* - v^{\\pi_k}) - \\gamma P_{\\pi_k} (v^* - v^{\\pi_k}) \\leq v^* - v^{\\pi_k}\\,,\\] where the last inequality follows because $P_{\\pi_k}$ is stochastic and hence monotone and because \\(v^* - v^{\\pi_k}\\ge 0\\). Our goal is to relate the right-hand side to \\(-g(\\pi_0,\\pi^*)\\). Since Eq. \\(\\eqref{eq:pig}\\) allows us to relate the right-hand side to \\(v^*-v^{\\pi_0}\\), and the value difference identity then lets us bring in \\(-g(\\pi_0,\\pi^*)\\), preparing to use Eq. \\(\\eqref{eq:pig}\\), we first take the max-norm of both sides of the above inequality, noting that this keeps the inequality by the definition of the max-norm. Then, as planned, we use Eq. \\(\\eqref{eq:pig}\\) and the value difference identity to get . \\[\\begin{align} \\|g(\\pi_k,\\pi^*)\\|_\\infty &amp; \\leq \\|v^* - v^{\\pi_k}\\|_\\infty \\leq \\gamma^k \\|v^* - v^{\\pi_0}\\|_\\infty = \\gamma^k \\|(I - \\gamma P_{\\pi_0})^{-1} (-g(\\pi_0,\\pi^*))\\|_\\infty \\nonumber \\\\ &amp; \\leq \\frac{\\gamma^k}{1 - \\gamma} \\|g(\\pi_0,\\pi^*)\\|_\\infty\\,, \\label{eq:plmain} \\end{align}\\] where the last inequality follows by noting that \\((I - \\gamma P_{\\pi_0})^{-1} = \\sum_{i\\ge 0} \\gamma^i P_{\\pi_0}^i\\) and thus from the triangle inequality and because \\(P_{\\pi_0}\\) is a max-norm non-expansion, \\(\\| (I - \\gamma P_{\\pi_0})^{-1} x \\|_\\infty \\le \\frac{1}{1-\\gamma}\\| x \\|_\\infty\\) holds for any \\(x\\in \\mathbb{R}^{\\mathrm{S}}\\). Now, define $s_0\\in \\mathcal{S}$ to be the state that satisfies \\(-g(\\pi_0,\\pi^*)(s_0) = \\| g(\\pi_0,\\pi^*)(s_0)\\|_\\infty\\). Since $\\mathcal{S}$ is finite, this exists. Noting that \\(0\\le -g(\\pi_k,\\pi^*)(s_0)\\le \\| g(\\pi_k,\\pi^*)\\|_\\infty\\), we get from Eq. \\(\\eqref{eq:plmain}\\) that . \\[-g(\\pi_k,\\pi^*)(s_0) \\leq \\|g(\\pi_k,\\pi^*)\\|_\\infty \\leq \\frac{\\gamma^k}{1 - \\gamma} (-g(\\pi_0,\\pi^*)(s_0)).\\] Now when \\(k\\ge k^*\\), \\(\\frac{\\gamma^k}{1 - \\gamma} &lt; 1\\). Since \\(\\pi_0 \\neq \\pi^*\\), \\(0&lt;\\|g(\\pi_0,\\pi^*)\\|_\\infty = -g(\\pi_0,\\pi^*)(s_0)\\) and thus, . \\[\\begin{align*} -g(\\pi_k,\\pi^*)(s_0) \\leq \\frac{\\gamma^k}{1 - \\gamma} (-g(\\pi_0,\\pi^*)(s_0)) &lt; -g(\\pi_0,\\pi^*)(s_0)\\,, \\end{align*}\\] which is Eq. \\(\\eqref{eq:strictprogress}\\), and thus, by our earlier discussion, \\(\\pi_k(s_0)\\ne \\pi_0(s_0)\\). The proof is done because this holds for any \\(k\\ge k^*\\). \\(\\qquad\\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec4/#policy-iteration",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec4/#policy-iteration"
  },"113": {
    "doc": "4. Policy Iteration",
    "title": "Is Value Iteration Inferior?",
    "content": "Our earlier result on the runtime of value iteration involves a $\\log(1/\\delta)$ term which grows without bounds as $\\delta$, the required precision level, decreases towards zero. However, at this stage it is not clear whether this extra term is the result of a loose analysis or whether it is a property of value-iteration. Can value iteration be guaranteed to find an optimal policy with computation which is polynomial in $\\mathrm{S}$, $\\mathrm{A}$ and the planning horizon $1/(1-\\gamma)$, assuming all value functions takes values in $[0,1/(1-\\gamma)]$? . Calling any algorithm that achieves the above strongly polynomial, we see that with this terminology we can say that policy iteration is strongly polynomial. Note that in the above definition rather than assuming that the rewards lie in $[0,1]$, we use the assumption that the value functions for all policies take values in $[0,1/(1-\\gamma)]$. This is a weaker assumption, but checking our proof for the runtime on policy iteration we see that it only needed this assumption. However, as it turns out, value-iteration is not strongly polynomial: . Proposition: There exists a family of MDPs with deterministic transitions, three states, two actions and value functions for all policies taking values in $[0,1/(1-\\gamma)]$ such that the worst-case iteration complexity of value iteration over this set of MDPs to find an optimal policy is infinite. Here, iteration complexity means the smallest number of iterations $k$ after which $\\pi_k$, as computed by value iteration, is optimal, for any of the MDPs in the family. Of course, an infinite iteration complexity also implies an infinite runtime complexity. Proof: The MDP is depicted in the following figure: . The circles show the states with their names in the circles, the arrows with labels $a_0$ and $a_1$ show the transitions between the states as a result of using the actions. The label $r=\\cdot$ shows how much reward is incurred along a transition. On the figure, $R$ is not a return, but a free parameter, which is chosen in the interval $[0,\\gamma/(1-\\gamma)]$ and which will govern the iteration complexity of value iteration. We consider value iteration initialized at $v_0 = \\boldsymbol{0}$. It is easy to see that the unique optimal action at $s_1$ is $a_0$, incurring a value of $\\gamma/(1-\\gamma)$ at this state. It is also easy to see that $\\pi_0(s_1)=a_1\\ne a_0$. We will show that value iteration can “hug” action $a_1$ at state $s_0$ indefinitely as $R$ approaches $\\gamma/(1-\\gamma)$ from below. For this, just note that $v_k(s_0)=0$ and that $v_k(s_2) =\\frac{\\gamma}{1-\\gamma}(1-\\gamma^k)$ for any $k\\ge 0$. Then, a little calculation shows that $\\pi_k(s_1)=a_1$ as long as $R&gt;v_k(s_2)$. If we want value iteration to spend more than $k_0$ iterations, all we have to do is to choose $R = \\frac{v^*(s_2)+v_{k_0}(s_2)}{2}&lt;\\gamma/(1-\\gamma)$. \\(\\blacksquare\\) . It is instructive to note how policy iteration avoids the blow-up of the iteration-counts. This result shows that value-iteration, as far as we are concerned with calculating an optimal policy, exactly, is clearly inferior to policy iteration. However, we also had our earlier positive result for value iteration that showed that the cost of achieving $\\delta$-suboptimal policies is at most $\\log(1/\\delta)$ (and polynomial in the remaining quantities). What does this all mean? Should we really care about that value-iteration is not finite for exact computation? We have many reasons to not to care much about exact calculations. In the end, we will do sampling, learning, all of which make exact calculations impossible. Also, recall that our models are just models: The models themselves introduce errors. Why would we want to care about exact optimality? In summary: . Exact optimality is nice to have, but approximate computations with runtime growing mildly with the required precision should be almost equally acceptable. Yet, it remains intriguing to think of how policy iteration can just “snap” into the right solution and how by changing just a few lines of code, a drastic improvement in runtime may be possible. We will keep returning to the question of whether an algorithm has some provable advantage over some others. When this can be shown, it is a true win: We do not need to bother with the inferior algorithm anymore. While this is great, remember that all this depends on how the problems are defined. As we have seen before, and we will see many more times, changing the problem definition can drastically change the landscape of what works and what does not work. And who knows, some algorithm may be inferior in some context, and be superior in some other. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec4/#is-value-iteration-inferior",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec4/#is-value-iteration-inferior"
  },"114": {
    "doc": "4. Policy Iteration",
    "title": "Notes",
    "content": "The runtime bound on policy iteration . The first result that showed that after $\\text{poly}(\\mathrm{S},\\mathrm{A},\\frac{1}{1-\\gamma})$ arithmetic and logic operations one can compute an optimal policy is due to Yinyu Ye (2011). This was a real breakthrough of the time. The theorem we proved is by Bruno Scherrer (2016) and we followed closely his proof. This proof is much simpler than the first one by Yinyu Ye, though the main ideas can be traced back to the proof of Yinyu Ye. Runtime of value iteration . The example that shows that value iteration is not strongly polynomial is due to Eugene A. Feinberg, Jefferson Huang and Bruno Scherrer (2014). Ties and stopping . More often than one may imagine, two actions may tie for the maximum in the above problem. Which one to use in this case? As it turns out, it matters only if we want to build a stopping condition for the algorithm that stops the first time it detects that $\\pi_{k+1}=\\pi_k$. This stopping condition takes $O(\\mathrm{S})$ operations, so is quite cheap. If we use this stopping condition, we better make sure that when there are ties, the algorithm resolves them in a systematic fashion, meaning that it has a fixed preference relation over the actions that it respects in case of ties. Otherwise, in the case when there are two optimal actions at some state $s$, $\\pi_k$ is an optimal policy, $\\pi_{k+1}$ may choose the optimal action that $\\pi_k$ did not choose, and then $\\pi_{k+2}$ could choose the same action as $\\pi_k$ at the same state, etc. and the stopping condition would fail to detect that all these policies are optimal. Alternatively to resolving ties systematically one may simply change the stopping condition to checking whether $v^{\\pi_k} = v^{\\pi_{k+1}}$. The reader is invited to check that this would work. “In practice”, though, this may be problematic if $v^{\\pi_k}$ and $v^{\\pi_{k+1}}$ are computed with finite precision and somehow the approximation errors that arise in this calculation lead to different answers. Can this happen at all? It can! We may have $v^{\\pi_k} = v^{\\pi_{k+1}}$ (with infinite precision), while $r_{\\pi_k}\\ne r_{\\pi_{k+1}}$ and $I-\\gamma P_{\\pi_k} \\ne I-\\gamma P_{\\pi_{k+1}}$. And so with finite precision calculations, there is no guarantee that we get the same outcomes in the two cases! The only guarantee that we get with finite precision calculations is that with identical inputs, the outputs are identical. An easy way out, of course, is just to use the theorem above and stop after the number of iterations is sufficiently large. However, this may be, needlessly, wasteful. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec4/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec4/#notes"
  },"115": {
    "doc": "4. Policy Iteration",
    "title": "References",
    "content": ". | Feinberg, E. A., Huang, J., &amp; Scherrer, B. (2014). Modified policy iteration algorithms are not strongly polynomial for discounted dynamic programming. Operations Research Letters, 42(6-7), 429-431. [link] | Scherrer, B. (2016). Improved and generalized upper bounds on the complexity of policy iteration. Mathematics of Operations Research, 41(3), 758-774. [link] | Ye, Y. (2011). The simplex and policy-iteration methods are strongly polynomial for the Markov decision problem with a fixed discount rate. Mathematics of Operations Research, 36(4), 593-603. [link] | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec4/#references",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec4/#references"
  },"116": {
    "doc": "4. Policy Iteration",
    "title": "4. Policy Iteration",
    "content": "PDF Version . In this lecture we . | formally define policy iteration and | show that with $\\tilde O( \\textrm{poly}(\\mathrm{S},\\mathrm{A}, \\frac{1}{1-\\gamma}))$ elementary arithmetic operations, it produces an optimal policy | . This latter bound is to be contrasted with what we found out about the runtime of value-iteration in the previous lecture. In particular, value-iteration’s runtime bound that we discovered previously grew linearly with $\\log(1/\\delta))$ where $\\delta$ was the targeted suboptimality level. This may appear as a big difference in the limit of $\\delta\\to 0$. Is this difference real? Is value-iteration truly inferior to policy-iteration? We will discuss these at the end of the lecture. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec4/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec4/"
  },"117": {
    "doc": "5. Online Planning - Part I.",
    "title": "What is Online Planning?",
    "content": "In a previous lecture we have seen that in discounted MDP with $S$ states and $A$ actions, no algorithm can output a $\\delta\\le \\gamma/(1-\\gamma)$ optimal or better policy with a computation cost less than $\\Omega( S^2 A )$ provided that the MDP is given with a table representation. One of the $SA$ factors here comes from that to specify a policy one needs to compute (and output) what action to take in every state. The additional $S$ factor comes from because to figure out whether an action is any good, one needs to read almost all entries of the next-state distribution vector. An unpleasant tendency of the world is that if a problem is modelled as an MDP (that is, the Markov assumption is faithfully observed), the size of the state space tends to blow up. Bellman’s curse of dimensionality is one reason why this happens. To be able to deal with such large MDPs, we expect our algorithm’s runtime to be independent of the size of the state space. However, our lower bound tells us that this is a pipe dream. But why did we require the planner to output a full policy? And why did we assume that the only way to get information about the MDP is to read big tables of transition probabilities? In fact, if the planner is used inside an “agent” that is embedded in an environment, there is no need for the planner to output a full policy: In every moment, the planner just needs to calculate the action to be taken in the state corresponding to the current circumstances of the environment. In particular, there is no need to specify what action to take under any other circumstances than the current one! . As we usually do in these lectures, assume that the environment is an MDP and the agent gets access to the state in every step when it needs to make a decision. Further, assume that the agent is lucky to also have access to a simulator of the MDP that describes its environment. Just think of the simulator as a black box that can be, fed with a state-action pair and responds with the immediate reward and a random next state from the correct next-state distribution. One can then perhaps build a planner that uses this black box with a “few” queries and quickly returns an action, to be taken by the agent, moving the environment to a random next state, from where the process continues. Now, the planner does not need to output actions at all states and it does not need to spend time on reading long probability vectors. Hence, in theory, the obstacles that led to the lower bound are removed. The question still remains whether in this new situation planner’s can indeed get away with runtime independent of the size of the state space. To break the suspense, the answer is yes and it comes very easily for deterministic environments. For stochastic environments a little more work will be necessary. In the remainder of this lecture we give a formal problem definition for the online planning problem that was described informally above. Next, the result is explained for deterministic environments. This result will be matched with a lower bound. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec5/#what-is-online-planning",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec5/#what-is-online-planning"
  },"118": {
    "doc": "5. Online Planning - Part I.",
    "title": "Online Planning: Formal Definitions",
    "content": "We start with the definition of MDP simulators. We use a language similar to that used to describe optimization problems where one talks about optimization in the presence of various oracles (zeroth-order, first order, noisy, etc.). Because we assume that all MDPs are finite, we identify the state and action spaces with subsets of the natural numbers and for the action set we also require that the action set is $[\\mathrm{A}]$ where $\\mathrm{A}$ is the number of actions. This simplifies the description quite a bit. Definition (MDP simulator): A simulator implementing an MDP \\(M=(\\mathcal{S},\\mathcal{A},P,r)\\) is a “black-box oracle” that when queried with a state action pair \\((s,a)\\in \\mathcal{S}\\times\\mathcal{A}\\) returns the reward $r_a(s)$ and a random state \\(S' \\sim P_a(s)\\), where \\(r=(r_a(s))_{s,a}\\) and \\(P = (P_a(s))_{s,a}\\). Users of the black-box must pay attention avoid querying it for state-action pairs outside of $\\mathcal{S}\\times \\mathcal{A}$. Our next notion is that of an online planner: . Definition (Online Planner): An online planner takes as input the number of actions $\\mathrm{A}$, a state $s\\in \\mathbb{N}$, an MDP simulator “access point”. After querying this simulator finitely many times, the planner needs to return an action from $[\\mathrm{A}]$. (Online) planners may randomize their calculation. Even if they do not randomize, the action returned by a planner is in general random due to the randomness of the simulator that the planner uses. A planner is well-formed if no matter what MDP it interfaces with through a simulator, it returns an action after querying the simulator finitely many times. This also means that the planner can never feed the simulator with state-action pair outside of the set of such pairs. If an online planner is given access to a simulator of $M$, the planner and the MDP $M$ together induce a policy of the MDP. We will just refer to this policy as the planner-induced policy $\\pi$ when the MDP is clear from the context. Yet, this policy depends on the MDP implemented by the simulator. If an online planner is well-formed, this policy is well-defined no matter the MDP that is implemented by the simulator. Online planners are expected to produce good policies: . Definition ($\\delta$-sound Online Planner): We say that an online planner is $\\delta$-sound if it is well-formed and for any MDP $M$, the policy $\\pi$ induced by it and a simulator implementing $M$ is $\\delta$-optimal in $M$. In particular, . \\[v^\\pi \\ge v^* - \\delta \\boldsymbol{1}\\] must hold where $v^*$ is the optimal value function in $M$. The (per-state, worst-case) query-cost of an online planner is the maximum number of queries it submits to the simulator where the maximum is over both the MDPs and the initial states. The following vignette summarizes the problem of online planning: . | Model: | Any finite MDP $M$ | . | Oracle: | Black-box simulator of $M$ | . | Local input: | State $s$ | . | Local output: | Action $A$ | . | Outcome: | Policy $\\pi$ | . | Postcondition: | \\(v^\\pi_M \\ge v^*_M-\\delta \\boldsymbol{1}\\) | . As an optimization, we let online planners also take as input $\\delta$, the target suboptimality level. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec5/#online-planning-formal-definitions",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec5/#online-planning-formal-definitions"
  },"119": {
    "doc": "5. Online Planning - Part I.",
    "title": "Online Planning through Value Iteration and Action-value Functions",
    "content": "Recall value iteration: . | Let \\(v_0 = \\boldsymbol{0}\\) | For \\(k=1,2,\\dots\\) let \\(v_{k+1} = Tv_k\\) | . As we have seen, if the iteration is stopped so that $k\\ge H_{\\gamma,\\delta(1-\\gamma)/(2\\gamma)}$, the policy $\\pi_k$ defined via . \\[\\pi_k(s) = \\arg\\max_a r_a(s) + \\gamma \\langle P_a(s),v_k \\rangle\\] is guaranteed to be $\\delta$-optimal. Can this be used for online planning? As we shall see, in a way, yes. But before showing this, it will be worthwhile to introduce some additional notation that, in the short term, will save us some writing. More importantly, the new notation will also be seen to influence algorithm design. The observation is that to decide about what action to take, we need to calculate the one-step lookahead value of the various actions. Rather than doing this in a separate step as shown above, we could have as well chosen to keep track of these lookahead values throughout the whole procedure. Indeed, define \\(\\tilde T: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}\\) as . \\[\\tilde T q = r + \\gamma P M q, \\qquad (q \\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}})\\,,\\] where $r\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ and the operators $P: \\mathbb{R}^{\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ and $M: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}}$ are defined via . \\[\\begin{align*} r(s,a) = r_a(s)\\,, \\quad (P v)(s,a) = \\langle P_a(s), v \\rangle\\,, \\quad (M q)(s) = \\max_{a\\in \\mathcal{A}} q(s,a) \\end{align*}\\] with \\(s\\in \\mathcal{S}\\), \\(a\\in \\mathcal{A}\\), \\(v\\in \\mathbb{R}^{\\mathcal{S}}\\), \\(q\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}\\). Then the definition of $\\pi_k$ can be shortened to . \\[\\pi_k(s) = \\arg\\max_a (\\tilde T^{k+1} \\boldsymbol{0})(s,a)\\,.\\] It is instructive to write the above computation in a recursive, algorithmic form. Let . \\[q_k = \\tilde T^k \\boldsymbol{0}.\\] Using a Python-like pseudocode, our function to calculate the values $q_k(s,\\cdot)$ looks as follows: . 1. define q(k,s): 2. if k = 0 return [0 for a in A] # base case 3. return [ r(s,a) + gamma * sum( [P(s,a,s') * max(q(k-1,s')) for s' in S] ) for a in A ] 4. end . Line 3, which is where the recursive call happens uses Python’s list comprehensions: the brackets create lists and the function itself returns a list. This is a recursive function (since it calls itself in line 3. The runtime is easily seen to be $(\\mathrm{A}\\mathrm{S})^k$, which is not very hopeful until we notice that if the MDP was deterministic, that is, $P(s,a,\\cdot)$ has a single one entry, and we have a way of looking up which entry is this without going through all the states, say, $g: \\mathcal{S}\\times \\mathcal{A} \\to \\mathcal{S}$ is a function that gives the next states, we can rewrite the above as . 1. define q(k,s): 2. if k = 0 return [0 for a in A] # base case 3. return [ r(s,a) + gamma * max(q(k-1,g(s,a))) for a in A ] 4. end . As in line 3 there is no loop over the next states (no summing up over these), the runtime becomes . \\[O(A^k)\\,\\] which is the first time we see that a good action can be calculated with effort regardless of the size of the state space! And of course, if one is given a simulator of the underlying MDP, which is deterministic, calling $g$ is the same as calling the simulator (once). But will this idea extend to the stochastic case? The answer is yes, but the details will be given in the next lecture. Instead, in this lecture we take a brief look at whether there is any possibility to do better than the above recursive procedure. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec5/#online-planning-through-value-iteration-and-action-value-functions",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec5/#online-planning-through-value-iteration-and-action-value-functions"
  },"120": {
    "doc": "5. Online Planning - Part I.",
    "title": "Lower Bound",
    "content": ". Theorem (online planning lower bound): Take any online planner $p$ that is $\\delta$-sound with $\\delta&lt; 1$ for discounted MDPs with rewards in $[0,1]$. Then there exist some MDPs on which $p$ uses at least \\(\\Omega(\\mathrm{A}^{k})\\) queries at some state with . \\[\\begin{align} k=\\left\\lceil \\frac{\\ln( 1/(\\delta(1-\\gamma)) )}{\\ln(1/\\gamma)}\\right\\rceil, \\label{eq:kdeflb} \\end{align}\\] where \\(\\mathrm{A}\\) is the number of actions in the MDP. Denote by $k_\\gamma$ the value defined in \\eqref{eq:kdeflb}. Then, for $\\gamma\\to 1$, $k_\\gamma =\\Omega( H_{\\gamma,\\delta} )$. Proof: This is a typical needle-in-the-haystack argument. We saw in Question 5 on Homework 0 that no algorithm can find out which element of a binary array of length $m$ is one with less than $\\Omega(m)$ queries. Take a rooted regular $\\mathrm{A}$-ary tree of depth $k$. The tree has exactly $\\mathrm{A}^k$ leafs. Consider an MDP with states corresponding to the nodes of this tree. Call the root $s_0$. Let the dynamics be deterministic: Taking an action at a node (of the tree) makes the next state the child of that node, unless the node is a leaf node, which are absorbing states: The next state under any action at any leaf state $s$ is $s$ itself. Let all the rewards be zero except at exactly one of the leaf nodes, where the reward under any action is set to one. If a planner is $\\delta$-sound, we claim that it must find the optimal action at $s_0$. This holds because the value of this action is $\\sum_{i=k}^\\infty \\gamma^i=\\gamma^k/(1-\\gamma)$ and, by our choice of $k$, $\\gamma^k/(1-\\gamma) \\ge \\delta$, while the value of any other action at $s_0$ is zero. It follows that the planner needs to be able to identify the unique action at the unique leaf node whose reward is one, which, by Question 5 on Homework 0, needs at least $\\Omega(\\mathrm{A}^{k})$ queries. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec5/#lower-bound",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec5/#lower-bound"
  },"121": {
    "doc": "5. Online Planning - Part I.",
    "title": "Notes",
    "content": "Dealing with larger state spaces . For a fully formal specification the reader may worry about how a state is described to an online planner, especially, if we allowed uncountably many states. Because the online planner will only have access to the state that it receives as its input and the other states that are returned from the simulator, for the purpose of communication between the online planner and its environment and the simulator, all these states can just be assigned unique numbers to identify them. Gap between the lower and upper bound . There is an obvious gap between the lower and the upper bound that should be closed. Local planning vs. online planning . Last year’s lecture notes used the expression local planning in place of online planning. There are pros and cons for both expressions, but perhaps online planning better expresses that the planner will be used in an online fashion, that is, every time after a transition happens. On simulators and access modes . Simulators come in many shapes and forms. A general planner needs to be prepared to be used in an interconnection with any simulator. But this is too much: Every simulator provides an interface to the planners and planners need to be designed around these interfaces. Therefore, planners will be specialized to the specific interface used. Here, we distinguish three types of interfaces based on what access the interface allows to generating data. The access can be global, local or online. Global access means that the simulator provides a function that returns a description of the full state space. For finite MDPs this would just mean returning the number of states $S$. Then, the simulator can be called for any $(s,a)$ pair where $s\\in [S]$ and $a\\in [A]$ (the simulator should also have a function that returns the number of actions, $A$). Internally, the simulator then needs to translate the integer indices $s$ and $a$ into appropriate data for which the simulation can be done. Then, the simulator would generate the next state, and translate it back to an integer in $[S]$, which is the data returned from the call. The simulator should also return the associated reward. Often, the reward would also be random (in the lecture, we are concerned with deterministic rewards, but this is just done for the sake of simplicity: random rewards at this stage would not create further difficulties). Local access means that the simulator allows the planner to generate transitions starting only from states that were passed to the planner previously. To implement a local access simulator, one can just introduce an array that is used to remember all the states that have been returned to the planner. For the sake of interfacing with the planner, one can then use the indexing into this array. This way, the planner does not need to know the details of how states are internally represented and it also becomes possible to interface with simulators where the number of states is infinite, or when it is finite, but calculating this number would be impractical or intractable. Of course, the simulator needs the ability to “go back” to a previously visited state and generate new transition data from there. This can be usually implemented on the top of existing simulators without much trouble (the ability to do this is known as “checkpointing”). Online access simulators have an “internal state”, which the planners can manipulate in two ways: they can reset this internal state to the initial state (which is provided to the planner when the planner is called), or they can ask for a transition from the current internal state, by providing an action. As a result of this, the simulator’s internal state would move to a random next state, which is what would be returned to the planner (along with the associated reward). Clearly, any planner prepared to work with online access, can also be used with simulator that provide either local access or global access, and any planner prepared to work with local access can be used with simulators providing global access. In this way, online access is the most general of the access modes, local access is least general, and global access is the most restrictive. Note that even with online access there is the issue that state information about the state of the environment has to be communicated to the planner in a way that is consistent with how state information can be passed from the planner to the simulator. To keep planners general, the environment and the simulator need to work on an appropriate consistent way of serializing information about the state, which is a pure engineering issue and can usually be done without much trouble. “Planning with a generative models” is an alternative, early terminology that is still used in the literature today. Most commonly, this is means online planning with a global access simulator. However, as the expression itself is not as easy to adopt to different situations as described here, we will refrain from using it. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec5/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec5/#notes"
  },"122": {
    "doc": "5. Online Planning - Part I.",
    "title": "5. Online Planning - Part I.",
    "content": "PDF Version . In this lecture we . | introduce online planning; | show that for deterministic MDPs there is an online planner whose runtime per call is independent of the size of the state space; | show that this online planner has in fact a near-optimal runtime in a worst-case sense. | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec5/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec5/"
  },"123": {
    "doc": "6. online planning - Part II.",
    "title": "Sampling May Save the Day?",
    "content": "Assume now that the MDP is stochastic. Recall the pseudocode of the recursive form of value iteration from the last lecture that computes $(T^k \\boldsymbol{0})(s,\\cdot)$: . 1. define q(k,s): 2. if k = 0 return [0 for a in A] # base case 3. return [ r(s,a) + gamma * sum( [P(s,a,s') * max(q(k-1,s')) for s' in S] ) for a in A ] 4. end . Obviously, the size of the state space creeps in because in line 3 we need to calculate an expected value over the next state distribution at $(s,a)$. As noted beforehand, in deterministic systems when a simulator is available, the sum over the next-states can be replaced with a single simulator call. But the reader may remember from Probability 101 that sampling allows one to approximate expected values, where the error of approximation is independent of the cardinality of the set over which we average the values. Here, this set is $\\mathcal{S}$, the state space. This is extremely lucky! . To quantify the size of these errors, we recall Hoeffding’s inequality: . Lemma (Hoeffding’s Inequality): Given $m$ independent, identically distributed (i.i.d.) random variables that take values in the $[0,1]$ interval, for any \\(0 \\leq \\zeta &lt; 1\\), with probability at least \\(1 - \\zeta\\) it holds that . \\[\\left| \\frac{1}{m} \\sum_{i=1}^m X_i - \\mathbb{E}[X_1] \\right| \\leq \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} }\\,.\\] . Letting $S_1’,\\dots,S_m’ \\stackrel{\\textrm{i.i.d.}}{\\sim} P_a(s)$ for some state-action pair $(s,a)$ and $v:S \\to [0,v_{\\max}]$, by this result, for any $0\\le \\zeta &lt;1$, with probability $1-\\zeta$, . \\[\\begin{align} \\left|\\frac1m \\sum_{i=1}^m v(S_i') - \\langle P_a(s), v \\rangle\\right| \\le v_{\\max} \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} }. \\label{eq:hoeffbop} \\end{align}\\] This suggests the following approach: For each state action pair $(s,a)$ draw $S_1’,\\dots,S_m’ \\stackrel{\\textrm{i.i.d.}}{\\sim} P_a(s)$ and store it in a list $C(s,a)$. Then, whenever for some function $v$ we need the value of $\\langle P_a(s), v \\rangle$, just use the sample average . \\[\\frac1m \\sum_{s'\\in C(s,a)} v(s')\\,.\\] Plugging this approximation into our previous pseudocode gives the following new code: . 1. define q(k,s): 2. if k = 0 return [0 for a in A] # base case 3. return [ r(s,a) + gamma/m * sum( [max(q(k-1,s')) for s' in C(s,a)] ) for a in A ] 4. end . The total runtime of this function is now $O( (m\\mathrm{A})^{k+1} )$. What is important is that this will give us a compute time independent of the size of the state space as long as we can show that $m$ can be set independently of $\\mathrm{S}$ while meeting our target for the suboptimality of the induced policy. This pseudocode sweeps under the rug on who creates the lists $C(s,a)$ and when? A simple and effective approach is to use “lazy evaluation” (or memoization): Create $C(s,a)$ at the first time it is needed (and do not create it otherwise). An alternative to the approach we follow here is to avoid storing these lists and just create them on demand. Both procedures are valid, but we will stick to the procedure that creates the lists only once and will comment on the other approach at the end in the notes. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec6/#sampling-may-save-the-day",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec6/#sampling-may-save-the-day"
  },"124": {
    "doc": "6. online planning - Part II.",
    "title": "Good Action-Value Approximations Suffice",
    "content": "As a first step towards understanding the strength and weaknesses of this approach, let us define $\\hat T: \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}}$ by . \\[(\\hat T q)(s,a) = r_a(s) + \\frac{\\gamma}{m} \\sum_{s'\\in C(s,a)} \\max_{a'\\in \\mathcal{A}} q(s',a')\\,.\\] With the help of this definition, when called with state $s=s_0$, the planner computes . \\[\\begin{align*} A = \\arg\\max_{a\\in \\mathcal{A}} \\underbrace{ (\\hat T^H \\boldsymbol{0})(s_0,a) }_{Q_H(s_0,a)}\\,, \\end{align*}\\] The conciseness of this formulae, if anything, must please everyone! . Let us now turn to the question of whether the policy $\\hat \\pi$ induced by this planners is a good one. We start with a lemma that parallels our earlier result that bounded the suboptimality of a policy that is greedy w.r.t. a function over the states as a function of how well the function approximates the optimal value function. To state the lemma, we need the analog of optimal value functions but with action values. Suboptimality of $\\epsilon$-optimizing policies . Define . \\[q^*(s,a) = r_a(s) + \\gamma \\langle P_a(s), v^* \\rangle\\,.\\] We call this function $q^*$ the optimal action-value function (in our MDP). The function \\(q^*\\) is easily seen to satisfy \\(M q^* = v^*\\) and thus also \\(q^* = T q^*\\). The promised lemma is as follows: . Lemma (Policy error bound - I.): Let $\\pi$ be a memoryless policy and choose a function $q:\\mathcal{S}\\times\\mathcal{A} \\to \\mathbb{R}$ and $\\epsilon\\ge 0$. Then, the following hold: . | If $\\pi$ is $\\epsilon$-optimizing in the sense that \\(\\sum_a \\pi(a\\vert s) q^*(s,a) \\ge v^*(s)-\\epsilon\\) holds for every state $s\\in \\mathcal{S}$ then $\\pi$ is $\\epsilon/(1-\\gamma)$ suboptimal: \\(v^\\pi \\ge v^* - \\frac{\\epsilon}{1-\\gamma} \\boldsymbol{1}\\,.\\) . | If $\\pi$ is greedy with respect to $q$ then $\\pi$ is $2\\epsilon$-optimizing with \\(\\epsilon= \\|q-q^*\\|_\\infty\\) and thus . | . \\[v^\\pi \\ge v^* - \\frac{2\\|q-q^*\\|_\\infty}{1-\\gamma} \\boldsymbol{1}\\,.\\] . For the proof, which is partially left to the reader, we need to introduce a bit more notation. In particular, for a memoryless policy, define the operator $M_\\pi: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}}$: . \\[(M_\\pi q)(s) = \\sum_{a\\in \\mathcal{A}} \\pi(a|s) q(s,a)\\,, \\qquad (q\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}, \\, \\, s\\in \\mathcal{S}).\\] With the help of this operator the condition that $\\pi$ is greedy with respect to $q$ can be written as . \\[M_\\pi q = M q\\,.\\] Further, the second claim of the lemma can be stated in the more concise form $M_\\pi q^* \\ge v^* - 2\\epsilon\\boldsymbol{1}$. For future reference, we will also find it useful to define $P_\\pi: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$: . \\[P_\\pi = P M_\\pi\\,.\\] Note that here we abused notation as $P_\\pi$ has already been used to denote the operator that maps functions of the states to functions of the state. From the context, the meaning of $P_\\pi$ will always be clear. Proof: The first part of the proof is standard and is left to the reader. For the second part note that . \\[\\begin{align*} M_\\pi q^* &amp; \\ge M_\\pi(q-\\epsilon \\boldsymbol{1}) = M_\\pi q - \\epsilon\\boldsymbol{1}=M q - \\epsilon \\boldsymbol{1} \\ge M(q^* - \\epsilon \\boldsymbol{1}) - \\epsilon \\boldsymbol{1} = M q^* - 2\\epsilon \\boldsymbol{1} = v^* - 2\\epsilon\\boldsymbol{1}\\,. \\end{align*}\\] Then use the first part. \\(\\qquad \\blacksquare\\) . Suboptimality of almost $\\epsilon$-optimizing policies . There are two issues that need to be taken care of. One is that the planner is randomizing when computing the values $Q_H(s_0,\\cdot)$. What happens when the random next states obtained from the simulator are not “representative”? We cannot expect the outcome of this randomized computation to be precise! Indeed, the best we can expect is that the outcome is “accurate” with some probability, hopefully close to one. In fact, from Hoeffding’s inequality, we see that if we want to achieve small errors in the computation for some target probability, we need to increase the sample size. But Hoeffding’s inequality, in all cases, allows errors which are uncontrolled on some failure event. All in all, the best we can hope for is that with each call, $Q_H(s_0,\\cdot)$ is a good approximation to \\(q^*(s_0,\\cdot)\\) outside of some “failure event” $\\mathcal{F}$ whose probability we will control separately. Let us say the probability of $\\mathcal{F}$ is at most $\\zeta$: . \\[\\mathbb{P}_{s_0}(\\mathcal{F})\\le \\zeta\\,.\\] Here, $\\mathbb{P}_{s_0}$ denotes the probability measure induced by the interaction of the planner and the MDP simulator on an appropriate probability space. We will choose $\\mathcal{F}$ so that on $\\mathcal{F}^c$, the complementer of $\\mathcal{F}$ (a “good” event), it holds that . \\[\\begin{align} \\delta_H = \\| Q_H(s_0,\\cdot) - q^*(s_0,\\cdot)\\|_\\infty \\le \\epsilon\\,. \\label{eq:d0def} \\end{align}\\] Then, on $\\mathcal{F}^c$, . \\[q^*(s_0, A) \\ge Q_H(s_0,A)-\\epsilon = \\max_a Q_H(s_0,a)-\\epsilon \\ge \\max_a (q^*(s_0,a)-\\epsilon)-\\epsilon = v^*(s_0)-2\\epsilon\\,.\\] That is, on the good event $\\mathcal{F}^c$, the action $A$ returned by the planner is $2\\epsilon$ optimizing at state $s_0$. Let $\\hat \\pi(a \\vert s_0)$ denote the probability that action $A$ returned by the planner is $a$: \\(\\hat \\pi(a \\vert s_0)=\\mathbb{P}_{s_0}(A=a)\\). Then, . \\[\\begin{align*} \\sum_{a} &amp; \\hat \\pi(a \\vert s_0) \\mathbb{I}( q^*(s_0, a) \\ge v^*(s_0) - 2\\epsilon ) \\\\ &amp; = \\mathbb{P}_{s_0}( q^*(s_0, A) \\ge v^*(s_0) - 2\\epsilon )\\\\ &amp;= \\mathbb{P}_{s_0}( q^*(s_0, A) \\ge v^*(s_0) - 2\\epsilon, \\mathcal{F}^c ) + \\mathbb{P}_{s_0}( q^*(s_0, A) \\ge v^*(s_0) - 2\\epsilon, \\mathcal{F} )\\\\ &amp;\\ge \\mathbb{P}_{s_0}( q^*(s_0, A) \\ge v^*(s_0) - 2\\epsilon, \\mathcal{F}^c )\\\\ &amp;= \\mathbb{P}_{s_0}( \\mathcal{F}^c )\\\\ &amp;\\ge 1-\\zeta\\,. \\end{align*}\\] In words, with probability at least $1-\\zeta$, $\\hat \\pi$ chooses $2\\epsilon$-optimizing actions: The policy is almost $2\\epsilon$-optimizing. While this is not as good as always choosing $2\\epsilon$-optimizing actions, we expect that as $\\zeta\\to 0$ the difference in performance between $\\hat \\pi$ and a policy that always chooses $2\\epsilon$-optimizing actions disappears because performance is expected to depend on action probabilities in a continuous fashion. The next lemma makes this precise: . Lemma (Policy error bound II): Let $\\zeta\\in [0,1]$, $\\pi$ be a memoryless policy that selects $\\epsilon$-optimizing actions with probability at least $1-\\zeta$ in each state. Then, . \\[v^\\pi \\ge v^* - \\frac{\\epsilon+2\\zeta \\|q^*\\|_\\infty}{1-\\gamma} \\boldsymbol{1}\\,.\\] . Proof: By Part 1 of the previous lemma, it suffices to show that $\\pi$ is \\(\\epsilon+2\\zeta \\|q^*\\|_\\infty\\)-optimizing in every state. This follows from algebra and is left to the reader. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec6/#good-action-value-approximations-suffice",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec6/#good-action-value-approximations-suffice"
  },"125": {
    "doc": "6. online planning - Part II.",
    "title": "Error control",
    "content": "What remains is to show that with high probability, the error $\\delta_H$, defined in \\(\\eqref{eq:d0def}\\) is small. Intuitively, $\\hat T \\approx T$. To firm up this intuition, we may note that for any fixed $q\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ function over the state-action pairs such that \\(\\|q\\|_\\infty \\le \\frac{1}{1-\\gamma}\\) and for any fixed $(s,a)\\in \\mathcal{S}\\times \\mathcal{A}$, by Eq. \\(\\eqref{eq:hoeffbop}\\) and the choice of the sets $\\mathcal{C}(s,a)$, with probability $1-\\zeta$, . \\[\\begin{align} |\\hat T q (s,a)-T q(s,a)| &amp; = \\gamma \\left| \\frac1m \\sum_{s'\\in \\mathcal{C}(s,a)} v(s')\\,\\, - \\langle P_a(s), v \\rangle \\right| \\le \\gamma \\|q\\|_\\infty\\, \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} } \\nonumber \\\\ &amp;\\le \\frac{\\gamma}{1-\\gamma}\\, \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} } =: \\Delta(\\zeta,m), \\label{eq:basicerror} \\end{align}\\] where, for brevity, we introduced $v = Mq$ in the above formula. Union bounds . So we know that for any fixed state-action pair $(s,a)$, outside of a low probability event, $(\\hat T q)(s,a)$ is close to $( T q)(s,a)$. But can we conclude from this that, outside of some low probability event, $(\\hat T q)(s,a)$ is close to $( T q)(s,a)$ everywhere? . To answer this question, it will be easier to turn it around and just try to come up with some event that, on the one hand, has low probability, while, in the other hand, outside of this event, $(\\hat T q)(s,a)$ is close to $( T q)(s,a)$ regardless of $(s,a)$. Denoting by $\\mathcal{E}(s,a)$ the event when $(\\hat T q)(s,a)$ is not close to $( T q)(s,a)$, i.e., . \\[\\mathcal{E}(s,a) = \\{ |(\\hat T q)(s,a) - (Tq)(s,a)|&gt; \\Delta(\\zeta,m) \\}\\,,\\] it is clear that if $\\mathcal{E} = \\cup_{(s,a)} \\mathcal{E}(s,a)$ then outside of $\\mathcal{E}$, none of $\\mathcal{E}(s,a)$ holds and hence . \\[\\max_{(s,a)\\in \\mathcal{S}\\times \\mathcal{A}} |(\\hat T q)(s,a) - (T q)(s,a)|\\le \\Delta(\\zeta,m)\\,.\\] But how large can the probability of $\\mathcal{E}$ be? For this, recall the following elementary result, which follows directly from the properties of measures: . Lemma (Union Bound): For any probability measure $\\mathbb{P}$ and any countable sequence of events \\(A_1, A_2, \\ldots\\) of the underlying measurable space, . \\[\\mathbb{P}\\left(\\cup_i A_i \\right) \\leq \\sum_i \\mathbb{P}(A_i).\\] . By this result, using that $\\mathcal{S}\\times \\mathcal{A}$ is finite, . \\[\\mathbb{P}(\\mathcal{E}) \\le \\sum_{(s,a)\\in \\mathcal{S}\\times \\mathcal{A}} \\mathbb{P}( \\mathcal{E}(s,a)) \\le \\mathrm{S} \\mathrm{A} \\zeta\\,.\\] If we want this probability to be $0\\le \\zeta’\\le 1$, we can set $\\zeta = \\frac{\\zeta’}{\\mathrm{S}\\mathrm{A}}$ and conclude that with probability $1-\\zeta’$, for any state-action pair $(s,a)\\in \\mathcal{S}\\times \\mathcal{A}$, . \\[\\begin{align} |(\\hat T q)(s,a) - (T q)(s,a)| \\le \\Delta\\left(\\frac{\\zeta'}{\\mathrm{S}\\mathrm{A}},m\\right) = \\frac{\\gamma}{1-\\gamma} \\, \\sqrt{\\frac{\\log \\frac{2\\mathrm{S}\\mathrm{A}}{\\zeta'}}{2m} }\\,. \\label{eq:ropec} \\end{align}\\] The following diagram summarizes the idea of union bounds: . To control the error of some bad event happening, we can break the the bad event into a number of elementary parts. By controlling the probability of each such part, we can control the probability of the bad event, or, alternatively, control the probability of the complementary “good” event. The worst case for controlling the probability of the bad event is if the elementary parts do not overlap, but the argument of course works even in this case. Returning to our calculations, from the last formula we see that the errors grew a little compared to \\(\\eqref{eq:basicerror}\\), but the growth is modest: the errors scale with the logarithm of the number of state-action pairs. While this logarithmic error-growth is mild, it is unfortunate that the number of states appears here. To control the errors, by this formulae we would need to choose $m$ to be proportional to the logarithm of the size of the state space, which is better than a linear dependence, but still. One must wonder whether this dependence is truly necessary? If it was, there would be a big gap between the complexity of planning in deterministic and stochastic MDPs. We should not give in for this just yet! . Avoiding dependence on state space cardinality . The key to avoiding the dependence on the cardinality of the state is to avoid taking union bounds over the whole state-action set. That this may be possible follows from that, thinking back to the recursive implementation of the planner, we can notice that the planner does not necessarily rely on all the sets $\\mathcal{C}(s,a)$. To get a handle on this, it will be useful to introduce a notion of a distance induced by the set $\\mathcal{C}(s):=\\cup_{a\\in \\mathcal{A}}\\mathcal{C}(s,a)$ between the states. This distance between states $s$ and $s’$ (denoted by $\\text{dist}(s,s’)$) will be the smallest number of steps that we can take to get from $s$ to $s’$, if in each step we choose one “neighbouring” state to the last state, starting from state $s$. Formally, this is the length $n$ of the shortest sequence $s_0,s_1,\\dots,s_n$ such that $s_0=s$, $s_n = s’$ and for each $i\\in [n]$, $s_i \\in \\mathcal{C}(s_{i-1})$ (this is the distance between states in the directed graph over the states with edges induced by $\\mathcal{C}$). With this, for $h\\ge 0$, define . \\[\\begin{align*} \\mathcal{S}_h &amp;= \\{s \\in \\mathcal{S} | \\text{ dist}(s_0,s) \\leq h \\} \\end{align*}\\] as the set of states accessible from $s_0$ by at most $h$ steps. Note that this is a nested sequence of sets and $\\mathcal{S}_0 = {s_0}$, $\\mathcal{S}_1$ contains $s_0$ and its immediate “neighbors”, etc. We may now observe that in the calculation of $Q_H(s_0,\\cdot)$ when function $q$ is called with a certain value of $0\\le k \\le H$, for the state that appears in the call we have . \\[s\\in \\mathcal{S}_{H-k}\\,.\\] This can be proved by induction on $k$, starting with $k=H$. Click here for the proof. The base case follows because when $q$ calls itself it decrements $k$. Hence, when $q$ is called with $k=H$ and state $s$, $s=s_0$ must be true. Hence, $s\\in \\mathcal{S}_0$. Now, assume that the claim holds for $k=i+1$ with some $0\\le i&lt; H$. Take any state $s'$ that $q$ is called on while $k=i$. Since $i&lt;H$, this call must be a recursive call (from line 3). Going up on the call chain, at the time this recursive call is made, $k=i+1$ (since in the recursive calls the value of $k$ is decremented). This call happens when $s'\\in \\mathcal{C}(s,a)$ for some action $a\\in \\mathcal{A}$ and some state $s$, which, by the induction hypothesis, satisfies $s\\in \\mathcal{S}_{H-(i+1)}$. It follows that $s$ is at a distance of at most $H-i-1$ from $s_0$, while $s'$, a \"neighbour\" of $s$, is at most of a distance of $H-i-1+1=H-i$ from $s_0$. Hence, $s\\in \\mathcal{S}_{H-i}$, finishing the induction. $$\\blacksquare$$ Taking into account that when $q$ is called with $k=0$, the sets $\\mathcal{C}(s,a)$ are not used (line 2), we see that only states $s$ from $\\mathcal{S}_{H-1}$ are such that the calculation ever uses the set $\\mathcal{C}(s,a)$. Since $|\\mathcal{C}(s,a)|=m$, . \\[\\mathcal{S}_h \\le 1 + (mA) + \\dots + (mA)^h \\le (mA)^{h+1}\\] and in particular, $\\mathcal{S}_{H-1}\\le (mA)^H$, which is independent of the size of the state space. Of course, all along, we knew this very well: This is why the total runtime is also independent of the size of the state space. The plan is to take advantage of this to avoid a union bound over all possible state-action pairs. We start with a recursive expression for the errors. Recall that \\(\\delta_H = \\| (\\hat T^H \\boldsymbol{0})(s_0,\\cdot)-q^*(s_0,\\cdot)\\|_\\infty\\). By the triangle inequality, . \\[\\begin{align*} \\delta_H &amp; = \\| (\\hat T^H \\boldsymbol{0})(s_0,\\cdot)-q^*(s_0,\\cdot)\\|_\\infty \\le \\| (\\hat T \\hat T^{H-1} \\boldsymbol{0})(s_0,\\cdot)- \\hat T q^*(s_0,\\cdot)\\|_\\infty + \\| \\hat T q^*(s_0,\\cdot)- q^*(s_0,\\cdot)\\|_\\infty\\,. \\end{align*}\\] Now, observing that . \\[\\vert \\hat T q (s,a)-\\hat T q^* (s,a) \\vert \\le \\frac{\\gamma}{m} \\sum_{s'\\in \\mathcal{C}(s,a)} \\vert Mq - v^* \\vert (s') \\le \\gamma \\max_{s'\\in \\mathcal{C}(s)} \\vert Mq - v^* \\vert (s')\\,,\\] we see that . \\[\\begin{align*} \\delta_H &amp; \\le \\gamma \\max_{s'\\in \\mathcal{C}(s_0),a\\in \\mathcal{A}} | (\\hat T^{H-1} \\boldsymbol{0})(s',a)-q^*(s',a) | + \\| \\hat T q^*(s_0,\\cdot)- q^*(s_0,\\cdot)\\|_\\infty\\,. \\end{align*}\\] In particular, defining . \\[\\delta_{h} = \\underbrace{\\max_{s'\\in \\mathcal{S}_{H-h},a\\in \\mathcal{A}} | \\hat T^{h} \\boldsymbol{0}(s',a)-q^*(s',a)|}_{=:\\| \\hat T^h \\boldsymbol{0}-q^*\\|_{\\mathcal{S}_{H-h}}}\\,,\\] we see that . \\[\\delta_H \\le \\gamma \\delta_{H-1} + \\| \\hat T q^* - q^* \\|_{\\mathcal{S}_0}\\,,\\] where we use the notation \\(\\| q \\|_{\\mathcal{U}} = \\max_{s\\in \\mathcal{U},\\max_{a\\in \\mathcal{A}}} |q(s,a)|\\). More generally, we can prove by induction on $1\\le h \\le H$ (starting with $h=H$) that . \\[\\delta_h \\le \\gamma \\delta_{h-1} + \\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-h}} \\le \\gamma \\delta_{h-1} + \\underbrace{ \\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-1}}}_{=:\\varepsilon'/(1-\\gamma)} \\,,\\] while . \\[\\delta_0 = \\| q^*\\|_{\\mathcal{S}_{H}} \\le \\| q^* \\|_\\infty \\le \\frac{1}{1-\\gamma}\\,,\\] where the last inequality uses that $r_a(s)\\in [0,1]$, which we shall assume for simplicity. Unfolding this recursion for $(\\delta_h)_h$, letting . we get . \\[\\begin{align} \\delta_H &amp;\\leq \\frac{\\gamma^H + \\varepsilon'(1 + \\gamma + \\cdots + \\gamma^{H-1})}{1 - \\gamma} \\leq \\left(\\gamma^H + \\frac{\\varepsilon'}{1 - \\gamma} \\right) \\frac{1}{1 - \\gamma} \\label{eq:delta_H}. \\end{align}\\] We see that the first term in the sum on the right-hand side (in the parenthesis) is controlled by $H$. It remains to show that $\\varepsilon’$ can also be controlled (by choosing $m$ appropriately). In fact, notice that $\\varepsilon’ / (1 - \\gamma)$ is the maximum-norm error with which \\(\\hat T q^*\\) approximates \\(q^* = T q^*\\), but only for states in \\(\\mathcal{S}_{H-1}\\) we need to control this error. By our earlier argument, this set has at most \\((mA)^H\\) states, hence, it is believable that this error can be controlled even when $m$ is chosen independently of the number of states. Controlling \\(\\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-1}}\\) . Since \\(\\mathcal{S}_{H-1}\\) has only $(mA)^H$ states in it, one’s first instinct is to take a union bound over the error events for the states in this set. The trouble is that the set \\(\\mathcal{S}_{H-1}\\) itself is random. As such, it is not clear, what the failure events should be? And how many failure events are we going to have? The size of this set is also random! Notice that if \\((A_i)_{i\\in [n]}\\) are some events with \\(\\mathbb{P}(A_i)\\le \\delta\\) and \\(I_1,\\dots,I_k\\in [n]\\) are random indices, it does not hold that \\(\\mathbb{P}( \\cup_{j=1}^k A_{I_j}) \\le k \\delta\\): One cannot apply the union bound to randomly chosen events. In fact, in the worst case, \\(\\mathbb{P}( \\cup_{j=1}^k A_{I_j}) = n \\delta\\). To exploit that \\(\\mathcal{S}_{H-1}\\) is a small set, we need to use one more time the structure. The reason that the randomness of \\(\\mathcal{S}_{H-1}\\) is not going to matter too much is because of the special way this set is constructed. First of all, clearly, \\(s_0\\in \\mathcal{S}_{H-1}\\) always and at this state the error \\(\\|(\\hat T q^*)(s_0,\\cdot)- Tq^*(s_0,\\cdot)\\|_\\infty\\) is under control by Hoeffding’s inequality. Next, we may consider the neighbors of \\(s_0\\). If \\(S\\in \\mathcal{C}(s_0)\\), either \\(S=s_0\\), in which case we already know that the error at \\(S\\) is under control, or \\(S\\) is a “bona fide neighbor” and we can think of then generating the elements in \\(\\mathcal{C}(S,a)\\) just inside the call of $q$. Ultimately, the error at such a neighbor is under control because, by definition, all the sets \\(\\mathcal{C}(s,a)\\) (with $(s,a)$ sweeping through all possible state-action pairs) are independently chosen. This suggests that we should consider the chronological order in which in the recursive call of function $q$ the states in \\(\\mathcal{S}_{H-1}\\) appear. Let this order be $S_1,S_2,\\dots,S_n$, where $n = 1+(mA)+\\dots+(mA)^{H-1}$, $S_1=s_0$, $S_2$ is the second state that $q$ is called on (necessarily, \\(S_2\\in \\mathcal{C}(s_0)\\)), \\(S_3\\) is the third such state. Note that states may reappear in this sequence multiple times. Furthermore, by construction, \\(\\mathcal{S}_{H-1} = \\{ S_1,\\dots,S_n \\}\\). Also note that the length of this sequence is not random: This length is exactly the number of times $q$ is called, which is clearly not random. That \\(\\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-1}}=\\| \\hat T q^* - Tq^* \\|_{\\mathcal{S}_{H-1}}\\) is under control directly follows from the next lemma: . Lemma: Assume that the immediate rewards belong to the $[0,1]$ interval. For any $0\\le \\zeta \\le 1$ with probability $1-\\mathrm{A}n\\zeta$, for any $1\\le i \\le n$, . \\[\\begin{align*} \\| \\hat T q^* (S_i, \\cdot)- q^* (S_i,\\cdot) \\|_{\\infty} \\le \\Delta(\\zeta,m)\\,, \\end{align*}\\] where \\(\\Delta\\) is given by \\(\\eqref{eq:basicerror}\\). Proof: Recall that $\\mathcal{C}(s,a) = (S_1’(s,a),\\dots,S_m’(s,a))$ where (i) the \\((\\mathcal{C}(s,a))_{(s,a)}\\) are mutually independent and (ii) for any $(s,a)$, $(S_i’(s,a))_i$ is an i.i.d. sequence with common distribution $P_a(s)$. For \\(s\\in \\mathcal{S}\\), \\(a\\in \\mathcal{A}\\), \\(C\\in \\mathcal{S}^m\\), let . \\[\\begin{align*} g(s,a,C) =| \\frac{\\gamma}{m} \\sum_{s'\\in C} v^*(s') \\,\\, - \\langle P_a(s), v^* \\rangle | \\end{align*}\\] (as earlier, $s’\\in C$ means that $s’$ is an element of the set composed of the elements in the sequence $C$). Recall that by the definition of $\\hat T$ and the properties of $q^*$, . \\[\\begin{align} |\\hat T q^*(s,a)- q^*(s,a)| = | \\frac{\\gamma}{m} \\sum_{s'\\in \\mathcal{C}(s,a)} v^*(s') \\,\\, - \\langle P_a(s), v^* \\rangle | = g(s,a,\\mathcal{C}(s,a)) \\,. \\label{eq:dub} \\end{align}\\] Fix \\(1 \\le i \\le n\\). Let \\(\\tau = \\min\\{ 1\\le j \\le i\\,:\\, S_j = S_i \\}\\). That is, $\\tau$ is the time when $S_i$ first appears in the sequence \\(\\{S_i\\}_i\\). Fix $a\\in \\mathcal{A}$. We claim that given \\(S_{\\tau}\\), \\((S_j'(S_{\\tau},a))_{j=1}^m\\) is i.i.d. with common distribution \\(P_a(S_{\\tau})\\). That is, for any \\(s,s_1',\\dots,s_m'\\in \\mathcal{S}\\), . \\[\\begin{align} \\mathbb{P}( S_1'(S_{\\tau},a)=s_1',\\dots, S_m'(S_{\\tau},a)=s_m' \\, \\vert\\, S_{\\tau}=s) = \\prod_{j=1}^m P(s,a,s_j') \\label{eq:indep} \\end{align}\\] Note that given this, for any $\\Delta\\ge 0$, by \\(\\eqref{eq:dub}\\), . \\[\\begin{align*} \\mathbb{P}( &amp; |\\hat T q^*(S_i,a)- q^*(S_i,a)| &gt; \\Delta ) = \\mathbb{P}( g(S_i,a,\\mathcal{C}(S_i,a)) &gt; \\Delta ) \\\\ &amp; = \\mathbb{P}( g(S_{\\tau},a,\\mathcal{C}(S_\\tau,a)) &gt; \\Delta ) \\\\ &amp; = \\sum_{s} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, S_{\\tau}=s ) \\\\ &amp; = \\sum_{s} \\sum_{1\\le j \\le i} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, S_j=s, \\tau=j ) \\\\ &amp; = \\sum_{s} \\sum_{1\\le j \\le i} \\sum_{\\substack{s_{1:j-1} \\in \\mathcal{S}^{j-1}:\\\\ s\\not\\in s_{1:j-1}}} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, S_j=s, S_{1:j-1}=s_{1:j-1})\\\\ &amp; = \\sum_{s} \\sum_{1\\le j \\le i} \\sum_{\\substack{s_{1:j-1} \\in \\mathcal{S}^{j-1}:\\\\ s\\not\\in s_{1:j-1}}} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, \\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1 )\\,, \\end{align*}\\] for some binary valued functions $\\phi_1$, $\\dots$, $\\phi_i$ where for $1\\le j \\le i$, $\\phi_j$ is defined so that . \\[\\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1\\] holds if and only if $S_j=s, S_{1:j-1}=s_{1:j-1}$ holds, where $s\\in \\mathcal{S}$ and $s_{1:j-1}\\in \\mathcal{S}^{j-1}$ are arbitrary so that $s\\not\\in s_{1:j-1}$. That such functions exist follows because for any sequence $s_{1:j}$ to verify whether $S_{1:j}=s_{1:j}$ the knowledge of the sets $\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1})$ suffices: The appropriate function should first check $S_1=s_1$, then move on to checking $S_2=s_2$ only if $S_1=s_1$ holds, etc. Now, notice that by our assumptions, for $s\\not\\in s_{1:j-1}$, $\\mathcal{C}(s,a)$ and $\\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1$ are independent of each other. Hence, . \\[\\begin{align*} \\mathbb{P}( &amp; g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, \\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1 )\\\\ &amp; = \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta) \\cdot \\mathbb{P}(\\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1 )\\,. \\end{align*}\\] Plugging this back into the previous displayed equation, “unrolling” the expansion done using the law of total probability, we find that . \\[\\begin{align*} \\mathbb{P}( |\\hat T q^*(S_i,a)- q^*(S_i,a)| &gt; \\Delta ) &amp; = \\sum_{s} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta ) \\mathbb{P}( S_\\tau = s )\\,. \\end{align*}\\] Now, choose $\\Delta = \\Delta(\\zeta,m)$ from \\(\\eqref{eq:basicerror}\\) so that, thanks to $|q^*|_\\infty \\le 1/(1-\\gamma)$, for any fixed $(s,a)$, \\(\\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta(\\zeta,m) )\\le \\zeta\\) Plugging this in into the previous display we get . \\[\\begin{align*} \\mathbb{P}( |\\hat T q^*(S_i,a)- q^*(S_i,a)| &gt; \\Delta(\\zeta,m) ) \\le \\zeta \\sum_{s} \\mathbb{P}( S_\\tau = s ) = \\zeta\\,. \\end{align*}\\] The claim the follows by a union bound over all actions and all $1\\le i \\le n$. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec6/#error-control",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec6/#error-control"
  },"126": {
    "doc": "6. online planning - Part II.",
    "title": "Final error bound",
    "content": "Putting everything together, we get that for any $0\\le \\zeta \\le 1$, the policy $\\hat \\pi$ induced by the planner is $\\epsilon(m,H,\\zeta)$-optimal with . \\[\\epsilon(m,H,\\zeta):=\\frac{2}{(1-\\gamma)^2} \\left[\\gamma^H + \\frac{1}{1-\\gamma} \\sqrt{ \\frac{\\log\\left(\\frac{2n\\mathrm{A}}{\\zeta}\\right)}{2m} } + \\zeta \\right]\\,.\\] Thus, to obtain a planner that induces a $\\delta$-optimal policy, we can set $H$, $\\zeta$ and $m$ so that each term above contributes at most $\\delta/3$: . \\[\\begin{align*} \\frac{2\\gamma^H}{1-\\gamma} &amp; \\le (1-\\gamma)\\frac{\\delta}{3}\\,,\\\\ \\zeta &amp; \\le (1-\\gamma)^2\\frac{\\delta}{6}\\, \\qquad \\text{and}\\\\ \\frac{m}{\\log\\left(\\frac{2n\\mathrm{A}}{\\zeta}\\right)} &amp; \\ge \\frac{18}{\\delta^2(1-\\gamma)^6}\\,. \\end{align*}\\] For $H$ we get that we can set $H = \\lceil H_{\\gamma,(1-\\gamma)\\delta/6}\\rceil$. We can also set $\\zeta = (1-\\gamma)^2\\delta/6$. To solve for the smallest $m$ that satisfies the last inequality, recall that $n = (mA)^H$. To find the critical value of $m$ note the following elementary result which we cite without a proof: . Proposition: Let $a&gt;0$, $b\\in \\mathbb{R}$. Let \\(t^*=\\frac{2}{a}\\left[ \\log\\left(\\frac1a\\right)-b \\right]\\). Then, for any positive real $t$ such that $t\\ge t^*$, . \\[\\begin{align*} at+b &gt; \\log(t)\\,. \\end{align*}\\] . From this, defining . \\[c_\\delta = \\frac{18}{\\delta^2(1-\\gamma)^6}\\] and . \\[\\begin{align} m^*(\\delta,\\mathrm{A}) = 2c_\\delta \\left[ H \\log(c_\\delta H) + \\log\\left(\\frac{12}{(1-\\gamma)^2\\delta}\\right) + (H+1) \\log(\\mathrm{A}) \\right] \\label{eq:mstar} \\end{align}\\] if $m \\ge m^*$ then all the inequalities are satisfied. Putting things together, we thus get the following result: . Theorem: Assume that the immediate rewards belong to the $[0,1]$ interval. There is an online planner such that for any $\\delta\\ge 0$, in any discounted MDP with discount factor $\\gamma$, the planner induces a $\\delta$-optimal policy and uses at most \\(O( (m^* \\mathrm{A})^H )\\) elementary arithmetic and logic operations per its calls, where $m^*(\\delta,\\mathrm{A})$ is given by \\(\\eqref{eq:mstar}\\) and $H = \\lceil H_{\\gamma,(1-\\gamma)\\delta/3}\\rceil$. Overall, we see that the runtime did increase compared to the deterministic case (apart from logarithmic factors, in the above result $m = H^7/\\delta^2$ whereas in the deterministic case $m=1$!), but we managed to get a runtime that is independent of the cardinality of the state space. Again, what is troubling is the exponential dependence on the effective horizon, though as we have seen, in the worst-case, this is unavoidable. In the next lectures we will consider proving the planner with extra information so that this exponential dependence can be avoided. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec6/#final-error-bound",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec6/#final-error-bound"
  },"127": {
    "doc": "6. online planning - Part II.",
    "title": "Notes",
    "content": "Sparse lookahead trees . The idea of the algorithm that we analyzed comes from a paper by Kearns, Mansour and Ng from 2002. In their paper they consider the version of the algorithm which creates a fresh “new” random set $\\mathcal{C}(s,a)$ in every recursive call. This makes it harder to see their algorithm as approximating the Bellman operator, but in effect, the two approaches are by and large the same. In fact, if we introduce $H$ random operators, $\\hat T_1$, $\\dots$, $\\hat T_H$ which are the same as $\\hat T$ above but $\\hat T_h$ has its own “private” sets $( \\hat C_h(s,a) )_{(s,a)}$, then their algorithm can be written as computing . \\[A = \\arg\\max_{a} (\\hat T_1 \\dots \\hat T_h \\boldsymbol{0})(s_0,a)\\,.\\] It is not hard to modify the analysis given here to accommodate this change. With this, one can also interpret the calculations done by the algorithm as backing up values in a “sparse lookahead tree” built recursively from $s_0$. Much work has been devoted to improving these basic ideas and eventually these ideas led to various Monte-Carlo tree search algorithms, including yours truly’s UCT. In general, these algorithms attempt to improve on the runtime by building the trees when they need to be built. As it turns out, a useful strategy here is to expand nodes which in a way hold the greatest promise to improve the value at the “root”. This is known as the “optimisism in planning”. Note that A* (and its MDP relative, AO) are also based on optimism: A’s admissible heuristic functions in our language correspond to functions that upper bound the optimal value. The definite source on MCTS theory as of today is Remi Munos’s monograph. Measure concentration . Hoeffding’s inequality is a special case of what is known as measure concentration. This phrase refers to that the empirical measure induced by a sample is a good approximation to the whole measure. The simplest case is when one just compares the means of the measures (the empirical and the sample-generating one), giving rise to concentration inequalities around the mean. Hoeffding’s inequality is an example. What we like about Hoeffding’s inequality (besides that it is simple) is that the failure probability, $\\delta$ (later $\\zeta$) appears inside a logarithm. That means, that the price of being more stringent is mild. When the exact dependence is of type that appears in Hoeffding’s inequality (i.e., $\\sqrt{ \\log(1/\\delta)})$), we say that the deviation of the subgaussian type because Gaussian random variables also satisfy an inequality like this. Concentration of measure and concentration inequalities are a central topic in probability theory, with separate books devoted to them. A few favourites are given at the end of this notes . For learning purposes, Pollard’s mini-book is nice (but all these books have pros and cons), or Vershynin’s book. The comparison inequality . The comparison inequality between the logarithm and the linear function is given as Proposition 4 here. The proof is based on two observations: First, it is enough to consider the case when $b=0$. Then, if $a\\ge 1$, the result is trivial, while for $a&lt; 1$, the guess is based on doubling the value where the growth rate of $t\\mapsto at$ matches that of $t\\mapsto \\log(t)$. A model-centered view and random operators . A key idea of this lecture is that $\\hat T$ is a good (random) approximation to $T$, hence, it can be used in place of $T$. One can also tell this story by saying that the data underlying $\\hat T$ gives a random approximation to the MDP; the transition probabilities of this random approximating MDP would be defined using . \\[\\hat P(s,a,s') = \\frac1m \\sum_{s''\\in C(s,a)} \\mathbb{I}\\{ s''=s'\\}\\] It may seem quite miraculous that with only a few elements in $C(s,a)$ (i.e., small $m$) we get a good approximation to the next state distribution. But so is the magic of randomness! Using a random operator (or a sequence of them, if, as outlined above, one uses a fresh set of random next state every time an update is calculated) in a dynamic programming method has been coined empirical dynamic programming by Haskell et al.. A bigger point is that for a model to be a “good” approximation to the “true MDP”, it suffices that the Bellman optimality operator that it induces is a “close” approximation to the Bellman optimality operator of the true MDP. This in fact brings us to our next topic, which is what happens when the simulator is imperfect? . Imperfect simulation model? . We can rarely expect simulators to be perfect. Luckily, not all is lost in this case. As noted above, if the simulator induced an MDP whose Bellman optimality operator is in a way close to the Bellman optimality operator of the true MDP, we expect the outcome of planning to be still a good policy in the true MDP. In fact, the above proof has already all the key elements in place to show this. In particular, it is not hard to show that if $\\hat T$ is a $\\gamma$ max-norm contraction and $\\hat q^*$ is its fixed point then . \\[\\|\\hat q^* - q^*\\|_\\infty \\le \\frac{\\| \\hat T q^* - T q^* \\|_\\infty}{1-\\gamma}\\,,\\] which, combined with the our first lemma of this lecture on the policy error bound gives that the policy that is greedy with respect to $\\hat q^*$ is . \\[\\frac{2\\| \\hat T q^* - T q^* \\|_\\infty }{(1-\\gamma)^2}\\] optimal in the MDP underlying $T$. We will return to this in later lectures. In particular, in batch reinforcement learning, one of the basic methods is to learn a “model” of the environment and as such it is inevitable to study the error that results from modelling errors. See Lecture 17 and Lecture 18. Monte-Carlo methods . We saw in homework 0 that randomization may help a little, and today we saw that it can help in a more significant way. A major lesson again is that representations do matter: If the MDP is not given with a “generative simulator”, getting such a simulator may be really hard. This is good to remember when it comes to learning models: . One should insist on learning models that make the job of planners easier. Generative models are one such case, provably, as we have seen in today’s lecture put together with our previous lower bound that involved the number of states. Randomization, more generally, is a powerful tool in computing science, which brings us to a somewhat philosophical question: What is randomness? Does “true randomness” exist? Can we really build computers to harness this? . True randomness? . What is the meaning of “true” randomness? The margin is definitely not big enough to explain this. Hence, we just leave this there, hanging, for everyone to ponder about. But let’s also note that this is a thoroughly studied question in theoretical computing science, with many beautiful results and even books. Arora and Barak’s book on computational complexity (Chapters 7, 20 and 21) is a good start for exploring this. Can we recycle the sets $C(s,a)$ between the calls? . If simulation is expensive, it may be tempting to recycle the sets between calls of the planner. After all, even if we recycle these sets, \\(\\hat{\\pi}\\) will have the property that it selects $\\epsilon$-optimizing actions with high probability at every state. However, this may not be a good idea. The reader is challenged to think about what can go wrong? The proof actually uses that the planner construct a new random operator $\\hat T$ with every call. But where is this used? . The ubiquity of continuity arguments in the MDP literature . All the computations that we do with MDPs tend to be approximate. We evaluate policies approximately. We compute a Bellman back approximately. We have approximate models. We greedify approximately. If any of these operations could enlarge small errors, none of the approximate methods would work. The study of approximate computations (which is a necessity if one faces large MDPs) is a study of the sensitivity of the values of the resulting policies to the errors introduced in the computations. This, in numerical analysis, would be called error analysis. In other areas of mathematics, this is called sensitivity analysis. In fact, sensitivity analysis often involves computing derivatives to see how fast outputs change as the inputs change (which is that data that will be approximated). What should we be taking derivatives with respect to here? Well, it is always the data that is being changed. One can in fact use differentiation based sensitivity analysis everywhere. This has been tried a little in the “older” MDP literature and is also related to policy gradient theorems (that we will learn about laters). However, perhaps there are more nice things to be discovered about this approach. From local to online access . The algorithm that is analyzed in this lecture requires local access simulators. This is better than requiring global access, but worse than requiring online access. It remains an open question of whether with online access, one can also get a similar result than shown in the present lecture and if not, whether the sample complexity of planning remains finite under this setting. When the state space is small . For finite state-action MDPs where the rewards and transition probabilities are represented using tables, a previous lecture’s main result established that an optimal policy of the MDP can be calculated by using at most $O(H\\textrm{poly}(S,A))$ arithmetic and logic operations ($H=1/(1-\\gamma)$ here). In the current lecture we saw that even when $S$ is unbounded, given a simulator with local access, $\\tilde{O}((AH^7/\\delta^2)^H)$ such elementary operations and calls to a simulator are sufficient. In a finite MDP, depending on the values of $S,A$ and $H$, either policy iteration, or the online planner that builds the tree will be faster. But policy iteration (and value iteration) as described previously used a table representation. The question then arises of what is the sample complexity of planning with a simulator access to a finite MDP? If planning means outputting a policy, the complexity needs to scale with $S$. In the presence of global access simulators, a simple approach, is to sample an appropriate number of next states for each state-action pair to build an empirical (but “sparse”) transition model and use this in connection with any MDP solver. We will see later in Lecture 18 that in this case $O(H^3 SA/\\delta^2)$ samples (or $H^3/\\delta^2$ samples per state-action pair) are sufficient to obtain a $\\delta$-optimal policy. In the case of online planning with global access, the sample complexity cannot be worse, but it is unclear whether it can be improved. Similarly, it is unclear what the complexity is in the case of either local or online access. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec6/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec6/#notes"
  },"128": {
    "doc": "6. online planning - Part II.",
    "title": "References",
    "content": ". | Kearns, M., Mansour, Y., &amp; Ng, A. Y. (2002). A sparse sampling algorithm for near-optimal planning in large Markov decision processes. Machine learning, 49(2), 193-208. [link] | David Pollard (2015). A few good inequalities. Chapter 2 of a book under preparation with working title “MiniEmpirical”. [link] | Stephane Boucheron, Gabor Lugosi and Pascal Massart (2012). Concentration inequalities: A nonasymptotic theory of indepndence. Clarendon Press – Oxford. [link] | Roman Vershynin (2018). High-Dimensional Probability: An Introduction with Applications in Data Science. [link] | M. J. Wainwright (2019) High-dimensional statistics: A non-asymptotic viewpoint. Cambridge University Press. | Lafferty J., Liu H., &amp; Wasserman L. (2010). Concentration of Measure. [link] | Lattimore, T., &amp; Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press. | William B. Haskell, Rahul Jain, and Dileep Kalathil. Empirical dynamic programming. Mathematics of Operations Research, 2016. | Sanjeev Arora and Boaz Barak (2009). Computational Complexity: A Modern Approach. Cambridge University Press. | Remi Munos (2014). From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning. Foundations and Trends in Machine Learning: Vol. 7: No. 1, pp 1-129. | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec6/#references",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec6/#references"
  },"129": {
    "doc": "6. online planning - Part II.",
    "title": "6. online planning - Part II.",
    "content": "PDF Version . In the previous lecture online planning was introduced. The main idea is to amortize the cost of planning by asking a planner to produce an action to be taken at a particular state so that the policy induced by repeatedly calling the planner at the states just visited and then using the action returned by the planner is near-optimal. We have seen that with this, the cost of planning can be made independent of the size of the state space – at least for deterministic MDPs. For this, one can use just a recursive implementation of value iteration, which, for convenience, we wrote using action-value functions and the corresponding Bellman optimality operator, $T$, defined by . \\[\\begin{align*} T q(s,a) = r_a(s) + \\gamma \\langle P_a(s), M q \\rangle\\,. \\end{align*}\\] (in the previous lecture we used $\\tilde T$ to denote this operator, but to reduce clutter from now on, we will drop the tilde). We have also seen that no procedure can do significantly better in terms of its runtime (or query cost) than this simple recursive procedure. In this lecture we show that these ideas also extend to the stochastic case. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec6/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec6/"
  },"130": {
    "doc": "7. Function Approximation",
    "title": "Hints on value functions",
    "content": "The hints that we start with will concern the value functions. In particular, they state that either the optimal value, or the value function of all policies are effectively compressible. For motivation, consider the figure on the right. Imagine the state space is an interval of the real line and the optimal value function in an MDP looks like as shown on the figure: It is a nice, smooth function over the interval. As is well known, such relatively slowly changing functions can be well approximated by using the linear combination of a few fixed basis functions, like an appropriate polynomial, or Fourier basis, or using splines. Then, one hopes that even though the state space is large or even infinite as in this example, there could perhaps be a method that calculates the few coefficients needed get a good approximation to \\(v^*\\) with a runtime that depends polynomially on the horizon, the number of actions and the number of coefficients that one needs to calculate. Given the knowledge of $v^*$ and simulator access to the MDP, good actions can then be efficiently obtained by performing one-step lookahead computations. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec7/#hints-on-value-functions",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec7/#hints-on-value-functions"
  },"131": {
    "doc": "7. Function Approximation",
    "title": "Linear function approximation",
    "content": "If the basis functions mentioned are $\\phi_1,\\dots,\\phi_d: \\mathcal{S} \\to \\mathbb{R}$ then, formally, the hope is that with some coefficients $\\theta =(\\theta_1,\\dots,\\theta_d)^\\top\\in \\mathbb{R}^d$, we will have . \\[\\begin{align} v^*(s) = \\sum_{i=1}^d \\theta_i \\phi_i(s)\\, \\qquad \\text{for all } s\\in \\mathcal{S}\\,. \\label{eq:vstr} \\end{align}\\] In the reinforcement learning literature, the vector $(\\phi_1(s), \\dots, \\phi_d(s))^\\top$ is called the feature vector assigned to state $s$. For a more compact notation we also use $\\phi$ to be a map from $\\mathcal{S}$ to $\\mathbb{R}^d$ which assigns the feature vectors to the states: . \\[\\phi(s) = (\\phi_1(s),\\dots,\\phi_d(s))^\\top\\,.\\] Conversely, given $\\phi: \\mathcal{S}\\to \\mathbb{R}^d$, its component are denoted using $\\phi_1,\\dots,\\phi_d$. It will also be useful to introduce a matrix notation: Recall that the number of states is $\\mathrm{S}$ and without loss of generality we may assume that $\\mathcal{S} = [\\mathrm{S}]$. Then, we can treat each of $\\phi_1,\\dots,\\phi_d$ as $\\mathrm{S}$-dimensional vectors: The $i$th component of $\\phi_j$ is $\\phi_j(i)$. Then, we can stack $\\phi_1,\\dots,\\phi_d$ next to each other to form a matrix: . \\[\\Phi = \\begin{pmatrix} | &amp; | &amp; \\dots &amp; | \\\\ \\phi_1 &amp; \\phi_2 &amp; \\dots &amp; \\phi_d \\\\ | &amp; | &amp; \\dots &amp; | \\end{pmatrix} \\in \\mathrm{R}^{\\mathrm{S}\\times d}\\,.\\] That is, $\\Phi$ is a $\\mathrm{S}\\times d$ matrix. The set of real-valued functions over the state space that can be described with the linear combination of the basis functions is . \\[\\mathcal{F} = \\{ f: \\mathcal{S} \\to \\mathbb{R} \\,:\\, \\exists \\theta\\in \\mathbb{R}^d \\text{ s.t. } f(s) = \\langle \\phi(s),\\theta \\rangle \\}\\,.\\] Identifying the space of real-valued functions with the vector space $\\mathbb{R}^{\\mathrm{S}}$ in the natural way, $\\mathcal{F}$ is a $d$-dimensional subspace of $\\mathbb{R}^{\\mathrm{S}}$, which is the same as the “column space”, or the span, or the range space of $\\Phi$: . \\[\\mathcal{F} = \\{ \\Phi \\theta \\,:\\, \\theta\\in \\mathbb{R}^d \\} = \\text{span}(\\Phi)\\] If we need to indicate the dependence of $\\mathcal{F}$ on the choice of features, we will write either \\(\\mathcal{F}_{\\phi}\\) or \\(\\mathcal{F}_{\\Phi}\\). Now, we have three equivalent ways of specifying the “features”, either by specifying the basis functions $\\phi_1,\\dots,\\phi_d$, or the feature-map $\\phi$, or the feature matrix $\\Phi$, and we have a four equivalent way of specifying the functions that can be obtained via the linear combination of features. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec7/#linear-function-approximation",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec7/#linear-function-approximation"
  },"132": {
    "doc": "7. Function Approximation",
    "title": "Delivering the hint",
    "content": "Note that in the above problem description it is tacitly assumed that the feature-map, in some form or another, is available to the planner. In fact, the feature map can be made available in multiple ways. When we argue for lower bounds, especially for query complexity, we often assume that the whole feature-map is available for the algorithm. For upper bounds with online planning, the most natural assumption is that the planner gets from the simulator the feature vector of the states that it encounters. In particular, when it comes to online planning, the natural assumption is that the planner gets the feature vector of the initial state together with the state and with any subsequent calls to the simulator, the simulator returns the feature vector of the next states, together with the next states. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec7/#delivering-the-hint",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec7/#delivering-the-hint"
  },"133": {
    "doc": "7. Function Approximation",
    "title": "Typical hints",
    "content": "In what follows we will study planning under a number of different hints (or assumptions) that connect the MDP and a feature-map. The simplest of this just states that $\\eqref{eq:vstr}$ holds: . Assumption A1 ($v^*$-realizibility): The MDP $M$ and the featuremap $\\phi$ are such that \\(v^* \\in \\mathcal{F}_\\phi\\) . A second variation is when all value functions are realizable: . Assumption A2 (universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(v^\\pi \\in \\mathcal{F}_\\phi\\). Clearly, A2 implies A1, because by the fundamental theorem of MDPs, there exists a memoryless policy \\(\\pi\\) such that \\(v^\\pi = v^*\\). The figure on the right illustrates the set of all finite MDPs with some state space and within those the set of those MDPs that satisfy A1 with a specific feature map $\\phi$ (denoted by A1\\(\\mbox{}_\\phi\\) on the figure), as well as those MDPs that satisfy A2 with the same feature map (denoted by A2\\(\\mbox{}_\\phi\\)). Both of these sets represent a very small fraction of all MDPs. However, of one changes the feature map, the union of all these sets clearly covers the set of all MDPs: The hint is general. There are many variations of these assumptions. Often, we will find it useful to relax the assumption value functions are exactly realizable. Under the modified assumptions the value function does not need to lie in the span of the feature-map, but only in some vicinity of it. The natural error metric to be used is the maximum norm for reasons that will become clear later. To help with stating these assumptions in a compact form, introduce the notation . \\[v\\in_{\\varepsilon} \\mathcal{F}\\] to denote that . \\[\\inf_{f\\in \\mathcal{F}} \\| f - v \\|_\\infty \\le \\epsilon\\,.\\] That is, $v\\in_{\\varepsilon} \\mathcal{F}$ means that the best approximator to $v$ from $\\mathcal{F}$ approximates it within a uniform error of $\\varepsilon$. Fixing $\\varepsilon\\ge 0$ and replacing $\\in$ with $\\in_{\\varepsilon}$ in the above two assumptions gives the following: . Assumption A1$\\mbox{}_{\\varepsilon}$ (approximate $v^*$ realizability): The MDP $M$ and the featuremap $\\phi$ are such that \\(v^* \\in_{\\varepsilon} \\mathcal{F}_\\phi\\) . Assumption A2$\\mbox{}_{\\varepsilon}$ (approximate universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(v^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\phi\\). ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec7/#typical-hints",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec7/#typical-hints"
  },"134": {
    "doc": "7. Function Approximation",
    "title": "Action-value hints",
    "content": "We obtain new variants if we consider feature-maps that map state-action pairs to vectors. Concretely, (by abusing notation) let $\\phi: \\mathcal{S}\\times\\mathcal{A}\\to \\mathbb{R}^d$. Then, the analog of A1 is as follows: . Assumption B1 ($q^*$-realizibility): The MDP $M$ and the featuremap $\\phi$ are such that \\(q^* \\in \\mathcal{F}_\\phi\\) . Here, as expected, $\\mathcal{F}_\\phi$ is defined as the set of functions that lie in the span of the feature-map. The analog of A2 is as follows: . Assumption B2 (universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(q^\\pi \\in \\mathcal{F}_\\phi\\). We can also introduce positive approximation errors $\\varepsilon&gt;0$, which lead to B1\\(_{\\varepsilon}\\) and B2\\(_{\\varepsilon}\\): . Assumption B1$\\mbox{}_{\\varepsilon}$ (approximate $q^*$-realizibility): The MDP $M$ and the featuremap $\\phi$ are such that \\(q^* \\in_{\\varepsilon} \\mathcal{F}_\\phi\\) . Assumption B2$\\mbox{}_{\\varepsilon}$ (approximate universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(q^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\phi\\). One may wonder why not choose one of these assumptions? When one assumption implies another, then clearly there is a preference to choose the weaker assumption. But often, there is going to be a price and sometimes the assumptions are just not comparable. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec7/#action-value-hints",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec7/#action-value-hints"
  },"135": {
    "doc": "7. Function Approximation",
    "title": "Notes",
    "content": "Origin . The idea of using value function approximation in planning dates back to at least the 1960s if not earlier. I include some intriguing early references at the end. That these ideas already appeared at the down of computing where computers hardly even existed is quite intriguing. Infinite spaces . Function approximation is especially appealing when the state space, or the action space, or both are “continuous” (i.e., they are a subset of a Euclidean space). In this case, the compression is “infinite”. Experimental evidence suggests that function approximation can work quite well in the context of MDP planning in a surprisingly large number of different scenarios. When the spaces are infinite, all the “math” will still go through, except that occasionally one has to be a bit more careful. For example, one cannot clearly say that $\\Phi$ is a matrix, but $\\Phi$ can clearly be defined as a linear operator mapping $\\mathbb{R}^d$ to the vector space of all real-valued functions over the (say) state space (when the feature map is also over states). Where do the features come from? . It will be instructive to start with a special case. Low-rank MDPs are those where the transition kernel factorizes: For any $s,a,s’$ state-action-state triple, . \\[P(s'|s,a) = \\langle \\phi(s,a), \\nu(s') \\rangle\\] for some $\\phi: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}^d$ and $\\nu(s’)\\in \\mathbb{R}^d$. If in addition to the above, . \\[\\begin{align} r(s,a) = \\langle \\phi(s,a), \\nu' \\rangle \\label{eq:rfact} \\end{align}\\] also holds for some $\\nu’\\in \\mathbb{R}^d$, it is not hard to see that any action-value function lies in the space of the features $\\phi$. But what are the cases when the transition kernel factorizes? (If the transition kernel factorizes with some feature map $\\phi_0$, one can always arrange for $\\eqref{eq:rfact}$ to hold by adding an extra dimension to the feature map, filled with the values of the rewards.) A simple case is when state-action pairs can be clustered into non-overlapping groups such that for any two pairs $(s_1,a_1),(s_2,a_2)$ that belong to the same group, the transitions are identical: $P(\\cdot|s_1,a_1) = P(\\cdot|s_2,a_2)$. Assuming $d$ groups number from $1$ to $d$, $\\phi_i(s,a)$ can be chosen as the indicator that $(s,a)$ belongs to the $i$th group ($i\\in [d]$). Another interesting case which leads to a factored transition kernel is when the state-space is $\\mathbb{R}^p$ with some $p&gt;0$ and the dynamics takes the form . \\[S_{t+1} = f(S_t,A_t) + \\eta_{t+1}\\] with some function $f$, and $(\\eta_t)_t$ is a sequence of independent random variables with common density $g$. Then, the transition kernel takes the form \\(P(ds'|s,a) = g(s'-f(s,a)) ds'\\). The important point here is that the noise introduced is homoscedastic (does not change with $(s,a)$). Take, for example, the case when $g(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$, i.e., $(\\eta_t)_t$ are standard normal random variables. It is well known then that . \\[g(x-y) = \\langle u(x;\\cdot,\\cdot), u(y;\\cdot,\\cdot) \\rangle\\,,\\] where . \\[u(x;\\omega,b) = \\sqrt{2} \\cos(\\omega^\\top x + b )\\] and for $n,m: \\mathcal{D} \\to \\mathbb{R}$, $\\mathcal{D}:=\\mathbb{R}^p \\times [0,2\\pi]$, . \\[\\langle n,m \\rangle = \\int_{\\mathbb{R}^p}\\, \\frac{1}{2\\pi} \\int_0^{2\\pi} n(\\omega,b) m(\\omega,b) \\, \\, db \\, \\prod_{i=1}^p g(\\omega_i)\\, d\\omega \\,.\\] From this, we get . \\[P(ds'|s,a) = g(s'-f(s,a)) ds' = \\langle u(s';\\cdot,\\cdot), u(f(s,a);\\cdot,\\cdot) \\rangle ds'\\,.\\] It follows that if we define \\(\\phi: \\mathcal{S}\\times\\mathcal{A} \\to \\mathbb{R}^{\\mathcal{D}}\\) via . \\[(\\phi(s,a))(\\omega,b) = u(f(s,a);\\omega,b)\\] then . \\[P(ds'|s,a) = \\langle \\phi(s,a), u(s';\\cdot,\\cdot) \\rangle ds'\\,,\\] which is the same as above, except here $\\phi$ is infinite dimensional. In a way, what happens here is that the noise introduces smoothness of the value functions. Smoothness of value functions can arise in some other ways. In the related topic of numerical computation of solutions of partial differential equations, Galerkin’s method also starts from assuming that the solution lies in the span of some features. In the relavant literature, various methods have been proposed to find appropriate features (or, basis functions, as they are called there). The book of Quarteroni et. al. gives several methods for automating the construction of these basis functions, and they also make a connection to optimal control. Nonlinear value function approximation . The most successful use of the idea of compressing value functions uses neural networks. Readers are most likely are already familiar with the ideas underlying neural networks. The hope here is that whatever we find in the case of linear function approximation will have implications in how to use nonlinear function approximation in MDP planning. In a way, the very first question is whether one can decouple the design of the planning algorithm from what function approximation technique it is used with. We will study this question by asking for planners that work with any feature map. If we find that we can identify planners that are performant no matter the feature map, the decoupling is successful and we can hope that the ideas will generalize to nonlinear function approximation. However, if we find that successful planners need to use intricate properties of the feature maps, then this is must be taken as a warning that complications may arise when the results are generalized to nonlinear function approximation. In any case, it appears to be a prudent strategy to first investigate the simpler, more straightforward linear case, before considering the nonlinear case. Computation with advice/Non-uniform Computation . Computation with advice is a general approach in computer science where a problem of computing a map is changed to computing a map which has an additional input, the advice. Clearly, the approach taken here can be seen as a special case of computation with advice. There is also the closely related notion of non-uniform computation studied in computability/complexity theory. In non-uniform computation, the Turing machine, in addition to its input, also receives some “advice” string. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec7/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec7/#notes"
  },"136": {
    "doc": "7. Function Approximation",
    "title": "References",
    "content": "The classical reference is a paper of Bellman et al. from 1963, where they proposed to use linear function approximation in a specific context for approximating the optimal value functions (Bellman et al. 1963). Other early papers are by Daniel (1976) and Schweitzer and Seidmann (1985). In the latter paper, the authors generalized the earlier constructions of Bellman and others and, with modern terminology, they introduced fitted value iteration, fitted policy iteration and approximate linear programming as possible approaches. The observation that homoscedastic noise makes it so that the transition kernel factorizes is due to Ren et al. (2022). The book of Quarteroni et al. (2016) describes various methods for automating the construction of basis functions for the solution of parametric family of partial differential equations. | Richard Bellman, Robert Kalaba and Bella Kotkin. 1963. Polynomial Approximation–A New Computational Technique in Dynamic Programming: Allocation Processes. Mathematics of Computation, 17 (82): 155-161 | Daniel, James W. 1976. “Splines and Efficiency in Dynamic Programming.” Journal of Mathematical Analysis and Applications 54 (2): 402–7. | Schweitzer, Paul J., and Abraham Seidmann. 1985. “Generalized Polynomial Approximations in Markovian Decision Processes.” Journal of Mathematical Analysis and Applications 110 (2): 568–82. | Brattka, Vasco, and Arno Pauly. 2010. Computation with Advice. arXiv [cs.LO]. | Quarteroni, Alfio, Andrea Manzoni, and Federico Negri. “Reduced Basis Methods for Partial Differential Equations”. Springer International Publishing. 2016. | Ren, T., T. Zhang, C. Szepesvári, and B. Dai. 2022. “A Free Lunch from the Noise: Provable and Practical Exploration for Representation Learning.” UAI. abstract | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec7/#references",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec7/#references"
  },"137": {
    "doc": "7. Function Approximation",
    "title": "7. Function Approximation",
    "content": "PDF Version . Our lower bound for online planners show that there are no online planners that lead to good policies in all MDPs while satisfying the following three requirements . | the planner induces policies that achieve some positive fraction of the optimal value in all MDPs; | the per-state runtime shows polynomial dependence on the planning horizon $H$ and | it shows a polynomial dependence on the number of actions and | it shows no dependence on the number of states in the MDP. | . Thus, one is left with no choice than to give up on one of the requirements. Since efficiency is clearly nonnegotiable (otherwise the runner just would not be practical), the only requirement that can be replaced is the first one. In what follows we will look at ways of relaxing this requirement. In all the relaxations we will look at, we will essentially restrict the set of MDPs that the planner is expected to work on. However, we will do this in such a way that no MDP will be ever ruled out. We achieve this by giving the planner some extra hint about the MDP and we demand good performance only when the hint is correct. Since the hint will take a general form, some hint is always correct for any MDP. Hence, no MDP is left behind and the planner can again demanded to be efficient and effective. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec7/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec7/"
  },"138": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Approximate Policy Evaluation: Done Well",
    "content": "Recall that in phase $k$ of policy iteration, given a policy $\\pi_k$, the next policy $\\pi_{k+1}$ is obtained as the policy that is greedy with respect to $q^{\\pi_k}$. If we found some coefficients $\\theta_k\\in \\mathbb{R}^d$ such that . \\[\\begin{align*} q^{\\pi_k} \\approx \\Phi \\theta_k\\,, \\end{align*}\\] then when it comes to “using” policy $\\pi_{k+1}$, we could just use $\\arg\\max_{a} \\langle \\theta_k,\\varphi(s,a)\\rangle$ when an action is needed at state $s$. Note that this action can be obtained at the cost of $O(d)$ elementary operations, a small overhead compared to a table lookup (with idealized $O(1)$ access times). Hence, the main question is how to obtain this parameter in an efficient manner. To be more precise, here we want to control the uniform error committed in approximating $q^{\\pi_k}$. To simplify the notation, let $\\pi = \\pi_k$. A simple idea is rolling out with the policy $\\pi$ from a fixed set $\\mathcal{C}\\subset \\mathcal{S}\\times \\mathcal{A}$ to “approximately” measure the value of $\\pi$ at the pairs in $\\mathcal{C}$. For concreteness, let $(s,a)\\in \\mathcal{C}$. Rolling out with policy this pair means using the simulator to simulate what would happen if we used policy $\\pi$ for a number of consecutive time steps when the initial state is $s$, the first action $a$, but for subsequent time steps the actions are chosen using policy $\\pi$ for whatever states are encountered. If the simulation goes on for $H$ steps, this way we get \\(m\\) trajectories starting in \\(z = (s, a)\\). For $1\\le j \\le m$ let the trajectory obtained be \\(\\tau_\\pi^{(j)}(s, a)\\). Thus, . \\(\\begin{align*} \\tau_\\pi^{(j)}(s, a) = \\left( S_0^{(j)}, A_0^{(j)}, S_1^{(j)}, A_1^{(j)}, \\ldots, S_{H-1}^{(j)}, A_{H-1}^{(j)} \\right)\\, \\end{align*}\\), . where \\(S_0^{(j)}=s\\), \\(A_0^{(j)}=a\\), and for $1\\le t \\le H-1$, \\(S_{t}^{(j)} \\sim P_{A_t^{(j)}} ( S_{t-1}^{(j)} )\\), and \\(A_t^{(j)} \\sim \\pi ( \\cdot | S_{t}^{(j)} )\\). The figure on the right illustrates these trajectories. Given these trajectories, the empirical mean of the discounted sum of rewards along these trajectories is used for approximating $q^\\pi(z)$: . \\[\\begin{align} \\hat R_m(z) = \\frac{1}{m} \\sum_{j=1}^m \\sum_{t=0}^{H-1} \\gamma^t r_{A_t^{(j)}}(S_t^{(j)}). \\label{eq:petargetsbiased} \\end{align}\\] Under the usual condition that the rewards are in the $[0,1]$ interval, the expected value of $\\hat{q}^\\pi(z)$ is in the $\\gamma^H/(1-\\gamma)$ vicinity of the $q^\\pi(z)$ and by averaging a large number of independent trajectories, we also achieve that the empirical means are tightly concentrated around their mean. Using a randomization device, it is possible to remove the error (“bias”) introduced by truncating the trajectories at a fixed time. For this, just let $(H^{(j)})_{j}$ be independent geometrically distributed random variables with parameter $1-\\gamma$, which are also independently chosen from the trajectories. By definition \\(H^{(j)}\\) is the number of $1-\\gamma$-parameter Bernoulli trials needed to get one success. With the help of these variables, define now $\\hat R_m(z)$ by . \\[\\begin{align} \\hat R_m(z) = \\frac{1}{m} \\sum_{j=1}^m \\sum_{t=0}^{H^{(j)}-1} r_{A_t^{(j)}}(S_t^{(j)})\\,. \\label{eq:petargetsunbiased} \\end{align}\\] Note that in the expression of \\(\\hat R_m(z)\\) the discount factor is eliminated. To calculate \\(\\hat R_m(z)\\) one can just perform a rollout with policy $\\pi$ as before, just in each time step $t=0,1,\\dots$, after obtaining $r_{A_t^{(j)}}(S_t^{(j)})$, draw a Bernoulli variable with parameter $(1-\\gamma)$ to decide whether the rollout should continue. To see why the above definition works, fix $j$ and note that by definition, for $h\\ge 1$, \\(\\mathbb{P}(H^{(j)}=h) = \\gamma^{h-1}(1-\\gamma)\\) and thus \\(\\mathbb{P}(H^{(j)}\\ge t+1) = \\gamma^t\\). Therefore, . \\[\\begin{align*} \\mathbb{E}[ \\sum_{t=0}^{H^{(j)}-1} r_{A_t^{(j)}}(S_t^{(j)}) ] &amp; = \\sum_{t=0}^\\infty \\mathbb{E}[ \\mathbb{I}\\{ t \\le H^{(j)}-1\\} r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = \\sum_{t=0}^\\infty \\mathbb{E}[ \\mathbb{I}\\{ t \\le H^{(j)}-1\\} ]\\, \\mathbb{E}[ r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = \\sum_{t=0}^\\infty \\mathbb{P}( t+1 \\le H^{(j)} )\\, \\mathbb{E}[ r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}[ r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = q^\\pi(z)\\,. \\end{align*}\\] All in all, this means, that we expect that if we solve for the least-squares problem . \\[\\begin{align} \\hat\\theta = \\arg\\min_{\\theta\\in \\mathbb{R}^d} \\sum_{z\\in \\mathcal{C}} \\left( \\langle \\theta,\\varphi(z) \\rangle - \\hat R_m(z)\\right)^2\\,, \\label{eq:lse} \\end{align}\\] we expect $\\Phi \\hat\\theta$ to be a good approximation to $q^\\pi$. Or at least, we can expect this hold at the points of $\\mathcal{C}$, where we are taking our measurements. The question is what happens outside of $\\mathcal{C}$: That is, what guarantees can we get for extrapolating to points of $\\mathcal{Z}:= \\mathcal{S}\\times \\mathcal{A}$. The first thing to observe that unless we are choosing $\\mathcal{C}$ carefully, there is no guarantee about the extrapolation error will be kept under control. In fact, if the choice of $\\mathcal{C}$ is so unfortunate that all the feature vectors for points in $\\mathcal{C}$ are identical, the least-squares problem will have many solutions. Our next lemma gives an explicit error bound on the extrapolation error. For the coming results we slightly generalize least-squares by introducing a weighting of the various errors in \\(\\eqref{eq:lse}\\). For this, let $\\varrho: \\mathcal{C} \\to (0,\\infty)$ be a weighting function assigning a positive weight to the various error terms and let . \\[\\begin{align} \\hat\\theta = \\arg\\min_{\\theta\\in \\mathbb{R}^d} \\sum_{z\\in \\mathcal{C}} \\varrho(z) \\left( \\langle \\theta,\\varphi(z) \\rangle - \\hat R_m(z)\\right)^2 \\label{eq:wlse} \\end{align}\\] be the minimizer of the resulting weighted squared-loss. A simple calculation gives that provided the (weighted) moment matrix . \\[\\begin{align} G_\\varrho = \\sum_{z\\in \\mathcal{C}} \\varrho(z) \\varphi(z) \\varphi(z)^\\top \\label{eq:mommx} \\end{align}\\] is nonsingular, the solution to the above weighted least-squares problem is unique and is equal to . \\[\\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\hat R_m(z') \\varphi(z')\\,,\\] From this expression we see that there is no loss of generality in assuming that the weights in the weighting function sum to one: \\(\\sum_{z\\in \\mathcal{C}} \\varrho(z) = 1\\). We will denote this by writing $\\varrho \\in \\Delta_1(\\mathcal{C})$ (here, $\\Delta_1$ refers to the fact that we can see $\\varrho$ as an element of a $|\\mathcal{C}|-1$ simplex). To state the lemma recall the notation that for a positive definite, $d\\times d$ matrix $Q$ and vector $x\\in \\mathbb{R}^d$, . \\[\\|x\\|_Q^2 = x^\\top Q x\\,.\\] ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec8/#approximate-policy-evaluation-done-well",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec8/#approximate-policy-evaluation-done-well"
  },"139": {
    "doc": "8. Approximate Policy Iteration",
    "title": "",
    "content": "Lemma (extrapolation error control in least-squares): Fix any \\(\\theta \\in \\mathbb{R}^d\\), \\(\\varepsilon: \\mathcal{Z} \\rightarrow \\mathbb{R}\\), $\\mathcal{C}\\subset \\mathcal{Z}$ and \\(\\varrho\\in \\Delta_1(\\mathcal{C})\\) such that the moment matrix $G_\\varrho$ is nonsingular. Define . \\[\\begin{align*} \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big(\\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z')\\,. \\end{align*}\\] Then, for any \\(z\\in \\mathcal{Z}\\) we have . \\[\\left| \\varphi(z)^\\top \\hat{\\theta} - \\varphi(z)^\\top \\theta \\right| \\leq \\| \\varphi(z) \\|_{G_{\\varrho}^{-1}}\\, \\max_{z' \\in C} \\left| \\varepsilon(z') \\right|\\,.\\] . Before the proof note that what his lemma tells us is that as long as we guarantee that the moment matrix is full rank, the extrapolation errors relative to predicting with some $\\theta\\in \\mathbb{R}^d$ can be controlled by controlling . | the value of \\(g(\\varrho):= \\max_{z\\in \\mathcal{Z}} \\| \\varphi(z) \\|_{G_{\\varrho}^{-1}}\\); and | the maximum deviation of the targets used in the weighted least-squares problem and the predictions with $\\theta$. | . Proof: First, we relate $\\hat\\theta$ to $\\theta$: . \\[\\begin{align*} \\hat{\\theta} &amp;= G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big(\\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z') \\\\ &amp;= G_\\varrho^{-1} \\left( \\sum_{z' \\in C} \\varrho(z') \\varphi(z') \\varphi(z')^\\top \\right) \\theta + G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\varepsilon(z') \\varphi(z') \\\\ &amp;= \\theta + G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\varepsilon(z') \\varphi(z'). \\end{align*}\\] Then for a fixed \\(z \\in \\mathcal{Z}\\), . \\[\\begin{align*} \\left| \\varphi(z)^\\top \\hat{\\theta} - \\varphi(z)^\\top \\theta \\right| &amp;= \\left| \\sum_{z' \\in C} \\varrho(z') \\varepsilon(z') \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') \\right| \\\\ &amp;\\leq \\sum_{z' \\in C} \\varrho(z') | \\varepsilon(z') | \\cdot | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') | \\\\ &amp;\\leq \\Big( \\max_{z' \\in C} |\\varepsilon(z')| \\Big) \\sum_{z' \\in C} \\varrho(z') | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') |\\,. \\end{align*}\\] To get a sense of how to control the sum notice that if $\\varphi(z)$ in the last sum was somehow replaced by $\\varphi(z’)$, using the definition of $G_\\varrho$ could greatly simplify the last expression. To get here, one may further notice that having the term in absolute value squared would help. Now, to get the squares, recall Jensen’s inequality, which states that for any convex function \\(f\\) and probability distribution \\(\\mu\\), \\(f \\left(\\int u \\mu(du) \\right) \\leq \\int f(u) \\mu(du)\\). Of course, this also works when $\\mu$ is a finitely supported, which is the case here. Thus, applying Jensen’s inequality with \\(f(x) = x^2\\), we thus get . \\[\\begin{align*} \\left(\\sum_{z' \\in C} \\varrho(z') | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') |\\right)^2 &amp; \\le \\sum_{z' \\in C} \\varrho(z') | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') |^2 \\\\ &amp;= \\sum_{z' \\in C} \\varrho(z') \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') \\varphi(z')^\\top G_\\varrho^{-1} \\varphi(z) \\\\ &amp;= \\varphi(z)^\\top G_\\varrho^{-1} \\left( \\sum_{z' \\in C} \\varrho(z') \\varphi(z') \\varphi(z')^\\top \\right) G_\\varrho^{-1} \\varphi(z) \\\\ &amp;= \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z) = \\|\\varphi(z)\\|_{G_\\varrho^{-1}}^2 \\end{align*}\\] Plugging this back into the previous inequality gives the desired result. \\(\\qquad \\blacksquare\\) . It remains to be seen of whether \\(g(\\varrho)=\\max_z \\|\\varphi(z)\\|_{G_\\varrho^{-1}}\\) can be kept under control. This is the subject of a classic result of Kiefer and Wolfowitz: . Theorem (Kiefer-Wolfowitz): Let $\\mathcal{Z}$ be finite. Let $\\varphi: \\mathcal{Z} \\to \\mathbb{R}^d$ be such that the underlying feature matrix $\\Phi$ is rank $d$. There exists a set \\(\\mathcal{C} \\subseteq \\mathcal{Z}\\) and a distribution \\(\\varrho: C \\rightarrow [0, 1]\\) over this set, i.e. \\(\\sum_{z' \\in \\mathcal{C}} \\varrho(z') = 1\\), such that . | \\(\\vert \\mathcal{C} \\vert \\leq d(d+1)/2\\); | \\(\\sup_{z \\in \\mathcal{Z}} \\|\\varphi(z)\\|_{G_\\varrho^{-1}} \\leq \\sqrt{d}\\); | In the previous line, the inequality is achieved with equality and the value of $\\sqrt{d}$ is best possible under all possible choices of $\\mathcal{C}$ and $\\rho$. | . We will not give a proof of the theorem, but we give references at the end where the reader can look up the proof. When $\\varphi$ is not full rank (i.e., $\\Phi$ is not rank $d$), one may reduce the dimensionality (and the cardinality of $C$ reduces accordingly). The problem of choosing $\\mathcal{C}$ and $\\rho$ such that $g(\\rho)$ is minimized is called the $G$-optimal design problem in statistics. This is a specific instance of optimal experimental design. Combining the Kiefer-Wolfowitz theorem with the previous lemma shows that least-squares amplifies the “measurement errors” by at most a factor of \\(\\sqrt{d}\\): . Corollary (extrapolation error control in least-squares via optimal design): Fix any $\\varphi:\\mathcal{Z} \\to \\mathbb{R}^d$ full rank. Then, there exists a set $\\mathcal{C} \\subset \\mathcal{Z}$ with at most $d(d+1)/2$ elements and a weighting function \\(\\varrho\\in \\Delta_1(\\mathcal{C})\\) such that for any \\(\\theta \\in \\mathbb{R}^d\\) and any \\(\\varepsilon: \\mathcal{C} \\rightarrow \\mathbb{R}\\), . \\[\\max_{z\\in \\mathcal{Z}}\\left| \\varphi(z)^\\top \\hat{\\theta} - \\varphi(z)^\\top \\theta \\right| \\leq \\sqrt{d}\\, \\max_{z' \\in C} \\left| \\varepsilon(z') \\right|\\,.\\] where $\\hat\\theta$ is given by . \\[\\begin{align*} \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big(\\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z')\\,. \\end{align*}\\] . Importantly, note that $\\mathcal{C}$ and $\\varrho$ are chosen independently of $\\theta$ and $\\epsilon$, that is, they are independent of the target. This suggests that in approximate policy evaluation, one should choose $(\\mathcal{C},\\rho)$ as in the Kiefer-Wolfowitz theorem and use the $\\rho$ weighted moment matrix. This leads to \\(\\begin{align} \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\hat R_m(z') \\varphi(z')\\,. \\label{eq:lspeg} \\end{align}\\) where $\\hat R_m(z)$ is defined by Eq. \\(\\eqref{eq:petargetsbiased}\\) and $G_\\varrho$ is defined by Eq. \\(\\eqref{eq:mommx}\\). We call this procedure least-square policy evaluation based on rollouts from $G$-optimal design points, or LSPE-$G$, for short. Note that we stick to the truncated rollouts, because this allows a simpler probabilistic analysis. That this properly controls the extrapolation error is as attested by the next result: . Lemma (LSPE-$G$ extrapolation error control): Fix any full-rank feature-map $\\varphi:\\mathcal{Z} \\to \\mathbb{R}^d$ and take the set $\\mathcal{C} \\subset \\mathcal{Z}$ and the weighting function \\(\\varrho\\in \\Delta_1(\\mathcal{C})\\) as in the Kiefer-Wolfowitz theorem. Fix an arbitrary policy $\\pi$ and let $\\theta$ and $\\varepsilon_\\pi$ such that $q^\\pi = \\Phi \\theta + \\varepsilon_\\pi$ and assume that immediate rewards belong to the interval $[0,1]$. Let $\\hat{\\theta}$ be as in Eq. \\eqref{eq:lspeg}. Then, for any $0\\le \\delta \\le 1$, with probability $1-\\delta$, . \\[\\begin{align} \\left\\| q^\\pi - \\Phi \\hat{\\theta} \\right\\|_\\infty &amp;\\leq \\|\\varepsilon_\\pi\\|_\\infty (1 + \\sqrt{d}) + \\sqrt{d} \\left(\\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}}\\right). \\label{eq:lspeee} \\end{align}\\] . Notice that that from the Kiefer-Wolfowitz theorem, \\(\\vert C \\vert = O(d^2)\\) and therefore nothing in the above expression depends on the size of the state space. Now, say we want to make the above error bound at most \\(\\|\\varepsilon_\\pi\\|_\\infty (1 + \\sqrt{d}) + 2\\varepsilon\\) with some value of $\\varepsilon&gt;0$. From the above we see that it suffices to choose $H$ and $m$ so that . \\[\\begin{align*} \\frac{\\gamma^H}{1 - \\gamma} \\leq \\varepsilon/\\sqrt{d} \\qquad \\text{and} \\qquad \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}} \\leq \\varepsilon/\\sqrt{d}. \\end{align*}\\] This, together with \\(\\vert\\mathcal{C}\\vert\\le d(d+1)/2\\) gives . \\[\\begin{align*} H \\geq H_{\\gamma, \\varepsilon/\\sqrt{d}} \\qquad \\text{and} \\qquad m \\geq \\frac{d}{(1 - \\gamma)^2 \\varepsilon^2} \\, \\log \\frac{d(d+1)}{\\delta}\\,. \\end{align*}\\] Proof: In a nutshell, we use the previous corollary, together with Hoeffding’s inequality and using that $|q^\\pi-T_\\pi^H \\boldsymbol{0}|_\\infty \\le \\gamma^H/(1-\\gamma)$, which follows since the rewards are bounded in $[0,1]$. Click here for the full proof. Fix $z\\in \\mathcal{C}$. Let us write $\\hat{R}_m(z) = q^\\pi(z) + \\hat{R}_m(z) - q^\\pi(z) = \\varphi(z)^\\top \\theta + \\varepsilon(z)$ where we define $\\varepsilon(z) = \\hat{R}_m(z) - q^\\pi(z) + \\varepsilon_\\pi(z)$. Then $$ \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big( \\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z'). $$ Now we will bound the difference between our action-value function estimate and the true action-value function: $$ \\begin{align} \\| q^\\pi - \\Phi \\hat\\theta \\|_\\infty &amp; \\le \\| \\Phi \\theta - \\Phi \\hat\\theta\\|_\\infty + \\| \\varepsilon_\\pi \\|_\\infty \\le \\sqrt{d}\\, \\max_{z\\in \\mathcal{C}} |\\varepsilon(z)|\\, + \\| \\varepsilon_\\pi \\|_\\infty \\label{eq:bound_q_values} \\end{align} $$ where the last line follows from the Corollary above. For bounding the first term above, first note that $\\mathbb{E} \\left[ \\hat{R}_m(z) \\right] = (T_\\pi^H \\mathbf{0})(z)$. Then, $$ \\begin{align*} \\varepsilon(z) &amp;= \\hat{R}_m(z) - q^\\pi(z) + \\varepsilon_\\pi(z) \\nonumber \\\\ &amp;= \\underbrace{\\hat{R}_m(z) - (T_\\pi^H \\mathbf{0})(z)}_{\\text{sampling error}} + \\underbrace{(T_\\pi^H \\mathbf{0})(z) - q^\\pi(z)}_{\\text{truncation error}} + \\underbrace{\\varepsilon_\\pi(z)}_{\\text{fn. approx. error}}. \\end{align*} $$ Since the rewards are assumed to belong to the unit interval, the truncation error is at most $\\frac{\\gamma^H}{1 - \\gamma}$. Concerning the sampling error (first term), Hoeffding's inequality gives that for any given $z\\in \\mathcal{C}$, $ \\left \\vert \\hat{R}_m(z) - (T_\\pi^H \\mathbf{0})(z) \\right \\vert \\leq \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 / \\delta)}{2m}}$ with at least $1 - \\delta$ probability. Applying a union bound, we get that with probability at least $1 - \\delta$, for all $z \\in \\mathcal{C}$, $ \\left \\vert \\hat{R}_m(z) - (T_\\pi^H \\mathbf{0})(z) \\right \\vert \\leq \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}}$. Putting things together, we get that with probability at least $1 - \\delta$, $$ \\begin{equation} \\max_{z \\in \\mathcal{C}} | \\varepsilon(z) | \\leq \\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}} + \\|\\varepsilon_\\pi\\|_\\infty\\,. \\label{eq:bound_varepsilon_z} \\end{equation} $$ Plugging this into Eq. \\eqref{eq:bound_q_values} and algebra gives the desired result. \\(\\blacksquare\\) . In summary, what we have shown so far is that if the features can approximate well the action-value function of a policy, then there is a simple procedure (Monte-Carlo rollouts and least-squares estimation based on an optimal experimental design) to produce an reliable estimate of the action-value function of the policy. The question remains whether if we use these estimates in policy iteration, the whole procedure will still give good policies after a sufficiently large number of iterations. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec8/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec8/"
  },"140": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Progress Lemma with Approximation Errors",
    "content": "Here we give a refinement of the geometric progress lemma of policy iteration that allows for “approximate” policy improvement steps. This previous lemma stated that the value function of the improved policy $\\pi’$ is at least as large as the Bellman operator applied to the value function of the policy $\\pi$ to be improved. Our new lemma is as follows: . Lemma (Geometric progress lemma with approximate policy improvement): Consider a memoryless policy \\(\\pi\\) and its corresponding value function \\(v^\\pi\\). Let \\(\\pi'\\) be any policy and define $\\varepsilon:\\mathcal{S} \\to \\mathbb{R}$ via . \\[T v^\\pi = T_{\\pi'} v^{\\pi} + \\varepsilon\\,.\\] Then, . \\[\\|v^* - v^{\\pi'}\\|_\\infty \\leq \\gamma \\|v^* - v^{\\pi}\\|_\\infty + \\frac{1}{1 - \\gamma} \\, \\|\\varepsilon\\|_\\infty.\\] . Proof: First note that for the optimal policy \\(\\pi^*\\), \\(T_{\\pi^*} v^* = v^*\\). We have . \\[\\begin{align} v^* - v^{\\pi'} &amp; = T_{\\pi^*}v^* - T_{\\pi^*} v^{\\pi} + \\overbrace{T_{\\pi^*} v^\\pi}^{\\le T v^\\pi} - T_{\\pi'} v^\\pi + T_{\\pi'} v^{\\pi} - T_{\\pi'} v^{\\pi'} \\nonumber \\\\ &amp;\\le \\gamma P_{\\pi^*} (v^*-v^\\pi) + \\varepsilon + \\gamma P_{\\pi'} (v^\\pi-v^{\\pi'})\\,. \\label{eq:vstar_vpiprime} \\end{align}\\] Using the value difference identity and that $v_\\pi =T_\\pi v^\\pi\\le T v^\\pi$, we calculate . \\[\\begin{align*} v^\\pi - v^{\\pi'} = (I-\\gamma P_{\\pi'})^{-1} [ v^\\pi - T_{\\pi'}v^\\pi] \\le (I-\\gamma P_{\\pi'})^{-1} [ T v^\\pi - (T v^\\pi -\\varepsilon) ] = (I-\\gamma P_{\\pi'})^{-1} \\varepsilon\\,, \\end{align*}\\] where the inequality follows because $(I-\\gamma P_{\\pi’})^{-1}= \\sum_{k\\ge 0} (\\gamma P_{\\pi’})^k$, the sum of positive linear operators, is a positive linear operator itself and hence is also monotone. Plugging the inequality obtained into \\eqref{eq:vstar_vpiprime} gives . \\[\\begin{align*} v^* - v^{\\pi'} \\le \\gamma P_{\\pi^*} (v^*-v^\\pi) + (I-\\gamma P_{\\pi'})^{-1} \\varepsilon. \\end{align*}\\] Taking the maximum norm of both sides and using the triangle inequality and that \\(\\| (I-\\gamma P_{\\pi'})^{-1} \\|_\\infty \\le 1/(1-\\gamma)\\) gives the desired result. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec8/#progress-lemma-with-approximation-errors",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec8/#progress-lemma-with-approximation-errors"
  },"141": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Approximate Policy Iteration",
    "content": "Notice that the progress lemma makes no assumptions about the origin of the errors. This motivates considering a generic version of approximate policy iteration where for $k\\ge 1$ in the $k$th update set, the new policy $\\pi_k$ is approximately greedy with respect to $v^{\\pi_k}$ in that sense that . \\[\\begin{align} T v^{\\pi_k} = T_{\\pi_{k+1}} v^{\\pi_k} + \\varepsilon_k\\,. \\label{eq:apidef} \\end{align}\\] The progress lemma implies that the resulting sequence of policies will have value functions that converge to a neighborhood of $v^*$ where the size of the neighborhood is governed by the magnitude of the error terms \\((\\varepsilon_k)_k\\). ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec8/#approximate-policy-iteration",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec8/#approximate-policy-iteration"
  },"142": {
    "doc": "8. Approximate Policy Iteration",
    "title": "",
    "content": "Theorem (Approximate Policy Iteration): Let \\((\\pi_k)_{k\\ge 0}\\), \\((\\varepsilon_k)_k\\) be such that \\eqref{eq:apidef} holds for all \\(k\\ge 0\\). Then, for any \\(k\\ge 1\\), . \\[\\begin{align} \\|v^* - v^{\\pi_k}\\|_\\infty \\leq \\frac{\\gamma^k}{1-\\gamma} + \\frac{1}{(1-\\gamma)^2} \\max_{0\\le s \\le k-1} \\|\\varepsilon_{s}\\|_\\infty\\,. \\label{eq:apieb} \\end{align}\\] . Proof: Left as an exercise. \\(\\qquad \\blacksquare\\) . Consider now a version of approximate policy iteration where the sequence of policies \\((\\pi_k)_{k\\ge 0}\\) is defined as follows: . \\[\\begin{align} q_k = q^{\\pi_k} + \\varepsilon_k', \\qquad M_{\\pi_{k+1}} q_k = M q_k\\,, \\quad k=0,1,\\dots\\,. \\label{eq:apiavf} \\end{align}\\] That is, for each \\(k=0,1,\\dots\\), \\(\\pi_k\\) is greedy with respect to \\(q_{k-1}\\). ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec8/#-1",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec8/#-1"
  },"143": {
    "doc": "8. Approximate Policy Iteration",
    "title": "",
    "content": "Corollary (Approximate Policy Iteration with Approximate Action-value Functions): The sequence defined in \\eqref{eq:apiavf} is such that . \\[\\| v^* - v^{\\pi_k} \\|_\\infty \\leq \\frac{\\gamma^k}{1-\\gamma} + \\frac{2}{(1-\\gamma)^2} \\max_{0\\le s \\le k-1} \\|\\varepsilon_{s}'\\|_\\infty\\,.\\] . Proof: To simplify the notation consider policies \\(\\pi,\\pi'\\) and functions \\(q,\\varepsilon'\\) over the state-action space such that \\(M_{\\pi'} q = M q\\) and \\(q=q^\\pi+\\varepsilon'\\). We have . \\[\\begin{align*} T v^\\pi &amp; \\ge T_{\\pi'} v^\\pi = M_{\\pi'} (r+\\gamma P v^\\pi) = M_{\\pi'} q^\\pi = M_{\\pi'} q - M_{\\pi} \\varepsilon' = M q - M_\\pi \\varepsilon'\\\\ &amp; \\ge M (q^\\pi - \\|\\varepsilon'\\|_\\infty \\boldsymbol{1}) - M_\\pi \\varepsilon' \\ge M q^\\pi - 2 \\|\\varepsilon'\\|_\\infty \\boldsymbol{1} = T v^\\pi - 2 \\|\\varepsilon'\\|_\\infty \\boldsymbol{1}\\,, \\end{align*}\\] where we used that \\(M_\\pi\\) is linear, monotone, and that $M$ is monotone, and both are nonexpansions in the maximum norm. Hence, if $\\varepsilon_k$ is defined by \\eqref{eq:apidef} then \\(\\|\\varepsilon_k\\|_\\infty \\le 2 \\|\\varepsilon_k'\\|_\\infty\\) and the result follows from the previous theorem. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec8/#-2",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec8/#-2"
  },"144": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Global planning with least-squares policy iteration",
    "content": "Putting things together gives the following planning method: . | Given the feature map $\\varphi$, find \\(\\mathcal{C}\\) and \\(\\rho\\) as in the Kiefer-Wolfowitz theorem | Let \\(\\theta_{-1}=0\\) | For \\(k=0,1,2,\\dots,K-1\\) do | \\(\\qquad\\) Roll out with policy \\(\\pi:=\\pi_k\\) for $H$ steps to get the targets \\(\\hat R_m(z)\\) where \\(z\\in \\mathcal{C}\\) \\(\\qquad\\) and \\(\\pi_k(s) = \\arg\\max_a \\langle \\theta_{k-1}, \\varphi(s,a) \\rangle\\) | \\(\\qquad\\) Solve the weighted least-squares problem given by Eq. \\(\\eqref{eq:wlse}\\) to get \\(\\theta_k\\). | Return \\(\\theta_{K-1}\\) | . We call this method least-squares policy iteration (LSPI) for obvious reasons. Note that this is a global planning method: The method makes no use of an input state and the parameter vector returned can be used to get the policy $\\pi_{K}$ (as in the method above). Theorem (LSPI performance): Fix an arbitrary full rank feature-map $\\varphi: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}^d$ and let $K,m,H\\ge 1$. Assume that B2\\(_{\\varepsilon}\\) holds. Then, for any $0\\le \\zeta \\le 1$, with probability at least $1-\\zeta$, the policy $\\pi_{K}$ which is greedy with respect to $\\Phi \\theta_{K-1}$ is $\\delta$-suboptimal with . \\[\\begin{align*} \\delta \\le \\underbrace{\\frac{2(1 + \\sqrt{d})}{(1-\\gamma)^2}\\, \\varepsilon}_{\\text{approx. error}} + \\underbrace{\\frac{\\gamma^{K-1}}{1-\\gamma}}_{\\text{iter. error}} + \\underbrace{\\frac{2\\sqrt{d}}{(1-\\gamma)^3} \\left(\\gamma^H + \\sqrt{\\frac{\\log( d(d+1)K / \\zeta)}{2m}}\\right)}_{\\text{pol.eval. error}} \\,. \\end{align*}\\] In particular, for any $\\varepsilon’&gt;0$, choosing $K,H,m$ so that . \\[\\begin{align*} K &amp; \\ge H_{\\gamma,\\gamma\\varepsilon'/2} \\\\ H &amp; \\ge H_{\\gamma,(1-\\gamma)^2\\varepsilon'/(8\\sqrt{d})} \\qquad \\text{and} \\\\ m &amp; \\ge \\frac{32 d}{(1-\\gamma)^6 (\\varepsilon')^2} \\log( (d+1)^2 K /\\zeta ) \\end{align*}\\] policy $\\pi_K$ is $\\delta$-optimal with . \\[\\begin{align*} \\delta \\le \\frac{2(1 + \\sqrt{d})}{(1-\\gamma)^2}\\, \\varepsilon + \\varepsilon'\\,, \\end{align*}\\] while the total computation cost is $\\text{poly}(\\frac{1}{1-\\gamma},d,\\mathrm{A},\\frac{1}{\\varepsilon’},\\log(1/\\zeta))$. Thus, with a polynomial cost, LSPI with the specific configuration at the cost of polynomial computation cost, but importantly, with a cost that is independent of the size of the state space, can result in a good policy as long as $\\varepsilon$, the worst-case error of approximating action-value functions of policies using the features provided, is sufficiently small. Proof: Note that B2\\(_\\varepsilon\\) and that $\\Phi$ is full rank implies that for any memoryless policy $\\pi$ there exists a parameter vector $\\theta\\in \\mathbb{R}^d$ such that \\(\\| \\Phi \\theta - q^\\pi \\|_\\infty \\le \\varepsilon\\) (cf. Part 2 of Question 3 of Assignment 2). Hence, we can use the “LSPE extrapolation error bound” (cf. \\(\\eqref{eq:lspeee}\\)). By this result, a union bound and of course by B2$_\\varepsilon$, we get that for any $0\\le \\zeta \\le 1$, with probability at least $1-\\zeta$, for any $0 \\le k \\le K-1$, . \\[\\begin{align*} \\| q^{\\pi_k} - \\Phi \\theta_k \\|_\\infty &amp;\\leq \\varepsilon (1 + \\sqrt{d}) + \\sqrt{d} \\left(\\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log( d(d+1)K / \\zeta)}{2m}}\\right)\\,, \\end{align*}\\] where we also used that \\(\\vert \\mathcal{C} \\vert \\le d(d+1)\\). Call the quantity on the right-hand side in the above inequality $\\kappa$. Take the event when the above inequalities hold and for now assume this event holds. By the previous theorem, $\\pi_K$ is $\\delta$-optimal with . \\[\\delta \\le \\frac{\\gamma^{K-1}}{1-\\gamma} + \\frac{2}{(1-\\gamma)^2} \\kappa \\,.\\] To obtain the second part of the result, we split $\\varepsilon’$ into two equal parts: $K$ is set to force the iteration error to be at most $\\varepsilon’/2$, while $H$ and $m$ are chosen to force the policy evaluation error to be at most $\\varepsilon’/2$. Here, to choose $H$ and $M$, $\\varepsilon’/2$ is again split into two equal parts. The details of this calculation are left to the reader. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec8/#global-planning-with-least-squares-policy-iteration",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec8/#global-planning-with-least-squares-policy-iteration"
  },"145": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Notes",
    "content": "Approximate Dynamic Programming (ADP) . Value iteration and policy iteration are specific instances of dynamic programming methods. In general, dynamic programming refers to methods that use value functions to calculate good policies. In approximate dynamic programming the methods are modified by introducing “errors” when calculating the values. The idea is that the origin of the errors does not matter (e.g., whether they come due to imperfect function approximation, linear, or nonlinear, or due to the sampling): The analysis is done in a general form. While here we met approximate policy iteration, one can also use the same ideas as shown here to study an approximate version of value iteration. A homework in problem set 2 asks you to study this method, which is usualy called approximate value iteration. In an earlier homework you were asked to study how linear programming can also be used to compute optimal value functions. Adding approximations we then get approximate linear programming. What function approximation technique to use? . We note in passing that fans of neural networks should like that the general, ADP-style results, like the theorem in the middle of this lecture, can be also applied to the case when neural networks are used as the function approximation technique. However, one main lesson of the lecture is that to control extrapolation errors, one should be quite careful in how the training data is chosen. For linear prediction and least-squares fitting, optimal design gives a complete answer, but the analog questions are completely open in the case of nonlinear function approximation, such as neural networks. There is also a sizable literature that connects nonparametric techniques (an analysis friendly relative of neural networks) to ADP methods. Concentrability coefficients and all that jazz . The idea of introducing approximate calculations has been introduced at the same time people got interested in Markov Decision Processes in the 1960s. Hence, the literature is quite enormous. However, the approach taken here which asks for error bounds where the algorithmic (not approximation-) error is uniformly controlled regardless of the MDP is quite recent and where the term that involves the approximation error is also uniformly bounded (for a fixed dimension and discount factor). Earlier literature often presented bounds where the magnification factor of the approximation and the algorithmic error involved terms which depended on the MDP. Often these came in the form of “concentrability coefficients” (and yours truly was quite busy with working on these results a while ago). The main conclusion of this earlier analysis is that more stochasticity in the transitions means less control, less concentrability, which is advantageous for the ADP algorithms. While this makes sense and this indicates that these earlier results are complementary to the results presented here, the issue is that these results are quite pessimistic for example when the MDP is deterministic (as in this case the concentrability coefficients can be as large as the size of the state space). While here we emphasized the importance of using a good design to control the extrapolation errors, in these earlier results, no optimal design was used. The upshot is that this saves the effort of coming up with a good design, but the obvious downside is that the extrapolation error may become uncontrolled. In the batch setting (which we will come back to later), of course, there is no way to control the sample collection, and this is in fact the setting where this earlier analysis was done. The strength of hints . A critical assumption in the analysis of API was that the approximation error is controlled uniformly for all policies. This feels limiting. Yet, there are some interesting sufficient conditions when this assumption is clearly satisfied. In general, these require that the transition dynamics and the reward are both “compressible”. For example, if the MDP is such that $r$, the immediate reward as a function of the state-action pairs satisfies \\(r = \\Phi \\theta_r\\) and the transition matrix, \\(P\\in [0,1]^{\\mathrm{S}\\mathrm{A} \\times \\mathrm{S}}\\) satisfies \\(P = \\Phi H\\) with some matrix \\(H\\in \\mathbb{R}^{d\\times \\mathrm{S}}\\), then for any policy policy \\(\\pi\\), \\(T_\\pi q = r+ \\gamma P M_\\pi q\\) has a range which is a subset of \\(\\text{span}(\\Phi)=\\mathcal{F}_{\\varphi}\\). Since \\(q^\\pi\\) is the fixed-point of \\(T_\\pi\\), i.e., \\(q^\\pi = T_\\pi q^\\pi\\), it follows that \\(q^\\pi\\) is also necessarily in the range space of \\(T_\\pi\\). As such, \\(q^\\pi \\in \\mathcal{F}_{\\varphi}\\) and \\(\\varepsilon_{\\text{apx}}=0\\). MDPs that satisfy the above two constraints are called linear in \\(\\Phi\\) (or sometimes, just “linear MDPs”). Exact linearity can be relaxed: If \\(r = \\Phi \\theta_r + \\varepsilon_r\\) and \\(P = \\Phi H +E\\), then for any policy \\(\\pi\\), \\(q^\\pi\\in_{\\varepsilon} \\mathcal{F}_{\\varphi}\\) with \\(\\varepsilon \\le \\|\\varepsilon_r\\|_\\infty+\\frac{\\gamma}{1-\\gamma}\\|E\\|_\\infty\\). Nevertheless, later we will investigate whether this assumption can be relaxed. The tightness of the bounds . It is not known whether the bound presented in the final result is tight. In fact, the dependence of $m$ on the $1/(1-\\gamma)$ is almost certainly not tight; in similar scenarios it has been shown in the past that replacing Hoeffding’s inequality with Bernstein’s inequality allows the reduction of this factor. It is more interesting whether the amplification factor of the approximation error, $\\sqrt{d}/(1-\\gamma)^2$, is best possible. In the next lecture we will show that the $\\sqrt{d}$ approximation error amplification factor cannot be removed while keeping the runtime under control. In a later lecture, we will show that the dependence on $1/(1-\\gamma)$ cannot be improved either – at least for this algorithm. However, we will see that if the main concern is the amplification of the approximation error, while keeping the runtime polynomial (perhaps with a higher order though) then under B2\\(_{\\varepsilon}\\) better algorithms exist. The cost of optimal experimental design . The careful reader would not miss that to run the proposed method one needs to find the set $\\mathcal{C}$ and the weighting function $\\rho$. The first observation here is that it is not crucial to find the best possible $(\\mathcal{C},\\rho)$ pair. The Kiefer-Wolfowitz theorem showed that with this best possible choice, $g(\\rho) = \\sqrt{d}$. However, if one finds a pair such that $g(\\rho)=2\\sqrt{d}$, the price of this is that wherever $\\sqrt{d}$ appears in the final performance bound, a submultiplicative factor of $2$ will also need to be introduced. This should be acceptable. In relation to this note that by relaxing this optimality requirement, the cardinality of $\\mathcal{C}$ can be reduced. For example, by introducing the factor of $2$ as suggested above allows one to reduce the cardinlity to $O(d \\log \\log d)$; which may actually be a good tradeoff as this can save much on the runtime. However, the question still remains of who computes these (approximately) optimal designs and at what cost. While this calculation only needs to be done once and is independent of the MDP (just depends on the feature map), the value of these methods remains unclear because of this compute cost. General methods to compute approximately optimal designs needed here are known, but their runtime for our case will be proportional to the number of state-action pairs. In the very rare cases when simulating transitions is very costly but the number of state-action pairs is not too high, this may be a viable option. However, these cases are rare. For special choices of the feature-map, optimal designs may be known. However, this reduces the general applicability of the method presented here. Thus, a major question is whether the optimal experimental design can be avoided. What is known is that for linear prediction with least-squares, clearly, they cannot be avoided. One suspects that this is true more generally. Can optimal designs be avoided while keeping the results essentially unchanged? Of particular interest would be if the feature-map would also be only “locally explored” as the planner interacts with the simulator. Altogether, one suspects that two factors contributed here for the appearance of optimal experimental design: One factor is that the planner is global: It comes up with a parameter vector that leads to a policy that can be used regardless of the state. The other (perhaps) factor is that the approach was based on simple “patching up” a dynamic programming algorithm with a function approximator. While this is a common approach, controlling the extrapolation errors in this approach is critical and is likely only possible with something like an optimal experimental design. As we shall see soon, there are indeed approaches that avoid the optimal experimental design step and which are based on online planning and they also deviate from the ADP approach. Policy evaluation alternatives . The policy evaluation method presented here feels unsophisticated. It uses simple Monte-Carlo rollouts, with truncation, averaging and least-squares regression. The reinforcement learning literature offers many alternatives, such as the “temporal difference” learning type methods that are based on solving the fixed point equation $q^\\pi = T_\\pi q^\\pi$. One can indeed try to use this equation to avoid the crude Monte-Carlo approach presented here, in the hope of reducing the variance (which is currently rather crudely upper bounded using the $1/(1-\\gamma)$ term in the Hoeffding bound). Rewriting the fixed point as $(I-\\gamma P_\\pi) q^\\pi = r$, and then plugging in $q^\\phi = \\Phi \\theta + \\varepsilon$, we see that the trouble is that to control the extrapolation errors, the optimal design must likely depend on the policy to be evaluated (because of the appearance of $(I-\\gamma P_\\pi)\\Phi$). Alternative error control: Bellman residuals . Let \\((\\pi_k)_{k\\ge 0}\\) and \\((q_k,\\varepsilon_k)_{k\\ge 0}\\) be so that . \\[\\varepsilon_k = q_k - T_{\\pi_k} q_k\\] Here, \\(\\varepsilon_k\\) is called the “Bellman residual” of \\(q_k\\). The policy evaluation alternatives above aim at controlling these residuals. The reader is invited to derive the analogue of the “approximate policy iteration” error bound in \\eqref{eq:apieb} for this scenario. The role of $\\rho$ in the Kiefer-Wolfowitz result . One may wonder about how critical is the presence of $\\rho$ in the results presented. For this, we can say that it is not critical. Unweighted least-squares does not perform much worse. Least-squares error bound . The error bound presented for least-squares does not use the full power of randomness. When part of the errors $\\varepsilon(z)$ with $z\\in \\mathcal{C}$ are random, some helpful averaging effects can appear, which we ignored for now, but which could be used in a more refined analysis. Optimal experimental design – a field on its own . Optimal exoerimental design is a subfield of statistics. The design considered here is just one possibility. In fact, this design which is called G-optimal design (G stands, uninspiringly, for the word “general”). The Kiefer-Wolfowitz theorem actually also states that this is equivalent to the D-optimal designs. Lack of convergence . The results presented show convergence to a ball around the optimal target. Some people think this is a major concern. While having a convergent method may look more appealing, as long as one controls the size of the ball, I will not be too concerned. Approximate value iteration (AVI) . Similarly to what is done here, one can introduce an approximate version of value-iteration. This is the subject of Question 3 of homework 2. While the conditions are different, the qualitative behavior of AVI is similar to that of approximate policy iteration. In particular, as for approximate policy iteration, there are two steps to this proof: One is to show that the residuals $\\varepsilon_k = q_k - T q_{k-1}$ can be controlled and the second is that if they are controlled then the policy that is greedy with respect to (say) $q_K$ is $\\delta$-optimal with $\\delta$ controlled by \\(\\varepsilon_{1:K}:=\\max_{1\\le k \\le K} \\| \\varepsilon_k \\|_\\infty\\). For this second part, we have the following bound: . \\[\\begin{align} \\delta \\le 2 H^2 (\\gamma^K + \\varepsilon_{1:K})\\,. \\label{eq:lsvibound} \\end{align}\\] where $H=1/(1-\\gamma)$. The procedure that uses least-squares fitting to get the iterates $(q_k)_k$ is known under various names, such as least-squares value iteration (LSVI), fitted Q-iteration (FQI), least-squares Q iteration (LSQI). This proliferation of abbreviations and names is unfortunate, but there is not much that can be done at this stage. To add insult to injury, when neural networks are used to represent the iterates and an incremental stochastic gradient descent algorithm is used for “fitting” the weights of these networks by resampling old data from a “replay buffer”, the resulting procedure is coined “Deep Q-Networks” (training), or DQN for short. Bounds on the parameter vector . The Kiefer-Wolfowitz theorem implies the following: . Proposition: Let $\\phi:\\mathcal{Z}\\to\\mathbb{R}^d$ and $\\theta\\in \\mathbb{R}^d$ be such that $\\sup_{z\\in \\mathcal{Z}}|\\langle \\phi(z),\\theta \\rangle|\\le 1$ and \\(\\sup_{z\\in \\mathcal{Z}} \\|\\phi(z)\\|_2 &lt;+\\infty\\). Then, there exist a matrix $S\\in \\mathbb{R}^{d\\times d}$ such that for $\\tilde \\phi$ . \\[\\begin{align*} \\tilde\\phi(z) &amp; = S\\phi(z)\\,, \\qquad z\\in \\mathcal{Z} \\end{align*}\\] there exists \\(\\tilde \\theta\\in \\mathbb{R}^d\\) such that the following hold: . | \\(\\langle \\phi(z),\\theta \\rangle = \\langle \\tilde \\phi(z),\\tilde \\theta \\rangle\\), \\(z\\in \\mathcal{Z}\\); | \\(\\sup_{z\\in \\mathcal{Z}} \\| \\tilde \\phi(z) \\|_2 \\le 1\\); | \\(\\|\\tilde \\theta \\|_2 \\le \\sqrt{d}\\). | . Proof: Let $\\rho:\\mathcal{Z} \\to [0,1]$ be the $G$-optimal design whose existence is guaranteed by the Kiefer-Wolfowitz theorem. Let \\(M = \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\phi(z)\\phi(z)^\\top\\) be the underlying moment matrix. Then, by the definition of $\\rho$, \\(\\sup_{z\\in \\mathcal{Z}}\\|\\phi(z)\\|_{M^{-1}}^2 \\le d\\). Define \\(S= (dM)^{-1/2}\\) and \\(\\tilde \\theta = S^{-1} \\theta\\). The first property is clearly satisfied. As to the second property, . \\[\\|\\tilde \\phi(z)\\|_2^2 = \\| (dM)^{-1/2}\\phi(z)\\|_2^2 = \\phi(z)^\\top (dM)^{-1} \\phi(z) \\le 1\\,.\\] Finally, for the third property, . \\[\\| \\tilde \\theta \\|_2^2 = d \\theta^\\top \\left( \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\phi(z) \\phi(z)^\\top \\right) \\theta = d \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\underbrace{(\\theta^\\top \\phi(z))^2}_{\\le 1} \\le d\\,,\\] finishing the proof. \\(\\qquad \\blacksquare\\) . Thus, if one has access to the full feature-map then knowing that a function realized is bounded, one may as well assume that the feature map is bounded and the parameter vector is bounded just by $\\sqrt{d}$. Regularized least-squares . The linear least-squares predictor given by a feature-map $\\phi$ and data $(z_1,y_1),\\dots,(z_n,y_n)$ predicts a response at $z$ via $\\langle \\phi(z),\\hat\\theta \\rangle$ where . \\[\\begin{align} \\hat\\theta = G^{-1}\\sum_{i=1}^n \\phi_i y_i\\,, \\label{eq:ridgesol} \\end{align}\\] with . \\[G = \\sum_{i=1}^n \\phi_i \\phi_i^\\top\\,.\\] Here, by abusing notation for the sake of minimizing clutter, we use $\\phi_i=\\phi(z_i)$, $i=1,\\dots,n$. The problem is that $G$ may not be invertible (i.e., $\\hat \\theta$ may not be defined as written above). “By continuity”, it is nearly equally problematic when $G$ is ill-conditioned (i.e., its minimum eigenvalue is “much smaller” than its maximum eigenvalue). In fact, this leads to poor “generalization”. One remedy, often used, is to modify $G$ by shifting it with a small constant multiple of the identity matrix: . \\[G = \\lambda I + \\sum_{i=1}^n \\phi_i \\phi_i^\\top\\,.\\] Here, $\\lambda&gt;0$ is a tuning parameter, whose value is often chosen based on cross-validation or with a similar process. The modification guarantees that $G$ is invertible and it overall improves the quality of predictions, especially when $\\lambda$ is tuned base on data. Above, the choice of the identity matrix, while is common in the literature, is completely arbitrary. In particular, invertibility will be guaranteed if $I$ is replaced with any other positive definite matrix $P$. In fact, the matrix one should use here should be one that makes $|\\theta|_P^2$ small (while, say, keeping the minimum eigenvalue of $P$ at constant). That this is the choice that makes sense can be argued for by noting that with . \\[G = \\lambda P + \\sum_{i=1}^n \\phi_i \\phi_i^\\top\\,.\\] the $\\hat\\theta$ vector defined in \\eqref{eq:ridgesol} is the minimizer of . \\[L_n(\\theta) = \\sum_{i=1}^n ( \\langle \\phi_i,\\theta \\rangle - y_i)^2 \\,\\,+ \\lambda \\| \\theta\\|_P^2\\,,\\] and thus, the extra penalty has the least impact for the choice of $P$ that makes the norm of $\\theta$ the smallest. If we only know that $\\sup_{z} |\\langle \\phi(z),\\theta \\rangle|\\le 1$, by our previous note, a good choice is $P=d M$, where \\(M = \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\phi(z)\\phi(z)^\\top\\) where \\(\\rho\\) is a $G$-optimal design. Indeed, with this choice, \\(\\|\\theta\\|_P^2 = d \\|\\theta \\|_M^2 \\le d\\). Note also that if we apply the feature-standardization transformation of the previous note, we have . \\[(dM)^{-1/2} (\\sum_i \\phi_i \\phi_i^\\top + \\lambda d M ) (dM)^{-1/2} = \\sum_i \\tilde \\phi_i \\tilde \\phi_i^\\top + \\lambda I\\,,\\] showing that the choice of using the identity matrix is justified when the features are standardized as in the proposition of the previous note. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec8/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec8/#notes"
  },"146": {
    "doc": "8. Approximate Policy Iteration",
    "title": "References",
    "content": "We will only scratch the surface now; expect more references to be added later. The bulk of this lecture is based on . | Tor Lattimore, Csaba Szepesvári, and Gellért Weisz. 2020. “Learning with Good Feature Representations in Bandits and in RL with a Generative Model.” ICML and arXiv:1911.07676, | . who introduced the idea of using \\(G\\)-optimal designs for controlling the extrapolation errors. A very early reference on error bounds in “approximate dynamic programming” is the following: . | Whitt, Ward. 1979. “Approximations of Dynamic Programs, II.” Mathematics of Operations Research 4 (2): 179–85. | . The analysis of the generic form of approximate policy iteration is a refinement of Proposition 6.2 from the book of Bertsekas and Tsitsiklis: . | Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont, Massachusetts, 1996. | . However, there are some differences between the “API” theorem presented here and Proposition 6.2. In particular, the theorem presented here appears to capture all sources of errors in a general way, while Proposition 6.2 is concerned with value function approximation errors and errors introduced in the “greedification step”. The form adopted here appears, for example, in Theorem 1 of a technical report of Scherrer, who also gives earlier references: . | Scherrer, Bruno. 2013. “On the Performance Bounds of Some Policy Search Dynamic Programming Algorithms.” arxiv. | . The earliest of these references is perhaps . | Munos, R. 2003. “Error Bounds for Approximate Policy Iteration.” ICML. | . Least-squares policy iteration appears in . | Lagoudakis, M. G. and Parr, R. Least-squares policy iteration. The Journal of Machine Learning Re-search, 4:1107–1149, 2003. | . The particular form presented in this work though uses value function approximation based on minimizing the Bellman residuals (using the so-called LSTD method). Two books that advocate the ADP approach: . | Powell, Warren B. 2011. Approximate Dynamic Programming. Solving the Curses of Dimensionality. Hoboken, NJ, USA: John Wiley &amp; Sons, Inc. | Lewis, Frank L., and Derong Liu. 2013. Reinforcement Learning and Approximate Dynamic Programming for Feedback Control. Hoboken, NJ, USA: John Wiley &amp; Sons, Inc. | . And a chapter: . | Bertsekas, Dimitri P. 2009. “Chapter 6: Approximate Dynamic Programming,” January, 1–118. | . A paper that is concerned with API and least-squares methods, but uses concentrability is: . Antos, Andras, Csaba Szepesvári, and Rémi Munos. 2007. “Learning near-Optimal Policies with Bellman-Residual Minimization Based Fitted Policy Iteration and a Single Sample Path.” Machine Learning 71 (1): 89–129. Optimal experimental design has a large literature. A nice book concerned with computation is this: . | M. J. Todd. Minimum-volume ellipsoids: Theory and algorithms. SIAM, 2016. | . The Kiefer-Wolfowitz theorem is from: . | J. Kiefer and J. Wolfowitz. The equivalence of two extremum problems. Canadian Journal of Mathematics, 12(5):363–365, 1960. | . More on computation here: . | E. Hazan, Z. Karnin, and R. Meka. Volumetric spanners: an efficient exploration basis for learning. Journal of Machine Learning Research, 17(119):1–34, 2016 | M. Grötschel, L. Lovász, and A. Schrijver. Geometric algorithms and combinatorial optimization, volume 2. Springer Science &amp; Business Media, 2012. | . The latter book is a very good general starting point for convex optimization. That the features are standardized as shown in the notes is assumed (and discussed), e.g., in . | Wang, Ruosong, Dean P. Foster, and Sham M. Kakade. 2020. “What Are the Statistical Limits of Offline RL with Linear Function Approximation?” arXiv [cs.LG]. arXiv | . which we will meet later. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec8/#references",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec8/#references"
  },"147": {
    "doc": "8. Approximate Policy Iteration",
    "title": "8. Approximate Policy Iteration",
    "content": "PDF Version . Note: On March 13, 2021, these notes were updated as follows: . | Tighter bounds are derived; the old analysis was based on bounding \\(\\| q^*-q^{\\pi_k} \\|_\\infty\\); the new analysis directly bounds \\(\\| v^* - v^{\\pi_k} \\|_\\infty\\), which leads to a better dependence on the approximation error; | Unbiased return estimates are introduced that use rollouts of random length. | . One simple idea to use function approximation in MDP planning is to take a planning method that uses internal value functions and add a constraint that restrict the value functions to have a compressed representation. As usual, two questions arise: . | Does this lead to an efficient planner? That is, can the computation be carried out in time polynomial in the relevant quantities, but not the size of the state space? In the case of linear functions the question is whether we can calculate the coefficients efficiently. | Does this lead to an effective planner? In particular, how good a policy can we arrive at with a limited compute effort? | . In this lecture, as a start into exploring the use of value function approximation in planning, we look at modifying policy iteration in the above described way. The resulting algorithm belongs to the family of approximate policy iteration algorithms, which consists of all algorithms derived from policy iteration by adding approximation to it. We will work with linear function approximation. In particular, we will assume that the planner is given as a hint a feature-map $\\varphi: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$. In this setting, since policy iteration hinges upon evaluating the policies obtained, the hint given to the planner is considered to be “good” if the (action-)value functions of all policies are well-represented with the features. This means, that we will work under assumption B2$_\\varepsilon$ from the previous lecture, which we copy here for convenience. In what follows we fix $\\varepsilon&gt;0$. Assumption B2$\\mbox{}_{\\varepsilon}$ (approximate universal value function realizibility) The MDP $M$ and the featuremap $\\varphi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(q^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\varphi\\). Recall that here the notation $q^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\varphi$ means that $q^\\pi$ can be approximated up to a uniform error of $\\varepsilon$ using linear combinations of the basis functions underlying the feature-map $\\varphi$: . For any policy $\\pi$, . \\[\\begin{align*} \\inf_{\\theta\\in \\mathbb{R}^d} \\max_{(s,a)} | q^\\pi(s,a) - \\langle \\theta, \\varphi(s,a) \\rangle | \\left(= \\inf_{\\theta\\in \\mathbb{R}^d} \\| q^\\pi - \\Phi\\theta \\|_\\infty\\right) \\le \\varepsilon\\,. \\end{align*}\\] One may question whether it is reasonable to expect that the value functions of all policies can be compressed. We will come back to this question later. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec8/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec8/"
  },"148": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Query lower bound for MDPs with large action sets",
    "content": "For the statement of our results, the following definitions will be useful: . Definition (soundness): An online planner is $(\\delta,\\varepsilon)$-sound if for any finite discounted MDP $M=(\\mathcal{S},\\mathcal{A},P,r,\\gamma)$ and feature-map $\\varphi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ such that $\\varepsilon^*(M,\\Phi)\\le \\varepsilon$, when interacting with $(M,\\varphi)$, the planner induces a $\\delta$-suboptimal policy of $M$. Definition (memoryless planner): Call a planner memoryless if it does not retain any information between its calls. The announced result is as follows: . Theorem (Query lower bound: large action sets): For any $\\varepsilon&gt;0$, $0&lt;\\delta\\le 1/2$, positive integer $d$ and for any $(\\delta,\\varepsilon)$-sound online planner $\\mathcal{P}$ there exists a “featurized-MDP” $(M,\\varphi)$ with rewards in $[0,1]$ with $\\varepsilon^*(M,\\Phi)\\le \\varepsilon$ such that when interacting with a simulator of $(M,\\varphi)$, the expected number of queries used by $\\mathcal{P}$ is at least . \\[\\begin{align*} \\Omega\\left( \\exp\\left( \\frac{1}{32} \\left(\\frac{\\sqrt{d}\\varepsilon}{\\delta}\\right)^2 \\right) \\right)\\,. \\end{align*}\\] . Note that if \\(\\delta &lt; \\sqrt{d}\\varepsilon\\) the number of queries is exponential in \\(d\\). For the proof we need a result that shows that one can pack the \\(d\\)-dimensional unit sphere with exponential in \\(d\\) many vectors that are nearly orthogonal. The precise result, which is stated without proof, is as follows: . Lemma (Johnson-Lindenstrauss (JL) Lemma) For every \\(\\tau &gt; 0\\) and integers \\(d,k\\) such that . \\[\\left\\lceil\\frac{8 \\ln k}{\\tau^2}\\right\\rceil \\leq d \\leq k\\] then there exists \\(v_1,...,v_k\\) vectors of the \\(d\\)-dimensional unit sphere such that for all \\(1\\le i&lt;j\\le k\\), . \\[\\lvert \\langle v_i,v_j \\rangle | \\leq \\tau\\,.\\] . Note that for a fixed dimension \\(d\\), the valid range for \\(k\\) is . \\(\\begin{align} d\\le k \\le \\exp\\left(\\frac{d\\tau^2}{8}\\right)\\,. \\label{eq:krange} \\end{align}\\) In particular, \\(k\\) can be “exponentially large” in \\(d\\) when \\(\\tau\\) is a constant. We can directly relate this lemma to our feature matrices. In particular, the lemma is equivalent to the following result: . Proposition (JL feature matrix): For any $\\tau,d,k$ as in the JL lemma there exists a matrix $\\Phi \\in \\mathbb{R}^{k\\times d}$ such that for any $i\\in[k]$, . \\[\\begin{align} \\max_{i\\in [k]} \\inf_{\\theta\\in \\mathbb{R}^d} \\|\\Phi \\theta - e_i \\|_\\infty \\le \\tau\\,, \\label{eq:featjl} \\end{align}\\] where \\(e_i\\) is the \\(i\\)th basis vector of standard Euclidean basis of \\(\\mathbb{R}^k\\), and in particular if \\(\\varphi_i^\\top\\) is the \\(i\\)th row of \\(\\Phi\\), \\(\\|\\Phi \\varphi_i - e_i\\|_\\infty \\le \\tau\\) holds. Proof: Choose $v_1,\\dots,v_k$ from the JL lemma as the rows of $\\Phi$. Fix $i\\in [k]$. Then, \\(\\begin{align*} \\Phi v_i - e_i = (v_1^\\top v_i,\\dots,v_i^\\top v_i,\\dots, v_k^\\top v_i)^\\top - e_i = (v_1^\\top v_i,\\dots,0,\\dots, v_k^\\top v_i)^\\top\\,. \\end{align*}\\) Since by construction $|v_j^\\top v_i|\\leq \\tau$ for $j\\ne i$, the statement follows. \\(\\qquad \\blacksquare\\) . Finally, we need a variation of the result of Question 6 of Assignment 0. This question asked for proving that any algorithm that identifies the single nonzero entry in a binary array of length \\(k\\) requires to look at at least \\((k+1)/2-1/k\\) entries of the array on expectation. A similar lower bound applies if we require the algorithm to be correct with, say, probability \\(1/2\\): . Lemma (High-probability needle lemma): Let $p&gt;0$. Any algorithm that correctly identifies the single nonzero entry in any binary array of length \\(k\\) with probability at least $p$ has the property that the expected number of queries that the algorithm uses is at least \\(\\Omega(p k)\\). In fact, if $q_k$ is the worst-case expected number of queries used by an algorithm that is correct with probability $p$ then one can show that for $k\\ge 2$, $q_k \\ge p( \\frac{k+1}{2}-\\frac{1}{k})$. Proof: Left as an exercise. \\(\\qquad \\blacksquare\\) . With this we are ready to give the proof of the theorem: . Proof (of the theorem): We only give a sketch. Fix the planner $\\mathcal{P}$ with the said properties. Let $k$ be a positive integer to be chosen later. We construct a feature map $\\varphi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ and $k$ MDPs $M_1,\\dots,M_k$ that share $\\mathcal{S}=\\{s,s_{\\text{end}}\\}$ and $\\mathcal{A}=[k]$ as state and action-spaces, respectively. Here $s$ will be chosen as the initial state where the planners will be tested from and $s_{\\text{end}}$ will be an absorbing state with zero reward. The MDPs share the same deterministic transition dynamics: All actions in $s$ end up in $s_{\\text{end}}$ with probability one and all actions taken in $s_{\\text{end}}$ end up in $s_{\\text{end}}$ with probability one. The rewards for actions taken in $s_{\\text{end}}$ are all zero. Finally, we choose the reward of MDP $M_i$ in state $s$ to be . \\[\\begin{align*} r_a^{(i)}(s)=\\mathbb{I}(a=i) r^*\\,, \\end{align*}\\] where the value of $r^*\\in (0,1]$ is left to be chosen later. Then, denoting by $A$ the action returned by the planner when called with state $s$, one can see that the value of the policy induced at $s$ in MDP $M_i$ is \\(r^*\\mathbb{P}_i(A=i)\\), where $\\mathbb{P}_i$ is the distribution induced by the interconnection of the planner and MDP $M_i$. Thus, for \\(r^*=2\\delta\\), the planner needs to return $A$ so that $\\mathbb{P}_i(A=i)\\ge 1/2$. Hence, it needs at least $\\Omega(k)$ calls by the high-probability needle lemma. Finally, the JL feature matrix construction allows us to construct a feature-map for this MDP as the action-value functions take the form \\(q^\\pi(s,a)=\\mathbb{I}(a=i)r^*\\), $q^\\pi(s_{\\text{end}},a)=0$ in this MDP. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec9/#query-lower-bound-for-mdps-with-large-action-sets",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec9/#query-lower-bound-for-mdps-with-large-action-sets"
  },"149": {
    "doc": "9. Limits of query-efficient planning",
    "title": "A lower bound when the number of actions is constant",
    "content": "The previous result leaves open whether query-efficient planners exist with a fixed number of actions. Our next result shows that the problem does not get much easier in this setting either. The result is stated for fixed-horizon MDPs. Given an MDP \\(M=(\\mathcal{S},\\mathcal{A},P,r)\\), a policy \\(\\pi\\), a positive integer \\(h&gt;0\\) and state \\(s\\in \\mathcal{S}\\) of the MDP, let . \\[\\begin{align*} v_h^\\pi(s) = \\mathbb{E}_s^{\\pi}[ \\sum_{t=0}^{h-1} r_{A_t}(S_t)] \\end{align*}\\] be the total reward collected by \\(\\pi\\) when it is used for \\(h\\) steps. The action-value functions \\(q_h^\\pi: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}\\) are defined similarly. The optimal \\(h\\)-step value function is . \\[\\begin{align*} v_h^*(s) = \\sup_{\\pi} v_h^\\pi(s)\\,, \\qquad s\\in \\mathcal{S}\\,. \\end{align*}\\] The Bellman optimality operator \\(T: \\mathbb{R}^{\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}}\\) is defined via . \\[\\begin{align*} T v(s) = \\max_{a\\in \\mathcal{A}} r_a(s) + \\langle P_a(s), v \\rangle\\,. \\end{align*}\\] The policy evaluation operator \\(T_\\pi: \\mathbb{R}^{\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}}\\) of a memoryless policy \\(\\pi\\) is . \\[\\begin{align*} T_\\pi v(s) = \\sum_{a\\in \\mathcal{A}} \\pi(a|s) \\left( r_a(s) + \\langle P_a(s), v \\rangle \\right)\\,. \\end{align*}\\] A policy \\(\\pi\\) is \\(h\\)-step optimal if \\(v_h^\\pi = v_h^*\\). Also, \\(\\pi\\) is greedy with respect to \\(v:\\mathcal{S}\\to \\mathbb{R}\\) if \\(T_\\pi v = T v\\). The analogue of the fundamental theorem looks as follows: . Theorem (fixed-horizon fundamental theorem): We have \\(v_0^*\\equiv \\boldsymbol{0}\\) and for any \\(h\\ge 0\\), \\(v_{h+1}^* = T v_h^*\\). Furthermore, for any \\(\\pi_0^*,\\dots,\\pi_h^*, \\dots\\) such that for \\(i\\ge 0\\), \\(\\pi_i^*\\) is greedy with respect to \\(v_i^*\\), for any \\(h&gt;0\\) it holds that \\(\\pi=(\\pi_{h-1}^*,\\dots,\\pi_0^*,\\dots)\\) (i.e., the policy which in step \\(1\\) uses \\(\\pi_{h-1}^*\\), in step \\(2\\) uses \\(\\pi_{h-2}^*\\), \\(\\dots\\), in step \\(h\\) uses \\(\\pi_0^*\\), after which it continues arbitrarily) is \\(h\\)-step optimal: . \\[\\begin{align*} v_h^{\\pi} = v_h^*\\,. \\end{align*}\\] . Proof: Left as an exercise. Hint: Use induction. \\(\\qquad \\blacksquare\\) . In the theorem our earlier notion of policies is slightly abused: \\(\\pi\\) is only specified for $h$ steps. In any case, according to this result for a fixed horizon \\(H&gt;0\\), the natural analogue for memoryless policies are these $H$-step nonstationary memoryless policies. Let us denote the set of these by \\(\\Pi_H\\). In the next result, we will only care about optimality with respect to a fixed initial state \\(s_0\\in \\mathcal{S}\\). Then, without loss of generality, we also assume that the set of states \\(\\mathcal{S}_h\\) reachable from \\(s_0\\) in \\(h\\ge 0\\) steps are disjoint: \\(\\mathcal{S}_h\\cap \\mathcal{S}_{h'}=\\emptyset\\) for \\(h\\ne h'\\) (why?). It follows that we can also find a memoryless policy \\(\\pi\\) that is optimal at \\(s_0\\): \\(v^{\\pi}_H(s_0)=v_H^*(s_0)\\). In fact, one can even find a memoryless policy that also satisfies . \\[\\begin{align} v^{\\pi}_{H-i}(s)=v_{H-i}^*(s), \\qquad s\\in \\mathcal{S}_i \\end{align}\\] simultaneously for all \\(0\\le i \\le H-1\\). Furthermore, the same holds for the action-value functions: . \\[\\begin{align} q^{\\pi}_{H-i}(s,a)=q_{H-i}^*(s,a), \\qquad s\\in \\mathcal{S}_i, a\\in \\mathcal{A}, 0\\le i \\le H-1\\,. \\end{align}\\] Thus, the natural analogue that all action-value functions are well-approximated with some feature-map is that there are feature-maps \\((\\varphi_h)_{0\\le h \\le H-1}\\) such that for \\(0\\le h \\le H-1\\), \\(\\varphi_h: \\mathcal{S}_h \\times \\mathcal{A} \\to \\mathbb{R}^d\\) and for any memoryless policy \\(\\pi\\), the \\(H-h\\)-step action value function of \\(\\pi\\), when restricted to \\(\\mathcal{S}_h\\), is well-approximated by the linear combination of the basis functions induced by \\(\\varphi_h\\). Since we will not need \\(q^{\\pi}_{H-h}\\) outside of \\(\\mathcal{S}_h\\), in what follows, we assume that these are restricted to \\(\\mathcal{S}_h\\). Writing \\(\\Phi_h\\) for the feature matrix induced by \\(\\varphi_h\\) (the rows of \\(\\Phi_h\\) are the feature vectors under \\(\\varphi_h\\) for some ordering of the state-action pairs from \\(\\mathcal{S}_{h}\\times \\mathcal{A}\\)), we redefine \\(\\varepsilon^*(M,\\Phi)\\) as follows: . \\(\\begin{align} \\varepsilon^*(M,\\Phi) : = \\sup_{\\pi \\text{ memoryless}} \\max_{0\\le h \\le H-1}\\inf_{\\theta\\in \\mathbb{R}^d} \\| \\Phi_h \\theta - q^{\\pi}_{H-h} \\|_\\infty\\,. \\end{align}\\) . Since we changed the objective, we also need to change the definition of $(\\delta,\\varepsilon)$-sound online planners: These planners now need to induce policies that are $\\delta$-suboptimal or better when evaluated with the $H$-horizon undiscounted total reward criterion from the designated start-state \\(s_0\\) provided that the MDP satisfies $\\varepsilon^*(M,\\Phi)\\le \\varepsilon$. In what follows, we call these planners $(\\delta,\\varepsilon)$-sound for the $H$-step criterion. With this, we are ready to state the main result of this section: . Theorem (Query lower bound: small action sets, fixed-horizon objective): For $\\varepsilon&gt;0$, $0&lt;\\delta\\le 1/2$ and positive integer $d$, let . \\[\\begin{align*} u(d,\\varepsilon,\\delta) = \\left\\lfloor\\exp\\left(\\frac{d (\\frac{\\varepsilon}{2\\delta})^2}{8}\\right)\\right\\rfloor\\,. \\end{align*}\\] Then, for any $\\varepsilon&gt;0$, $0&lt;\\delta\\le 1/2$, positive integers $\\mathrm{A},H,d$ such that \\(d\\le\\mathrm{A}^H\\) and for any online planner $\\mathcal{P}$ that is $(\\delta,\\varepsilon)$-sound for MDPs with at most $\\mathrm{A}$ actions and the $H$-step criterion, there exists a “featurized-MDP” $(M,\\varphi)$ with \\(\\mathrm{A}\\) actions and rewards in $[0,1]$ such that when interacting with a simulator of $(M,\\varphi)$, the expected number of queries used by $\\mathcal{P}$ is at least . \\[\\begin{align*} \\tilde\\Omega\\left( \\frac{u(d,\\varepsilon,\\delta)}{\\mathrm{A} d(\\varepsilon/\\delta)^2}\\right)\\, \\end{align*}\\] provided that \\(\\mathrm{A}^H&gt;u(d,\\varepsilon,\\delta)\\) (“large horizons”), while it is . \\[\\begin{align*} \\tilde\\Omega\\left( \\frac{\\mathrm{A}^H}{ H }\\right)\\, \\end{align*}\\] otherwise (“small horizon”). In words, if the horizon is large enough, the previous exponential-in-\\(d\\) lower bound continues to hold, while for horizons that are smaller, a lower bound that is exponential in the horizon holds. Note that above \\(\\tilde\\Omega(\\cdot)\\) hides logarithmic terms. Note that the condition \\(d\\le\\mathrm{A}^H\\) is reasonable: We do not expect the feature-space dimension to be comparable to \\(\\mathrm{A}^H\\). Proof: At a high level the proof constructs an MDP (tree) with $\\mathrm{A}^H$ trajectories, only one of which has a non-zero reward at the leaf node. Then, taking a trajectory can be thought of as quering an array with $A^H$ entries with only one non-zero entry. Thus, since our planner is gauranteed to be $\\delta$-optimal, and the MDP can be thought of as a needle in a haystack problem (as described above), a total of $\\Omega(\\mathrm{A}^H)$ trajectories must be searched through to find the single non-zero reward. The details are as follows. Fix a planner $\\mathcal{P}$ with the required properties. We consider $k=\\mathrm{A}^H$ MDPs $M_1,\\dots,M_k$ that share the state space \\(\\mathcal{S} = \\cup_{0\\le h \\le H} \\mathcal{A}^h\\) and action space \\(\\mathcal{A}\\). Here, by convention, \\(\\mathcal{A}^0\\) is a singleton with the single element \\(\\perp\\), which will play the role of the start state \\(s_0\\). The transition dynamics are also shared by these MDPs: When in state \\(s\\in \\mathcal{S}\\) and action \\(a\\in \\mathcal{A}\\) is taken, the next state is \\(s'=(a)\\) when \\(s=\\perp\\), while if \\(s=(a_1,\\dots,a_h)\\) with some \\(1\\le h \\le H-1\\) then \\(s'=(a_1,\\dots,a_h,a)\\) and when \\(h=H\\) then the next state is \\(s\\) (ever state in \\(\\mathcal{A}^H\\) is absorbing). The MDPs differ in their reward functions. To describe the rewards let \\(f\\) be a bijection from \\([k]\\) to \\(\\mathcal{A}^H\\). Now, fix \\(1\\le i \\le k\\) and define \\((a_0^*,\\dots,a_{H-1}^*)\\) by \\(f(i)=(a_0^*,\\dots,a_{H-1}^*)\\). Let \\(s_0^*=s_0\\), \\(s_1^*=(a_0^*)\\), \\(s_2^*=(a_0^*,a_1^*)\\), \\(\\dots\\), \\(s_H^*=(a_0^*,\\dots,a_{H-1}^*)\\). Then, in MDP \\(M_i\\), \\(r_{a_{H-1}^*}(s_{H-1}^*)=2\\delta\\) while \\(r_a(s)=0\\) for any other state-action pair. Note that the optimal reward in \\(H\\) steps from \\(\\perp\\) is \\(2\\delta\\) and the only policy that achieves this reward is the one that goes through the states in \\(s_0^*,s_1^*,\\dots,s_{H-1}^*\\). We can visualize MDP \\(M_i\\) as a tree, as seen on the figure on the right. The green nodes on the figure correspond to the states \\(s_0^*,s_1^*,\\dots,s_{H-1}^*,s_H^*\\). Note also that \\(\\mathcal{S}_h = \\mathcal{A}^h\\) for \\(0\\le h \\le H\\). We will now describe the action-value functions of the memoryless policies in \\(M_i\\) as this will be useful later. Fix \\(0\\le h \\le H-1\\). Then, \\(q^{\\pi}_{H-h}\\), by our convention, is defined over \\(\\mathcal{S}_h\\). Then, for any \\(s\\in \\mathcal{S}_h (=\\mathcal{A}^h)\\) and \\(a\\in \\mathcal{A}\\), . \\[\\begin{align} q^\\pi_{H-h}(s,a) = \\begin{cases} 2\\delta\\,, &amp; \\text{if } h=H-1, s=s_{H-1}^*, a=a_{H-1}^*\\,;\\\\ v^\\pi_{H-h-1}(s_{h+1}^*)\\,, &amp; \\text{if } h&lt;H-1, s=s_h^*, a=a_h^*\\,;\\\\ 0\\,, &amp; \\text{otherwise}\\,. \\end{cases} \\label{eq:qpiinfh} \\end{align}\\] Note that here \\(0\\le v^\\pi_{H-h-1}(g(s,a))\\le 2\\delta\\). We see that for each stage \\(0\\le h \\le H-1\\), there is only one state-action pair such that the value of \\(q^\\pi_{H-h}\\) is nonzero, and in this case the value is in the \\([0,2\\delta]\\) interval. Now, since the planner induces a policy $\\pi$ with suboptimality \\(\\delta\\), . \\[v_H^\\pi(s_0) \\ge v_H^*(s_0) - \\delta = 2\\delta - \\delta = \\delta.\\] We also know that . \\[\\begin{align} v_H^\\pi(s_0) &amp;= \\mathbb{E}_{s_0}^\\pi \\left[\\sum_{t=0}^{H-1} r_{A_t}(S_t)\\right] \\nonumber \\\\ &amp;= 2\\delta \\mathbb{P}(A_0 = a_0^*, \\dots, A_{H-1} = a_{H-1}^*) \\nonumber \\end{align}\\] where \\(\\mathbb{P}\\) is as usual the canonical probability measure induced by the interaction of the policy $\\pi$ and environment. Thus, since \\(v_H^\\pi(s_0) \\ge \\delta\\) it must be that \\(\\mathbb{P}(A_0 = a_0^*, \\dots, A_{H-1} = a_{H-1}^*) \\ge 1/2\\). By our previous argument (reduction to the “needle in a haystack” problem), this whole process needs \\(\\Omega(k)\\) queries. If the expected number of queries issued by \\(\\mathcal{P}\\) is \\(q\\), the expected number of queries issued here is \\(H q\\), since the planner must called once for each step \\(h \\in \\{0, \\dots, H-1\\}\\). Hence, . \\[q = \\Omega\\left( \\frac{k}{H} \\right)\\,.\\] Let us now consider a choice for \\(\\Phi = (\\Phi_h)_{0\\le h \\le H-1}\\) such that \\(\\varepsilon^*(M,\\Phi)\\le \\varepsilon\\). For \\(\\Phi_h\\) choose first a “JL feature matrix” \\(\\tilde \\Phi_h\\in \\mathbb{R}^{|\\mathcal{S}_h| \\times d}\\) such that Eq. \\eqref{eq:featjl} holds. Then let \\(\\Phi_h = \\sqrt{2\\delta} \\tilde \\Phi_h\\). Choose \\(\\theta_h = v^\\pi_{H-h-1}( s_{h+1}^* ) \\varphi_h( s_h^*,a_{h}^* )/(2\\delta)\\) if \\(h&lt;H-1\\) and choose \\(\\theta_h = \\varphi_h( s_h^*,a_{h}^* )\\), otherwise. Then, by Eq. \\eqref{eq:qpiinfh}, for \\((s,a)\\ne (s_h^*,a_{h}^*)\\), \\(|\\varphi_h(s,a)^\\top \\theta_h-q_{H-h}(s,a)| \\le |v^\\pi( s_{h+1}^* )| \\, |\\tilde \\varphi_h(s,a)^\\top \\tilde \\varphi_h(s_h^*,a_{h}^*)|\\le 2\\delta \\tau\\) and for \\((s,a)=(s_h^*,a_{h}^*)\\), \\(\\varphi_h(s,a)^\\top \\theta_h=q_{H-h}(s,a)\\). Hence, \\(\\varepsilon^*(M,\\Phi)\\le \\varepsilon\\) holds if we set \\(\\tau=\\varepsilon/(2\\delta)\\). From Eq. \\eqref{eq:krange}, \\(\\tilde \\Phi_h\\) exists if \\(d\\le k\\) and . \\[\\begin{align*} k \\le u:=\\left\\lfloor\\exp\\left(\\frac{d (\\frac{\\varepsilon}{2\\delta})^2}{8}\\right)\\right\\rfloor\\,. \\end{align*}\\] Recall that \\(k = \\mathrm{A}^H\\). Thus, the required claim holds for the case when \\(\\mathrm{A}^H\\le u\\) (“small horizon case”). In the opposite case (“large horizon”), let \\(\\tilde H\\) be the largest positive number such that \\(\\mathrm{A}^{\\tilde H}\\le u\\) holds. A quick calculation shows \\(\\tilde H \\le \\frac{d (\\epsilon/\\delta)^2}{32 \\log(\\mathrm{A})}\\) Repeating the above argument with horizon \\(\\tilde H\\) and noting that since \\(\\mathrm{A}^{\\tilde H}\\le u \\le \\mathrm{A}^{\\tilde H + 1}\\) we know that \\(u/\\mathrm{A} \\le \\mathrm{A}^{\\tilde H}\\) gives the lower bound \\(q = \\Omega\\left( \\frac{\\mathrm{A}^{\\tilde H}}{\\tilde H} \\right) = \\Omega\\left( \\frac{u}{\\mathrm{A} \\tilde H} \\right) = \\Omega\\left( \\frac{u \\log(\\mathrm{A})}{\\mathrm{A} d (\\epsilon/\\delta)^2} \\right)\\,,\\) which finishes the proof. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec9/#a-lower-bound-when-the-number-of-actions-is-constant",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec9/#a-lower-bound-when-the-number-of-actions-is-constant"
  },"150": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Proof of the JL lemma",
    "content": "For completeness, we include a proof of the JL lemma. The proof uses the so-called probabilistic method The idea of this is that sometimes it is easier to establish the existence of some “good configuration” (like the nearly orthogonal vectors on the unit sphere in the JL lemma) by establishing that such a configuration has positive probability under some probability distribution over possible configurations. In our case, this works as follows: Let $V_1,\\dots,V_k$ be random vectors, each uniformly distributed on the $d$-dimensional unit sphere and so that the distinct vectors in this sequence are pairwise independent of each other. Take $i\\ne j$. If we show that $|\\langle V_i, V_j \\rangle| \\le \\tau$ holds with probability at least $1-1/k^2$, by a union bound over the $k(k-1)/2$ pairs $1\\le i &lt;j \\le k$, it follows that $\\max_{i\\ne j} |\\langle V_i,V_j | \\le \\tau$ holds with probability at least $1/2$, from which, the lemma follows. Thus, it remains to show that the angle between the random vectors $V_i$ and $V_j$ is “small” with the claimed probability. Since the uniform distribution is rotation invariant and $V_i$ and $V_j$ are independent of each other, $\\langle V_i, V_j \\rangle$ has the same distribution as $\\langle e_1, V_1 \\rangle = V_{11}\\in [-1,1]$. To see this take a rotation $R$ that rotates $V_i$ to $e_1$; then $\\langle V_i, V_j \\rangle = \\langle R V_i, R^{-1} V_j \\rangle = \\langle e_1, R^{-1} V_j \\rangle$. Now, since $R$ and $V_j$ are independent of each other, $R^{-1} V_j$ is still uniformly distributed on the sphere, hence, $\\langle e_1, R^{-1} V_j \\rangle$ and $\\langle e_1, V_1 \\rangle$ share the same distribution. A tedious calculation shows that for any $x\\ge 6$, . \\[\\begin{align} \\mathbb{P}( V_{11}^2 &gt; x/d) \\le \\exp(-x/4)\\,. \\label{eq:dgtail} \\end{align}\\] (The idea of proving this is to notice that if $X$ is $d$-dimensional standard normal variable then \\(V=X/\\|X\\|_2\\) is uniformly distributed on the sphere. Then, one proceeds using Chernoff’s method.) The result now follows from \\eqref{eq:dgtail} by choosing $x$ so that $\\tau^2 = x/d$ holds. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec9/#proof-of-the-jl-lemma",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec9/#proof-of-the-jl-lemma"
  },"151": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Notes",
    "content": ". | The lower bound for the discounted case is missing the planning horizon. In the fixed-horizon setting, the lower bound is again missing the horizon when the horizon is large. It remains to be seen whether the extra “horizon terms” in Eq. \\eqref{eq:limit} are necessary. | In any case, the main conclusion is that even when we require “strong features”, high-accuracy planning is intractable. | The reader familiar with the TCS literature may recognize a close resemblance to questions studied there which are concerned with the existence of “fully polynomial time approximation schemes” (FPTAS). | There are many open questions. For one, is there a counterpart of the second theorem for the discounted setting? . | . ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec9/#notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec9/#notes"
  },"152": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Bibliographical notes",
    "content": "The idea of using the Johnson-Lindenstrauss lemma in this context is due to Du, Kakade, Wang and Yang (DKWY, for short). The first theorem is a variant of a result from this paper. The second theorem is a variation of Theorem 4.1 from the paper of Du et al. mentioned above who prove the analoge result for global planners. The proof of the lemma also follows the proof given in this paper. The proof of inequality \\eqref{eq:dgtail} is given in a paper of Dasgupta and Gupta, which also gives the “full version” of the Johnson-Lindenstrauss lemma which states that logarithmically many dimensions are sufficient to keep pairwise distances between a finite set of points. | Dasgupta, Sanjoy; Gupta, Anupam (2003), “An elementary proof of a theorem of Johnson and Lindenstrauss” link, Random Structures &amp; Algorithms, 22 (1): 60–65 | . The presentation of the first result which is for “bandits” (fixed horizon problems with $H=1$) follows closely that of a paper by Lattimore, Weisz and yours truly. This, and a paper by van Roy and Dong were both prompted by the DKWY paper, whose initial version focused on the case when $\\delta \\ll \\sqrt{d} \\varepsilon$, which made the outlook for designing robust RL methods quite bleak. While it is true that in this high-precision regime nothing much can be done (unless further restricting the features), both papers emphasized that the hardness result disappears when the algorithm can deliver $\\delta$ optimal policies with $\\delta \\gtrsim \\sqrt{d} \\varepsilon$. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec9/#bibliographical-notes",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec9/#bibliographical-notes"
  },"153": {
    "doc": "9. Limits of query-efficient planning",
    "title": "9. Limits of query-efficient planning",
    "content": "PDF Version . In the last lecture we have seen that given a discounted MDP $M = (\\mathcal{S},\\mathcal{A},P,r,\\gamma)$, a feature-map $\\varphi: \\mathcal{S}\\times\\mathcal{A}\\to \\mathbb{R}^d$ and a precomputed, suitably small core set, for any $\\varepsilon’&gt;0$ target and any confidence parameter $0\\le \\zeta \\le 1$, interacting with a simulator of $M$, with at most $\\text{poly}(\\frac{1}{1-\\gamma},d,\\mathrm{A},\\frac{1}{(\\varepsilon’)^2},\\log(1/\\zeta))$, compute time, LSPI returns some weight vector $\\theta\\in \\mathbb{R}^d$ such that with probability $1-\\zeta$, the policy that is greedy with respect to $q = \\Phi \\theta$ is $\\delta$-suboptimal with . \\[\\begin{align} \\delta \\le \\frac{2(1 + \\sqrt{d})}{(1-\\gamma)^2}\\, \\varepsilon + \\varepsilon'\\,, \\label{eq:suboptapi} \\end{align}\\] where $\\varepsilon$ is the error with which the features can approximate the action-value functions of the policies of the MDP: . \\[\\begin{align} \\varepsilon = \\varepsilon^*(M,\\Phi) : = \\sup_{\\pi \\text{ memoryless}} \\inf_{\\theta\\in \\mathbb{R}^d} \\| \\Phi \\theta - q^\\pi \\|_\\infty\\,. \\label{eq:polerr} \\end{align}\\] Here, following our earlier convention, $\\Phi$ refers to the \\(| \\mathcal{S}\\times\\mathcal{A} | \\times d\\) matrix that is obtained by stacking the feature vectors $\\varphi^\\top(s,a)$ of all possible state-action pairs on the top of each other in some fixed order. Setting $\\varepsilon’$ to match the first term in Eq. \\eqref{eq:suboptapi}, we can keep the effort polynomial in the relevant quantities (including $1/\\varepsilon$), but even in the limit of infinite computation, the best bound we can obtain is . \\[\\begin{align} \\delta \\le \\frac{2(1 + \\sqrt{d})}{(1-\\gamma)^2}\\, \\varepsilon\\,. \\label{eq:limit} \\end{align}\\] While it makes sense that with a reasonable compute effort $\\delta$ cannot be better than $\\varepsilon$ or a constant multiple of $\\varepsilon$, it is unclear whether the extra $\\sqrt{d}/(1-\\gamma)^2$ factor is an artifact of the proof. We may suspect that some power of $1/(1-\\gamma)$ may be necessary, because even if we knew the parameter vector that gives the best approximation to \\(q^*\\), the error incurred by acting greedily with respect to $q^*$ could be as large as . \\[\\frac{\\varepsilon}{1-\\gamma}\\,.\\] However, at this point, it is completely unclear whether the extra \\(\\sqrt{d}\\) factor is necessary. The main question asked in this lecture: Are the “extra” factors truly necessary in the above bound? Or are there some other polynomial runtime algorithms that are able to produce policies with smaller suboptimality? . In this lecture we will give a partial answer to this question: We will justify the presence of $\\sqrt{d}$. We start with a lower bound that shows that when there is no limit on the number of actions, efficient algorithms are limited to $\\delta = \\Omega( \\varepsilon\\sqrt{d})$. ",
    "url": "/2024/lecture-notes/planning-in-mdps/lec9/",
    
    "relUrl": "/lecture-notes/planning-in-mdps/lec9/"
  },"154": {
    "doc": "Planning in MDPs",
    "title": "Planning in MDPs",
    "content": "PDF Version . ",
    "url": "/2024/lecture-notes/planning-in-mdps",
    
    "relUrl": "/lecture-notes/planning-in-mdps"
  },"155": {
    "doc": "Batch RL",
    "title": "Batch RL",
    "content": "PDF Version . ",
    "url": "/2024/w2022-lecture-notes/batch-rl",
    
    "relUrl": "/w2022-lecture-notes/batch-rl"
  },"156": {
    "doc": "17. Introduction",
    "title": "How good is the plug-in method?",
    "content": "The plug-in method estimates a model and uses the estimated model in place of the real one to solve the problem at hand. Let $M = (\\mathcal{S},\\mathcal{A},P,r)$ be a finite MDP, $\\hat M = (\\mathcal{S},\\mathcal{A},\\hat P,\\hat r)$ be an estimate. The estimate can be produced in a number of ways, but from the perspective of the result that comes, how the estimate is produced does not matter. We consider the discounted case with a discount factor $0\\le \\gamma&lt;1$. We will use $\\hat v^\\pi$ to denote the value function of a policy $\\pi$ in $\\hat M$ (as opposed to $v^\\pi$, which is the value function of policy in $M$), and similarly, we will use \\(\\hat v^*\\) to denote the optimal value function in $\\hat M$. We analogously use $\\hat q^\\pi$ and \\(\\hat q^*\\). Every other quantity that is usually associated with an MDP but which now is associated with $\\hat M$ receives a “hat”. For example, we use $\\hat T_\\pi$ for the policy evaluation operator of memoryless policy $\\pi$ in $\\hat M$ (either for the state values, or the action-values), while we use $\\hat T$ to denote the Bellman optimality operator underlying $\\hat M$ (again, both for the state and action-values). We start with a generic result about contraction mappings: . Proposition (residual bound): Let $F: V \\to V$ be a $\\gamma$-contraction over a normed vector space $V$ and let $x\\in V$ be a fixed-point of $F$. Then for any $y\\in V$, . \\[\\begin{align} \\| x - y \\| \\le \\frac{\\| Fy - y \\|}{1-\\gamma}\\,. \\label{eq:resbound} \\end{align}\\] . Proof: By the triangle inequality, . \\[\\| x- y \\| \\le \\| Fx - Fy \\| + \\| F y - y \\| \\le \\gamma \\| x-y \\| + \\| Fy - y \\|\\,.\\] Reordering and solving for $| x-y |$ gives the result. \\(\\qquad \\blacksquare\\) . An immediate implication is that good model estimates are guaranteed to give rise to (relatively) good value estimates. Proposition (value estimation error): Let $H_\\gamma = 1/(1-\\gamma)$ and assume that the rewards in $M$ are in the $[0,1]$ interval. For any policy $\\pi$, the following holds: . \\[\\begin{align} \\| v^\\pi - \\hat v^\\pi \\|_\\infty &amp; \\le H_\\gamma \\left( \\| r_\\pi-\\hat r_\\pi\\|_\\infty + \\gamma \\| ( P_\\pi - \\hat P_\\pi) v^\\pi \\|_\\infty \\right) \\\\ &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma H_\\gamma \\| P - \\hat P\\|_\\infty \\right)\\,. \\end{align}\\] Also, . \\[\\begin{align} \\| v^* - \\hat v^* \\|_\\infty &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma \\| (P - \\hat P ) v^*\\|_\\infty \\right) \\\\ &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma H_\\gamma \\| P - \\hat P\\|_\\infty \\right)\\,. \\end{align}\\] Similarly, . \\[\\begin{align} \\| q^\\pi - \\hat q^\\pi \\|_\\infty &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma \\| (P - \\hat P ) v^\\pi\\|_\\infty \\right) \\\\ &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma H_\\gamma \\| P - \\hat P\\|_\\infty \\right)\\,. \\end{align}\\] and . \\[\\begin{align} \\| q^* - \\hat q^* \\|_\\infty &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma \\| (P - \\hat P)v^*\\|_\\infty \\right) \\label{eq:qsdiff1} \\\\ &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma H_\\gamma \\| P - \\hat P\\|_\\infty \\right)\\,. \\label{eq:qsdiff} \\end{align}\\] . Note that in general the value estimates are more sensitive to errors in the transition probabilities then in the rewards. In particular, the transition errors can be magnified by a factor as large as $H_\\gamma$, while the reward errors are magnified by at most $H_\\gamma$. Also note that sometimes one can obtain tighter estimates with stopping earlier in the derivations of these bounds. We will see some examples of how this can help later. Proof: To reduce clutter, we write \\(\\| \\cdot \\|\\) for \\(\\|\\cdot\\|_\\infty\\). Let $F = \\hat T_\\pi$, where $\\hat T_\\pi$ is defined via $\\hat T_\\pi v = \\hat r_\\pi + \\gamma \\hat P_\\pi v$. By the residual bound \\eqref{eq:resbound}, . \\[\\| \\hat v^\\pi - v^\\pi \\| \\le H_\\gamma \\| \\hat T_\\pi v^\\pi - v^\\pi \\| = H_\\gamma \\| \\hat T_\\pi v^\\pi - T_\\pi v^\\pi \\| \\le H_\\gamma \\left( \\| r_\\pi-\\hat r_\\pi\\| + \\gamma \\| (P_\\pi - \\hat P_\\pi) v^\\pi \\|\\right).\\] The second inequality follows from separating $v^\\pi$ from the second term and bounding it using \\(\\| v^\\pi \\| \\le H_\\gamma\\) and also using that $r_\\pi = M_\\pi r$, $\\hat r_\\pi = M_\\pi \\hat r$, $P_\\pi = M_\\pi P$ and $\\hat P_\\pi = M_\\pi \\hat P$ and finally using that $M_\\pi$ is a nonexpansion. The remaining inequalities can be obtained in an entirely analogous manner and hence their proof is omitted. \\(\\qquad \\blacksquare\\) . The result just shown suffices to quantify the size of the value errors. For quantifying the policy optimization error that results from finding an optimal (or near optimal) policy for $\\hat M$, recall the Policy Error Bound from Lecture 6: . Lemma (Policy error bound - I.): Let $\\pi$ be a memoryless policy and choose a function $q:\\mathcal{S}\\times\\mathcal{A} \\to \\mathbb{R}$ and $\\epsilon\\ge 0$. Then, the following hold: . | If $\\pi$ is $\\epsilon$-optimizing in the sense that \\(\\sum_a \\pi(a\\vert s) q^*(s,a) \\ge v^*(s)-\\epsilon\\) holds for every state $s\\in \\mathcal{S}$ then $\\pi$ is $\\epsilon/(1-\\gamma)$ suboptimal: \\(v^\\pi \\ge v^* - \\frac{\\epsilon}{1-\\gamma} \\boldsymbol{1}\\,.\\) . | If $\\pi$ is greedy with respect to $q$ then $\\pi$ is $2\\epsilon$-optimizing with \\(\\epsilon= \\|q-q^*\\|_\\infty\\) and thus . | . \\[v^\\pi \\ge v^* - \\frac{2\\|q-q^*\\|_\\infty}{1-\\gamma} \\boldsymbol{1}\\,.\\] . This leads to the following result: . Theorem (bound on policy optimization error): Assume that the rewards both in $M$ and $\\hat M$ belong to the $[0,1]$ interval. Take any $\\varepsilon&gt;0$ and $\\varepsilon$-optimal policy $\\pi$ in $\\hat M$: $\\hat v^\\pi \\ge \\hat v^* - \\varepsilon \\boldsymbol{1}$. Then, $\\pi$ is $\\delta$-optimal in $M$ with $\\delta$ satisfying . \\[\\begin{align*} \\delta \\le (1+2\\gamma) H_\\gamma \\varepsilon + 2 H_\\gamma^2\\left\\{ \\|r-\\hat r\\|_\\infty + \\gamma \\| (P-\\hat P)v^*\\|_\\infty \\right\\}\\,. \\end{align*}\\] . Note that, up to a small constant factor, the optimization error is magnified by a factor of $H_\\gamma$, the reward errors are magnified by a factor of $H_\\gamma^2$, while the transition errors can get magnified by a factor of up to $H_\\gamma^3$, depending on the magnitude of $v^*$. Proof: Let $\\pi$ be a policy as in the theorem statement. Our goal now is to use the first part of the “Policy error bound”, i.e., that $\\pi$ is $\\varepsilon’$-optimizing with some $\\varepsilon’&gt;0$. On the one hand, we have . \\[M_\\pi \\hat q^\\pi = \\hat v^\\pi \\ge \\hat v^* - \\varepsilon \\boldsymbol{1} = M \\hat q^* - \\varepsilon\\boldsymbol{1} \\ge M \\hat q^\\pi - \\varepsilon \\boldsymbol{1}\\,.\\] Let $z$ be defined by $M_\\pi \\hat q^\\pi = M \\hat q^\\pi + z$. From the previous inequality, we know that \\(\\| z \\|_\\infty \\le \\varepsilon\\). We also have . \\[\\begin{align*} M_\\pi q^* &amp; = M_\\pi \\hat q^\\pi + M_\\pi (q^* - \\hat q^\\pi) \\\\ &amp; = M \\hat q^\\pi + M_\\pi(q^*-\\hat q^\\pi) + z \\\\ &amp; = M q^* + M\\hat q^\\pi-M q^* + M_\\pi(q^*-\\hat q^\\pi) + z \\\\ &amp; \\ge M q^* - (2\\| \\hat q^\\pi - q^* \\|+\\varepsilon) \\boldsymbol{1}\\\\ &amp; = v^* - (2\\| \\hat q^\\pi - q^* \\|+\\varepsilon) \\boldsymbol{1}\\,. \\end{align*}\\] Hence, by Part 1. of the “Policy Error Bound I.” lemma from above, . \\[v^\\pi \\ge v^* - H_\\gamma (2\\| \\hat q^\\pi - q^* \\|+\\varepsilon) \\boldsymbol{1}\\,.\\] By the triangle inequality and the assumption on $\\pi$, . \\[\\begin{align*} \\| \\hat q^\\pi - q^* \\|_\\infty &amp; \\le \\| \\hat q^\\pi - \\hat q^* \\|_\\infty + \\| \\hat q^* - q^* \\|_\\infty \\le \\gamma \\varepsilon + \\| \\hat q^* - q^* \\|_\\infty\\,. \\end{align*}\\] By Eq. \\eqref{eq:qsdiff1}, . \\[\\begin{align*} \\| q^* - \\hat q^* \\|_\\infty &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma \\| (P - \\hat P) v^*\\|_\\infty \\right)\\,. \\end{align*}\\] The result is obtained by chaining the inequalities: . \\[\\begin{align*} \\| v^*-v^\\pi\\|_\\infty &amp; \\le H_\\gamma (2\\| \\hat q^\\pi - q^* \\|+\\varepsilon)\\\\ &amp; \\le H_\\gamma \\left\\{2\\gamma \\varepsilon + 2 H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma \\| (P - \\hat P) v^*\\|_\\infty \\right)+\\varepsilon\\right\\}\\,. \\qquad \\qquad \\qquad \\qquad \\blacksquare \\end{align*}\\] ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec17/#how-good-is-the-plug-in-method",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec17/#how-good-is-the-plug-in-method"
  },"157": {
    "doc": "17. Introduction",
    "title": "Model estimation error: Tabular case",
    "content": "As usual, it is worthwhile to clean up the foundations by considering the tabular case. In this case, the model can be estimared by using sample means. To allow for a unified presentation, let the data available be given in the form of triplets of the form $E_i=(S_i,A_i,R_i,S_{i+1})$ where $i=1,\\dots,n$ and $S_{i+1}\\sim P_{A_i}(S_i)$ given $E_1,\\dots,E_{i-1},S_i,A_i$ and $\\mathbb{E}[R_i|S_i,A_i,E_1,\\dots,E_{i-1}] = r_{A_i}(S_i)$. Introducing the visit counts . \\[\\begin{align*} N(s,a,s') = \\sum_{i=1}^n \\mathbb{I}(S_i=s,A_i=a,S_{i+1}=s') \\end{align*}\\] and \\(N(s,a) = \\sum_{s'} N(s,a,s')\\), provided that the visit count for $(s,a)$ is positive, for the transition probability estimates we have . \\[\\begin{align*} \\hat P_{a}(s,s') = \\frac{N(s,a,s')}{N(s,a)} \\end{align*}\\] and for the reward estimate we have . \\[\\begin{align*} \\hat r_{a}(s) = \\frac{1}{N(s,a)} \\sum_{i=1}^n \\mathbb{I}( S_i=s,A_i=a) R_i\\,. \\end{align*}\\] For ensuring that these are always defined, let $\\hat P_a(s)$ be the uniform distribution over the states and let $\\hat r_a(s)=0$ when $N(s,a)=0$. From the perspective of the results to be presented, the particular values chosen here do not matter. Consider now the simple case when the above triplets are so that for each state-action pair $(s,a)$, $N(s,a)=n(s,a)$ for some deterministic counts $(n(s,a))_{s,a}$. Say, one has access to a generative model (simulator) and for each state-action pair the model is used to generate a fixed number of independent transitions. In this case, one can use Hoeffding’s inequality. In particular, defining . \\[\\beta(n,\\zeta) = \\sqrt{ \\frac{\\log\\left(\\frac{\\mathrm{SA}}{\\zeta}\\right)}{2 n}}\\] provided that $R_i\\in [0,1]$, Hoeffding’s inequality gives that with probability $1-2\\zeta$, for any $s,a$, . \\[\\begin{align*} | \\hat r_a(s) - r_a(s) | &amp; \\le \\beta(n(s,a),\\zeta)\\,,\\\\ | \\langle \\hat P_a(s) - P_a(s), v^* \\rangle | &amp; \\le H_\\gamma \\beta(n(s,a),\\zeta)\\,, \\end{align*}\\] from which it follows that with probability $1-2\\zeta$, . \\[\\begin{align*} \\| \\hat r - r \\|_{\\infty} &amp; \\le \\beta(n_{\\min},\\zeta)\\,,\\\\ \\| ( \\hat P - P) v^* \\|_\\infty &amp; \\le H_\\gamma \\beta(n_{\\min},\\zeta) \\,, \\end{align*}\\] where $n_{\\min} = \\min_{s,a} n(s,a)$. Plugging the obtained deviation bound into our policy suboptimality bound, we get that with probability $1-\\zeta$, . \\[\\begin{align*} \\delta \\le (1+2\\gamma) H_\\gamma \\varepsilon + 2 H_\\gamma^2 (1+\\gamma H_\\gamma) \\beta(n_{\\min},\\zeta) \\,. \\end{align*}\\] One can alternatively write this in terms of the total number of observations, $n$. The best case is when $n(s,a)=n_{\\min}$ for all $(s,a)$ pairs, in which case $n = \\mathrm{SA} n_{\\min}$ and the above bound gives . \\[\\begin{align*} \\delta \\le (1+2\\gamma) H_\\gamma \\varepsilon + 2 H_\\gamma^2 (1+\\gamma H_\\gamma) \\sqrt{ \\mathrm{SA} \\frac{\\log\\left(\\frac{\\mathrm{SA}}{\\zeta}\\right)}{2 n}} \\,. \\end{align*}\\] It follows that for any target suboptimality $\\delta_{\\text{trg}}$, as long as $n$, the number of observations satisfies . \\[n \\ge \\frac{8 H_\\gamma^6 SA \\log\\left(\\frac{\\mathrm{SA}}{\\zeta}\\right)}{\\delta_{\\text{trg}}^2}\\,,\\] we are guaranteed that the optimal policy of the estimated model is at most $\\delta_{\\text{trg}}$ suboptimal. As we shall see soon, the optimal dependence on the horizon $H_\\gamma$ is cubic, unlike the dependence shown here. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec17/#model-estimation-error-tabular-case",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec17/#model-estimation-error-tabular-case"
  },"158": {
    "doc": "17. Introduction",
    "title": "Notes",
    "content": "Between batch and online learning . In applications it may happen that one can change the data collection strategy a limited number of times. This creates a scenario that is in between batch and online learning. This setting can be thought to be between batch and online learning. From the perspective of online learning, this is learning in the presence of constraints on the data collection strategy. One such widely studied constraint is the number of switches of the data collection strategy. As it happens, only very few switches are necessary to get the full power of online learning and this is not really specific to reinforcement learning but follows because the empirical distribution converges are a slow rate to the true distribution. For parametric problems, the rate is $O(1/\\sqrt{n})$ where $n$ is the number of observations. Thus, to change “accuracy” of the estimates of any quantity in a significant fashion, the sample size should increase by much, which means, few changes to the data collection are sufficient. In other words, there is no reason to change the data collection strategy before one obtains sufficient new evidence that can help with deciding in what way the data collection strategy should be changed. This usually means that with only logarithmically many changes in the total sample size, one gets the full power of online methods. Batch RL with no access to state information . For simplicity, we stated the batch learning problem in a way that assumes that the states in the transitions are observed. This may be seen as problematic. One “escape” is to treat the whole history as the state: Indeed, in a causal, controlled stochastic process, the history can always be used as a Markov state. Because of this, the assumption that the state is observed is not restrictive, though the state space becomes exponential in the length of the trajectories. This reduces to the problem to learning in large state-space MDPs. Of course, even lower bounds for planning tell us that in lack of extra structure, all algorithms need a sample size proportional to the size of the state-action space, hence, one needs to add extra structure to deal with this case, such as function approximation. It also holds that if one uses, say, linear function approximation, then only the features of the states (or state-action pairs) need to be recorded in the data. Causal reasoning and batch RL . Whether a causal effect can be learned from a batch of data (to be more precise, from data drawn from a specific distribution) is the topic of causal reasoning. In batch RL, the “effect” is the value of a policy, which, in the language of causal reasoning, would be called a multistage treatment. As the example in the text shows, in batch RL, just because of our assumptions on how the data is collected, the identifiability problem is just “assumed away”. When the assumption on how the data is generated/collected is not met, the tools of causal reasoning can potentially be still used. It is important to emphasize though that there is no causality without assuming causality. The statements that causal reasoning can make are conditional on the data sampling assumptions met. Even “causal discovery” is contingent on these assumptions. However, with care, oftentimes it is possible to argue for that some suitable assumptions are met (e.g., arguing based on what information is available at what time in a process), in which case, the nontrivial tools of causal reasoning may be very useful. Nevertheless, especially in engineered systems, our standard data collection assumptions are reasonable and can be arranged for, though in large engineered systems, mistakes, such as not logging critical quantities may happen. One example of this is an action to be taken is overriden by some part of a system, which will, say, later be turned off. Clearly, if no one logs the actual actions taken, the effects of actions become unidentifiable. As we shall see later, batch RL and the causality literature share some of their vocabulary, such as “instrumental variables”, “propensity scores”, etc. Plug-in or certainty equivalence . Plug-in generally means that a model is estimated and then is used as if it was the “true” model. In control, when a controller (policy) is derived with this approach, this is known as the “certainty equivalence” controller. The “certainty equivalence principle” states that the “random” errors can be neglected. The principle originates from the observation that in various scenarios, the optimal controller (optimal policy) has a special form that confirms this principle. In particular, this was first observed in the control of linear quadratic Gaussian control, where the optimal controller can be obtained by solving for the optimal control under perfect state information then substituting optimal state prediction for the the perfect state information. This strict optimality result is quite brittle. As we shall see soon, from the perspective of minimax optimality, certainty equivalent policies are not a bad choice. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec17/#notes",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec17/#notes"
  },"159": {
    "doc": "17. Introduction",
    "title": "Bibliographic remarks",
    "content": "In the early RL literature, online learning was dominant. When people tried to apply RL to various “industrial”/”applied” settings, they were forced to think about how to learn from data collected before learning starts. One of the first papers to push this agenda is the following one: . | Tree-Based Batch Mode Reinforcement Learning Damien Ernst, Pierre Geurts, Louis Wehenkel; 6(18):503−556, 2005. | . Earlier mentions of “batch-mode RL” include . | Efficient Value Function Approximation Using Regression Trees (1999) by Xin Wang , Thomas G. Dietterich, Proceedings of the IJCAI Workshop on Statistical Machine Learning for Large-Scale Optimization. pdf | . Even in online learning, efficient learning may force one to save all the data to be used for learning. The so-called LSTD algorithm, and later the LSPI algorithm, were explicitly proposed to address this challenge: . | J. A. Boyan. Technical update: least-squares temporal difference learning. Machine Learning, 49 (2-3):233–246, 2002. | M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107–1149, 2003a. | . Off-policy learning refers to the case when an algorithm needs to produce value function (or action-value function) estimates for some policy and the data available is not generated by the policy to be evaluated. In all the above examples, we are thus in the setting of off-policy learning. The policy evaluation problem, accordingly, is often called the off-policy policy evaluation (OPPE) problem, while the problem of finding a good policy is called the off-policy policy optimization (OPPO) problem. For a review of the literature of around 2012, consult the following paper: . | S. Lange, T. Gabel, M. Riedmiller (2012) Batch Reinforcement Learning. In: M. Wiering, M. van Otterlo (eds) Reinforcement Learning. Adaptation, Learning, and Optimization, vol 12. Springer, Berlin, Heidelberg pdf | . ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec17/#bibliographic-remarks",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec17/#bibliographic-remarks"
  },"160": {
    "doc": "17. Introduction",
    "title": "17. Introduction",
    "content": "PDF Version . Batch learning is concerned with problems when a learning algorithm must work with data collected in some manner that is not under the control of the learning algorithm: on a batch of data. In batch RL the data is given in the form of a sequence of trajectories of varying length, where each trajectory is of the form $\\tau=(S_0,A_0,R_0,S_1,A_1,R_1,\\dots,S_t,A_t,R_t,S_{t+1})$, where $A_i$ is chosen in a causal fashion (based on “past” data), $(R_t,S_{t+1})\\sim Q_{A_t}(S_t)$, where $Q = (Q_a(s))_{s,a}$ is a collection of probability distributions over pairs of reals and states, as usual (when we want to allow stochastic rewards). Batch RL problems fall into two basic categories: . | Value prediction: Predict the value $\\mu v^\\pi$ of using a policy $\\pi$ from the initial distribution $\\mu$, where both $\\mu$ and $v^\\pi$ are given in an explicit form. | Policy optimization: Find a good (ideally, near optimal) policy given the batch of data from an MDP. | . These two problems are intimately related. On the one hand, a good value predictor can potentially be used to find good policies. On the other hand, a good policy optimizer can also be used to decide about whether the value of some policy is above or below some fixed threshold by appropriately manipulating the data fed to the policy optimizers. One can then put a binary search procedure around this decision routine to find out the value of some policy. Value prediction problems have some common variations. In policy evaluation, rather than evaluating a policy for some fixed initial distribution, the goal is to estimate the entire value function of the policy. Of course, this is at least as hard as the simpler, initial value estimation problem. However, much of the hardness of the problem is already captured by the initial value estimation problem. In initial value prediction, oftentimes the goal is to predict an interval that contains the true unknown value with a prescribed probability, rather than just producing a “point estimate”. In the case of policy evaluation, the analogue is to predict a set that contains the true unknown value function with a prescribed probability. Here, a simpler goal is to estimate confidence intervals for each potential input (state), which when “pasted together” can be visualized as forming a confidence band. There is also the question of how to collect data. In statistics, the problem of designing a “good way” of collecting the data is called the experimental design problem. The best is of course, if data can be collected in an active manner: This is when the data collection strategy changes in response to what data has been collected so far. The problem of designing good active data collection strategies belongs to the bigger group of designing online learning algorithms. These are defined exactly based on that the data is collected in a way that depends on what data has been previously collected. The last segment of the part will be solely devoted to these online learning strategies. In many applications, active data collection is not an option. There can be many reasons for this: active data collection may be deemed to be risky, expensive, or just technically challenging. When data is collected in a passive fashion, it may simply miss key information that would allow for good solutions. Still, in this case, there may be better and worse ways collecting data. Optimizing experimental designs is the problem of choosing good passive data collection strategies that lead to good learning outcomes. This topic came up in the context planning algorithms as they also need to create value function estimates and for this the data collection is better to be planned so that learning can succeed. Oftentimes though, there is no control over how data is collected. Even worse, the method that was used to collect data may be unknown. When this is the case, not much can be done, as the following example shows: . Consider a bandit problem with two actions, denoted by ${0,1}$ and a Bernoulli reward. Assume that the reward distribution is Bernoulli with parameter $0.1$ when $a=1$ and Bernoulli with parameter $0.9$ when $a=0$. Let $Z$ be a random variable, which is normally unavailable, but which, together with the action $a$ taken completely determines the reward. For example, $Z$ could have a Bernoulli distribution with parameter $p=0.1$, and if action $a$ is chosen, the reward $R(a)$ obtained is . \\[\\begin{align*} R(a) = aZ + (1-a)(1-Z)\\,. \\end{align*}\\] This is indeed consistent with that $R(a)$ has Bernoulli $0.1$ distribution when $a=1$ and has Bernoulli $0.9$ distribution when $a=0$. Assume now that during data collection the actions are chosen based on $Z$: $A=\\pi(Z)$ with some $\\pi$. For concreteness, assume that during data collection $A=Z$. Then, the action is random, yet, if the data is composed of pairs that have the distribution shared by $(A,R(A))$, or $(Z,1)$, clearly no method will be able to properly estimate the mean of $R(0)$ or $R(1)$, let alone choosing the action that leads to a higher reward. It is not hard to construct examples when the conditional mean of the observed data makes an optimal action look worse than a suboptimal action. This is an example where the correct model cannot be estimated because of the way data is collected: The presence of the spurious correlation between a variable that controls outcomes but is not recorded can easily make the data collected useless, regardless of quantities. This is an instance when the model is unidentifiable even with an infinite amount of data. When data collection is as arbitrary as in the above example, only a very careful study of the domain can tell us whether the model is identifiable or not from the data. Note that this is an activity that involves thinking about the structure of the problem at hand. The best is of course if data collection can be influenced to avoid building up spurious correlations. When data is collected in a causal way (following a policy, while recording both the decisions made and the data is used to make those decisions), spurious correlations are avoided and the remaining problem is to guarantee sufficient “coverage” to achieve statistical efficiency. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec17/",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec17/"
  },"161": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "Improved analysis of the plug-in method: First attempt",
    "content": "The improvement in the analysis of the plug-in method comes from two sources: . | Using a version of the value-difference identity and avoiding the use of the policy error bound | Using Bernstein’s inequality in place of Hoeffding’s inequality | . In this section, we focus on the first aspect. The second aspect will be considered in the next section. We continue to use the same notation as in the previous lecture. In particular, $M$ denotes the “true” MDP, $\\hat M$ denotes the estimated MDP and we put $\\hat\\cdot$ on quantities related to this second MDP. We further let $\\pi^*$ be one of the memoryless optimal policies of $M$. For simplicity, we will assume that the reward function in $\\hat M$ is the same as in $M$: As we have seen, the higher order term in our error bound came from errors in the transition probability; the simplifying assumption allows us to focus on reducing this term while minimizing clutter. The arguments are easy to extend to the case when $\\hat r\\ne r$. Let $\\hat \\pi$ be a policy whose suboptimality in $M$ we want to bound. The idea is to bound the suboptimality of $\\hat \\pi$ by its suboptimality in $\\hat M$ and also by how much value functions for fixed policies differ when we switch from $P$ to $\\hat P$. In particular, we have . \\[\\begin{align} v^* - v^{\\hat \\pi} &amp; = v^* - \\hat v^* \\, + \\, \\hat v^* - v^{\\hat \\pi} \\nonumber \\\\ &amp; \\le v^{\\pi^*} - \\hat v^{\\pi^*} \\, + \\, \\underbrace{\\hat v^*-\\hat v^{\\hat \\pi}}_{\\text{opt. error}} \\, + \\, \\hat v^{\\hat \\pi} - v^{\\hat \\pi}\\,, \\label{eq:subbd} \\end{align}\\] where \\(\\hat \\pi^{*}\\) denotes an optimal policy in \\(\\hat M\\) and the inequality holds because \\(\\hat v^{*}=\\hat v^{\\hat \\pi^{*}}\\ge \\hat v^{\\pi^{*}}\\). The term marked as “opt. error” is the optimization error that arises when $\\hat \\pi$ is not (quite) optimal in $\\hat M$. This term is controlled by the choice of $\\hat \\pi$. For simplicity, assume for now that $\\hat \\pi$ is an optimal policy in $\\hat M$, so that we can drop this term. We further assume that $\\hat \\pi$ is a deterministic optimal policy of $\\hat M$. It remains to bound the first and last terms. Both of these terms have the form $v^\\pi - \\hat v^\\pi$, i.e., the difference between the value functions of the same policy $\\pi$ in the two MDPs (here, $\\pi$ is either $\\pi^*$ or $\\hat\\pi$). This difference, similar to the value difference identity, can be expressed as a function of the difference $P-\\hat P$, as shown in the next result: . Lemma (value difference from transition differences): Let $M$ and $\\hat M$ be two MDPs sharing the same state-action space, rewards, but differing in their transition probabilities. Let $\\pi$ be a memoryless policy over the shared state-action space of the two MDPs. Then, the following identities holds: . \\[\\begin{align} v^\\pi - \\hat v^\\pi &amp; = \\gamma \\underbrace{(I-\\gamma P_\\pi)^{-1} M_\\pi (P-\\hat P) \\hat v^{\\pi}}_{\\delta(\\hat v^\\pi) }\\,, \\label{eq:vdpd} \\\\ \\hat v^\\pi - v^\\pi &amp; = \\gamma \\underbrace{(I-\\gamma \\hat P_\\pi)^{-1} M_\\pi (\\hat P- P) v^{\\pi}}_{\\hat{\\delta}(v^\\pi) }\\,. \\label{eq:vdpd2} \\end{align}\\] . Proof: We only need to prove \\eqref{eq:vdpd} since \\eqref{eq:vdpd2} follows from this identity by symmetry. Concerning the proof of \\eqref{eq:vdpd}, we start with the closed form expression for value functions. From this we get . \\[\\begin{align*} v^\\pi - \\hat v^\\pi = (I-\\gamma P_\\pi)^{-1} r_\\pi - (I-\\gamma \\hat P_\\pi)^{-1} r_\\pi\\,. \\end{align*}\\] Inspired by the elementary identity that states that $\\frac{1}{1-x} - \\frac{1}{1-y} = \\frac{x-y}{(1-x)(1-y)}$, we calculate . \\[\\begin{align*} v^\\pi - \\hat v^\\pi &amp; = (I-\\gamma P_\\pi)^{-1} \\left[(I-\\gamma \\hat P_\\pi)-(I-\\gamma P_\\pi) \\right](I-\\gamma \\hat P_\\pi)^{-1} r_\\pi \\\\ &amp; = \\gamma (I-\\gamma P_\\pi)^{-1} \\left[P_\\pi -\\hat P_\\pi\\right](I-\\gamma \\hat P_\\pi)^{-1} r_\\pi \\\\ &amp; = \\gamma (I-\\gamma P_\\pi)^{-1} M_\\pi \\left[P -\\hat P\\right] \\hat v^\\pi \\,, \\end{align*}\\] finishing the proof. \\(\\qquad \\blacksquare\\) . Note that in \\eqref{eq:vdpd2}, the empirical transition kernel $\\hat P$ appears through its inverse by left-multiplying $M_\\pi (\\hat P-P)$, while in \\eqref{eq:vdpd}, through $\\hat v^\\pi$, it appears by right-multiplying the same deviation term. In the remainder of this section we use \\eqref{eq:vdpd2}, but in the next section we will use \\eqref{eq:vdpd}. Combining \\eqref{eq:vdpd2} with our previous inequality, we immediately get that . \\[\\begin{align} v^* - v^{\\hat \\pi} &amp; \\le \\frac{\\gamma}{1-\\gamma} \\left[ \\|(P-\\hat P) v^{\\pi^*}\\|_\\infty + \\|(P-\\hat P) v^{\\hat \\pi}\\|_\\infty \\right]\\,. \\label{eq:vdeb} \\end{align}\\] Assume that $\\hat P$ is obtained by sampling $m$ next states at each state-action pair. By Hoeffding’s inequality and a union bound over the state-action pairs, for any fixed $v\\in [0,H]^{SA}$ and $0\\le \\zeta &lt;1$, with probability $1-\\zeta$, we have . \\[\\begin{align} \\|(P-\\hat P) v \\|_\\infty = H\\sqrt{\\frac{\\log(SA/\\zeta)}{2m}}\\, \\label{eq:vbound} \\end{align}\\] and in particular with $v=v^{\\pi^*}$, we have . \\[\\|(P-\\hat P) v^{\\pi^*} \\|_\\infty = \\tilde O\\left( H/ \\sqrt{m} \\right) \\,.\\] Controlling the second term in \\eqref{eq:vdeb} requires more care as $\\hat \\pi$ is random and depends on the same data that is used to generate $\\hat P$. To deal with this term, we use another union bound. Let \\(\\tilde V = \\{ v^\\pi \\,:\\, \\pi: \\mathcal{S} \\to \\mathcal{A}\\}\\) be the set of all possible value functions that we can obtain by considering deterministic policies. Since by construction $\\hat \\pi$ is also a deterministic policy, \\(\\hat v^{\\hat \\pi}\\in \\tilde V\\). Hence, . \\[\\|(P-\\hat P)\\hat v^{\\hat \\pi} \\|_\\infty \\le \\sup_{v\\in \\tilde V}\\|(P-\\hat P)v \\|_\\infty\\,.\\] and thus by a union bound over the $|\\tilde V|\\le A^S$ functions $v$ in $\\tilde V$, we get that with probability $1-\\zeta$, . \\[\\begin{align*} \\|(P-\\hat P)\\hat v^{\\hat \\pi} \\|_\\infty &amp; \\le H\\sqrt{\\frac{\\log(SA|\\tilde V|/\\zeta)}{2m}} = H\\sqrt{\\frac{\\log(SA/\\zeta)+S \\log(A)}{2m}} = \\tilde O\\left(H\\sqrt{S/m}\\right)\\,. \\end{align*}\\] Putting things together, we see that . \\[v^* - v^{\\hat \\pi} = \\tilde O\\left(H^2\\sqrt{S/m}\\right)\\,,\\] which reduces the dependence on $H$ of the sample size bound from $H^6$ to $H^4$. As we shall see soon, this is not the best possible dependence on $H$. This method also falls short of giving the best possible dependence on the number of states. In particular, inverting the above bound, we see that with this method we can only guarantee a $\\delta$-optimal policy if the total number of samples, $n=SA m$ is at least . \\[\\tilde O( S^2 A H^4/\\delta^2)\\,\\] while below we will see that the optimal bound is $\\tilde O(SA H^3/\\delta^2)$. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec18/#improved-analysis-of-the-plug-in-method-first-attempt",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec18/#improved-analysis-of-the-plug-in-method-first-attempt"
  },"162": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "Improved analysis of the plug-in method: Second attempt",
    "content": "There are two further ideas that help one achieve the sample complexity which will be seen to be optimal. One is to use what is known as Bernstein’s inequality in place of Hoeffding’s inequality, together with a clever observation on the “total variance” and the second is to improve the covering argument. The first idea helps with improving the horizon dependence, the second helps with improving the dependence on the number of states. In this lecture, we will only cover the first idea and sketch the second. Bernstein’s inequality is a classic result in probability theory: . Theorem (Bernstein’s inequality): Let $b&gt;0$ and let $X_1,\\dots,X_m\\in [0,b]$ be an i.i.d. sequence and define $\\bar X_m$ as the sample mean of this sequence: $\\bar X_m = \\frac{1}{m}(X_1+\\dots+X_m)$. Then, for any $\\zeta\\in (0,1)$, with probability at least $1-\\zeta$, . \\[|\\bar X_m - \\mathbb{E}[X_1]| \\le \\sigma \\sqrt{ \\frac{2\\log(2/\\zeta)}{m}} + \\frac{2}{3} \\frac{b \\log(2/\\zeta)}{m}\\,,\\] where $\\sigma^2 = \\text{Var}(X_1)$. To set expectations, it will be useful to compare this bound to Hoeffding’s inequality. In particular, in the setting of the lemma Hoeffding’s inequality also applies and gives . \\[|\\bar X_m - \\mathbb{E}[X_1]| \\le b \\sqrt{ \\frac{\\log(2/\\zeta)}{2m}}\\,.\\] Since in our case $b=H$ (the value functions take values in the $[0,H]$ interval), using $m=H^3/\\delta^2$ (which would give rise to the optimal sample size), Hoeffding’s inequality gives a bound of size $H H^{-3/2}\\delta = H^{-1/2}\\delta$ (cf. \\eqref{eq:vbound}). This is a problem: Ideally, we would like to see $H^{-1}\\delta$ here, because inequality \\cref{eq:vdeb} introduces an additional $H$ factor. We immediately see that for Bernstein’s inequality to make a difference, just focusing on the first term in Bernstein’s inequality, we need $\\sigma=O(H^{1/2})$. In fact, since $b/m=H^{-2}\\delta^2 = o(H^{-1}\\delta)$, we see that this is also sufficient to take off the $H$ factor from the sample complexity bound. It thus remains to be seen whether the variance could indeed be this small. To find this out, fix a state-action pair $(s,a)$ and let $S_1’,\\dots,S_m’ \\sim P_a(s)$ be an i.i.d. sequence of next states at $(s,a)$. Then, $((\\hat P-P) v^\\pi)(s,a)=(\\hat P_a(s)-P_a(s))v^\\pi$ has the same distribution as . \\[\\Delta(s,a)=\\frac{1}{m} \\sum_{i=1}^m v^\\pi(S_i') - P_a(s) v^{\\pi}\\,.\\] Defining $X_i = v^\\pi(S_i’)$ and \\(\\sigma^2_\\pi(s,a)=\\mathrm{Var}(X_1)\\), we see that it is $\\sigma_\\pi(s,a)$ that appears when Bernstein’s inequality is used to bound $((\\hat P-P) v^\\pi)(s,a)$. It remains to be seen how large values $\\sigma_\\pi(s,a)$ can take on. Sadly, one can quickly discover that the range of $\\sigma_\\pi(s,a)$ is sometimes also as large as $H$. Is Bernstein’s inequality a dead-end then? . Of course, it is not, otherwise we would not have introduced it. In particular, a better bound is possible by directly bounding the maximum-norm of . \\[\\delta(v^\\pi) = (I-\\gamma P_\\pi)^{-1} M_\\pi (P-\\hat P) v^{\\pi} \\,,\\] which is close to the actual term that we need to bound. Indeed, by \\cref{\\eqref{eq:vdpd}} from the value difference lemma, $v^\\pi - \\hat v^\\pi = \\gamma \\delta(\\hat v^\\pi)$ and thus . \\[v^\\pi - \\hat v^\\pi = \\gamma \\delta(v^\\pi) + \\gamma( \\delta(\\hat v^\\pi)-\\delta(v^\\pi))\\,.\\] The second term on the right-hand side is of order $1/m$ (since $(P-\\hat P)(\\hat v^\\pi-v^\\pi)$ appears there and both $P-\\hat P$ and $\\hat v^\\pi - v^\\pi$ have been seen to be of order $1/\\sqrt{m}$). As we expect $\\delta(v^\\pi)$ to be of order $1/\\sqrt{m}$, we will focus on this term. For simplicity, take now the case when $\\pi$ is a fixed, nonrandom policy (we need to bounded $\\delta(v^\\pi)$ for $\\pi=\\pi^*$ and also for $\\pi=\\hat \\pi$, the second of which is random). In this case, by a union bound and Bernstein’s inequality, with probability $1-\\zeta$, . \\[\\begin{align*} |(P-\\hat P) v^{\\pi}| &amp; \\le \\sqrt{ \\frac{2\\log(2SA/\\zeta)}{m} } \\sigma_\\pi + \\frac{2H}{3} \\frac{\\log(2/\\zeta)}{m} \\boldsymbol{1}\\,. \\end{align*}\\] Multiplying both sides by $(I-\\gamma P_\\pi)^{-1} M_\\pi$, using a triangle inequality and the special properties of $(I-\\gamma P_\\pi)^{-1} M_\\pi$, we get . \\[\\begin{align} |\\delta(v^\\pi)| &amp; \\le (I-\\gamma P_\\pi)^{-1} M_\\pi |(P-\\hat P) v^{\\pi}| \\nonumber \\\\ &amp; \\le \\sqrt{ \\frac{2\\log(2SA/\\zeta)}{m} } (I-\\gamma P_\\pi)^{-1} M_\\pi \\sigma_\\pi + \\frac{2H^2}{3} \\frac{\\log(2SA/\\zeta)}{m} \\boldsymbol{1}\\,. \\label{eq:dvpib} \\end{align}\\] The following beautiful result, whose proof is omitted, gives an $O(H^{3/2})$ bound on the first term appearing on the right-hand side of the above display: . Lemma (total discounted variance bound): For any discounted MDP $M$ and policy $\\pi$ in $M$, \\(\\| (I-\\gamma P_\\pi)^{-1} M_\\pi \\sigma_{\\pi} \\|_\\infty \\le \\sqrt{ \\frac{2}{(1-\\gamma)^3}}\\,.\\) . Since the bound that we get from here is $H^{3/2}$ and not $H^2$, “we are saved”. Indeed, plugging this into \\eqref{eq:dvpib} gives . \\[\\| \\delta(v^\\pi) \\|_\\infty \\le 2\\sqrt{ \\frac{H^3\\log(2SA/\\zeta)}{m} } + \\frac{2H^2}{3} \\frac{\\log(2SA/\\zeta)}{m} \\,,\\] which holds with probability $1-\\zeta$. Choosing $m=H^3/\\delta^2$, we see that both terms are $O(\\delta)$. It remains to show that a similar result holds for $\\pi = \\hat \\pi$. If we use the union bound that we used before, we introduce an extra $S$ factor. Avoiding this extra $S$ factor requires new ideas, but with these we get the following result: . Theorem (upper bound for $Z$-designs): Let $\\hat \\pi$ be an optimal policy in the MDP whose transition kernel is $\\hat P$, a kernel estimated based on a sample of $m$ next states from each state-action pair. Letting $0\\le \\zeta &lt;1$ and $0\\le \\delta \\le \\sqrt{H}$, if . \\[m \\ge \\frac{ c \\gamma H^3 \\log(SAH/\\delta) }{\\delta^2}\\] then with probability $1-\\zeta$, $\\hat \\pi$ is $\\delta$-optimal, where $c$ is a universal constant. In short, for any $0\\le \\delta \\le \\sqrt{H}$ there exist an algorithm that produces a $\\delta$-optimal policy from a total number of . \\[\\tilde O\\left( \\frac{ \\gamma SA H^3 }{ \\delta^2 } \\right)\\] samples under a uniform $Z$-design. It remains to be seen whether the same sample complexity holds for larger values of $\\delta$, e.g., for $\\delta = H/2$. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec18/#improved-analysis-of-the-plug-in-method-second-attempt",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec18/#improved-analysis-of-the-plug-in-method-second-attempt"
  },"163": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "Lower bound for $Z$-designs",
    "content": "A natural question is whether we can improve on the $H^3 SA/\\delta^2$ upper bound, or whether this can be matched by a lower bound. For this, we have the following result: . Theorem (lower bound for $Z$-designs): Any algorithm that uses $Z$-designs and is guaranteed to produce a $\\delta$-optimal policy needs at least $\\Omega( H^3 SA/\\delta^2)$ samples. Proof (sketch): As we have seen in the proof of the upper bound, the key to achieve the cubic dependence was that the sample mean $\\bar X_m$ of $m$ i.i.d. bounded random variables is within a distance of $\\sigma \\sqrt{1/m}$ to the true mean. In a way, the converse of this is also true: It is “quite likely” that the distance between the sample and true mean is this large. This is not too hard to see for specific distributions, such as when the $X_i$ are normally distributed, or when $X_i$ are Bernoulli distributed (in a way, this is the essence of the central-limit theorem, though the central-limit theorem is restricted for $m\\to\\infty$). So how can we use this to establish the lower bound? In an MDP the randomness comes either from the rewards or the transitions. But in the upper bound above, the rewards were given, so the only source of randomness is transitions. Also, the cubic dependence must hold even if the number of states is a constant. What all this implies is that somehow learning the transition structure with a few states is what makes the sample complexity large as $\\gamma\\to 1$ (or $H\\to \\infty$). Clearly, this can only happen if the (small) MDP has self-loops. The smallest example of an MDP with a self-loop is if one has an action and state such that taking that action from that state leads to same action with some positive probability, while with the complementary probability the next state is some other state. This leads to the structure shown on the figure on the right. As can be seen, there are two states. The transition at the first state, call it state $1$, is stochastic and leads to itself with probability $p$, while it leads to state $2$ with probability $1-p$. The reward associated with both transitions is $1$. The second state, call it state $2$, has a self-loop. The reward associated with this transition is zero. There are no actions (alternatively, there is only one action at both states). However, if we can show that in the lack of knowledge of $p$, estimating the value of state $1$ up to a precision of $\\delta$ takes $\\Omega(H^3/\\delta^2)$ samples, the sample complexity result will follow. In particular, if we repeat the transition structure $A$ times (sharing the same two states), one can make the value of $p$ for one of this actions ever slightly so different from the others so that its value differs by (say) $2\\delta$ from the others. Then, by construction, a learner who uses fewer than $\\Omega(A H^3/\\delta^2)$ total samples at state $1$ will not be able to reliably tell the difference between the value of the special action and the other actions, hence, will not be to choose the right action and will thus be unable to produce a $\\delta$-optimal policy. To also add the state dependence, the structure can then be repeated $S$ times. So it remains to be seen whether the said sample complexity result holds for estimating the value of state $1$. Rather than giving a formal proof, we give a quick heuristic argument, hoping that readers will find this more intuitive. The starting point for this heuristic argument is the general observation that sample complexity questions concerning estimation problems are essentially questions about the sensitivity of the quantity to be estimated to the unknown parameters. Here, sensitivity means how much the quantity changes if we change the underlying parameter. This sensitivity for small deviations and a single parameter is exactly the derivative of the quantity of interest with respect to the parameter. In our special case, the value of state $1$, call it $v_p(1)$ (also showing the dependence on $p$) is the quantity to be estimated. Since the value of state $2$ is zero, $v_p(1)$ must satisfy $v_p(1)=p(1+\\gamma v_p(1))+(1-p)1$. Solving this we get . \\[v_p(1) = \\frac{1}{1-p\\gamma}\\,.\\] The derivative of this with respect to $p$ is . \\[\\frac{d}{d p} v_p(1) =\\frac{\\gamma}{(1-\\gamma p)^2}\\,.\\] To get a $\\delta$-accurate estimate of $v_{p_0}(1)$, we need . \\[\\begin{align*} \\delta &amp; \\ge |v_{p_0}(1)-v_{\\bar X_m}(1)| \\approx \\frac{d}{dp} v_p(1)|_{p=p_0} |p_0-\\bar X_m| = \\frac{\\gamma}{(1-\\gamma p_0)^2} |p_0-\\bar X_m|\\\\ &amp; \\approx \\frac{\\gamma}{(1-\\gamma p_0)^2} \\sqrt{ \\frac{p_0(1-p_0)}{m}}\\,. \\end{align*}\\] Inverting for $m$, we get that . \\[m \\gtrsim \\frac{\\gamma^2 p_0(1-p_0)}{(1-\\gamma p_0)^4 \\delta^2}\\,.\\] It remains to choose $p_0$ as a function of $\\gamma$ to show that the above can be lower bounded by $1/(1-\\gamma)^3$. If we choose $p_0=\\gamma$, we have $1-\\gamma p_0 = 1-\\gamma^2 = (1-\\gamma)(1+\\gamma)\\le 2(1-\\gamma)$ and hence . \\[\\frac{\\gamma^2 p_0(1-p_0)}{(1-\\gamma p_0)^4 \\delta^2} \\ge \\frac{\\gamma^2 \\gamma(1-\\gamma)}{2^4(1-\\gamma)^4 \\delta^2} = \\frac{\\gamma^3 }{2^4(1-\\gamma)^3 \\delta^2}\\,.\\] Putting things together finishes the proof sketch. \\(\\qquad \\blacksquare\\) . A homework problem is included which explains how to fill in the gaps in the last section of the proof, while pointers to the literature are given that one can use to figure out how to fill the remaining gaps. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec18/#lower-bound-for-z-designs",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec18/#lower-bound-for-z-designs"
  },"164": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "Policy-based designs",
    "content": "When the data is generated by following some policy, we talk about policy based designs. Here, the design decision is what policy to use to generate the data. The sample complexity of learning with policy based designs is the number of observations necessary and sufficient for some algorithm to figure out a policy of a fixed target suboptimality, from a fixed initial state, based on data generated by following a policy where the MDP where the policy is followed can be any of the MDPs within the class. Three questions arise then. (i) The first question (the design question) is what policy to follow during data collection. If the policy can use the full history, the problem is not much different than online learning, which we will consider later. From this perspective the interesting (and perhaps more realistic) case is when the data-collection policy is memoryless and is fixed before the data collection begins. Hence, in what follows, we will restrict our attention to this case. (ii) The second question is what algorithm to use to compute the policy given the data generated. (iii) The final, third question is how large is the sample complexity of learning with policy induced data for some fixed MDP class. Learning and estimating a good policy from policy induced data is much closer to reality than the same problem from $Z$-designs. Practical problems, such as problems in health care, robotics, etc., are so that we can obtain data generated by following some fixed policy, while it is usually not possible to demand obtaining sample transitions from arbitrary state-action pairs. For simplicity, let us still consider the case of finite state-action MDPs, but to further simplify matters, let us now consider (homogenous kernel) finite horizon problems with a horizon $H$. As it turns out, the plug-in algorithm of the previous section is still a good algorithm in the sense that it achieves the optimal (minimax) sample complexity. However, the minimax sample complexity is much higher than it is for $Z$-designs: . Theorem (sample complexity lower bound with policy induced data): For any $S,A,H$, $0\\le \\delta$, any (memoryless) data collection policy $\\pi$ over the state-action spaces $[S]$ and $[A]$, for any $n\\le c A^{\\min(S-1,H)}/\\delta^2$ and any algorithm $\\mathcal{L}$ that maps data that has $n$ transitions to a policy there exist an MDP $M$ with state space $[S]$ and action space $[A]$ such that with constant probability, the policy $\\hat \\pi$ produced by $\\mathcal{L}$ is not $\\delta$-optimal with respect to the $H$-horizon total reward criterion when the algorithm is fed with data by following $\\pi$ in $M$. Proof (sketch): Without loss of generality assume that $S=H+1$; if there are more states, just ignore them, while if there are fewer states then just decrease $H$. Consider an MDP where states ${1,\\dots,H}$ are organized in a chain under the effect of some actions, and state $H+1$ is an absorbing state with zero associated reward. For $1\\le i \\le H$, let action $a_i$ be the one that gets to be chosen with the smallest probability in state $i$ under the data generating policy $\\pi$: $a_i = \\arg\\min_{a\\in [A]} \\pi(a|i)$. We choose action $a_i$ as the action that moves the state from $i$ to $i+1$, deterministically. Any other action leads to state $H+1$, deterministically. All rewards are zero, except when transitioning from state $H$ to state $H+1$ under action $a_H$, where the reward is stochastic with a normal distribution with mean $\\mu$ either $-2\\delta$ or $+2\\delta$ and a variance of one. The structure of the MDP is shown on the figure in the left-hand side. Now, because of the choice of $a_i$, $\\pi(a_i|i)\\le 1/A$. Hence, the probability that starting from state $1$, following policy $\\pi$ for $H$ steps will generate the sequence of states $1,2,\\dots,H,H+1$, including the critical transition from state $H$ to state $H+1$, is at most $(1/A)^H$. This transition is critical in the sense is that only data from this transition decides whether in state $1$ it is worth taking action $a_1$ or not. In particular, if $\\mu=-2\\delta$, taking $a_1$ is a poor choice, while if $\\mu=2\\delta$, taking $a_1$ is the optimal choice. The expected number of times this critical transition is seen is at most $m=n(1/A)^H$. With $m$ observations, the value of $\\mu$ will be estimated up to an accuracy of $O(\\sqrt{1/m})$. When this is smaller than $2\\delta$, with constant probability, the sign of $\\mu$ cannot be decided and thus with constant probability, any algorithm will fail to identify whether $a_1$ should be taken in state $1$ or not (with a probability, of say, at least $1/2$). Plugging in the expected value of $m$, we get that the condition on $n$ is that $\\sqrt{c A^H/n}\\le 2\\delta$ where $c&gt;0$ is some universal constant. Equivalently, the condition is that $n\\ge c A^H/(4\\delta^2)$, which is the statement to be proven. \\(\\qquad \\blacksquare\\) . The lower bound construction suggests that the best policy to be used in the lack of extra information about the MDPs is the uniform policy. Note that a similar statement holds for the discounted setting. The contrast between this lower bound and the polynomial upper bound of the previous section are in strike contrast: Data obtained from following policies can be very poor. One may wonder whether the situation can be improved assuming that the data is obtained from a good policy (say, $2\\delta$ optimal policy), but the proof of the previous result in fact shows that this is not the case. While the exponential lower bound on the sample complexity of learning from policy induced data is already bad enough, one may worry that the situation could be even worse. Could it happen that even the best algorithm needs double exponential number of samples? Or even infinite? A moment of thought shows that the latter is the case is switch to the average reward setting: This is because in the average reward setting the value of an action can depend on the value of a state whose hitting probability within an arbitrary fixed number of transitions is positive, just arbitrarily low. Can something similar happen perhaps in the finite-horizon setting, or the discounted setting? As it turns out, the answer is no. The previous lower bound gives the correct order of the sample complexity of finding a near-optimal policy using policy induced data: . Theorem (sample complexity upper bound with policy induced data): With $m=\\Omega(S^3 H^4 A^{\\min(H,S-1)+2}/\\delta^2)$ episodes of length $H$ collected with the uniform policy from a fixed initial distribution $\\mu$, with a constant probability, the plug-in algorithm produces a policy that is $\\delta$-optimal when started from $\\mu$. Proof (sketch): For simplicity assume that the reward function is known. Let $\\pi_{\\log}$ be the logging policy, which is uniform. Again, assume that the plug-in algorithm produces a deterministic policy. The proof is based on the decomposition of the suboptimality gap of the policy $\\hat \\pi$ produced that was used before. In particular, by \\eqref{eq:subbd}, . \\[\\begin{align} v^*(\\mu) - v^{\\hat \\pi}(\\mu) &amp; \\le v^{\\pi^*}(\\mu) - \\hat v^{\\pi^*}(\\mu) \\, + \\, \\hat v^{\\hat \\pi}(\\mu) - v^{\\hat \\pi}(\\mu)\\,, \\label{eq:subbd2} \\end{align}\\] where as before, $\\hat v^\\pi$ denotes the value function of policy $\\pi$ in the empirically estimated MDP. Further, we also used $v(\\mu)$ as a shorthand for $\\sum_s \\mu(s) v(s) (= \\langle \\mu, v \\rangle)$, where $v:[S]\\to \\mathbb{R}$, . One then needs a counterpart of the value difference lemma. In this case, the following version is convenient: For any policy $\\pi$, . \\[q^\\pi_H-\\hat q^\\pi_H = \\sum_{h=0}^{H-1} (P_\\pi)^h (P-\\hat P) \\hat v_{H-h-1}^\\pi\\,,\\] and . \\[\\hat q^\\pi_H- q^\\pi_H = \\sum_{h=0}^{H-1} (\\hat P_\\pi)^h (\\hat P-P) v_{H-h-1}^\\pi\\,,\\] where $P_\\pi$ and $\\hat P_\\pi$ are $SA \\times SA$ matrices. These can be proved by using the Bellman equation for the action-value functions and a simple recursion and noting that $q_0 = r = \\hat q_0$. Next, we can observe that $v^\\pi(\\mu) = \\langle \\mu^\\pi, q^\\pi \\rangle$ where $\\mu^\\pi$ is a distribution over $[S]\\times [A]$ which assigns probability $\\mu(s)\\pi(a|s)$ to $(s,a)\\in [S]\\times [A]$. This, combined with the value difference identity makes $\\nu_h^\\pi:=\\mu^\\pi (P_\\pi)^h$ appear in the bounds. This is the probability distribution over the state-action space after using $\\pi$ for $h$ steps when the initial distribution is $\\mu^\\pi$. Now, as this is multiplied by $P-\\hat P$, and for a given state-action pair $(s,a)$, \\(\\| P(s,a)-\\hat P(s,a) \\|_1 \\lesssim 1/\\sqrt{N(s,a)} \\le 1/\\sqrt{N_h(s,a)} \\approx 1/\\sqrt{m \\nu_h^{\\pi_{\\log}}}\\), using that $\\nu_h^\\pi(s,a)\\le \\sqrt{\\nu_h^\\pi(s,a)}$ which holds because $0\\le \\nu_h^\\pi\\le 1$, we see that it suffices if the ratios $\\rho_h^\\pi(s,a):=\\nu_h^\\pi(s,a)/\\nu_h^{\\pi_{\\log}}(s,a)$ (or their square root) are controlled. Above, $N(s,a)$ is the number of times $(s,a)$ is seen in the data, and $N_h(s,a)$ is the number of times $(s,a)$ is seen in the data in the $h$th transition. Here we should also mention that we only control these terms for state-action pairs $(s,a)$ that satisfy $\\nu_h^{\\pi}(s,a)\\gtrsim 1/m$ as the total contribution of the other state-action pairs is $O(1/m)$, i.e., small. For these state action pairs, $\\nu_h^{\\pi_{\\log}}(s,a)$ is also positive and with high probability, the counts are also positive. Next, one can show that . \\[\\rho_h^\\pi(s,a)\\le A^{\\min(h+1,S)}\\,.\\] This is done in two steps. First, show that $\\nu_h^{\\pi}(s,a) \\le A^{h+1} \\nu_h^{\\pi_{\\log}}(s,a)$. This follows from the law of total probability: Write $\\nu_h^{\\pi}(s,a)$ as the sum of probabilities of all trajectories that end with $(s,a)$ after $h$ transitions. Next, for a given trajectory, replace each occurrence of $\\pi$ with $\\pi_{\\log}$ at the expense of introducing a factor of $A^{h+1}$ (this comes from $\\pi(a’|s’)\\le 1 \\le A \\pi_{\\log}(a’|s’)$). The next step is to show that $\\nu_h^{\\pi}(s,a) \\le A^{S} \\nu_h^{\\pi_{\\log}}(s,a)$ also holds. This inequality follows by observing that the uniform policy and the uniform mixture of all deterministic (memoryless) policies induce the same distribution over the trajectories. Then by letting $\\textrm{DET}$ denote the set of all deterministic policies, using that $\\pi\\in \\textrm{DET}$, we have $\\nu_h^{\\pi}(s,a)\\le \\sum_{\\pi’ \\in \\textrm{DET}} \\nu_h^{\\pi’}(s,a) =A^S \\nu_h^{\\pi_{\\log}}(s,a)$, where we used that $\\vert \\textrm{DET} \\vert =A^S$. Putting things together, applying a union bound when it comes to argue for $\\hat \\pi$ and collecting terms gives the result. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec18/#policy-based-designs",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec18/#policy-based-designs"
  },"165": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "Bibliographic remarks",
    "content": "Finding a good policy from a sample drawn from a $Z$-design and finding a good policy from a sample given a generative model, or random access simulator of the MDP (which we extensively studied in previous lectures on planning) are almost the same. The random access model however allows the learner to determine which state-action pair the next transition data should be generated at in reaction to the sample collected in a sequential fashion. Thus, computing a good policy with a random access simulator gives more power to the “learner” (or planner). The lower bound presented for $Z$-design can in fact be shown to hold for the generative setting, as well (the proof in the paper cited below goes through in this case with no changes). This shows that in the tabular case, adaptive random access to the simulator provides no benefits to the planner over non-adaptive random access. The result of the $O(H^3 SA/\\delta^2)$ sample complexity bound to find a $\\delta$-optimal policy with uniform $Z$-design using the plug-in method is from the following paper: . | Agarwal, Alekh, Sham Kakade, and Lin F. Yang. 2020. “Model-Based Reinforcement Learning with a Generative Model Is Minimax Optimal.” COLT, 67–83. arXiv link | . This paper also contains a number of pointers to the literature. Interestingly, earlier approaches often used more complicated approaches which directly worked with value functions rather than the more natural plug-in approach. The problem of whether the plug-in method is minimax optimal in $Z$ design for finite-horizon problem is open. The result which was included in this lecture limits the range of $\\delta$ to $\\sqrt{H}$. Equivalently, the result is not applicable for a small number of observations $m$ per state-action pair. This limitation has been removed in a follow-up to this work: . | Li, Gen, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. 2020. “Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model.” NeurIPS | . This paper still uses the plug-in method, but adds random noise to the observed rewards to help with tie-breaking. The variance bound, which is the key to achieving the cubic dependence on the horizon is from the following paper: . | Mohammad Gheshlaghi Azar, Rémi Munos, and Hilbert J Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):325–349, 2013. | . This paper also has the essential ideas for the matching lower bound. The 2020 paper is notable for some novel proof techniques, which were developed to bound the error terms whose control is not included in this lecture. The results for learning with policy-induced data are from . | Xiao, Chenjun, Ilbin Lee, Bo Dai, Dale Schuurmans, and Csaba Szepesvari. 2021. “On the Sample Complexity of Batch Reinforcement Learning with Policy-Induced Data.” arXiv | . which also has the details that were omitted in these notes. This paper also gives a modern proof for the $Z$-design sample complexity lower bound. One may ask whether the results for $Z$-design that show cubic dependence on the horizon $H$ extend to the case of large MDPs when value function approximation is used. In a special case, this has been positively resolved in the following paper: . | Yang, Lin F., and Mengdi Wang. 2019. “Sample-Optimal Parametric Q-Learning Using Linearly Additive Features.” ICML arXiv version | . which uses an approach similar to Politex in a more restricted setting, but achieves an optimal dependence on $H$. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec18/#bibliographic-remarks",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec18/#bibliographic-remarks"
  },"166": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "18. Sample complexity in finite MDPs",
    "content": "PDF Version . Let $Z = \\mathcal{S}\\times \\mathcal{A}$ be the set of state-action pairs. A $Z$-design assigns a count to every member of $Z$, that is, to every state-action pair. In the last lecture we saw that . \\[n = \\tilde O\\left( \\frac{H^6 \\mathrm{SA}}{\\delta_{\\text{trg}}} \\right)\\] samples are sufficient to obtain a $\\delta_{\\text{trg}}$-suboptimal policy with high probability provided that data is generated from a $Z$-design that assigns the same count to each state-action pair and to get a policy one uses the straightforward plug-in approach that estimates the rewards and transitions using empirical estimates and uses the policy that is optimal with respect to the estimated model. Above, the dependence on the number of state-action pairs is optimal, but the dependence on the horizon $H = \\frac{1}{1-\\gamma}$ is suboptimal. In the first half of this lecture, I sketch how the analysis presented in the previous lecture can be improved to get the optimal cubic dependence, together with a sketch that shows that the cubic dependence is indeed optimal. In the second half of the lecture, we consider policy-based data collection, or experimental designs, where the goal is to find a near optimal policy from an initial state, where the data consists of trajectories obtained by rolling out the data-collection policy from the said initial state. Here, we will show a lower bound that shows that the sample complexity in this case is at least as large $\\Omega(\\mathrm{A}^{\\min(\\mathrm{S},H)})$, which shows that there exist an exponential separation between both $Z$-designs and policy-based designs, and also between passive and active learning. To see the latter, note that in the presence of a simulator, with only a reset to an initial state, one can use approximate policy iteration with rollouts, or Politex with rollouts, to get a policy that is near-optimal when started from the initial state that one can reset to but with polynomially many samples in $\\mathrm{S},\\mathrm{A}$ and $H$ (cf. Lecture 8 and Lecture 14). ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec18/",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec18/"
  },"167": {
    "doc": "19. Scaling with value function approximation",
    "title": "Batch-mode Approximate Dynamic Programming",
    "content": "The next question is whether the “featurized” planning algorithms that use value function approximation can be adopted to the batch setting and if so, how? . For planning in the presence of simulators, three basic settings that are a good fit to “approximate dynamic programming” methods are summarized in the following table: . | Algorithm family | Condition | Specific algorithms | . | AVI | $T \\mathcal{F} \\subset \\mathcal{F}$ | LSVI/LSQI/FQI, DQN | . | API | $\\forall \\pi$: $q^\\pi \\in \\mathcal{F}$ | LSPI, Politex, NGD | . | AMPI | $\\forall \\pi$: $T_\\pi \\mathcal{F} \\subset \\mathcal{F}$ | Actor-critic variants | . In this table $T$ stands for the Bellman optimality operator, $\\mathcal{F}$ is the set of value functions that can be compactly represented (e.g., when using linear function approximation via the linear combination of a few basis functions). Here, the domain of $T$ and $\\mathcal{F}$ obviously need to match. The condition $T\\mathcal{F}\\subset \\mathcal{F}$ is known as the Bellman completeness condition. The condition in the last row is a stronger version of Bellman completeness. Indeed, this condition implies the one in the first row: Under the condition of the last row, if $f\\in \\mathcal{F}$ and $\\pi$ is a greedy policy with respect to $f$ then $T f = T_\\pi f$ and by the condition, $T_\\pi f\\in \\mathcal{F}$, hence, $T \\mathcal{F} \\subset \\mathcal{F}$ also holds. In the first row, AVI stands for “approximate value iteration”, in the second row API stands for “approximate policy iteration”, while in the third row AMPI stands for “approximate modified policy iteration”. When a simulator is available, the algorithms are made to work by ensuring that the worst-case approximation error (or extrapolation error) is under control. This is done by generating data using the simulator, usually from a set of carefully chosen “anchor points” (e.g., using an approximate G-optimal design). When only a batch of data is available, choosing anchor points is not an option and this will obviously limit the extent to which the extrapolation error can be controlled. In fact, here, there is not much one can do: One can use all the data that is available, but nothing replaces data that is missing. Still, there is a difference between how easily the various methods can be adopted to the batch setting. The adoption of AVI and AMPI type methods that compute a sequence of value functions with the successive applications of a Bellman operator (either the Bellman optimality operator, or a policy evaluation operator) is relatively seamless. This is because these operators can be seen as the composition of an action selection operator ($M_\\pi$ for policy evaluation and $M$, the greedifying operator for the Bellman optimality operator) and the transition kernel viewed as an operator. The action selection operator needs no approximation and the transition kernel operator can be just replaced with the empirical transition kernel. Finally, to keep the value functions in $\\mathcal{F}$, a projection step is added. In what follows we will discuss the first and the second row and the special demands that these come with in the batch setting. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec19/#batch-mode-approximate-dynamic-programming",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec19/#batch-mode-approximate-dynamic-programming"
  },"168": {
    "doc": "19. Scaling with value function approximation",
    "title": "Batch-mode least-squares Q-iteration",
    "content": "For the sake of specificity, consider discounted problems and the case of approximating the Bellman optimality operator $T$ that maps action-value functions to action-value functions. Thus, . \\[T q = r + \\gamma P M q\\,,\\] where $M: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}\\to \\mathbb{R}^{\\mathcal{S}}$ is given by $(M q)(s)=\\max_{a\\in [A]} q(s,a)$ and, as usual, $\\gamma$ is the discount factor. Recall that value iteration then produces a sequence $q_k = T q_{k-1}$ and the policy returned is the policy that is greedy with respect to $q_K$ with some $K&gt;0$. When function approximation is involved, the relation $q_k = T q_{k-1}$ holds with some error: . \\[\\begin{align} q_k = T q_{k-1} + \\varepsilon_k\\,. \\label{eq:avi} \\end{align}\\] Recall the following proposition from an endnote on approximate value iteration from Lecture 8: . Proposition (AVI error bound): Let $q_0=0$. For any $K&gt;0$, the policy that is greedy with respect to $q_K$ is $\\delta$-optimal . \\[\\begin{align*} \\delta \\le 2 H^2 \\left(\\gamma^K + \\max_{1\\le k \\le K} \\| \\varepsilon_k \\|_\\infty\\right)\\,. \\end{align*}\\] . This shows that in whatever approximate way we calculate $q_k$ from $q_{k-1}$, the algorithm’s success will be controlled by the size of \\(\\max_{1\\le k \\le K} \\| \\varepsilon_k \\|_\\infty\\). Least-squares approximation to $T$ . In batch-mode least-squares Q-iteration these errors are controlled by performing a least-squares fit from $\\mathcal{F}\\subset \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ to a “noisy estimate” of $T q_{k-1}$, which is constructed on a batch of transition data $D_n = ( (S_1,A_1,R_1,S_1’),\\dots,(S_n,A_n,R_n,S_n’))$. This data is assumed to be so that for $i\\in [n]$, $(R_i,S_i’)\\sim Q(\\cdot|S_i,A_i)$, with $Q$ the reward-state transition kernel of the MDP, as this, together with some assumptions on $Z_i:=(S_i,A_i)$ allows one to approximate $T$. This is done as follows: For $q\\in \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}}$, let . \\[\\hat T q = \\arg\\min_{f\\in \\mathcal{F}} L_n(f,q)\\,,\\] where . \\[L_n(f,q) = \\frac1n \\sum_{i=1}^n \\left(R_i+\\gamma \\max_{a} q(S_i',a) - f(S_i,A_i) \\right)^2\\,.\\] Defining $\\hat P$ and $\\hat r$ as the empirical state transition kernel and empirical reward function the usual way and the semi-norm \\(\\|f\\|_n\\) through \\(\\|f\\|_n^2 = \\frac1n \\sum_{i=1}^n f^2(S_i,A_i)\\), it is not hard to see that . \\[\\hat T q = \\arg\\min_{f\\in \\mathcal{F}} \\| \\hat r + \\gamma \\hat P M q - f \\|_n^2,\\] or, as promised, . \\[\\hat T q = \\Pi_n (\\hat r + \\gamma \\hat P M q),\\] where \\(\\Pi_n g = \\arg\\min_{f\\in \\mathcal{F}} \\| g - f \\|_n^2\\). With this notation, least-squares Q-iteration (LSQI) then computes the sequence . \\[q_{k+1} = \\hat T q_k\\,.\\] When $\\mathcal{F} = { \\Phi \\theta\\,:\\, \\theta\\in \\mathbb{R}^d }$, letting $q_n = \\Phi \\theta_n$, one can perform the whole calculation through calculating the parameter sequence $(\\theta_n)_n$, where each update costs the same as finding the solution of a least-squares problem. If $\\mathcal{F}$ is the set of functions that can be represented through neural networks and instead of calculating the minimizer of the least-squares problems, only a few gradient steps are made by also subsampling the data, one gets a variant of the “deep Q-networks” (DQN) method. From our perspective, the main question is how good these methods can be? As it stands, the main issue is to control the worst-case error. In the linear case, under Bellman completeness, this (essentially) hinges upon how the eigenvalues of the expectation of the moment matrix, . \\[G_n = \\frac1n \\sum_{i=1}^n \\phi(S_i,A_i) \\phi(S_i,A_i)^\\top,\\] behave. To see how this works consider a simple case when $(S_i,A_i,R_i,S_i’)$ is an i.i.d. sequence. This is similar to the $Z$-design case, but this excludes the possibility when the data is serially correlated, which would be the case when it is generated by following some policy. To simplify notation let $Z_i = (S_i,A_i)\\in \\mathcal{Z}:=\\mathcal{S}\\times \\mathcal{A}$ and $\\phi_i = \\phi(Z_i)$. Fix $q\\in \\mathbb{R}^{\\mathcal{Z}}$. Assume further that $T q = \\Phi \\theta$ (realizability). Letting $\\epsilon_i = R_i + \\gamma \\max_a q(S_i’,a) - \\phi_i^\\top \\theta$, we see that $(\\epsilon_i)_i$ is a zero-mean i.i.d. sequence. Then, $\\Delta:=\\hat \\theta_n-\\theta = G_n^{-1} \\frac1n \\sum_i \\phi_i \\epsilon_i$ and . \\[\\begin{align*} \\mathbb{E}[ \\Delta \\Delta^\\top | Z_1, \\dots, Z_n ] &amp; = \\frac{1}{n^2} G_n^{-1} \\mathbb{E}[ \\sum_{i,j} \\phi_i \\phi_j^\\top \\epsilon_i \\epsilon_j | Z_1, \\dots, Z_n ] G_n^{-1} \\\\ &amp; = \\frac{\\sigma^2}{n^2} G_n^{-1} \\mathbb{E}[ \\sum_{i} \\phi_i \\phi_i^\\top | Z_1, \\dots, Z_n ] G_n^{-1} \\\\ &amp; = \\frac{\\sigma^2}{n} G_n^{-1} G_n G_n^{-1} \\\\ &amp; = \\frac{\\sigma^2}{n} G_n^{-1} \\,, \\end{align*}\\] where $\\sigma^2 = \\mathbb{E}[\\epsilon_1^2|Z_1]$. For $n$ large, we also have $G_n \\approx G:=\\mathbb{E}[ \\phi(Z_1)\\phi(Z_1)^\\top ]$. Hence, the squared expected error at $z$ satisfies . \\[\\begin{align*} \\mathbb{E}[ (\\phi(z)^\\top \\Delta)^2 ] = \\phi(z)^\\top \\mathbb{E}[ \\mathbb{E}[ \\Delta \\Delta^\\top | Z_1,\\dots,Z_n ] ] \\phi(z) \\approx \\frac{\\mathbb{E}\\sigma^2}{n} \\| \\phi(z) \\|^2_{G^{-1}} \\,. \\end{align*}\\] (A similar calculation was done earlier in Lecture 8.) Thus, the extrapolation errors are governed by the value of . \\[\\epsilon(\\Phi):=\\sup_{z\\in \\mathcal{Z}} \\| \\phi(z) \\|_{G^{-1}}\\,.\\] Note that this, in turn, is controlled by the common distribution of \\((Z_i)_i\\). Recalling what we learned about \\(G\\)-optimal designs in the lecture mentioned previously, we know that there is always a distribution such that this is at most $\\sqrt{d}$. However, if the distribution is not chosen in a careful manner then this value can be arbitrarily large. Indeed, to start with the the most significant issue, $G$ may even be singular. But even if $G$ is invertible, this value can be as large as \\(\\lambda^{-1/2}_{\\min}(G)\\), the square root of the inverse of the minimum eigenvalue of $G$. Altogether, this discussion shows that if the feature-space is a good fit to the MDP and the data generating distribution is favorable, the errors $\\varepsilon_k = q_k - T q_{k-1}$ will be under control. While the above discussion was developed for the realizable case (when $T\\mathcal{F}\\subset \\mathcal{F}$), the same conclusions hold for the misspecified case, as well. The problem is that of course batch mode RL gives little room for controlling $\\epsilon(\\Phi)$ as in general the distribution $P$ of $(\\phi(Z_i))_i$ is uncontrolled. When there is a chance to control this distribution, this discussion shows that it suffices to do it so that the minimum eigenvalue of $G$ is controlled (equivalently, $\\epsilon(\\Phi)$ is controlled). To get a sense of the quality of the bound before moving to the next topic, it is worthwhile to consider the tabular case, which is a special case of the linear case, as noted beforehand (choose $d=SA$, and let the feature vector of each state-action pair be a unit vector of $\\mathbb{R}^d$). In this case the choice for $P$ that maximizes the minimum eigenvalue is the uniform distribution over the state action space. With this choice, the minimum eigenvalue becomes $1/d=1/(SA)$. Choosing $K$ so that $H^2 \\gamma^K\\le \\delta/2$ (i.e., $K \\approx H_{\\gamma,\\delta/(2H)})$), ignoring logarithmic terms, we get that it suffices to have a dataset of size $n=(H SA/\\delta)^2$ to obtain a $\\delta$-optimal policy. While this is worse than the best bound we obtained for the plug-in method, it is comparable to the bounds we obtained with simpler methods. In fact, with some conditions, the procedure itself is optimal; the suboptimal results are obtained because of the simple analysis. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec19/#batch-mode-least-squares-q-iteration",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec19/#batch-mode-least-squares-q-iteration"
  },"169": {
    "doc": "19. Scaling with value function approximation",
    "title": "Batch-mode least-squares policy iteration",
    "content": "In policy iteration, $\\pi_k$ is chosen to be greedy with respect to the value function of the previous policy $\\pi_{k-1}$. Letting $q_k = q^{\\pi_k}$, this means that $\\pi_k$ satisfies $M q_{k-1} = M_{\\pi_k} q_{k-1}$. Again, a previous result in Lecture 8 states that even if . \\[q_k = q^{\\pi_k}+\\varepsilon_k\\] where $\\varepsilon_k$ may be nonzero, this process is stable in the sense that the policy computed after $K$ steps is near-optimal as long as $K$ is large enough and the error \\(\\varepsilon_{1:k}=\\max_{1\\le k \\le K} \\| \\varepsilon_k \\|_\\infty\\) is under control. In particular, a corollary in that lecture states the following: . Proposition: Let \\((\\pi_k)_{k\\ge 0}\\), \\((\\varepsilon_k)_k\\) be as above. Then, for any \\(k\\ge 1\\), . \\[\\| v^* - v^{\\pi_k} \\|_\\infty \\leq H \\gamma^k + H^2 \\varepsilon_{1:k}\\,,\\] where $H=1/(1-\\gamma)$. Now, to control the suboptimality of the policy it suffices to choose $k$ large enough, while controlling the error $\\varepsilon_{1:k}$. In the lecture on planning, these errors were controlled by rolling out with the policies $\\pi_i$ obtained in the process. Clearly, when working with a batch of data, this option is not available. As it turns out, evaluating arbitrary policies based on a batch of data can be challenging exactly because of this: How can we use data generated by following some policy (or a $Z$-design) to evaluate some other policy? In what follows, we will enumerate some options for doing this and discuss their pros and const. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec19/#batch-mode-least-squares-policy-iteration",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec19/#batch-mode-least-squares-policy-iteration"
  },"170": {
    "doc": "19. Scaling with value function approximation",
    "title": "Policy evaluation using likelihood ratio corrections",
    "content": "One simple idea that is applicable when the data is generated by following some logging policy is to use likelihood ratio corrections together with least-squares fitting. In particular, assume that the data consists of a number of trajectories generated by following policy $\\pi_{\\log}$ while our aim is to evaluate a policy $\\pi$. Let the initial state-action distribution be $\\mu$. As discussed in the part on planning, there is not much loss in assuming that we cut the trajectories after some $H$ steps with $H \\approx H_{\\gamma, \\delta/(1-\\gamma)^2}$. The expected total discounted reward under a policy along a trajectory of this length that is started from $\\mu$ is then . \\[q^\\pi_H(\\mu):=\\int R(\\tau) P_{\\mu}^{\\pi}(d\\tau)\\,,\\] where $P_{sa}^\\pi$ is the distribution induced over the $H$ step-trajectories $\\tau\\in (\\mathcal{S}\\times\\mathcal{A})^H$ by the interconnection of policy $\\pi$ and MDP $M$ when the policy is started from $(s,a)$, while $R(\\tau)$ is the discounted sum of immediate expected rewards along $\\tau = ((s_0,a_0),\\dots,(s_{H-1},a_{H-1}))$: . \\[R(\\tau) = \\sum_{h=0}^{H-1} r(s_h,a_h)\\,.\\] For discrete state-action spaces, the above integral reduces to a sum and becomes . \\[\\sum_{\\tau\\in \\mathcal{Z}^H} R(\\tau) p_{\\mu}^{\\pi}(\\tau)\\,,\\] where $p_{\\mu}^\\pi(\\tau)$ is the probability of trajectory $\\tau$. In particular, if $\\tau = ((s_0,a_0),\\dots,(s_{H-1},a_{H-1}))$ then . \\[p_{\\mu}^\\pi(\\tau) = \\mu(s_0,a_0) P_{a_0}(s_0,s_1) \\pi(a_1|s_1) \\dots P_{a_{H-2}}(s_{H-2},s_{H-1}) \\pi(a_{H-1}|s_{H-1})\\,.\\] In any case, for any $\\pi’$, as long as the support of $dP_{\\mu}^{\\pi}$ is at least as large as that of $dP_{\\mu}^{\\pi’}$, by a change of measure argument, we have . \\[q^{\\pi'}_H(\\mu) = \\int R(\\tau) P_{\\mu}^{\\pi'}(d\\tau) = \\int R(\\tau) \\frac{dP_{\\mu}^{\\pi'}}{dP_{\\mu}^{\\pi}}(\\tau) dP_{\\mu}^{\\pi}(d\\tau)\\,.\\] Here, $ \\rho:=\\frac{dP_{\\mu}^{\\pi’}}{dP_{\\mu}^{\\pi’}} $ denotes the density of measure $P_{\\mu}^{\\pi’}$ with respect to $P_{\\mu}^{\\pi}$ (this is also known as the Radon-Nikodym derivative of $P_{\\mu}^{\\pi’}$ with respect to $P_{\\mu}^{\\pi}$). (The definition of $\\rho$ is that it is a measurable function over the trajectories of required length so that for any measurable subset $U$ of these trajectories, $\\int_U \\rho(\\tau) P_{\\mu}^{\\pi}(d\\tau) = \\int_U P_{\\mu}^{\\pi’}(d\\tau)$). The function $\\rho$ is also known as the likelihood ratio of measure $P_{\\mu}^{\\pi’}$ relative to $P_{\\mu}^{\\pi}$ at $\\tau=(s_0,a_0,\\dots,s_{H-1},a_{H-1})$. Note that here, for the sake of minimizing clutter, by slightly abusing notation, we dropped some parentheses. As it turns $\\rho$ is simple to calculate and satisfies . \\[\\begin{align} \\rho(\\tau):= \\frac{dP_{\\mu}^{\\pi'}}{dP_{\\mu}^{\\pi'}}(s_0,a_0,\\dots,s_{H-1},a_{H-1}) = \\prod_{h=1}^{H-1} \\frac{\\pi'(a_h|s_h)}{\\pi(a_h|s_h)}\\,. \\label{eq:rhofinite} \\end{align}\\] From this, we see that the support condition holds if for any $(s,a)$ such that $\\pi’(a\\vert s)&gt;0$, $\\pi(a\\vert s)&gt;0$ also holds. Intuitively, the data generating policy should never omit actions that the target policy $\\pi’$ would use with positive probability. The above relation for $\\rho$ is easy to verify from the definition above when the state-action space is discrete. The important point here is that evaluating $\\rho$ at any trajectory requires only knowing $\\pi$ and $\\pi’$ and in particular the knowledge of the transition probabilities is not needed. It follows then that $R(\\tau)\\rho(\\tau)$ can be used as a target to estimate $q^{\\pi’}_H$. With linear function approximation this works as follows: Given $\\mathcal{F} = { \\Phi \\theta \\,:\\, \\theta\\in \\mathbb{R}^d }\\subset \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$, one can estimate the “projection” $\\Pi_mu q^{\\pi’}_H$ of $q^{\\pi’}_H$ to $\\mathcal{F}$ using least-squares fitting based on a number of independently obtained trajectories $\\tau^{(1)},\\dots,\\tau^{(n)}$. Here, the projection mentioned is defined by . \\[\\Pi_\\mu q = \\arg\\min_{g\\in \\mathcal{F}} \\int (g(z)-q(z))^2 \\mu(dz)\\,,\\] which we assume is uniquely defined. In particular, this holds if $G= \\int \\phi(z)\\phi(z)^\\top \\mu(dz)$ is full rank. The estimate of $\\Pi_{\\mu} q^{\\pi’}_H$ is then defined as $g_n=\\Phi \\theta_n$, where . \\[\\begin{align} \\theta_n = \\arg\\min_{\\theta} \\sum_{i=1}^n ( R(\\tau^{(i)})\\rho(\\tau^{(i)})- \\phi(\\tau^{(i)}_0) \\theta )^2\\,, \\label{eq:lspibtrg} \\end{align}\\] where $\\tau^{(i)}_0$ denotes the first state-action pair along trajectory $\\tau^{(i)}$. Indeed, with some calculations similar to the one done previously, it is not hard to see that under minimal regularity assumptions, . \\[\\| g_n - \\Pi_{\\mu} q_H^{\\pi'} \\|_\\infty \\lesssim \\frac{\\sigma}{n} \\lambda_{\\min}^{-1/2}(G),\\] where $\\sigma^2 = \\mathbb{E}[ \\sigma^2(\\tau^{(1)}_0) ]$, with the quantity inside the expectation defined by . \\[\\sigma^2(z) = \\mathbb{E}[ (R(\\tau^{(i)})\\rho(\\tau^{(i)}) - q^{\\pi}_H(\\tau^{(i)}_0) )^2 | \\tau^{(1)}_0 = z ] = \\mathrm{Var}[ R(\\tau^{(i)})\\rho(\\tau^{(i)}) | \\tau^{(1)}_0 = z ]\\,.\\] The problem now is twofold: On the one hand one needs to control the minimum eigenvalue of $G$ (the same optimal design problem that we met multiple times before), while, on the other hand, one also needs to control the variance terms $\\sigma^2(z)$. While the rewards along the trajectories are bounded, that is, the range of $R$ above is bounded, this may scale with $H$ (in the lack of access to the immediate reward function, but when the trajectory data includes rewards along the transitions observed, one can of course replace $R(\\tau^{(i)})$ with the sum of rewards in $\\tau^{(i)}$, at the price of potentially further increasing the variance). A potentially even more serious problem though is that the variance of the likelihood ratios can be quite large. Indeed, owning to $\\mathbb{E}[\\rho(\\tau)]=1$, $\\mathrm{Var}(\\rho(\\tau))=\\mathbb{E}[\\rho^2(\\tau)]-1$. Then, the Perron-Frobenius theory of nonnegative matrices implies the following: . Proposition: Assume that there are finitely many state-action pairs and the Markov chain over the state-action pairs induced by following $\\pi’$ is ergodic (aperiodic and irreducible). Assume further that $\\pi’(a|s)&gt;0$ implies that $\\pi(a|s)&gt;0$. Then, . \\[\\begin{align} \\mathbb{E}[\\rho^2(\\tau)] \\sim C e^{H \\alpha}\\,, \\label{eq:expasymptotics} \\end{align}\\] where the asymptotics is taken with $H\\to \\infty$, $C&gt;0$ is a positive constant and $\\alpha&gt;0$ is the largest magnitude eigenvalue of the $SA \\times SA$ matrix $F$ whose entry at $(s,a)$ and $(s’,a’)$ is zero if $\\pi(a’|s’)=0$ and otherwise it is . \\[P_{a}(s,s')\\pi(a'|s')\\left(\\frac{\\pi'(a'|s')}{\\pi(a'|s')}\\right)^2\\] . Proof: Let $g(s,a)=(\\frac{\\pi’(a|s)}{\\pi(a|s)})^2$. Note that $\\rho(\\tau) = g(Z_1)g(Z_2)\\dots g(Z_{H-1})$ where $Z_i = (S_i,A_i)$. For $n\\ge 1$, define the $SA \\times SA$ matrix $F(n)$ by . \\[\\begin{align*} F_{z,z'}(n) &amp; =\\mathbb{E}_{z}[g(Z_n)g(Z_{n-1})\\dots g(Z_0) \\mathbb{I}(Z_n=z')]\\,, \\end{align*}\\] where $Z_0,Z_1,\\dots$ are the state-action pairs of the Markov chain on $\\mathcal{Z}:=\\mathcal{S}\\times\\mathcal{A}$ induced by $\\pi$ and $\\mathbb{E}_z$ is the expectation under the distribution of this chain when the chain starts at $z$. It follows from elementary calculations that for any $n\\ge 1$, $F(n) = F^n$. We prove this by induction on $n$. Indeed, this holds for $n=1$ by inspecting the definitions. Now, assuming that $F(n)=F^n$ holds for some $n\\ge 1$, we have . \\[\\begin{align*} F_{z,z'}(n+1) &amp; = \\mathbb{E}_{z}[g(Z_{n+1})g(Z_n)\\dots g(Z_0) \\mathbb{I}(Z_{n+1}=z')]\\\\ &amp; = \\sum_u \\mathbb{E}_{z}\\left[ \\mathbb{E}_{z}[g(Z_{n+1})g(Z_n)\\dots g(Z_0) \\mathbb{I}(Z_n=u,Z_{n+1}=z')|Z_0,\\dots,Z_{n}]\\right] \\\\ &amp; = \\sum_u \\mathbb{E}_{z}\\left[ g(Z_n)\\dots g(Z_0) \\mathbb{I}(Z_n=u) \\mathbb{E}_{z}[g(Z_{n+1}) \\mathbb{I}(Z_{n+1}=z')|Z_{n}]\\right] \\\\ &amp; = \\sum_u \\mathbb{E}_{z}[ g(Z_n)\\dots g(Z_0) \\mathbb{I}(Z_n=u) F_{Z_n,z'}] \\\\ &amp; = \\sum_u F_{z,u}(n) F_{u,z'} = (F^{n+1})_{z,z'}\\,, \\end{align*}\\] where the last equality used the induction hypothesis. Now, since $F$ is nonnegative valued, by Perron-Frobenius theory, $F^n \\sim h \\kappa e^{n \\alpha}$ where $\\kappa$ is a left-eigenvector of $F$ and $h$ is a right-eigenvector of $F$, both corresponding to the eigenvalue $\\rho$ and they are normalized so that $\\kappa h=1$ ($h\\kappa$ is called the Perron projection underlying $F$). The result follows by noting that $\\mathbb{E}[\\rho^2(\\tau)]=\\mu F(H-1) \\boldsymbol{1} \\sim \\mu h \\kappa \\boldsymbol{1}e^{\\alpha}\\, e^{\\alpha H}$. The constant $C=\\mu h \\kappa \\boldsymbol{1} e^{\\alpha}$ is positive since $F$ is irreducible, hence both $h$ and $\\kappa$ have only positive entries. \\(\\qquad \\blacksquare\\) . We can further conclude that the growth rate of the variance is rapid. In particular, $\\alpha \\ge 1$. This follows, since also from Perron-Frobenius theory, we have . \\[\\alpha\\ge \\min_{s,a} (F \\boldsymbol{1})_{s,a}.\\] Now, from Jensen’s inequality, . \\[\\begin{align*} (F \\boldsymbol{1})_{s,a} &amp; = \\sum_{s',a':\\pi(a'|s')&gt;0} P_{a}(s,s')\\pi(a'|s')\\left(\\frac{\\pi'(a'|s')}{\\pi(a'|s')}\\right)^2 \\ge \\left( \\sum_{s',a':\\pi(a'|s')&gt;0} P_{a}(s,s')\\pi(a'|s')\\frac{\\pi'(a'|s')}{\\pi(a'|s')}\\right)^2\\\\ &amp; = \\left( \\sum_{s',a':\\pi(a'|s')&gt;0} P_{a}(s,s')\\pi'(a'|s')\\right)^2 = 1\\,. \\end{align*}\\] There is a good intuitive explanation as well as to why the variance of $\\rho$ grows really fast. Since the data is sampled from $\\pi$, we expect the ratios in $\\rho$ to be less than one: It is rare to sample an action from $\\pi$ which has a probability under $\\pi$ that is below the probability assigned to the action by $\\pi’$: sampling under $\\pi$ chooses actions which have high probability under $\\pi$. Thus, the typical terms in $\\rho$ are below 1. In fact, because of this, one can show that with probability one $\\rho$, as $H\\to\\infty$, converges to $0$. Now, since the expectation of $\\rho$ is one, there must be exponentially rare events when $\\rho$ takes on exponentially large values. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec19/#policy-evaluation-using-likelihood-ratio-corrections",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec19/#policy-evaluation-using-likelihood-ratio-corrections"
  },"171": {
    "doc": "19. Scaling with value function approximation",
    "title": "Variance reduction",
    "content": "There is a trivial approach to reduce variance, which is based on the observation that $R(\\tau)$ has an additive structure and that for a given $h$ index, the state-action pairs beyond index $h$ have no influence on the expectation of $r(S_h,A_h)\\rho(\\tau)$. In particular, we have that . \\[\\mathbb{E}[ r(S_h,A_h)\\rho_{H-1}(\\tau) ] = \\mathbb{E}[ r(S_h,A_h) \\rho_h(\\tau_h) ]\\,,\\] where $\\tau_h = (S_0,A_0,\\dots,S_h,A_h)$ and . \\[\\rho_h = \\frac{dP_{h,\\mu}^{\\pi'}}{dP_{h,\\mu}^{\\pi}}\\,,\\] where $P_{h,\\mu}^\\pi$ denotes the distribution induced over trajectories of length $h+1$ by using $\\pi$ in MDP $M$ starting from $\\mu$. Clearly, we still have . \\[\\rho_h(s_0,a_0,\\dots,s_h,a_h) = \\frac{\\pi'(a_1|s_1)}{\\pi(a_1|s_1)} \\dots \\frac{\\pi'(a_h|s_h)}{\\pi(a_h|s_h)} \\,.\\] One can then replace $R(\\tau^{(i)})\\rho(\\tau^{(i)})$ in \\eqref{eq:lspibtrg} by . \\[r(S_0^{(i)},A_0^{(i)})+\\sum_{h=1}^{H-1} r(S_h^{(i)},A_h^{(i)}) \\rho_h(S_0^{(i)},A_0^{(i)},\\dots,S_h^{(i)},A_h^{(i)})\\] where $\\tau^{(i)} = (S_0^{(i)},A_0^{(i)},\\dots,S_{H-1}^{(i)},A_{H-1}^{(i)})$. While this generally has lower variance than the previous expression, the fundamental problem remains. The likelihood ratio method is a general technique in the Monte-Carlo simulation literature. As such, there is a whole range of further ideas that can be applied to reduce the variance even further. Examples include baseline substraction, the doubly robust estimator, self-normalized importance weighting (the likelihood ratio in this context is also known as importance weighting), etc. However, rather than embarking on the futile mission of trying to survey all these and the other methods, we move on and consider the last case in our table. ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec19/#variance-reduction",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec19/#variance-reduction"
  },"172": {
    "doc": "19. Scaling with value function approximation",
    "title": "TD methods and variants",
    "content": "When $T_\\pi \\mathcal{F}\\subset \\mathcal{F}$ for the policy $\\pi$ to be evaluated, one can follow the approach . ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec19/#td-methods-and-variants",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec19/#td-methods-and-variants"
  },"173": {
    "doc": "19. Scaling with value function approximation",
    "title": "19. Scaling with value function approximation",
    "content": "PDF Version . ",
    "url": "/2024/w2022-lecture-notes/batch-rl/lec19/",
    
    "relUrl": "/w2022-lecture-notes/batch-rl/lec19/"
  },"174": {
    "doc": "22. Introduction",
    "title": "Sample complexity and regret: How good is the learner?",
    "content": "The goal of the learner is to collect as much reward as possible. We denote $V_k = \\sum_{h=0}^{H-1} r_{A_h^{(k)}}(S_h^{(k)})$ as the reward collected by the learner in episode $k$. The total reward is $\\sum_{k=1}^K V_k$. For the analysis it will be useful to introduce a normalization: Instead of directly arguing about the total reward, we compare the learner to the value \\(v_0^*(S_0^{(k)})\\) of the best policy in the MDP. This leads to the notation of regret defined as follows: . \\[R_K = \\sum_{k=1}^K \\big(v_0^*(S_0^{(k)}) - V_k\\big)\\] A learner has sublinear expected regret if $\\mathbb{E}[R_K/K]\\rightarrow 0$ as $K \\rightarrow \\infty$. Sublinear regret means that the average reward of the learner approaches the optimal value $v_0^*(\\mu)$ as the number of episodes increases. Certainly that is a desirable property! . Before we go on to construct learners with small regret, we briefly note that there are also other objectives. The most common alternative is PAC - which stands for probably approximately correct. A learner is said to be $(\\epsilon,\\delta)$-PAC if upon termination in episode $K$, it outputs a policy such that \\(v_0^{*}(s_0^{(k)}) - \\mathbb{E}[V_K] \\leq \\epsilon\\) with probability at least $1-\\delta$. We have discussed PAC bounds already in the context of planning. The difference to bounding regret is that in the first $K-1$ episodes, the learner does not ‘pay’ for choosing suboptimal actions. This is sometimes called a pure exploration problem. Note that a learner that achieves sublinear regret can be converted into a PAC learner (discussed in the notes). However, this may lead to a suboptimal (large) $K$ in the PAC framework. ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec22/#sample-complexity-and-regret-how-good-is-the-learner",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec22/#sample-complexity-and-regret-how-good-is-the-learner"
  },"175": {
    "doc": "22. Introduction",
    "title": "$\\epsilon$-greedy",
    "content": "There exist many ideas on how to design algorithms with small regret. We first note that a “greedy” agent can easily fail: Following the best actions according to some empirical estimate can easily get you trapped in a supoptimal policy (think of some examples where this can happen!). A simple remedy is to add a small amount of “forced” exploration: With (small) probability $\\epsilon$, we choose an action uniformly at random. Thereby we eventually collect samples from all actions to improve our estimates. With probabilty $(1-\\epsilon)$ we follow the “greedy” choice, that is the action that appears best under our current estimates. This gives raise to the name $\\epsilon$-greedy. It is often possible to show that $\\epsilon$-greedy converges. By carefully choosing the exploration probability $\\epsilon$, we may show that in finite MDPs, the regret is at most $R_K \\leq \\mathcal{O}(K^{2/3})$. As we will discuss later, there are multiple algorithms that achieve a regret of only $\\mathcal{O}(K^{1/2})$. Thus, $\\epsilon$-greedy is not the best algorithm to minimize regret. Not unexpectedly, this type of exploration can be quite sub-optimal. It is easy to construct examples, where $\\epsilon$-greedy takes exponential time (in the number of states) to reach an optimal policy. Can you find an example (Hint: construct the MDP such that each time the agent explores a suboptimal action, the agent is reset to the starting state)? . On the upside, $\\epsilon$-greedy is very simple and can easily used in more complex scenarios. In fact, it is a popular choice when using neural network function approximations, where theoretically grounded exploration schemes are much harder to obtain. ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec22/#epsilon-greedy",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec22/#epsilon-greedy"
  },"176": {
    "doc": "22. Introduction",
    "title": "Optimism Principle",
    "content": "A popular technique to construct regret minimizing algorithms is based on optimism in the face of uncertainty. To formally define the idea, let $\\mathcal{M}$ be the set of possible environments (e.g. finite MDPs). We make the realizability assumption that the true environment $M^* \\in \\mathcal{M}$ is in this set. After obtaining data in rounds $1,\\dots, k-1$, the learner uses the observations to compute a set of plausible models $\\mathcal{M}_k\\subset \\mathcal{M}$. The plausible model set is such that it contains the true model with high probabilty. Although this is not always required, it is useful to think of a decreasing sequence of sets $\\mathcal{M} \\supset\\mathcal{M}_1 \\supset\\mathcal{M}_2 \\supset \\cdots \\supset\\mathcal{M}_k$. This simply means that as more data arrives, the learner is able to exclude models that are statistically unlikely to produce the observation data. The optimism principle is to act according to the policy that achieves the highest reward among all plausible models, i.e. \\(\\begin{align} \\label{eq:opt-policy} \\pi_k = \\arg\\max_{\\pi} \\max_{M \\in \\mathcal{M}_k} v_M^\\pi \\end{align}\\) At this point it not be clear why this leads to an efficient learning algorithm (with small regret). The idea is that the learner systematically obtains data about the environment. For example, if data contradicts the optimistic model $\\tilde M_k = \\arg\\max_{M \\in \\mathcal{M}} \\max_\\pi v_M^\\pi$, then $\\tilde M_k \\notin \\mathcal{\\mathcal{M}}_{k+1}$ is excluded from the set of plausible models in the future. Consequently, the learner chooses a different policy in the next round. On the other hand, the learner ensures that \\(M^{*} \\in \\mathcal{M}_k\\) with high probability. In this case, it is often possible to show that the gap \\(v_{\\tilde M_k}^{\\pi_k} - v_{M^{*}}^{*} \\geq 0\\) is small (more specifically, behaves like a statistical estimation error of order $\\mathcal{O}(t^{-1/2})$ with a leading constant that depends on the “size” of $\\mathcal{M}$). One should also ask if the optimization problem \\(\\eqref{eq:opt-policy}\\) can be solved efficiently. This is far from always the case. Often one needs to rely on heuristics to implement the optimistic policy, or use other exploration techniques such as Thompson sampling (see below). How much regret the learner has of course depends on the concrete setting at hand. In the next lecture we will see how we can make use of optimism to design (and analyize) an online learning algorithm for finite MDPs. The literature has produced a large amount of papers with algorithms that use the optimism principle in many settings. This however does not mean that optimism is a universal tool. More recent literature has also pointed out limitations of the optimsm principle, and in lieu proposed other design ideas. ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec22/#optimism-principle",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec22/#optimism-principle"
  },"177": {
    "doc": "22. Introduction",
    "title": "Notes",
    "content": "Other Exploration Techniques . Some other notable exploration strategies are: . | Phased-Elimination and Experimental Design | Thompson Sampling | Information-Directed Sampling (IDS) and Estimation-To-Decisions (E2D) | . ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec22/#notes",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec22/#notes"
  },"178": {
    "doc": "22. Introduction",
    "title": "References",
    "content": "The paper showing the details behind how to convert between Regret and PAC bounds. | Dann, C., Lattimore, T., &amp; Brunskill, E. (2017). Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning. Advances in Neural Information Processing Systems, 30. [link] | . ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec22/#references",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec22/#references"
  },"179": {
    "doc": "22. Introduction",
    "title": "22. Introduction",
    "content": "PDF Version . Online learning in reinforcement learning refers to the idea that a learner is placed in an (initially) unknown MDP. By interacting with the MDP, the learner collects data about the unknown transition and reward function. The learner’s goal is to collect as much reward as possible, or output a near-optimal policy. The difference to planning is that the learner does not have access to the true MDP. Unlike in batch RL, the learner gets to decide what actions to play. Importantly, this means the learner’s action affect the data that is available to the learner (sometimes refered to as “closed loop”). The fact that the learner needs to create its own data leads to an important decision: Should the learner sacrifice reward to collect more data that will improve decision making in the future? Or should it act according to what seems currently best? Clearly, too much exploration will be costly if the learner chooses actions with low reward too often. On the other hand, playing actions that appear optimal with limited data comes at the risk of missing out on even better rewards. In the literature, this is commonly known as exploration-exploitation dilemma. The exploration-exploitation dilemma is not specific to the MDP setting. It already arises in the simpler (multi-armed) bandit setting (i.e. an MDP with only one state and stochastic reward). In the following, we focus on finite-horizon episodic (undiscounted) MDPs $M = (\\mathcal{S},\\mathcal{A},P,r, \\mu)$. The learner interacts with the MDP for $K$ episodes of length $H &gt; 0$. At the beginning of each episode $k=1\\,\\dots,K$, an initial state is sampled from the initial distribution $S_0^{k} \\sim \\mu$. The data collected during the $k^{th}$ episode is . \\[S_0^{(k)}, A_0^{(k)}, R_1^{(k)}, S_1^{(k)} A_1^{(k)}, R_2^{(k)}, S_2^{(k)}, \\dots S_{H-1}^{(k)}, A_{H-1}^{(k)},R_H^{(k)}, S_H^{(k)}\\] where $A_h^{k}$ is the action chosen by the learner at step $h$, $S_{h+1}^{(k)} \\sim P_{A_h^{(k)}}(S_h^{(k)})$ is the next state and $R_{h+1}^{(k)} \\sim r_{A_h^{(k)}}(S_h^{(k)})$ is the (possibly stochastic) reward. This model contains some important settings as a special case. Most notably, . | $H=1$ recovers the contextual bandit setting, where the “context” $S_0^{(k)}$ is sampled from the distribution $\\mu$ | $H=1$ and $S=1$ is the finite multi-armed bandit setting. | . ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec22/",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec22/"
  },"180": {
    "doc": "23. Tabular MDPs",
    "title": "UCRL: Upper Confidence Reinforcement Learning",
    "content": "The UCRL algorithm implements the optimism princple. For this we need to define a set of plausible models. First, we define the maximum likelihood estimates using data from rounds $1,\\dots, k-1$: . \\[P^{(k)}_a(s,s') = \\frac{N_k(s,a,s')}{1 \\vee N_k(s,a)}\\] The definition makes use of the notation $a \\vee b = \\max(a,b)$, and empirical counts: . \\[\\begin{align*} N_k(s,a) &amp;= \\sum_{k'&lt;k}\\sum_{h&lt;H} \\mathbb{I}(S_h^{(k)}=s,A_h^{(k)}=a)\\\\ N_k(s,a,s') &amp;= \\sum_{k'&lt;k}\\sum_{h&lt;H} \\mathbb{I}(S_h^{(k)}=s,A_h^{(k)}=a,S_{h+1}^{(k)}=s') \\end{align*}\\] Define the confidence set . \\[C_{k,\\delta} = \\{ P_a(s)\\,\\, :\\, \\,\\forall s,a\\,\\, \\|P_a^{(k)}(s) - P_a(s)\\|_1 \\leq \\beta_\\delta(N_k(s,a)) \\}\\] where $\\beta_\\delta : \\mathbb{N} \\rightarrow (0,\\infty)$ is a function that we will choose shortly. Our goal of choosing $\\beta_\\delta$ is to ensure that . | $P^* \\in C_{k,\\delta}$ for all $k=1,\\dots,K$ with probability at least $1-\\delta$ | $C_{k,\\delta}$ is “not too large” | . The second point will appear formually in the proof, however note that from a statistical perspective, we want the confidence set to be as efficient as possible. With the confidence set, we can now introduce the UCRL algorithm: . UCRL (Upper confidence reinforcement learning): . In episodes $k=1,\\dots,K$, . | Compute confidence set $C_{k,\\delta}$ | Use policy $\\tilde \\pi_k = \\arg\\max_\\pi \\max_{P \\in C_{k,\\delta}} v_P^\\pi$ | Observe episode data ${S_0^{(k)}, A_0^{(k)}, S_1^{(k)}, \\dots, S_{H-1}^{(k)}, S_{H-1}^{(k)}, S_H^{(k)}}$ | . Note that we omitted the rewards from the observation data. Since we made the assumption that the reward vector $r_a(s)$ is known, we can always recompute the rewards from the state and action sequence. For now we we also glance over the point of how to compute the optimistic policy $\\pi_k$ efficently, but we will get back to this point later. Step 1: Defining the confidence set . Lemma (L1-confidence set): Let $\\beta_\\delta(u) = 2\\sqrt{\\frac{S \\log(2) + \\log(u(u+1)SA/\\delta)}{2u}}$ and define the confidence sets . \\[C_{k,\\delta} = \\{ P_a(s)\\,\\, :\\, \\,\\forall s,a\\,\\, \\|P_a^{(k)}(s) - P_a(s)\\|_1 \\leq \\beta_\\delta(N_k(s,a)) \\}\\] Then, with probability at least $1-\\delta$, . \\[\\forall k \\geq 1, \\quad P^* \\in C_{k,\\delta}\\] . Proof: Let $s,a$ be fixed and denote by $X_v \\in \\mathcal{S}$ the next state observed upon visiting $(s,a)$ the $v^{\\text{th}}$ time. Assume that $(s,a)$ was visited in total $u$ times. Then \\(P_{u,a}(s,s') = \\frac{1}{u} \\sum_{v=1}^u \\mathbb{I}(X_v = s')\\). The Markov property implies that \\((X_v)_{v=1}^{u}\\) is i.i.d. Note that for any vector \\(p \\in \\mathbb{R}^{S}\\) we can write the 1-norm as \\(\\|p\\|_1 = \\sup_{\\|x\\|_\\infty \\leq 1} \\langle p,x\\rangle\\). Therefore . \\[\\|P_{u,a}(s) - P_a^*(s)\\|_1 = \\max_{x \\in \\{\\pm 1\\}^S} \\langle P_{u,a}(s) - P_a^*(s), x \\rangle\\] Fix some \\(x \\in \\{\\pm1\\}^S\\). \\[\\begin{align*} \\langle P_{u,a}(s) - P_a^*(s), x \\rangle &amp;= \\frac{1}{u} \\sum_{v=1}^u \\sum_{s'} x_{s'}\\big(\\mathbb{I}(X_v = s') - P_a^*(s,s')\\big)\\\\ &amp;=\\frac{1}{u} \\sum_{v=1}^u \\Delta_v \\end{align*}\\] where in the last line we defined $\\Delta_v = \\sum_{s’ \\in \\mathcal{S}} x_{s’}\\big(\\mathbb{I}(X_v = s’) - P_a^*(s,s’)\\big)$. Note that $\\mathbb{E}[\\Delta_v]=0$, $|\\Delta_v| \\leq 1$ and $(\\Delta_v)_{v=1}^u$ is an i.i.d. random variable. Therefore Hoeffding’s inequality implies that with probability at least $1-\\delta$, . \\[\\frac{1}{u} \\sum_{v=1}^u \\Delta_v \\leq 2\\sqrt{\\frac{\\log(1/\\delta)}{2u}}\\] Next note that \\(\\vert\\{\\pm1\\}^S\\vert = 2^S\\), therefore taking the union bound over all \\(x \\in \\{\\pm1\\}^S\\), we get that with probability at least $1-\\delta$, . \\[\\|P_{u,a}(s) - P_a^*(s)\\|_1 \\leq 2\\sqrt{\\frac{S \\log(2) + \\log(1/\\delta)}{2u}}\\] In a last step, we take a union bound over $s \\in \\mathcal{S}$, $a \\in \\mathcal{A}$ and $u \\geq 1$. For taking the union bound over the infinite set of natural numbers, we can use the following simple trick. Note that . \\[\\sum_{u=1}^\\infty \\frac{\\delta}{u(u+1)} = \\delta\\] This follows from the simple obseration that $\\frac{1}{u(u+1)} = \\frac{1}{u} - \\frac{1}{u+1}$ and using a telescoping sum argument. Therefore, with probability at least $1-\\delta$, for all $u \\geq 1$, $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$ . \\[\\|P_{u,a}(s) - P_a^*(s)\\|_1 \\leq 2\\sqrt{\\frac{S \\log(2) + \\log(u(u+1)SA/\\delta)}{2u}}\\] Lastly, the claim follows by noting that $P_a^{(k)}(s) = P_{N_k(s,a),a}(s)$. \\(\\qquad\\blacksquare\\) . Step 2: Bounding the regret . Theorem (UCRL Regret): The regret of UCRL defined with confidence sets $C_{k,\\delta}$ satisfies with probability at least $1-3\\delta$: . \\[R_K \\leq 4 c_\\delta H\\sqrt{SAHK} + 2 c_{\\delta} H^2 SA + 3H \\sqrt{\\frac{HK}{2} \\log(1/\\delta)}\\] where $c_{\\delta} = \\sqrt{2 S \\log(2) + \\log(HK(HK+1)SA/\\delta)}$. In particular, for large enough $K$, surpressing constants and logarithmic factors, we get . \\[R_K \\leq \\mathcal{\\tilde O}\\left( H^{3/2} S\\sqrt{AK \\log(1/\\delta)} \\right)\\] . Proof: Denote by $\\pi_k$ the UCRL policy defined as . \\[\\pi_k = \\arg\\max_{\\pi} \\max_{P \\in C_{k,\\delta}} v_{0,P}^\\pi(S_0^{(k)})\\] Further, let $\\tilde P^{(k)} = \\arg\\max_{P \\in C_{k,\\delta}} v_{0,P}^*(S_0^{(k)})$ be the optimistic model. In what follows we assume that we are on the event $\\mathcal{E} = \\cap_{k\\geq 1} C_{k,\\delta}$. By the previous lemma, $\\mathbb{P}(\\mathcal{E}) \\geq 1- \\delta$. Fix $k \\geq 1$ and decompose the (instantenous) regret in round $k$ as follows: . \\[\\begin{align*} v_0^*(S_0^{(k)}) - V_k =\\,\\, &amp; \\underbrace{v_{0,P^*}^*(S_0^{(k)}) - v_{0,\\tilde P_k}^*(S_0^{(k)})}_{\\text{(I)}}\\\\ &amp;+\\,\\, \\underbrace{v_{0,\\tilde P_k}^{\\pi_k}(S_0^{(k)}) - v_{0, P^*}^{\\pi_k}(S_0^{(k)})}_{\\text{(II)}}\\\\ &amp;+\\,\\, \\underbrace{v_{0, P^*}^{\\pi_k}(S_0^{(k)}) - V_k}_{\\text{(III)}} \\end{align*}\\] Note that we used that $v_{0,\\tilde P_k}^*(S_0^{(k)}) = v_{0,\\tilde P_k}^{\\pi_k}(S_0^{(k)})$ which holds because by definition $\\pi_k$ is an optimal policy for $\\tilde P_k$. The first term is easily bounded. This is the crucial step that makes use of the optimism principle. By \\(P^* \\in C_{k,\\delta}\\) and the choice of $\\tilde P_k$ it follows that $\\text{(I)} \\leq 0$. In particular, we already eliminated the dependence on the (unknown) optimal policy from the regret bound! . The last term is also relatively easy to control. Denote \\(\\xi_k = \\text{(III)}\\). Note that by the definition of the value function we have \\(\\mathbb{E}[ \\xi_k \\vert S_0^{(k)} ] = 0\\) and \\(\\vert\\xi_k\\vert \\leq H\\). Hence $\\xi_k$ behaves like noise! If $\\xi_k$ was an i.i.d variable we could directly apply Hoeffding’s inequality to bound \\(\\sum_{k=1}^K \\xi_k\\). The sequence $\\xi_k$ has a property that allows us to obtain a similar bound. Let . \\[\\mathcal{F}_k = \\{S_0^{(l)}, A_0^{(l)}, S_1^{(l)}, \\dots, S_{H-1}^{(l)}, S_{H-1}^{(l)}, S_H^{(l)}\\}_{l=1}^{k-1}\\] be the data available to the learner at the beginning of the episode $k$. Then by definition of the value function, $\\mathbb{E}[\\xi_k\\vert\\mathcal{F}_k, S_0^{(k)}] = 0$. A sequence of random variables $(\\xi_k)_{k\\geq 1}$ with this property is called a martingale difference sequence. Lucky for us, most properties that hold for (zero-mean) i.i.d. sequences can also be shown for martingale difference sequences. The analogue result to Hoeffding’s inequality is called the Azuma-Hoeffding’s inequalty. Applied to the sequence $\\xi_k$, Azuma-Hoeffdings inequality implies that . \\[\\sum_{k=1}^K \\xi_k \\leq H \\sqrt{\\frac{K}{2} \\log(1/\\delta)}\\] It remains to bound term (II) in the regret decomposition: . \\[\\text{(II)} = v_{0,P^*}^{\\pi_k}(S_0^{(k)}) - v_{0, \\tilde P^{(k)}}^{\\pi_k}(S_0^{(k)})\\] Using the Bellman equation, we can recursively compute the value function for any policy $\\pi$: . \\[\\begin{align*} v_{h,P}^{\\pi} &amp;= r^\\pi + M_\\pi Pv_{h+1,P}^\\pi\\,\\,,\\quad 0 \\leq h \\leq H-1\\\\ v_{H,P}^\\pi &amp;= 0 \\end{align*}\\] We introduce the following shorthand for the value difference of policy $\\pi_k$ under models $P^*$ and $\\tilde P^{(k)}$: . \\[\\delta_h^{(k)}= v_{h,\\tilde P^{(k)}}^{\\pi_k}(S_h^{(k)}) -v_{h,P^*}^{\\pi_k}(S_h^{(k)})\\] Let \\(\\mathcal{F}_{h,k}\\) contain all observation data up to episode $k$ and step \\(h\\) including \\(S_h^k\\). Using the Bellman equation, we can write . \\[\\begin{align*} \\delta_h^{(k)} &amp;= M_{\\pi_k} \\tilde P^{(k)} v_{h+1,\\tilde P^{(k)}}^{\\pi_k}(S_h^{(k)}) - M_{\\pi_k} P^* v_{h+1,P^*}^{\\pi_k}(S_h^{(k)}) \\pm M_{\\pi_k} P^*V_{h+1,\\tilde P^{(k)}}(S_h^{(k)})\\\\ &amp;= (M_{\\pi^k}(\\tilde P^{(k)} - P^*) v_{h+1, \\tilde P^{(k)}}^{\\pi_k})(S_h^{(k)}) + (M_{\\pi_k}P^*(v_{h+1,\\tilde P^{(k)}}^{\\pi_k} - v_{h+1,P^*}^{\\pi_k})(S_h^{(k)})\\\\ &amp;\\leq \\|P_{A_h^{(k)}}^*(S_h^{(k)}) - \\tilde P_{A_h^{(k)}}^{(k)}(S_h^{(k)}) \\|_1 H+ \\delta_{h+1}^{(k)} + \\underbrace{\\big(\\mathbb{E}[\\delta_{h+1}^{(k)}|\\mathcal{F}_{h,k}] - \\delta_{h+1}^{(k)}\\big)}_{=:\\eta_{h+1}^{(k)}}\\\\ &amp;\\leq 2 H \\beta_\\delta(N_k(S_h^{(k)}, A_h^{(k)})) + \\delta_{h+1}^{(k)} + \\eta_{h+1}^{(k)} \\end{align*}\\] The first inequality uses that for any two vectors $w,v$, we have \\(\\langle w,v\\rangle \\leq \\|w\\|_1 \\|v\\|_{\\infty}\\) and \\(\\|v_{h+1,\\tilde P^{(k)}}^{\\pi_k}\\|_\\infty \\leq H\\). Further we use that $\\pi_k$ is a deterministic policy, therefore \\(M_{\\pi_k} P(S_h^{(k)}) = P_{A_h^{(k)}}(S_h^{(k)})\\). The second follows from the definition of the confidence set in the previous lemma: . \\[\\begin{align*} &amp;\\|P_{A_h^{(k)}}^*(S_h^{(k)}) - \\tilde P_{A_h^{(k)}}^{(k)}(S_h^{(k)}) \\|_1 \\\\ &amp;\\leq \\|P_{A_h^{(k)}}^*(S_h^{(k)}) - P_{A_h^{(k)}}^{(k)}(S_h^{(k)}) \\|_1 + \\|P_{A_h^{(k)}}^{(k)}(S_h^{(k)}) - \\tilde P_{A_h^{(k)}}^{(k)}(S_h^{(k)}) \\|_1\\\\ &amp;\\leq 2 \\beta_\\delta(N_k(S_h^{(k)}, A_h^{(k)})) \\end{align*}\\] Telescoping and using that $\\delta_H^{(k)} = 0$ yields . \\[\\delta_0^{(k)} \\leq \\eta_1^{(k)} + \\cdots + \\eta_{H-1}^{(k)} + 2H \\underbrace{\\sum_{h=0}^{H-1}\\beta_\\delta(N_k(S_h^{(k)}, A_h^{(k)}))}_{\\text{(IV)}}\\] Note that $(\\eta_h^{(k)})_{h=1}^{H-1}$ is another martingale difference sequence (with \\(\\eta_h^{(k)}\\vert \\leq H\\)) that can be bounded by Azuma-Hoeffding: . \\[\\sum_{k=1}^K\\sum_{h=1}^{H-1} \\eta_h^{(k)} \\leq 2H \\sqrt{\\frac{HK}{2}\\log(1/\\delta)}\\] It remains to bound term $\\text{(IV)}$. For this we make use of the following algebraic lemma: . Lemma: . For any sequence \\(m_1, \\dots, m_k\\) that satisfies \\(m_1 + \\dots + m_k \\geq 0\\): . \\[\\sum_{k=1}^K \\frac{m_k}{\\sqrt{1 \\vee (m_1 + \\cdots + m_k)}} \\leq 2 \\sqrt{m_1 + \\cdots + m_k}\\] . Proof of Lemma: Let \\(f(x) = 1/\\sqrt{x}\\). \\(f(x)\\) is a concave function on \\((0,\\infty)\\). Therefore \\(f(A + x) \\leq f(A) + x f'(A)\\) for all $A, A + x, &gt;0$. This translates to: . \\[\\sqrt{A + x} \\leq \\sqrt{A} + \\frac{x}{2\\sqrt{A}}\\] The claim follows from telescoping. \\(\\qquad\\blacksquare\\) . Continuing the proof of the theorem where we need to bound $\\text{(IV)}$. Denote $c_{\\delta} = \\sqrt{2 S \\log(2) + \\log(HK(HK+1)SA/\\delta)}$. Further let \\(M_k(s,a) = \\sum_{h=1}^{H-1} \\mathbb{I}(S_h^{(k)}=s, A_h^{(k)} = a)\\) and note that \\(N_k(s,a) = M_1 + \\cdots + M_{k-1}\\). Then . \\[\\begin{align*} \\sum_{k=1}^K \\sum_{h=0}^{H-1}\\beta_\\delta(N_k(S_h^{(k)}, A_h^{(k)})) &amp;\\leq c_{\\delta} \\sum_{s,a} \\sum_{k=1}^K \\sum_{h=0}^{H-1}\\frac{\\mathbb{I}(S_h^{(k)}=s, A_h^{(k)} = a)}{\\sqrt{1 \\vee N_k(s,a)}}\\\\ &amp;=c_{\\delta} \\sum_{s,a} \\sum_{k=1}^K \\frac{M_k}{\\sqrt{1 \\vee (M_1 + \\dots + M_{k-1})}} \\end{align*}\\] Next, using the algebraic lemma above and the fact that $M_k(s,a) \\leq H$, we find . \\[\\begin{align*} \\sum_{k=1}^K \\sum_{h=0}^{H-1}\\beta_\\delta(N_k(S_h^{(k)}, A_h^{(k)})) &amp;\\leq c_{\\delta} \\sum_{s,a} \\sum_{k=1}^K \\frac{M_k(s,a)}{\\sqrt{1 \\vee( M_1(s,a) + \\dots + M_{k-1}(s,a))}}\\\\ &amp;\\leq c_{\\delta} \\sum_{s,a} \\sum_{k=1}^K \\frac{M_k(s,a)}{\\sqrt{1 \\vee (M_1(s,a) + \\dots + M_{k}(s,a) - H)}}\\\\ &amp;\\leq c_{\\delta} \\sum_{s,a} \\sum_{k=1}^K \\frac{M_k(s,a) \\mathbb{I}(M_1(s,a) + \\dots + M_{k}(s,a) &gt; H)}{\\sqrt{M_1(s,a) + \\dots + M_{k}(s,a) - H}} + c_{\\delta} HSA\\\\ &amp;\\leq 2 c_{\\delta}\\sum_{s,a} \\sqrt{N_k(s,a)} + c_{\\delta} HSA\\\\ &amp;\\leq 2 c_{\\delta} SA \\sqrt{\\sum_{s,a} N_k(s,a)/SA} + c_{\\delta} HSA\\\\ &amp;= 2 c_{\\delta}\\sqrt{SAHK} + c_{\\delta} HSA \\end{align*}\\] The last inequality uses Jensen’s inequality. Collecting all terms and taking the union bound over two applications of Azuma-Hoeffdings and the event $\\mathcal{E}$ completes the proof. \\(\\qquad\\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec23/#ucrl-upper-confidence-reinforcement-learning",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec23/#ucrl-upper-confidence-reinforcement-learning"
  },"181": {
    "doc": "23. Tabular MDPs",
    "title": "Unknown reward functions",
    "content": "In our analysis of UCRL we assumed that the reward function is known. While this is quite a common assumption in the literature, it is mainly for simplicity. We also don’t expect the bounds to change by much: Estimating the rewards is not harder than estimating the transition kernels. To modify the analysis and account for unkown rewards, we first consider the case with deterministic reward function \\(r_a(s) \\in [0, R_{\\max}]\\), where $R_{\\max}$ is some known upper bound on the reward per step. Embracing the idea of optimism, we define reward estimates . \\[\\hat r_a^{(k)}(s) = \\begin{cases} r_{A_h^{(k')}}(S_h^{(k')}) &amp; \\text{(s,a) was visited in a round $k' &lt; k$ and step $h$}\\\\ R_{\\max} &amp; \\text{else.} \\end{cases}\\] Clearly this defines an optimistic estimate, \\(\\hat r_a^{(k)}(s) \\geq r_a(s)\\). Moreover, we have \\(\\hat r_{A_h^{(k)}}^{(k)}(S_h^{(k)}) \\neq r_{A_h^{(k)}}(S_h^{(k)})\\) at most $SA$ times. Therefore the regret in the previous analysis is increased by at most $R_{\\max}SA$. When the reward is stochastic, we can use a maximum likelihood estimate of the reward and construct confidence bounds around the estimate. This way we can define an optimistic reward. Still not much changes, as the reward estimates concentrate at the same rate as the estimates of $P$. ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec23/#unknown-reward-functions",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec23/#unknown-reward-functions"
  },"182": {
    "doc": "23. Tabular MDPs",
    "title": "UCBVI: Upper Confidence Bound Value Iteration",
    "content": "Computing the UCRL policy can be quite challenging. However, we can relax the construction so that we can use backward induction. We define a time-inhomogenous relaxation of the confidence set: . \\[C_{k,\\delta}^H = \\underbrace{C_{k,\\delta} \\times \\cdots \\times C_{k,\\delta}}_{H \\text{ times}}\\] Let \\(\\tilde P_{1:H,k} := (\\tilde P_{1,k}, \\dots, \\tilde P_{H,k}) = \\arg\\max_{P \\in C_{k,\\delta}^H} v^*_P(s_0^{(k)})\\) be the optimistic (time-inhomogenous) transition matrices and \\(\\pi_k = \\arg\\max_{\\pi} v_{\\tilde P_{1:H,k}}^\\pi\\) the optimal policy for the optimistic model \\(\\tilde P_{1:H,k}\\). Then \\(v_{\\tilde P_{1:H,k}}^{\\pi^k} = v_{\\tilde P_{1:H,k}}^* = v^{(k)}\\) is defined by the following backwards induction: . \\[\\begin{align*} v^{(k)}_H(s) &amp;= 0 \\qquad\\forall s \\in [S]\\\\ Q_h^{(k)}(s,a) &amp;= r(s,a) + \\max_{P \\in C_{k,\\delta}} P_a(s) v_{h+1}^{(k)}\\\\ v^{(k)}_h(s) &amp;= \\max_{a} Q_h^{(k)}(s,a) \\end{align*}\\] Note that the maximum in the second line is a linear optimization with convex constraints that can be solved efficiently. Further, the proof of the UCRL regret still applies, because we used the same (step-wise) relaxation in the analysis. We can further relax the backward induction to avoid the optimization over $C_{k,\\delta}$ completely: . \\[\\begin{align*} \\max_{P \\in C_{k,\\delta}} P_a(s) v_{h+1}^{(k)}&amp;\\leq P_a^{(k)}(s) v_{h+1}^{(k)}+ \\max_{P \\in C_{k,\\delta}} (P_a(s) - P_a^{(k)}(s)) v_{h+1}^{(k)}\\\\ &amp;\\leq P_a^{(k)}(s) v_{h+1}^{(k)}+ \\max_{P \\in C_{k,\\delta}} \\|P_a(s) - P_a^{(k)}(s))\\|_1 \\| v_{h+1}^{(k)}\\|_\\infty\\\\ &amp;\\leq P_a^{(k)}(s) v_{h+1}^{(k)}+ \\beta_{\\delta}(N_k(s,a))H\\\\ \\end{align*}\\] This leads us to the the UCBVI (upper confidence bound value iteration) algorithm. In episode $k$, UCBVI uses value iteration for the estimated transition kernel $P_a^{(k)}(s)$ and optimistic reward function $r_a(s) + H \\beta_\\delta(N_k(s,a))$ to compute the policy. UCBVI (Upper confidence bound value iteration): . In episodes $k=1,\\dots,K$, . | Compute optimistic value function: | . \\[\\begin{align*} v^{(k)}_H(s) &amp;= 0 \\qquad\\forall s \\in [S]\\\\ b_k(s,a) &amp;= H\\beta_{\\delta}(N_k(s,a))\\\\ Q_h^{(k)}(s,a) &amp;= \\min\\left(r(s,a) + b_k(s,a) + P_a^{(k)}(s) v_{h+1}^{(k)}, H\\right)\\\\ v^{(k)}_h(s) &amp;= \\max_{a} Q_h^{(k)}(s,a) \\end{align*}\\] . | Follow greedy policy $A_{h}^{(k)} = \\arg\\max_{A} Q_h^{(k)}(S_h^{(k)}, A)$ | Observe episode data ${S_0^{(k)}, A_0^{(k)}, S_1^{(k)}, \\dots, S_{H-1}^{(k)}, S_{H-1}^{(k)}, S_H^{(k)}}$ | . Note that we truncate the $ Q_h^{(k)}$-function to be at most $H$, this avoids a blow up by a factor of $H$ in the regret bound. Carefully checking that the previous analysis still applies shows that UCBVI has regret at most $R_K \\leq \\mathcal{O}(H^{2}S \\sqrt{AK})$. By more carefully designing the reward bonuses for UCBVI, it is possible to achieve $R_K\\leq \\mathcal{\\tilde O}(H^{3/2}\\sqrt{SAK})$ which matches the lower bound up to logarithmic factors in the time in-homogeneous setting. ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec23/#ucbvi-upper-confidence-bound-value-iteration",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec23/#ucbvi-upper-confidence-bound-value-iteration"
  },"183": {
    "doc": "23. Tabular MDPs",
    "title": "Notes",
    "content": " ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec23/#notes",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec23/#notes"
  },"184": {
    "doc": "23. Tabular MDPs",
    "title": "References",
    "content": "The original UCRL paper. Notice that they consider the infinite horizon average reward setting, which is different from the episodic setting we present. Auer, P., &amp; Ortner, R. (2006). Logarithmic online regret bounds for undiscounted reinforcement learning. Advances in neural information processing systems, 19. [link] . The UCBVI paper. Notice that they consider the homogeneous setting, which is different from the in-homogeneous setting we present. Azar, M. G., Osband, I., &amp; Munos, R. (2017, July). Minimax regret bounds for reinforcement learning. In International Conference on Machine Learning (pp. 263-272). PMLR. [link] . The paper that presents the lower bound. Notice the they consider the infinite horizon average reward setting. Thus, there results contains a diameter term $D$ instead of a horizon term of $H$. Auer, P., Jaksch, T., &amp; Ortner, R. (2008). Near-optimal regret bounds for reinforcement learning. Advances in neural information processing systems, 21. [link] . ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec23/#references",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec23/#references"
  },"185": {
    "doc": "23. Tabular MDPs",
    "title": "23. Tabular MDPs",
    "content": "PDF Version . In this lecture we will analize an online learning algorithm for the finite-horizon episodic MDP setting. Let \\(M=(\\mathcal{S}, \\mathcal{A}, P^*, r, \\mu, H)\\) be an MDP with finite state and action spaces $\\mathcal{S}$ and $\\mathcal{A}$, unknown transition matrix \\(P^*\\), known reward function $r_a(s) \\in [0,1]$, an initial state distribution $\\mu$, and length of each episode $H \\ge 1$. The star-superscript in $P^*$ is used to distinquish the true environment from other (e.g. estimated) environments that occur in the algorithm and the analysis. The assumption that the reward function $r$ is known is for simplicity. In fact, most of the hardness (in terms of sample complexity and designing the algorithm) comes from unknown transition probabilities. We will focus on the finite-horizon setting where the learner interacts with the MDP over $k=1,\\dots, K$ episodes of length $H \\ge 1$. Most, but not all ideas translate to the infinite-horizon discounted or average reward settings. Recall that the regret is defined as follows: . \\[R_K = \\sum_{k=1}^K v_0^*(S_0^{(k)}) - V_k\\] where $V_k = \\sum_{h=0}^{H-1} r_{A_h^{(k)}}(S_h^{(k)})$. ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec23/",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec23/"
  },"186": {
    "doc": "24. Featurized MDPs",
    "title": "Linear Mixture MDPs",
    "content": "We focus on the episodic, finite-horzion MDPs \\(M=(\\mathcal{S}, \\mathcal{A}, P_h, r_h, \\mu, H)\\) with time in-homogenous reward \\(r_h\\) and transition matrix \\(P_h\\). We let \\(\\mathcal{S}\\) be a finite but possibly very large state space, and \\(\\mathcal{A}\\) be a finite action space. With care, most of the analysis can be extended to infinite state and action spaces. As before, we assume that the reward function \\(r_h(s,a) \\in [0,1]\\) is known. We now impose additional (linear) structure on the transition kernel $P_h$. For this we assume the learner has access to features \\(\\phi(s,a,s') \\in \\mathbb{R}^d\\) that satisfy \\(\\|\\phi(s,a,s')\\|_2 \\leq 1\\). In time-inhomogeneous linear mixture MDPs, the transition kernel is of the form . \\[P_{h,a}(s,s') = \\langle \\phi(s,a,s'), \\theta_h^* \\rangle\\] for some unkown parameter \\(\\theta_h^* \\in \\mathbb{R}^d\\) with \\(\\|\\theta_h^*\\|_2 \\leq 1\\). We remark that tabular MDPs are recovered using $\\phi(s,a,s’) = e_{s,a,s’}$, where $e_{s,a,s’}$ are the unit vectors in \\(\\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}\\times \\mathcal{S}}\\). For any function \\(V : \\mathcal{S} \\rightarrow \\mathbb{R}\\), we define . \\[\\phi_V(s,a) = \\sum_{s'} \\phi(s,a,s')V(s') \\in \\mathbb{R}^d\\] Note that \\(\\langle\\phi_V(s,a), \\theta^*\\rangle\\) predicts the expected value of \\(V(s')\\) when \\(s'\\) is sampled from \\(P_{h,a}(s)\\): . \\[P_{h,a}(s)V = \\sum_{s'} P_{h,a}(s,s') V(s') = \\sum_{s'} \\langle \\phi(s,a,s'), \\theta_h^* \\rangle V(s') = \\langle \\phi_V(s,a), \\theta_h^* \\rangle\\] Value Targeted Regression (VTR) . Now that we have specified the parametrized model, the next step is to construct an estimator of the unknown paramter. An estimator of $\\theta^*$ allows us to predict the value of any policy. For the algorithm, we are particularly interested in constructing optimistic estimates of the value function. Hence we will also need a confidence set. Let \\((V_h^{(j)})^{j&lt;k}_{h\\leq H}\\) be a sequence of value functions constructed up to episode \\(k-1\\). Let \\(\\phi_{h,j} =\\phi_{V_{h+1}^{(j)}}(S_h^{(j)}, A_h^{(j)})\\) and \\(y_{h,j} = V_{h+1}^{(j)}(S_{h+1}^{(j)})\\). By constrution, we have that \\(\\mathbb{E}[y_{h,j}] = \\langle \\phi_{h,j}, \\theta^* \\rangle\\) and \\(\\vert y_{h,j}\\vert \\leq H\\). Define the regularized least-squares estimator . \\[\\hat \\theta_{h,k} = \\arg\\min_{\\theta} \\sum_{j=0}^{k-1} \\big(\\langle \\phi_{h,j},\\theta\\rangle - y_{h,j}\\big)^2 + \\lambda \\|\\theta\\|^2\\] Let $\\mathbf{I}_d \\in \\mathbb{R}^{d\\times d}$ be the idenity matrix. We have the following closed form for \\(\\hat \\theta_{h,k}\\): . \\[\\begin{align*} \\hat \\theta_{h,k} = \\Sigma_{h,k}^{-1} \\sum_{j=0}^{k-1}\\phi_{h,j} y_{h,j}\\qquad \\text{where} \\quad \\Sigma_{h,k} = \\sum_{j=0}^{k-1} \\phi_{h,j}\\phi_{h,j}^{\\top} + \\lambda \\mathbf{I}_d \\end{align*}\\] The next step is to quantify the uncertainy in the estimation. Mirroring the steps in the tabular setting, we construct a confidence set for \\(\\hat \\theta_{h,k}\\). For a positive (semi-)definite matrix \\(\\Sigma \\in \\mathbb{R}^{d\\times d}\\) and vector \\(v \\in \\mathbb{R}^d\\), define the (semi-)norm \\(\\|a\\|_\\Sigma = \\sqrt{\\langle v, \\Sigma v \\rangle}\\). We make use of the following elliptical confidence set for \\(\\hat \\theta_{h,k}\\) . \\[C_{h,\\delta}^{(k)} = \\{\\theta : \\|\\theta - \\hat \\theta_{h,k}\\|_{\\Sigma_{h,k}}^2 \\leq \\beta_{h,k,\\delta} \\}\\] where . \\[\\beta_{h,k,\\delta}^{1/2} = H\\sqrt{\\log \\det(\\Sigma_{h,k}) - \\log \\det(\\Sigma_{h,0}) + 2 \\log(1/\\delta)} + \\sqrt{\\lambda}\\] The log determinant of \\(\\Sigma_{h,k}\\) can be computed online by the algorithm. For the analysis, it is useful to further upper bound \\(\\beta_{h,k,\\delta}\\). It is possible to show the following upper bound on \\(\\beta_{h,k,\\delta}\\) that holds independent of the data sequence: . \\[\\beta_{h,k,\\delta}^{1/2} \\leq H \\sqrt{d\\log (1 + k/(d\\lambda)) + 2\\log(1/\\delta)} + \\sqrt{\\lambda}\\] For a derivation of the above inequality see Lemma 19.4 of the Bandit Book. The next lemma formally specifies the confidence probabilty. Lemma (Online Least-Squares Confidence) Fix some \\(0 \\leq h &lt; H\\). Then . \\[\\mathbb{P}[\\theta_h^* \\in \\cap_{k \\geq 1}C_{h,k,\\delta}] \\geq 1-\\delta\\] . Proof: The above result is presented as Theorem 2 in Abbasi-Yadkori et al (2011), where the proof can also be found. \\(\\qquad\\blacksquare\\) . The confidence set can be used to derive bounds on the estimation error with probability at least \\(1-\\delta\\) as follows: . \\[|\\langle \\phi_V(s,a), \\hat \\theta_{h,k} - \\theta^* \\rangle| \\leq \\| \\phi_V(s,a)\\|_{\\Sigma_{h,k}^{-1}} \\|\\hat \\theta_{h,k} - \\theta^*\\|_{\\Sigma_{h,k}} \\leq \\beta_{h,k,\\delta}^{1/2} \\| \\phi_V(s,a)\\|_{\\Sigma_{h,k}^{-1}}\\] The first inequality is by Cauchy-Schwarz and the second inequality uses the confidence bound from the previous lemma. UCRL-VTR . Similar to the tabular UCRL and UCBVI algorithms, UCRL-VTR uses the estimates \\(\\hat \\theta_{h,k}\\) to compute an optimistic policy. One way of obtaining an optimistic policy is from optimistic Q-estimates \\(Q_h^{(k)}(s,a)\\) defined via backwards induction. Then UCRL-VTR follows the greedy policy w.r.t. the optimistic Q-values. UCRL-VTR . In episodes \\(k=1,\\dots,K\\), . | Set \\(V^{(k)}_H(s) = 0\\). Compute \\(\\hat \\theta_{h,k}\\) and \\(\\Sigma_{h,k}\\). Recursively define optimistic value functions . For \\(h=H-1,\\dots,0\\): . \\[\\begin{align*} \\hat \\theta_{h,k} &amp;= \\arg\\min_{\\theta} \\sum_{j=1}^{k-1} \\big(\\langle\\phi_{h,j}, \\theta \\rangle - y_{h,j}\\big)^2 + \\lambda \\|\\theta\\|_2^2\\\\ \\Sigma_{h,k} &amp;= \\sum_{j=1}^k \\phi_{h,j}\\phi_{h,j}^\\top + \\lambda \\mathbf{I}_d\\\\ Q_h^{(k)}(s,a) &amp;= \\big(r_h(s,a) + \\langle \\phi_{V_{h+1}^{(k)}}(s,a), \\hat \\theta_{h,k} \\rangle + \\beta_{h,k,\\delta/H}^{1/2}\\|\\phi_{V_{h+1}^{(k)}}(s,a)\\|_{\\Sigma_{h,k}^{-1}} \\big)\\wedge H\\\\ V_h^{(k)}(s) &amp;= \\max_{a} Q_h^{(k)}(s,a) \\end{align*}\\] | Follow greedy policy w.r.t. $Q_h^{(k)}(s,a)$. For \\(h = 0, \\dots, H-1\\): . \\[\\begin{align*} A_{h}^{(k)} &amp;= \\arg\\max_{a \\in \\mathcal{A}} Q_h^{(k)}(S_h^{(k)}, a) \\end{align*}\\] Let \\(\\phi_{h,k} =\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\) and \\(y_{h,k} = V_{h+1}^{(k)}(S_{h+1}^{(k)})\\). | . We are now in the position to state a regret bound for UCRL-VTR. Theorem (UCRL-VTR Regret) The regret of UCRL-VTR satisfies with probability at least \\(1-2\\delta\\): . \\[R_K \\leq \\mathcal{O}\\big(d H^{2}\\log(K) \\sqrt{K \\log(KH/\\delta)} \\big)\\] . Note that the bound scales with the feature dimension \\(d\\), but not the size of the state space or action space. The lower bound for this setting is \\(R_K \\geq \\Omega(dH^{3/2} \\sqrt{K})\\), therefore our upper bound is tight except for a factor \\(\\sqrt{H}\\). Proof: . Our proof strategy follows the same steps as in the proof of UCRL. Step 1 (Optimism): . Taking the union bound over $h=0,\\dots, H-1$, the previous lemma implies that with probability at least \\(1-\\delta\\), for all $h \\in [H-1]$ and all $k \\geq 0$, \\(\\theta_h^* \\in C_{h,\\delta/H}^{(k)}\\). In the following, we condition on this event. Using induction over \\(h=H, H-1, \\dots, 0\\), we can show that . \\[V^*_0(S_h^{(k)}) \\leq V_{0}^{(k)}(S_h^{(k)})\\] Step 2 (Bellman recursion and estimation error): . For any $h =0, \\dots, H-1$, we find . \\[\\begin{align*} &amp;V_{h}^{(k)}(S_h^{(k)}) - V_h^{\\pi_k}(S_h^{(k)}) \\\\ &amp;\\leq \\langle \\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)}), \\hat \\theta_{h,k}\\rangle + \\beta_{h,k,\\delta/H}^{1/2} \\|\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\|_{\\Sigma_{h,k}^{-1}} - P_{h, A_{h}^{(k)}}^*(S_h^{(k)}) V_{h+1}^{\\pi_k}\\\\ &amp;= \\langle \\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)}), \\hat \\theta_{h,k} - \\theta^*\\rangle + \\beta_{h,k,\\delta/H}^{1/2} \\|\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\|_{\\Sigma_{h,k}^{-1}} + P_{h, A_{h}^{(k)}}^*(S_h^{(k)}) (V_{h+1}^{(k)}- V_{h+1}^{\\pi_k}) \\end{align*}\\] The inequality is by the definition of $V_h^{(k)}$ and dropping the truncation, and in the last line we add and subtract \\(P_{h, A_{h}^{(k)}}^*(S_h^{(k)}) V_{h+1}^{(k)} = \\langle \\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)}), \\theta^*\\rangle\\). Further, by Cauchy-Schwarz on the event \\(\\theta^* \\in C_{k,\\delta/H}\\) we get . \\[\\langle \\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)}), \\hat \\theta_{h,k} - \\theta^*\\rangle \\leq \\beta_{h,k,\\delta/H}^{1/2} \\|\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\|_{\\Sigma_{h,k}^{-1}}\\] Continuing the previous display, we find . \\[\\begin{align*} &amp;V_{h}^{(k)}(S_h^{(k)}) - V_h^{\\pi_k}(S_h^{(k)}) \\\\ &amp;\\leq 2 \\beta_{h,k,\\delta/H}^{1/2} \\|\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\|_{\\Sigma_{h,k}^{-1}} + P_{h, A_{h}^{(k)}}^*(S_h^{(k)}) (V_{h+1}^{(k)}- V_{h+1}^{\\pi_k})\\\\ &amp;= 2 \\beta_{h,k,\\delta/H}^{1/2} \\|\\phi_{V_{h+1}^{(k)}}(S_h^{(k)}, A_h^{(k)})\\|_{\\Sigma_{h,k}^{-1}} + V_{h+1}^{(k)}(S_{h+1}^{(k)}) - V_{h+1}^{\\pi_k}(S_{h+1}^{(k)}) + \\xi_{h,k} \\end{align*}\\] where we defined . \\[\\xi_{h,k} = \\big(P_{h, A_{h}^{(k)}}^*(S_h^{(k)}) (V_{h+1}^{(k)}- V_{h+1}^{\\pi_k})\\big)- \\big(V_{h+1}^{(k)}(S_{h+1}^{(k)}) - V_{h+1}^{\\pi_k}(S_{h+1}^{(k)})\\big)\\] Recursively appliying the previous inequality and summing over all episodes yields . \\[\\sum_{k=1}^K V_{0}^{(k)}(S_0^{(k)}) - V_0^{\\pi_k}(S_0^{(k)}) \\leq \\sum_{k=1}^K \\sum_{h=0}^{H-1} 2 \\beta_{h,k,\\delta}^{1/2} \\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}} + \\xi_{h,k}\\] Note that $\\xi_{h,k}$ is a martingale difference sequence, hence by Azuma-Hoeffdings inequality we have with probability at least $1-\\delta$, . \\[\\sum_{k=1}^K \\sum_{h=0}^{H-1} \\xi_{h,k} \\leq H \\sqrt{\\frac{HK}{2} \\log(1/\\delta)}\\] Step 3 (Cauchy-Schwarz): . Note that \\(\\beta_{h,k,\\delta}\\) is non-decreasing in both \\(h\\) and \\(k\\). Very little is lost by bounding \\(\\beta_{h,k,\\delta} \\leq \\beta_{H,K,\\delta}\\). From the previous step, we are left to bound the sum over uncertainties \\(\\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}}\\). We start with an application of the Cauchy-Schwarz inequality. Applied to sequences \\((a_i)_{i=1}^n\\), \\((b_i)_{i=1}^n\\), we have that \\(\\vert\\sum_{i=1}^n a_i b_i \\vert \\leq \\sqrt{\\sum_{i=1}^n a_i^2 \\sum_{j=1}^n b_i^2}\\). Applied to the regret, we get: . \\[\\sum_{k=1}^K \\sum_{h=0}^{H-1} 2 \\beta_{h,k,\\delta}^{1/2} \\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}} \\leq \\sum_{h=0}^{H-1} 2 \\beta_{h,K,\\delta}^{1/2} \\sqrt{K \\sum_{k=1}^K \\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}}^2}\\] Step 4 (Elliptic potential lemma): . The penultima step is to control the sum over squared uncertainties \\(\\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}}^2\\). This classical result is sometimes refered to as the elliptic potential lemma: . \\[\\sum_{k=1}^K \\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}}^2 \\leq \\mathcal{O}(d \\log(K))\\] The proof, as mentioned earlier, can be found as Lemma 19.4 in the Bandit Book. Step 5 (Summing up): . It remains to chain the previous steps and take the union bound over the event where the confidence set contains the true parameter and the application of Azuma-Hoeffdings. \\[\\begin{align*} R_K &amp;= \\sum_{k=1}^K V_{0}^{(k)}(S_0^{(k)}) - V_0^{\\pi_k}(S_0^{(k)}) \\\\ &amp;\\leq \\sum_{k=1}^K \\sum_{h=0}^{H-1} \\big(2 \\beta_{h,k,\\delta}^{1/2} \\|\\phi_{h,k}\\|_{\\Sigma_{h,k}^{-1}} + \\xi_{h,k}\\big)\\\\ &amp;\\leq C \\cdot H \\beta_{H,K,\\delta}^{1/2} \\sqrt{d \\log(K) K} + H^{3/2} \\sqrt{2K \\log(1/\\delta)} \\end{align*}\\] For some universal constant \\(C\\). This completes the proof. \\(\\qquad\\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec24/#linear-mixture-mdps",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec24/#linear-mixture-mdps"
  },"187": {
    "doc": "24. Featurized MDPs",
    "title": "Linear MDPs",
    "content": "So far we have seen the linear mixture MDP model. This is not the only way one can parameterize the transition matrix. An alternative is the linear MDP model, defined as follows for features $\\phi(s,a) \\in \\mathbb{R}^d$ and parameters \\(\\psi_h^{*} \\in \\mathbb{R}^{d\\times S}\\) and \\(\\theta_h^* \\in \\mathbb{R}^d\\): . \\[\\begin{align*} P_h^*(s,s') &amp;= \\langle\\phi(s,a), \\psi_h^*(s')\\rangle\\\\ r_h(s,a) &amp;= \\langle \\phi(s,a), \\theta_h^* \\rangle \\end{align*}\\] Note that tabular MDPs are recovered using $\\phi(s,a) = e_{s,a}$, where $e_{s,a}$ are the unit vectors in \\(\\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}}\\). Compared to the linear mixture model, an immediate observation is that the dependence on the the next state $s’$ is pushed into the parameter \\(\\psi_h(s') \\in \\mathbb{R}^d\\). Consequently, the dimension of the parameter space scales with the number of states, and it is not immediately clear how we can avoid the $S$ dependence in the regret bounds. Another consequence of this model is that the $Q$-function for any policy is linear in the features $\\phi(s,a)$. Lemma: . Under the linear MDP assumption, for any policy $\\pi$ the Q-function $Q_h^\\pi(s,a)$ is linear in the features $\\phi(s,a)$. That is, there exist parameters $w_h^\\pi \\in \\mathbb{R}^d$ such that . \\[Q_h^\\pi(s,a) = \\langle \\phi(s,a), w_h^\\pi \\rangle\\] . Proof: The claim follows directly from the definition of \\(Q_h^\\pi\\) and the assumptions on \\(r_h(s,a)\\) and \\(P_{h,a}(s)\\). \\[\\begin{align*} Q_h^\\pi(s,a) &amp;= r_h(s,a) + P_{h,a}(s)V_{h+1}^\\pi\\\\ &amp;= \\langle \\phi(s,a), \\theta_h^* \\rangle + \\sum_{s'} V_{h+1}^\\pi(s') \\langle \\phi(s,a), \\psi_h^*(s') \\rangle\\\\ &amp;=\\langle \\phi(s,a), w_h^\\pi \\rangle \\end{align*}\\] where we defined \\(w_h^\\pi = \\theta_h^* + \\sum_{s'} \\psi_h^*(s') V_{h+1}^\\pi(s')\\) for the last equation. \\(\\qquad\\blacksquare\\) . In light of this lemma, our goal is to estimate \\(w_h^{\\pi^*}\\). This can be done using least-squares value iteration (LSVI). Let \\(\\{S_1^{(j)}, A_1^{(j)}, \\dots, S_{H-1}^{(j)}, A_{H-1}^{(j)}, S_{H}^{(j)}\\}_{j=1}^{k-1}\\) be the data available at the beginning of episode \\(k\\). Denote \\(\\phi_{h,j} = \\phi(S_h^{(j)}, A_h^{(j)})\\) and define targets \\(y_{h,j} = r_h(S_h^{(j)}, A_h^{(j)}) + \\max_{a \\in \\mathcal{A}} Q_{h+1}^{(j)}(S_h^{(j)},a)\\) based on \\(Q_{h+1}^{(j)}(s,a)\\) estimates obtained in episodes \\(j=1,\\dots,k-1\\). Least-squares value iteration solves the following problem: . \\[\\begin{align*} \\hat w_{h,k} &amp;= \\arg\\min_{w \\in \\mathbb{R}^d} \\sum_{j=1}^{k-1} \\big(\\langle\\phi_{j,h}, w\\rangle - y_{j,h}\\big)^2 + \\lambda \\|w\\|_2^2 \\end{align*}\\] The closed form solution is $w_{h,k} = \\Sigma_{h,k}^{-1}\\sum_{j=1}^{k-1} \\phi_{h,j}y_{h,j}$ where \\(\\Sigma_{h,k} = \\sum_{j=1}^{k-1} \\phi_{j,h}\\phi_{j,h}^\\top + \\lambda \\mathbf{I}_d\\). Based on the estimate \\(\\hat w_{h,k}\\), we can define optimistic \\(Q\\)- and \\(V\\)-estimates: . \\[\\begin{align*} Q_h^{(k)}(s,a) &amp;= (\\langle\\phi(s,a), \\hat w_{h,k}\\rangle + \\tilde \\beta_{k,\\delta}^{1/2} \\|\\phi(s,a)\\|_{\\Sigma_{h,k}^{-1}}) \\wedge H\\\\ V_h^{(k)}(s) &amp;= \\max_{a \\in \\mathcal{A}} Q_h^{(k)} \\end{align*}\\] Assuming that the features satisfy \\(\\|\\phi(s,a)\\|_2\\leq 1\\) and the true parameters satisfy \\(\\|\\theta_h^*\\|_2 \\leq 1\\) and \\(\\| \\psi_{h}^*v\\|_2 \\leq \\sqrt{d}\\) for all $v \\in \\mathbb{R}^S$ with \\(\\|v\\|_\\infty \\leq 1\\), one can choose the confidence parameter as follows: . \\[\\tilde \\beta_{k,h,\\delta} = \\mathcal{O}\\left(d^2 \\log(\\frac{HK}{\\delta})\\right)\\] This result is the key to unlock a regret bound that is independent of the size of the state space \\(S\\). The proof requires a delicate covering argument. For details refer to chapter 8 of the RL Theory Book . LSVI-UCB . Algorithm: LSVI-UCB . In episodes \\(k=1,\\dots,K\\), . | Initialize \\(V_H^{(j)}(s) = 0\\) for \\(j=1,\\dots, k-1\\). For \\(h=H-1,\\dots,0\\), compute optimistic $Q$ estimates: . \\[\\begin{align*} y_{h,j} &amp;= r_h(S_h^{(j)}, A_h^{(j)}) + V_{h+1}^{(j)}(S_h^{(j)})\\quad \\forall\\,j=1,\\dots,k-1\\\\ \\phi_{h,j} &amp;= \\phi(S_h^{(j)}, A_h^{(j)})\\quad \\forall\\,j=1,\\dots,k-1\\\\ \\hat w_{h,k} &amp;= \\arg\\min_{w \\in \\mathbb{R}^d} \\sum_{j=1}^{k-1} \\big(\\langle\\phi_{j,h}, w\\rangle - y_{j,h}\\big)^2 + \\lambda \\|w\\|_2^2\\\\ \\Sigma_{h,k} &amp;= \\sum_{j=1}^{k-1} \\phi_{j,h}\\phi_{j,h}^\\top + \\lambda \\mathbf{I}_d\\\\ Q_h^{(k)}(s,a) &amp;= (\\langle\\phi(s,a), \\hat w_{h,k}\\rangle + \\tilde \\beta_{k,\\delta}^{1/2} \\|\\phi(s,a)\\|_{\\Sigma_{h,k}^{-1}}) \\wedge H \\end{align*}\\] | For $h=0,\\dots, H-1$, follow greedy policy . \\[A_{h}^{(k)} = \\arg\\max_{a \\in \\mathcal{A}}Q_h^{(k)}(S_h^{(k)},a)\\] | . Note that computing the optimistic policy in episode \\(k\\) can be done in time \\(\\mathcal{O}(Hd^2 + HAd)\\) by incrementally updating the least-square estimates \\(\\hat w_{h,k}\\) using the Sherman-Morrison formula. Compared to UCRL-VTR, this avoids iteration over the state space \\(S\\), which is a big advantage! . Theorem (LSVI-UCB Regret) . The regret of LSVI-UCB is bounded up to logarihmic factors and with probability at least \\(1-\\delta\\) as follows: . \\[R_K \\leq \\mathcal{\\tilde O}(d^{3/2} H^2 \\sqrt{K})\\] . Proof: The proof idea follows a similar strategy as the proof we presented for UCRL-VTR. As mentioned before, the crux is to show a confidence bound for LSVI that is indepenent of the size of the state space. For details, we again refer you to chapter 8 of the RL Theory Book. \\(\\qquad\\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec24/#linear-mdps",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec24/#linear-mdps"
  },"188": {
    "doc": "24. Featurized MDPs",
    "title": "Notes",
    "content": "Bernstein-type bounds for VTR (UCRL-VTR$^+$) . The UCRL-VTR$^+$ algorithm is computationally efficient and able to obtain a regret upper bound of \\(\\mathcal{O}(dH\\sqrt{K})\\), and \\(\\mathcal{O}(d\\sqrt{T}(1-\\gamma)^{-1.5})\\) in the episodic and discounted, infinite horizon setting respectively. These results rely on using bernstein-type bounds. Better regret bounds for Linear MDPs (Eleanor)? . A careful reader might have noticed that the regret bound for LSVI-UCB, \\(\\mathcal{\\tilde O}(d^{3/2} H^2 \\sqrt{K})\\), is not tight with the tabular lower bound, \\(\\Omega(d \\sqrt{K})\\). The difference is in a factor of \\(\\sqrt{d}\\). The Eleanor algorithm (Algorithm 1 in Zanette et al (2020)) is able to shave of the factor of \\(\\sqrt{d}\\), obtaining a regret upper bound of \\(\\mathcal{\\tilde O}(d H^2 \\sqrt{K})\\). However, it is not currently known if the alogrithm can be implemented in a computationally efficient way. The Eleanor algorithm operates under the assumption of low inherent Bellman error (Definition 1 in Zanette et al (2020)), which means the function class is approximately closed under the Bellman optimality operator. It is interesting to note that this assumption is more general than the Linear MDP, thus Eleanor is also able to operate under the Linear MDP assumption. ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec24/#notes",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec24/#notes"
  },"189": {
    "doc": "24. Featurized MDPs",
    "title": "References",
    "content": "The UCRL-VTR paper. Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., &amp; Yang, L. (2020, November). Model-based reinforcement learning with value-targeted regression. In International Conference on Machine Learning (pp. 463-474). PMLR. [link] . The UCRL-VTR$^+$ paper. It also shows the regret lower bound for linear mixture MDPs \\(\\Omega(d H^{3/2} \\sqrt{K})\\). Zhou, D., Gu, Q., &amp; Szepesvari, C. (2021, July). Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In Conference on Learning Theory (pp. 4532-4576). PMLR. [link] . The LSVI-UCB paper. Jin, C., Yang, Z., Wang, Z., &amp; Jordan, M. I. (2020, July). Provably efficient reinforcement learning with linear function approximation. In Conference on Learning Theory (pp. 2137-2143). PMLR. [link] . The Eleanor paper. Zanette, A., Lazaric, A., Kochenderfer, M., &amp; Brunskill, E. (2020, November). Learning near optimal policies with low inherent bellman error. In International Conference on Machine Learning (pp. 10978-10989). PMLR. Link . ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec24/#references",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec24/#references"
  },"190": {
    "doc": "24. Featurized MDPs",
    "title": "24. Featurized MDPs",
    "content": "PDF Version . In tabular (finite-horizon) MDPs \\(M=(\\mathcal{S}, \\mathcal{A}, P, r, \\mu, H)\\), roughly speaking, the learner has to learn about reward and transition probabilities for all states and actions in the worst-case. This is reflected in lower bounds on the regret that scale with \\(R_K \\geq \\Omega(H^{3/2}\\sqrt{ASK})\\) (in the time in-homogeneous case). In many applications the state space can be huge, and reinforcement learning is often used together with function approximation. In such settings, we want to avoid bounds that scale directly with the number of states \\(S\\). The simplest parametric models often rely on state-action features and linearly parametrized transition and reward functions. The goal is to obtain bounds that scale with the complexity of the function class (e.g. the feature dimension in linear models), and are independent of \\(S\\) and \\(A\\). Historically, many ideas for online learning in linear MDP models are borrowed from the linear bandit model. Beyond what is written here, you may find it helpful to read about stochstic linear bandits and LinUCB (see chapters 19 and 20 of the Bandit Book). ",
    "url": "/2024/w2022-lecture-notes/online-rl/lec24/",
    
    "relUrl": "/w2022-lecture-notes/online-rl/lec24/"
  },"191": {
    "doc": "Online RL",
    "title": "Online RL",
    "content": "PDF Version . ",
    "url": "/2024/w2022-lecture-notes/online-rl",
    
    "relUrl": "/w2022-lecture-notes/online-rl"
  },"192": {
    "doc": "1. Introductions",
    "title": "Introduction",
    "content": "Hello everyone and welcome to CMPUT 653: Theoretical Foundations of Reinforcement Learning at the University of Alberta. We are very excited to be teaching this course and hope that you are excited to journey with us through reinforcement learning theory. The course will cover three sub-topics of RL theory: (1) Planning, (2) Batch RL, and (3) Online RL: . | Planning refers to the problem of computing plans, or policies, or just action by interacting with some model. | Batch RL refers to the problem of coming up with plans, policies, value predictions but when the input is just some data obtained by interacting with an environment. | Online RL refers to the problem of coming up with actions that maximize total reward while interacting with an environment. | . In all of these subproblems, we will use Markov Decision Processes, to describe how either the simulation models, or the environments work. Thus, we start by introducing the formal definition of a Markov Decision Process (MDP). ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec1/#introduction",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec1/#introduction"
  },"193": {
    "doc": "1. Introductions",
    "title": "Markov Decision Process",
    "content": "A Markov Decision Process is a mathematical model for modelling sequential decision making in an environment that undergoes stochastic transitions. An MDP consists of the following elements: states, actions, rules of stochastic transitions between states, rewards, and an objective, which we take for now to be the discounted total expected reward, or return. States are considered to be primitive thus we do not explicitly define what they are. The set of states will be denoted by \\(\\mathcal{S}\\). Actions are also primitive and their set is denoted by \\(\\mathcal{A}\\). For simplicity, we assume that both sets are finite. We will let the number of states be denoted by \\(\\mathrm{S}\\), and similarly, we let the number of actions be denoted by \\(\\mathrm{A}\\). Stochastic transitions between states \\(s\\) and \\(s'\\) are the result of choosing some action $a$ in a given state. For a fixed state \\(s\\) and action \\(a\\), the probabilities of landing in the various states \\(s'\\) is collected into a probability vector, which is denoted by \\(P_a(s)\\). To minimize clutter, by slightly abusing notation, we will write \\(P_a(s,s')\\) as the \\(s'\\in \\mathcal{S}\\) component of this probability vector. This is the probability that the process will transition into state \\(s'\\), when in state \\(s\\) it takes action \\(a\\). Rewards are scalars and the reward incurred as a result of taking action \\(a\\) in state \\(s\\) is denoted by \\(r_a(s)\\). Since the number of states and actions are finite, there is no loss in generality by assuming that all the rewards belong to the \\([0,1]\\) interval. Taking action \\(A_t\\) at time step \\(t\\) gives rise to an infinitely long trajectory of state-action pairs \\(S_0,A_0,S_1,A_1,...\\): here, \\(S_{t+1}\\) is the state that results from taking action \\(A_t\\) in time step \\(t\\ge 0\\) and the assumption is that as long as \\(A_t\\) is chosen based on the “past” only, the distribution of \\(S_{t+1}\\) given \\(S_0,A_0,\\dots,S_t,A_t\\) is solely determined by \\(P_{A_t}(S_t)\\), and, in particular, \\(\\mathbb{P}\\)-almost surely, . \\[\\begin{align} \\label{eq:markov} \\mathbb{P}(S_{t+1}=s|S_0,A_0,\\dots,S_t,A_t) = P_{A_t}(S_t,s)\\,. \\end{align}\\] The objective is to find a way of choosing the actions that result in the largest possible return along the trajectories that arise. The return along a trajectory is defined as . \\[R = r_{A_0}(S_0) + \\gamma r_{A_1}(S_1) + \\gamma^2 r_{A_2}(S_2) + \\dots + \\gamma^t r_{A_t}(S_t) + \\dots\\] where \\(\\gamma \\in [0,1)\\) is the discount factor. Formally, a (discounted) MDP will thus be described by the \\(5\\)-tuple \\(M = (\\mathcal{S},\\mathcal{A},P,r,\\gamma)\\), where \\(P=(P_a(s))_{s,a}\\) and \\(r=(r_a(s))_{s,a}\\) collect the transitions and the rewards, respectively. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec1/#markov-decision-process",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec1/#markov-decision-process"
  },"194": {
    "doc": "1. Introductions",
    "title": "On Discounting",
    "content": "Note that \\(\\gamma\\) makes it so that the future reward does not matter as much as the present reward. Also, if we truncate the above sum after \\(H\\ge 0\\) terms, by our assumption on the rewards, the difference between the return and the truncated return is between zero and . \\[\\gamma^H \\Big[r_{A_H}(S_H)+\\gamma r_{A_{H+1}}(S_{H+1})+\\dots \\Big]\\le \\gamma^H \\sum_{s\\ge 0} \\gamma^s = \\frac{\\gamma^H}{1-\\gamma}\\] by using the summation rule for geometric series. Solving for the largest \\(H\\) under which the above upper bound on the difference is below \\(\\varepsilon\\), we get that this bound on the difference holds as long as \\(H\\) satisfies . \\[H \\ge \\underbrace{\\frac{\\ln \\left( \\frac{1}{\\varepsilon(1-\\gamma)} \\right)}{\\ln(1/\\gamma)}}_{H_{\\gamma,\\varepsilon}^*} \\,.\\] For $H$ satisfying this, the return is maximized already when considering only the first \\(H\\) time steps. Notice that the critical value of \\(H\\) depends on not only \\(\\varepsilon\\) but also \\(\\gamma\\). For a fixed \\(\\varepsilon\\), this critical value is called the effective horizon. Oftentimes, for the sake of simplicity, we replace $H_{\\gamma,\\varepsilon}^*$ with the following quantity: . \\[H_{\\gamma,\\varepsilon}:=\\frac{\\ln \\left( \\frac{1}{\\varepsilon(1-\\gamma)} \\right)}{1-\\gamma}\\,.\\] (In fact, the literature often calls the latter the effective horizon). This quantity is an upper bound on \\(H_{\\gamma,\\varepsilon}^*\\). Furthermore, it is not hard to verify that the relative difference between these two quantities is of order $o(1-\\gamma)$ as $\\gamma\\to 1$. Thus, \\(H_{\\gamma,\\varepsilon}^*\\) behaves the same as $H_{\\gamma,\\varepsilon}$ up to a first-order approximation as $\\gamma\\to 1$. Since we are typically interested in this regime (large horizons), there is no loss in switching from \\(H_{\\gamma,\\varepsilon}^*\\) to $H_{\\gamma,\\varepsilon}$. The discounted setting may occasionally feel a bit cringey. Where is the discount factor coming from? One approach is to think about how many time steps in the future we think the optimization should look into for some level of desired accuracy and then work backwards to set \\(\\gamma\\) so that the resulting effective horizon matches our expectation. However, it is more honest to admit that the discounted objective may not faithfully capture the nature of a decision problem. Indeed, there are other objectives that one can consider, such as the finite horizon, undiscounted (or discounted) setting, the infinite horizon setting with no discounting (“total reward”), or the infinite horizon with the average reward. All these have their own pros and cons and we will consider some of these objectives and their relationships in future lectures. For now, we will stick to the discounted objective for pedagogical reasons: the math underlying the discounted objective is simple and elegant. Also, many results transfer to the other settings mentioned, perhaps with some extra conditions, or a little change. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec1/#on-discounting",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec1/#on-discounting"
  },"195": {
    "doc": "1. Introductions",
    "title": "Policies",
    "content": "A policy is a rule that describes how the actions should be taken in light of the past. Here, the past at time step \\(t\\ge 0\\) is defined as . \\[H_t = (S_0,A_0,\\dots,S_{t-1},A_{t-1},S_t)\\] which is the sequence of state-action pairs leading up to the state of the process at the current time step \\(t\\). We allow policies to randomize. As such, formally, a policy becomes an infinite sequence \\(\\pi = (\\pi_t)_{t\\ge 0}\\) of maps of histories to distributions over actions. For a (finite) set \\(X\\) let \\(\\mathcal{M}_1(X)\\) denote the set of probability distributions over \\(X\\). These probability distributions are uniquely determined by what probability they assign to the individual elements of \\(X\\). Hence, they will be identified with the probability vectors with \\(|X|\\) components, each component giving the probability of some $x\\in X$. If $p\\in \\mathcal{M}_1(X)$, we use both $p_x$ and $p(x)$ to denote this probability (whichever is more convenient). With this, we can write that the \\(t\\)th “rule” in \\(\\pi\\), which will be used in the \\(t\\)th time step to come up with the action for that time step, as . \\[\\pi_t: \\mathcal{H}_t \\to \\mathcal{M}_1(\\mathcal{A})\\,,\\] where . \\[\\mathcal{H}_t = (\\mathcal{S} \\times \\mathcal{A})^{t-1} \\times \\mathcal{S}\\,.\\] Note that \\(\\mathcal{H}_0 = \\mathcal{S}\\). Intuitively, following a policy \\(\\pi\\) means that in time step \\(t\\ge 0\\), the distribution of the action \\(A_t\\) to be chosen for that timestep is \\(\\pi_t(H_t)\\): the probability that \\(A_t=a\\) is \\(\\pi_t(H_t)(a)\\). Since writing \\(\\pi_t(H_t)(a)\\) is quite cumbersome, we abuse notation and will write $\\pi_t(a|H_t)$ instead. Thus, when following a policy \\(\\pi\\), in time step \\(t\\ge 0\\) we get that, \\(\\mathbb{P}\\)-almost surely, . \\[\\begin{align} \\label{eq:pol} \\mathbb{P}(A_t=a|H_t) = \\pi_t(a|H_t)\\,. \\end{align}\\] ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec1/#policies",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec1/#policies"
  },"196": {
    "doc": "1. Introductions",
    "title": "Initial State Distributions, Distributions over Trajectories",
    "content": "When a policy is interconnected with an MDP, the interconnection, together with an initial distribution \\(\\mu\\in \\mathcal{M}_1(\\mathcal{S})\\) over the states, uniquely determines a distribution over the infinite-long trajectories . \\[T = (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}\\] such that for every time step \\(t\\ge 0\\), both \\(\\eqref{eq:markov}\\) and \\(\\eqref{eq:pol}\\) hold, in addition to that . \\[\\begin{align} \\label{eq:init} \\mathbb{P}(S_0=s) = \\mu(s)\\,, \\qquad s\\in \\mathcal{S}\\,. \\end{align}\\] In fact, this distribution could be over some potentially bigger probability space, in which case uniqueness does not hold. When we want to be specific and take the distribution that is defined over the infinite-long state-action trajectories, we will say that this is the distribution over the canonical probability space induced by the interconnection of the policy and the MDP. To emphasize the dependence of the probability distribution \\(\\mathbb{P}\\) on \\(\\mu\\) and \\(\\pi\\), we will often use \\(\\mathbb{P}_\\mu^\\pi\\), but we will also take the liberty to drop any of these indices when its identity can be uniquely deduced from the context. When needed, the expectation operator corresponding to \\(\\mathbb{P}\\) (or \\(\\mathbb{P}_\\mu^\\pi\\)) will be denoted by \\(\\mathbb{E}\\) (respectively, \\(\\mathbb{E}_\\mu^\\pi\\)). What is the probability assigned to a trajectory \\(\\tau = (s_0,a_0,s_1,a_1,\\dots)\\in T\\)? Let \\(h_t = (s_0,a_0,\\dots,s_{t-1},a_{t-1},s_t)\\). Recall that \\(H_t = (S_0,A_0,\\dots,S_{t-1},A_{t-1},S_{t})\\). By a repeated application of the chain rule of probabilities, we get . \\[\\begin{align*} \\mathbb{P}(&amp;H_t=h_t)\\\\ &amp;= \\mathbb{P}(S_0=s_0,A_0=a_0,S_1=s_1,\\dots,S_t=s_t)\\\\ &amp;= \\mathbb{P}(S_t=s_t|H_{t-1}=h_{t-1},A_{t-1}=a_{t-1}) \\mathbb{P}(H_{t-1}=h_{t-1},A_{t-1}=a_{t-1}) \\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\mathbb{P}(H_{t-1}=h_{t-1},A_{t-1}=a_{t-1}) \\tag{by \\eqref{eq:markov}}\\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\mathbb{P}(A_{t-1}=a_{t-1}|H_{t-1}=h_{t-1}) \\mathbb{P}(H_{t-1}=h_{t-1})\\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\pi_{t-1}(a_{t-1}|h_{t-1}) \\mathbb{P}(H_{t-1}=h_{t-1}) \\tag{by \\eqref{eq:pol}}\\\\ &amp; \\;\\; \\vdots \\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\pi_{t-1}(a_{t-1}|h_{t-1}) \\times \\dots\\times P_{a_0}(s_0,s_1) \\pi_{0}(a_0|s_0) \\mathbb{P}(S_0=s_0) \\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\pi_{t-1}(a_{t-1}|h_{t-1}) \\times \\dots\\times P_{a_0}(s_0,s_1) \\pi_{0}(a_0|s_0) \\mu(s_0)\\,. \\tag{by \\eqref{eq:init}} \\end{align*}\\] Collecting the terms, . \\[\\mathbb{P}(H_t=h_t) = \\mu_0(s_0) \\left\\{ \\Pi_{i=0}^{t-1} \\pi_i(a_i|h_i)\\right\\} \\, \\left\\{ \\Pi_{i=0}^{t-1} P_{a_i}(s_i,s_{i+1})\\right\\}\\,.\\] Similarly, . \\[\\mathbb{P}(H_t=h_t,A_t=a_t) = \\mu_0(s_0) \\left\\{ \\Pi_{i=0}^{t} \\pi_i(a_i|h_i)\\right\\} \\, \\left\\{ \\Pi_{i=0}^{t-1} P_{a_i}(s_i,s_{i+1})\\right\\}\\,.\\] ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec1/#initial-state-distributions-distributions-over-trajectories",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec1/#initial-state-distributions-distributions-over-trajectories"
  },"197": {
    "doc": "1. Introductions",
    "title": "Value Functions, the Optimal Value Function and the Objective",
    "content": "The total expected discounted reward, or the expected return of policy \\(\\pi\\) in MDP \\(M\\) when the initial state is sampled from $\\mu$ is . \\[v^\\pi(\\mu) = \\mathbb{E}_\\mu^\\pi \\left[ R \\right] \\,.\\] When \\(\\mu=\\delta_s\\) where \\(\\delta_s\\) is the “Dirac” probability distribution that puts a point mass at \\(s\\), we use \\(v^\\pi(s)\\) to denote the resulting value. Since this assigns a value to every state, \\(v^\\pi\\) can be viewed as a function assigning a value to every state in \\(\\mathcal{S}\\). This function will be called the value function of policy \\(\\pi\\). When the dependence on the MDP is important, we may add “in MDP \\(M\\)” and denote the dependence by introducing an index: \\(v^\\pi_M\\). The best possible value in state \\(s\\in \\mathcal{S}\\) that can be obtained by optimizing over all possible policies is . \\[v^*(s) = \\sup_{\\pi} v^\\pi(s)\\,.\\] Then, \\(v^*: \\mathcal{S}\\to \\mathbb{R}\\), viewed as a function, is called the optimal value function. A policy is optimal in state \\(s\\) if \\(v^\\pi(s)=v^*(s)\\). A policy is uniformly optimal if it is optimal in every state. In what follows, we will drop uniformly as we will usually be interested in finding uniformly optimal policies. Given an MDP, we are interested in efficiently computing an optimal policy. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec1/#value-functions-the-optimal-value-function-and-the-objective",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec1/#value-functions-the-optimal-value-function-and-the-objective"
  },"198": {
    "doc": "1. Introductions",
    "title": "Planning=Computation",
    "content": "Computing an optimal policy can be seen as a planning problem: the optimal policy answers the question of how to take actions so that the expected return is maximized. This is also an algorithmic problem. The input, in the simplest case, is a big table (or a number of tables) that describes the transition probabilities and rewards. The interest is to develop algorithms that read in this table and then as output should return a description of an optimal policy. At this stage, it may seem unlikely that an efficient algorithm could do this: in the above unrestricted form, policies have an infinite description. As we shall find out soon though, we will be lucky with finite MDPs as in such MDPs one can always find optimal policies that have a short description. Then, the algorithmic question becomes interesting! . As for any algorithmic problem, the main question is how many elementary computational steps are necessary to solve an MDP? As can be suspected, the number of steps will need to scale with the number of states and actions. Indeed, even the size of the input scales with these. If computation indeed needs to scale with the number of state-action pairs, is there still any reason to consider this problem given that the number of states and actions in MDPs that one typically encounters in practical problems is astronomically large, if not infinite? Yes, there are: . | Not all MDPs are in fact large and it may be useful to know what it takes to “solve” a small MDP. Good solvers for “small” MDPs may serve as benchmarks for solvers developed for the “large MDP” case. | Even if a problem is large (or infinite), one may be able to approximate it well with a small MDP. Then, a solver for a small MDP may be useful. | Some ideas and tools developed for this problem also generalize (perhaps) with some twists to the “large” MDP setting. | . At this stage, the reader may be wondering about what is meant by “small” and “large”? As a rough guideline, by “small” we mean problems where the tables describing the MDP (and/or policy) comfortably fit in the memory of whatever computer one has access to. Large is everything else. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec1/#planningcomputation",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec1/#planningcomputation"
  },"199": {
    "doc": "1. Introductions",
    "title": "Miscellaneous Remarks",
    "content": "Probabilities of infinite long trajectories? . Based on the above calculations, one expects that the probability of a trajectory $\\tau = (s_0,a_0,s_1,a_1,\\dots)$ that never ends is . \\[\\begin{align*} \\mathbb{P}(S_0=s_0,A_0=a_0,S_1=s_1,A_1=a_1,\\dots) &amp;= \\mu(s_0) \\times \\pi_0(a_0|h_0) \\times P_{a_0}(s_0,s_1) \\\\ &amp; \\qquad\\quad\\;\\; \\times \\pi_1(a_1|h_1) \\times P_{a_1}(s_1,s_2) \\\\ &amp; \\qquad\\quad\\;\\; \\times \\cdots \\\\ &amp; \\qquad\\quad\\;\\; \\times \\pi_{t}(a_t|h_t) \\times P_{a_t}(s_t,s_{t+1}) \\\\ &amp; \\qquad\\quad\\;\\; \\times \\cdots \\end{align*}\\] where \\(h_t = (s_0,a_0,\\dots,s_{t-1},a_{t-1},s_t)\\) as before. However, this does not work: if in the trajectory, each action is taken with probability $1/2$ by the policy on the given history, the infinite product on the right-hand side is zero! This should make one pause at least for a moment: how is then \\(\\mathbb{P}\\) even defined? Does this distribution even exist? If yes, and it assigns zero probability to trajectories like above, could not it be that it assigns zero to all the trajectories of infinite length? In the world of infinite, one must tread carefully! The way out of this conundrum is that we must use measure theoretic probabilities, or we need to give up on objects like the return, \\(R= \\sum_{t\\ge 0}\\gamma^t r_{A_t}(S_t)\\), which is defined on trajectories of infinite length. The alternative to measure theoretical probability is to define everything through by taking limits (and always taking expectations over finite-length prefixes of the infinite long trajectories). As this would be quite cumbersome, we will take the measure-theoretic route, which will be explained in the next lecture. Why Markov? . Equation \\(\\eqref{eq:markov}\\) tells us that the only thing that matters from the history of the process as far as the prediction of the next state is concerned is the last action and the last state. This is known as the Markov property. More generally, Markov chains, which are specific stochastic processes, have a similar property. Bellman’s curse of dimensionality . Richard Bellman, who has made many foundational contributions to the early theory, coined the term the “curse of dimensionality”. By this, Bellman meant the following: oftentimes when MDPs are used to model a practical decision making problem, the state space oftentimes takes the product form \\(\\mathcal{S} = \\mathcal{S}_1 \\times \\dots \\times \\mathcal{S}_d\\) with some $d&gt;0$. If each set \\(\\mathcal{S}_i\\) here has at only two(!) elements, the state space will have at least \\(2^d\\) elements. This is an exponential growth as a function of \\(d\\), which is taken as the fundamental scaling quantity. Thus, any algorithm that needs to even just enumerate the states in the state space is “cursed” to perform a very lengthy calculation. While we start with considering the case when both the state and the action space are small (as described above), the main focus will be on the case when this is not true anymore. In this way, the problem will be to figure out ways of breaking the curse. But just to make things clear, in the worst-case, there is no cure to this curse, as we shall see it soon in a rigorous fashion. Any cure will come by changing the problem, either by changing the objective, or by changing the inputs available, or both. Actions shared across states? . We described MDPs as if the same set of actions was available in all the states. This may create the (false) impression that action $a_1$ in state $s_1$ has something to do with action $a_1$ in state $s_2$ (i.e., their rewards, or next state distributions are shared or are similar). Given the MDP definition though, clearly, no such assumptions are made. In a way, a better way of describing an MDP is using a set \\(Z\\) and an equivalence relation over \\(Z\\), or, equivalently, the partition induced by it over \\(Z\\). We should think of \\(Z\\) as the set of possible state-action pairs: The equivalence relation over \\(Z\\) then gives which of these share a common state. Alternatively, if \\(z_1\\) and \\(z_2\\) are in the same partition, they share a state, which we can identify with the partition. Then, for every \\(z\\in Z\\), the MDP would specify a distribution over the parts of the partition (the “next states”) and one should specify a reward. While this description is appealing from a mathematical perspective, it is nonstandard and would make it harder to relate everything to the literature. Furthermore, the description chosen, apart from the inconvenience that one need to forcefully remember that actions do not keep their identity across states, is quite intuitive and compact. A common variation in the literature, which avoids the “sharing issue” is to assume that every state is equipped with a set \\(\\mathcal{A}(s)\\) of actions admissible to the state and these sets are disjoint across the states. This description allows the number of actions to be varied across the states. While this has a minor advantage, our notation is simpler and tends not to lose much in comparison to these more sophisticated alternatives. Are states observed? . In many practical problems it is not a priori clear whether the problem has a good approximate description as an MDP. One critical aspect that is missing from the MDP description is that the states of the MDP may not be available for measurement and thus the control (the choice of the action) cannot use state information. For now, we push this problem aside, but we shall return to it time-to-time. The reason is that it is best to start with the simpler questions and, at least intuitively, the problem of finding a policy that can use state information feels easier than finding one that cannot even access the state information. First, at least, we should find out what can be done in this case (and how efficiently), hoping that the more complex cases will either be reducible to this case, or will share some common patterns. On the notation . Why use \\(r_a(s)\\) rather than, say, \\(r(s,a)\\)? Or \\(P_a(s)\\), or \\(P_a(s,s')\\) rather than \\(P(s'|s,a)\\)? All these notations have pros and cons. None of them is ideal for all purposes. One explanation for using this notation is that later we will replace $a$ with $\\pi$, where $\\pi$ will be a special policy (a memoryless, or stationary Markov policy). When doing so, the notation of $r_\\pi$ (suppressing $s$) and \\(P_\\pi\\) (a stochastic matrix!) will be tremendously useful. A bigger question is why use $s$ for states and $a$ for actions. Is not the answer in the words? Well, people working in control would disagree. They would prefer to use $x$ for state and $u$ for actions, and I am told by Maxim Raginsky, that these come from Russian abbreviations, so they make at least as much sense as the notation used here. That is, if one speaks Russian (and if not, why not learn it?). Dimitri Bertsekas likes using $i,j$ etc. for states, which seems fine if one has discrete (countable) state spaces. Stochastic rewards . Some authors (e.g., this author in some of their papers or even in his book) considers rewards which are stochastic. This may matter when the problem is to learn a good policy, or to find a good plan while interacting with a stochastic simulator. However, when it comes to defining the object of computation, we can safely ignore (well-behaved) stochastic rewards. Here, the well-behaved stochastic rewards are those whose conditional expectation given an arbitrary history up to a state $s$ and an action $a$ taken in that state depends only on $(s,a)$. Which is what we start here from. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec1/#miscellaneous-remarks",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec1/#miscellaneous-remarks"
  },"200": {
    "doc": "1. Introductions",
    "title": "References",
    "content": "“The” book about MDPs is: . Puterman, Martin L. 2005. Markov Decision Processes (Discrete Stochastic Dynamic Programming). Wiley-Interscience. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec1/#references",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec1/#references"
  },"201": {
    "doc": "1. Introductions",
    "title": "1. Introductions",
    "content": "PDF Version . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec1/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec1/"
  },"202": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Planning under \\(q^*\\) realizability",
    "content": "We consider fixed horizon online planning in large finite MDPs $(\\mathcal{S},\\mathcal{A},P,r)$. As usual, the horizon is denoted by $H&gt;0$ and we consider planning with a fixed initial state $s_0$, as in the previous lecture. Let us denote by \\(\\mathcal{S}_i\\) the states that are reachable from $s_0$ in $0\\le i \\le H$ steps. As before, we assume that \\(\\mathcal{S}_i\\cap \\mathcal{S}_j=\\emptyset\\) when $i\\ne j$. Recall that in this case the action-value functions depend on the number of steps left, of the current stage. For a fixed $0\\le h\\le H-1$, let \\(q^*_h:\\mathcal{S}_{h} \\times \\mathcal{A}\\to \\mathbb{R}\\) be the optimal action-value function with $h$ stages in the process, $H-h$ stages left. Since we do not need the values of $q^*_h$ outside of $\\mathcal{S}_h\\times \\mathcal{A}$, we abuse notation by redefining it restricted to this set. Important note: The indexing of $q^*_h$ used here is not consistent with the indexing used in the previous lecture, where it was more convenient to index value functions based on the number of stages left. The planner will be given a feature map $\\phi_h$ for every stage $0\\le h\\le H-1$ such that \\(\\phi_h:\\mathcal{S}_h \\times \\mathcal{A} \\to \\mathbb{R}^d\\). The realizability assumption means that . \\[\\begin{align} \\inf_{\\theta\\in \\mathbb{R}^d} \\max_{0\\le h \\le H-1}\\|\\Phi_h \\theta - q^*_{h} \\|_\\infty = 0\\,. \\label{eq:qsrealizability} \\end{align}\\] Note that we demand that the same parameter vector is shared between all stages. As it turns out, this makes our result stronger. Regardless, at the price of increasing the dimension from $d$ to $dH$, one can always assume that the parameter vector is shared. Since we will give a negative result concerning the query-efficiency of planners, we allow the planners access to the full feature-map: The negative result still applies even if the planner is allowed to perform any sort of computation with the feature-map during or before the planning process. For $\\delta&gt;0$, we call an online planner $\\delta$-sound for the $H$-step criterion if for any MDP $M$ and feature map $\\phi = (\\phi_h)_h$ pair such that the optimal action-value function of $M$ is realizable with the features $\\phi$ in the sense that \\eqref{eq:qsrealizability} holds, the planner induces a policy that is $\\delta$-suboptimal or better when evaluated with the $H$-horizon undiscounted total reward criterion from the designated start-state \\(s_0\\) in MDP $M$. Note that this is very much the same as the previous $(\\delta,\\varepsilon=0)$ soundness criterion, except that the definition of the approximation error is relaxed, while we demand $\\varepsilon=0$. The result below uses MDPs where the immediate reward (obtained from the simulator) can be random. The random reward is used to make the job of the planners harder and it allows us to consider MDPs with deterministic dynamics. (The result could also be proven for MDPs with deterministic rewards and random transitions.) . The usual definition of MDPs with random transitions and rewards is in a way even simpler: Such a (finite) MDP is given by the tuple \\(M=(\\mathcal{S},\\mathcal{A},Q)\\) where \\(Q = (Q_a(s))_{s,a}\\) is a collection of distributions over state-reward pairs. In particular, for all state-action pairs $(s,a)$, \\(Q_a(s)\\in \\mathcal{M}_1(\\mathcal{S}\\times\\mathbb{R})\\). Letting \\((S',R)\\sim Q_a(s)\\) (i.e., $(S’,R)$ is drawn from \\(Q_a(s)\\) at random), we can recover $P_a(s)$ as the distribution of $S’$ and $r_a(s)$ as the expected value of $R$. That the reward can be random forces a change to the notion of the canonical probability spaces, since histories now also show include rewards, $R_0,R_1,\\dots$ incurred in each time step $t=0,1,\\dots$. With appropriate modifications, we can nevertheless still introduce \\(\\mathbb{P}_\\mu^\\pi\\) and the corresponding expectation operator, \\(\\mathbb{E}_\\mu^\\pi\\), as well. The natural definition of the value of a policy $\\pi$ at state $s$, say, in the discounted setting is then \\(v^\\pi(s) = \\mathbb{E}_s^\\pi[ \\sum_{t=0}^\\infty \\gamma^t R_t]\\). However, it is easy to see that for any $t\\ge 0$, \\(\\mathbb{E}_\\mu^\\pi[R_t]=\\mathbb{E}_\\mu^\\pi[r_{A_t}(S_t)]\\), and, as such, nothing changes in the theoretical results derived so far. For $a,b$ reals, let $a\\wedge b = \\min(a,b)$. The main result of this lecture is as follows: . Theorem (worst-case query-cost is exponential under $q^*$-realizability): For any $d,H$ large enough and any online planner $\\mathcal{P}$ that is $9/128$-sound for the $H$-horizon planning problem, there exists a triplet $(M,s_0,\\phi)$ where $M$ is a finite MDP with random rewards taking values in $[0,1]$ and deterministic transitions, $s_0$ is a state of this MDP and $\\phi$ is a $d$-dimensional feature-map such that \\eqref{eq:qsrealizability} holds for the optimal action-value function \\(q^* = (q^*_h)_{0\\le h \\le H-1}\\) and the expected number of queries $q$ that $\\mathcal{P}$ uses when interconnected with $(M,s_0,\\phi)$ satisfies . \\[q = e^{\\Omega(d\\wedge H )}\\] . Note that with random rewards with no control on their tail behavior (e.g., unbounded variance) it would not be hard to make the job of any planner arbitrarily hard. As such, it is quite important that the MDPs that are constructed for the result, the rewards, while random, lie in a fixed interval. Note that the specific choice of this interval does not matter: If there is a hard example with some interval, that example can be translated into another by shifting and scaling, and at the price of introducing an extra dimension in the feature map to account for the shifts. A similar comment applies to $\\delta = 9/128$ (which, nevertheless, needs to be scaled to the range of the rewards). ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec10/#planning-under-q-realizability",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec10/#planning-under-q-realizability"
  },"203": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "The main ideas of the proof",
    "content": "Rather than giving the full proof, we will just explain the main ideas behind it. At a high-level, the proof merges the ideas behind the lower bound for the small action-set case and the lower bound of the large action-set case. That is, we will consider an action set that is exponentially large in $d$. In particular, we will consider action sets that have $k=e^{\\Theta(d)}$ elements. Note that because realizability holds, having a large action set but with a trivial dynamics (as in the lower bound in the last lecture) does not lead to the lower bound of the desired form. In particular, if the dynamics are trivial (i.e., $\\mathcal{S}_i={s_i}$, see the figure on the right) then the optimal action to be taken at $s_0$ does not depend on what actions are taken at later stages and can be efficiently found by just maximizing for the reward received in that stage, which can be done efficiently due to our realizability assumption, even in the presence of random rewards. Whether an example exists with only a few actions but with a more complicated dynamics remains open. With the construction provided here (which is based on tree dynamics and zero intermediate reward in the tree), this clearly fails, as we will make it clear below. In any case, since the “chain dynamics” does not work, the next simplest approach is to have a tree, but with exponentially many actions in every node. Since this creates many many states ($e^{\\Theta(dh)}$ states at stage $h$) the next question then is how to ensure realizability. There are two issues: We need to be able to keep the dimension fixed at $d$ at every stage and somehow we will need to have a way of controlling which action should be optimal at each state at each stage. Indeed, realizability means that we need to ensure that for all $0\\le h \\le H-1$ and $(s,a)\\in \\mathcal{S}_h \\times \\mathcal{A}$, . \\[\\begin{align} q_{h}^*(s,a) = r_a(s)+v_{h+1}^*(sa) \\label{eq:cons} \\end{align}\\] Here, $sa$ stands for the state that is reached by taking action $a$ in state $s$ (in the tree, every node, or state is uniquely indexed by the action sequence that reaches it). Now, in the definition of $v_{h}^*$, for all $h$, we also have \\(v_{h}^*(s) = \\max_{a\\in \\mathcal{A}} q_{h+1}^*(s,a)\\), which calls for the need to know the identity of the maximizing action. What is more, since the solution to the Bellman optimality equations is unique, if we guarantee that \\eqref{eq:cons} holds at all state-action pairs for \\(q_h(s,a) = \\langle \\phi_h(s,a), \\theta^* \\rangle\\) with some features and parameter vectors, it also follows that \\(q_h = q^*_h\\) for all \\(h\\ge 0\\), that is, \\(q^*\\) is realizable with the features. A simple approach to resolve all of these issues is to let a fixed action $a^*\\in \\mathcal{A}$ be the optimal action at all the states, together with using the JL features from the previous lecture (the identity of this action is of course hidden from the planner). In particular, the JL feature-matrix lemma from the previous lecture furnishes us with $k$ $d$-dimensional unit vectors $(u_a)_{a\\in \\mathcal{A}}$ such that for $a\\ne a’$, . \\[\\begin{align*} \\vert \\langle u_a, u_a' \\rangle \\vert \\le \\frac{1}{4}\\,. \\end{align*}\\] Fix these vectors. That $a^*$ should be optimal at all states $s$ is equivalent to that . \\[\\begin{align} q_h^*(s,a)\\le q_h^*(s,a^*) (=v_h^*(s)), \\qquad 0\\le h \\le H-1, s\\in \\mathcal{S}_h, a\\in \\mathcal{A}\\,. \\label{eq:aopt} \\end{align}\\] In our earlier proof we used \\(\\phi_h(s,a) = u_a\\) and \\(\\theta^* = u_{a^*}\\). Will this still work? Unfortunately, it does not. The first observation is that from this it follows that for any $h$, $s$, $a$, . \\[\\begin{align*} q_{h}^*(s,a) = \\langle u_{a^*}, u_a \\rangle\\,. \\end{align*}\\] As such, for almost all the actions $a$, we expect \\(|q_h^*(s,a)|\\) to be close to \\(1/4\\). Now, under this choice we also have that \\(v_h^*(s)=1\\) for all states and all stages $0\\le h \\le H-1$. This creates essentially the same problem as what we saw above with the trivial chain dynamics. In particular, from \\eqref{eq:cons} we get that \\(q_h^*(s,a) = r_a(s)+1\\). As such, we expect $r_a(s)$ to be close to either $-3/4$ or $-5/4$ (since \\(|q_h^*(s,a)|\\) is close to $1/4$). Putting aside the issue that we wanted the immediate reward be in $[0,1]$, we see that if the reward noise is not large, \\(\\theta^*\\) and thus the identity of $a^*$ can be obtained with just a few queries: The signal to noise ratio is just too good! . This problem replicates itself at the very last stage: Here, \\(v_H^*(s')=0\\) for any state $s’$, hence . \\[\\begin{align} q^*_{H-1}(s,a)=r_a(s) \\label{eq:laststage} \\end{align}\\] for any $(s,a)$ pair. Unless we choose \\(q^*_{H-1}(s,a)\\) to be small, say, \\(e^{-\\Theta(H)}\\), a planner will succeed with fewer queries than in our desired bound. This motivates us to introduce a scaling of the features (recall that the parameter vector is shared between the stages) with some scaling factors. For maximum generality, we allow for the scaling factor of the feature vector of \\((s,a)\\in \\mathcal{S}_h\\times \\mathcal{A}\\) to depend on \\((s,a)\\) itself (since states between stages are not shared, scaling can depend on the stage with this choice). Let \\((3/2)^{-h+1}\\sigma_{sa}\\) be the scaling factor we intend to use with \\((s,a)\\) where we intend to keep $\\sigma_{sa}$ in a constant range (so the scaling with the stage index works as intended) while we aim to use \\(\\phi_h(s,a) =(3/2)^{-h+1} \\sigma_{sa} u_a\\). Now, we can explain the need for many actions. By the Bellman optimality equation \\eqref{eq:cons} we have that for any suboptimal action, $a$, . \\[r_{a^*}(s)-r_a(s) =q_h^*(s,a^*)-q_h^*(s,a) \\approx (3/2)^{-h} \\langle u_{a^*}-u_a,u_{a^*} \\rangle \\ge (3/2)^{-h} (3/4),\\] where \\(\\approx\\) uses that \\(\\sigma_{sa}\\approx\\sigma_{sa^*}\\approx \\text{const}\\). From this we see that close to the initial state \\(s_0\\) the reward gaps are of constant order. In particular, if there were only a few actions per state, a planner could identify the optimal action by finding the action whose reward is significantly larger than that of the others. By choosing to have many actions, the planner faces a “needle-in-a-haystack” situation, which makes their job hopeless even with perfect signal (no noise). The next idea is to force “clever” planners to only experiment with actions in the last stage. Since here, the signal-to-noise ratio will be very poor, if we manage to achieve this, even clever planners will need to use a large number of queries. A simple way of forcing this is to choose all the rewards while transitioning in the tree and taking suboptimal actions to be identically zero except for stage $h=H-1$, where, in accordance to our earlier plan, the rewards are chosen at random to ensure consistency but the signal to noise ratio will be poor. Since the dynamics in the tree is known, and it is known that all rewards are zero with the possible exception of when using the optimal action (one of exponentially many actions and is thus hard to find), planners are either left with either solving the needle in a haystack problem of identifying the optimal action by randomly stumbling upon it, or they need to experiment with actions in the last stage. That the rewards are chosen to be identically zero is not critical: From the point of view of this argument, what is critical is that they are all the same. It remains to be seen that consistency can be achieved and also that the optimal action at $s_0$ has a large value compared to the values of suboptimal actions at the same state. Here, we still face some challenges with consistency. Since we want the immediate rewards to belong to the $[0,1]$ interval, all the action values have to be nonnegative. As such, it will be easier if we introduce an additional bias component $c_h$ in the feature vectors, which we allow to scale with the stage. To summarize, we let . \\[\\begin{align*} \\phi_h(s,a) = ( c_h, (3/2)^{-h+1} \\sigma_{sa} u_a^\\top )^\\top\\,. \\end{align*}\\] while we propose to use . \\[\\begin{align*} \\theta^* = \\frac{1}{3} (1, u_{a^*}^\\top)^\\top \\,. \\end{align*}\\] It remains to show that \\eqref{eq:aopt} and \\eqref{eq:cons} can be satisfied with \\(q_h(s,a):=\\langle \\phi_h(s,a), \\theta^* \\rangle\\), while also keeping the suboptimal gap of \\(a^*\\) at \\(s_0\\) large, and while the last stage rewards (\\eqref{eq:laststage}) are in $[0,1]$ and are of size \\(e^{-\\Theta(H)}\\) as planned. Assume for a moment that \\(a^*\\) is optimal in all states, i.e., that \\eqref{eq:aopt} holds. Then, \\(a^*\\) is also optimal in state $sa$, hence, under \\(q^*_h=q_h\\), \\eqref{eq:cons} for any \\(a\\ne a^*\\) is equivalent to . \\[\\begin{align*} q_h(s,a) = q_{h+1}(sa,a^*) \\end{align*}\\] where we also used that by assumption $r_a(s)=0$ because \\(a\\ne a^*\\). Plugging in the definitions, . \\[\\begin{align} \\sigma_{sa,a^*} = \\left(\\frac{3}{2}\\right)^h \\left(c_h-c_{h+1}\\right) + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a^*} \\rangle\\,. \\label{eq:sigmarec} \\end{align}\\] Define \\((c_h)_{0\\le h\\le H-1}\\) so that . \\[\\begin{align*} \\left(\\frac{3}{2}\\right)^h \\left(c_h-c_{h+1}\\right) =\\frac{5}{8}\\,. \\end{align*}\\] with \\(C_{H-1} = \\frac{1}{2}\\left(\\frac32\\right)^{-H}\\) (i.e., $c_h$ is a decreasing geometric sequence) This has two implications: \\eqref{eq:sigmarec} simplifies to . \\[\\begin{align} \\sigma_{sa,a^*} = \\frac{5}{8} + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a^*} \\rangle\\,, \\label{eq:sigmarec2} \\end{align}\\] and also for the last stage rewards, from \\eqref{eq:laststage} we get . \\[\\begin{align*} r_a(s) = \\frac{1}{3} \\left(\\frac32\\right)^{-H} \\left( \\frac{1}{2} + \\sigma_{sa} \\frac32 \\langle u_a,u_{a^*}\\rangle\\right)\\,. \\end{align*}\\] Clearly, if $\\sigma_{sa}\\in [-4/3,4/3]$, since for \\(a\\ne a^*\\), \\(\\vert \\langle u_a,u_{a^*}\\rangle \\vert \\le 1/4\\), \\(r_a(s)\\in [0,(3/2)^{-H}/3]\\) while also \\(r_{a^*}(s)\\in [0,1]\\). With this, to satisfy \\eqref{eq:cons}, on the one hand we choose to define $\\sigma_{sa}$ with the following “downward recursion” in the tree: For any $s$ in the tree and actions $a,a’$, . \\[\\begin{align} \\sigma_{sa,a'} = \\frac{5}{8} + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a'} \\rangle\\,. \\label{eq:sigmarec3} \\end{align}\\] Note that this is consistent with \\eqref{eq:sigmarec2}. The next challenge is to show that $\\sigma_{sa}$ stays within a constant range. In fact, with the above definition, this will not hold. In particular, when $a=a’$, the right-hand side can be as large as \\(5/8+3/2 \\sigma_{sa} \\ge 3/2 \\sigma_{sa}\\), which means that the scaling coefficients will exponentially increase with a base of $(3/2)$. Note, however, that if $a\\ne a’$, then provided that \\(\\sigma_{sa}\\in [1/4,1]\\) (which can be ensured at the root by choosing \\(\\sigma_{s_0,a}=1\\) for all actions \\(a\\)), . \\[\\frac{1}{4} = \\frac{5}{8} - \\frac{3}{8} \\le \\frac{5}{8} + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a'} \\rangle \\le \\frac{5}{8} + \\frac{3}{8} \\le 1\\,,\\] and thus \\(\\sigma_{sa,a'}\\in [1/4,1]\\) will also hold. Hence, we modify the construction so that the definition \\eqref{eq:sigmarec3} is never needed for $a=a’$. This is achieved by changing the dynamics: We introduce a special set of states, ${e_1,\\dots,e_H}$, the exit lane. Once, the process gets into this lane, there is now return and in fact all the remaining rewards up the end are zero. Specifically, all the actions in $e_h$ lead to state $e_{h+1}$ and we set the feature vector of all states in the exit-lane zero: . \\[\\phi_h(e_h,a) = \\boldsymbol{0}\\,.\\] This way, regardless the choice of the parameter vector, we ensure that the Bellman optimality equations hold at these state and the optimal values are correctly set to zero. The exit lane is introduced to remove the need to use \\eqref{eq:sigmarec3} with repeat actions. In particular, for any \\(s\\in \\mathcal{S}_h\\) with some $h\\ge 1$, say, \\(s=(a_1,\\dots,a_h)\\) (i.e., $s$ is obtained by following these actions) then if for \\(a\\in \\{a_1,\\dots,a_h\\}\\), the next state is $e_{h+1}$. Since the optimal value of $e_{h+1}$ is zero and we don’t intend to introduce an immediate reward, we set . \\[\\phi_h(s,a)=\\boldsymbol{0}\\,,\\] making the value of repeat actions zero. The next complication is that this ruins our plan to keep \\(a^*\\) optimal at all states: Indeed, \\(a^*\\) could be applied multiply times in a path from \\(s_0\\) to a leaf of the tree, and by the second application, the new rule forces the value of \\(a^*\\) to be zero. Hence, we need to modify this rule when the action is \\(a^*\\). Clearly, whether a suboptimal action, or \\(a^*\\) is repeated is problematic for the recursive definition of $\\sigma_{sa}$. Hence, it is better if \\(a^*\\) is also forced to use the exit lane. Thus, if \\(a^*\\) is used in \\(s\\in \\mathcal{S}_h\\) with \\(h\\ge 0\\), the next state is \\(e_{h+1}\\). However, we do not zero out \\(\\sigma_{sa^*}\\), but keep the recursive definition and we rather introduce an immediate reward to match \\(q_h(s,a^*) = \\langle \\phi_h(s,a^*), \\theta^* \\rangle\\). It is not hard to check that this reward is also in the \\([0,1]\\) range. Note that here if \\(s = (a_1,\\dots,a_h)\\) then by definition \\(a^*\\not\\in \\{a_1,\\dots,a_h\\}\\). This completes the description of the structure of the MDPs. That the action gap at \\(s_0\\) is large follows from the choice of the JL feature vectors. It remains to be seen that \\(a^*\\) is indeed the optimal action at any state. This boils down to checking that for \\(a'\\ne a^*\\), \\(q_{h+1}(sa,a^*)-q_{h+1}(sa,a')\\ge 0\\). When \\(a'\\) is a repeat action, this is trivial. When \\(a'\\) is not a repeat action, we have . \\[q_{h+1}(sa,a^*)-q_{h+1}(sa,a') = \\frac{1}{3}\\left(\\frac{3}{2}\\right)^{-h} \\left[ \\sigma_{sa,a^*}-\\sigma_{sa,a'}\\langle u_{a'},u_{a^*}\\rangle \\right] \\ge \\frac{1}{3}\\left(\\frac{3}{2}\\right)^{-h} \\left[ \\frac{1}{4}-\\frac{1}{4} \\right] = 0\\] where we used that \\(\\sigma_{sa,a^*}\\ge 1/4\\) and \\(1/4\\le \\sigma_{sa,a'}\\le 1\\) and thus \\(\\sigma_{sa,a'}\\langle u_{a'},u_{a^*}\\rangle\\ge -\\frac{1}{4}\\) by the choice of \\((u_a)_a\\) and since \\(a\\ne a'\\). Let \\(M_{a^*}\\) denote the MDP constructed this way when the optimal action is \\(a^*\\) (the feature maps, of course, are common between these MDPs). For a formal proof, one also needs to argue that planners that do not use many queries cannot distinguish between these MDPs. Intuitively, this is because such planners will receive, with high probability, identical observations under different MDPs in this class. As such, these planners can at best randomly choose an action (“needle in a haystack”) and since in MDP \\(M_{a}\\) only action \\(a\\) incurs high values, they cannot induce a policy with a near-optimal value. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec10/#the-main-ideas-of-the-proof",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec10/#the-main-ideas-of-the-proof"
  },"204": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Computation with many actions",
    "content": "In the construction given the number of actions was allowed to scale exponentially with the dimension. The above proof would show a separation between the query and computation complexity of planning, if one could demonstrate that there is a choice of the JL feature vectors when the optimization problems . \\[\\begin{align*} \\arg\\max_{a\\in \\mathcal{A}} \\langle \\phi(s,a), \\theta \\rangle \\end{align*}\\] admits a computationally efficient solver regardless of the choice of $\\theta\\in \\mathbb{R}^d$ and $s\\in \\mathcal{S}$ (for simplicity, we suppress dependence on $h$). Whether such a solver exist will depend on the choice of the feature-map and this is a fascinating question on its own. One approach to arrive at such a solver is to rewrite this problem as the problem of finding . \\[\\begin{align} \\arg\\max_{v\\in V_s} \\langle v, \\theta \\rangle \\label{eq:linopt} \\end{align}\\] where $V_s \\subset \\mathbb{R}^d$ is the convex hull of the feature vectors \\(\\{ \\phi(s,a) \\}_{a\\in \\mathcal{A}}\\). Provided that this problem admits an efficient solution and given any extreme point of $v\\in V_s$, we can efficiently recover an action $a\\in \\mathcal{A}$ such that $\\phi(s,a)=v$ (this amounts to “inverting” the feature map), the first problem can also be solved efficiently. Note that \\eqref{eq:linopt} is a linear optimization problem over a convex set $V_s$ and the question whether this problem admits an efficient solver lies at the heart of computer science. The general lesson is that the answer can be expected to be yes when $V_s$ has some “convenient” description other than the one that is used to define it. The second problem of inverting the feature map is known as the “decomposition problem” and the same conclusions hold for this problem. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec10/#computation-with-many-actions",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec10/#computation-with-many-actions"
  },"205": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Notes",
    "content": ". | It is possible to modify the construction to make it work in the discounted setting. The paper cited below shows how. | Back to the finite horizon setting, for an upper bound, one can employ the least-squares value iteration algorithm with $G$-optimal design (LSVI-G), which we have met in Homework 2. What results is that to get a $\\delta$-sound (global) planner with this approach, . | . \\[\\begin{align*} O\\left( \\frac{H^5(2d)^{H+1}}{\\delta^2}\\right) \\end{align*}\\] queries are sufficient (and the compute cost is also of similar order). We see that as far as the exponents in the lower and upper bounds are concerned, in the upper bound the exponent is \\(\\Theta(H \\log_2(d))\\) while in the lower bound it is \\(O(H\\wedge d)\\). Thus, there remains a logarithmic gap between them when $H\\ll d$, while the gap is unbounded when \\(H \\gg d\\), i.e., for long horizon problems. In particular, in the constant dimension and long-horizon featurized planning problem, the LSVI-G algorithm seems to be suboptimal because it calculates the optimal action-value function stage-wise. One conjectures that the upper bound for LSVI-G is tight, while the lower bound in this lecture is also essentially correct. This would means that there is an alternate algorithm that could perform much better than LSVI-G in large-horizon planning with constant feature-dimension. Clearly, for the specific construction used in this lecture, a planner that tries all actions, say at \\(s_0\\), will find the optimal action and the cost of this planner is independent of the horizon. Hence, at least in this case, the lower bound can be matched with an alternate algorithm. One may think that this problem is purely of theoretical interest. To counter this note that long-horizon planning is a really important practical question: Many applications require thousands of steps, if not millions, while perhaps the feature space dimension does not need to be very large. Whether there exist an algorithm that works better than LSVI-G thus remains to be a fascinating open problem with good potential for having a real impact on applications. | For infinite horizon undiscounted problems and \\(v^*\\) realizability, there is a simple example that shows that with \\(\\Theta(d)\\) actions and $d$-dimensional features, any query efficient planner that guarantees a constant suboptimality gap needs \\(\\Omega(2^d/d)\\) queries per state. This is based on a shortest path problem on a regular grid. Here, the obstruction is simply algebraic: There is no noise in either the transitions or the rewards. | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec10/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec10/#notes"
  },"206": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Bibliographical notes",
    "content": "This lecture is entirely based on the paper . | Weisz, Gellert, Philip Amortila, and Csaba Szepesvári. 2020. “Exponential Lower Bounds for Planning in MDPs With Linearly-Realizable Optimal Action-Value Functions.”, | . which is available on arXiv and which will also soon appear at ALT. The second lower for the undiscounted setting mentioned in the notes is from . | Weisz, Gellert, Philip Amortila, Barnabás Janzer, Yasin Abbasi-Yadkori, Nan Jiang, and Csaba Szepesvári. 2021. “On Query-Efficient Planning in MDPs under Linear Realizability of the Optimal State-Value Function.” | . available on arXiv. A beautiful book that is a very good source on reading about the linear optimization problem mentioned above is . | Grotschel, Martin, László Lovász, and Alexander Schrijver. 1993. Geometric Algorithms and Combinatorial Optimization. Vol. 2. Algorithms and Combinatorics. Berlin, Heidelberg: Springer Berlin Heidelberg. | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec10/#bibliographical-notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec10/#bibliographical-notes"
  },"207": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "10. Planning under $q^*$ realizability",
    "content": "PDF Version . The lesson from the last lecture is that efficient planners are limited to induce policies whose suboptimaly gap is polynomially larger than the misspecification error of the feature-map supplied to the planner. We have also seen) that if we accept this polynomial in the feature-space-dimension error amplification, a relatively straightforward adaptation of policy iteration gives rise to a computationally efficient (global) planner – at least, when the planner is furbished with the solution to an underlying optimal experimental design problem. In any case, the planner is query efficient. All this was shown in the context when the misspecification error is relative to the set of action value functions underlying all possible policies. In this lecture we look into whether this error metric could be changed so that the misspecification error is measured by how well the optimal action-value function, $q^*$, is approximated by the features, while still retaining the positive result. As the negative result already implies that there are no efficient planners unless the suboptimality gap of the induced policy is polynomially larger than the approximation error, we look into the case when the optimal action-value function is perfectly representable with the features supplied to the planner. This assumption is also known as “\\(q^*\\)-realizability”, or, “\\(q^*\\) linear realizability”, if we want to be more specific about the nature of the function approximation technique used. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec10/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec10/"
  },"208": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "TensorPlan: An optimistic planner",
    "content": "The planner that is referred to in the previous theorem is called TensorPlan. The reason for this name will become clear after we describe the algorithm. TensorPlan belongs to the class of optimistic algorithms. Since knowing \\(\\theta^*\\), the parameter vector that realizes \\(v^*\\), would be sufficient for acting near-optimally, the algorithm aims to find a good approximation to this vector. A suitable estimate is constructed in a two-step process: . | The algorithm maintains a non-empty “hypothesis” set \\(\\Theta\\subset \\mathbb{R}^d\\), which contains those parameter vectors that are consistent with the data that the algorithm has seen. The details of the construction of this set are at the heart of the algorithm and will come soon. | Given \\(\\Theta\\), an estimate $\\theta^+$ is produced by solving a maximization problem: | . \\[\\begin{align} \\theta^+=\\arg\\max_{\\theta\\in \\Theta} \\phi_0(s_0)^\\top \\theta\\,. \\label{eq:optplanning} \\end{align}\\] Here, \\(s_0\\) is the initial state of the episode, i.e., this is the state the planner is called when $h=0$. Recalling that \\(\\phi_0(s_0)^\\top \\theta^* = v_0^*(s_0)\\), we see that provided that \\(\\theta^*\\in \\Theta\\), . \\[v_0(s_0;\\theta^+)\\ge v_0^*(s_0)\\,,\\] where, for convenience, we introduce \\(v_h(s;\\theta) = \\phi_h(s)^\\top \\theta\\). When $\\theta^+$ is close enough to \\(\\theta^*\\), one hopes that the policy induced by \\(\\theta^+\\) will be near-optimal. Hence, the approach is to “roll out” with the induced policy (using the simulator) and verify whether during the rollout the data received is consistent with the Bellman equation, and as a result of this, also whether the episode return observed is close to \\(v_0(s_0;\\theta^+)\\). When a contradiction to any of these is detected, the data can be used to shrink the set \\(\\Theta\\) of consistent parameter vectors. The approach described leaves open the question of what we mean by a policy “induced” by \\(\\theta^+\\). The naive approach is to base this on the Bellman optimality equation, which states that . \\[\\begin{align} v_h^*(s) = \\max_a r_a(s)+ \\langle P_a(s), v_{h+1}^* \\rangle \\label{eq:fhboe} \\end{align}\\] holds for $h=0,1,\\dots,H-1$ with \\(v_H^* = \\boldsymbol{0}\\). If \\(\\theta^+= \\theta^*\\), \\(v_h(\\cdot;\\theta^+)\\) will also satisfy this equation and thus one might define the policy induced by \\(\\theta^+\\) that achieves the maximum above when \\(v_{h+1}^*\\) is replaced by \\(v_{h+1}(\\cdot;\\theta^+)\\). Consistency of \\(\\theta^+\\) would also mean checking whether \\eqref{eq:fhboe} holds (approximately) when \\(v^*_{\\cdot}(\\cdot)\\) is replaced in this equation by \\(v_{\\cdot}(\\cdot;\\theta^+)\\), which, one may imagine can be checked by generating data from the simulator. While this may approach work, it is not easy to see whether it does. (It is open problem whether this works!) TensorPlan defines induced policies and consistency slightly differently. The changed definition allows not only for proving that TensorPlan is query-efficient, but it even makes the guarantees for TensorPlan stronger than what was announced above in the theorem. What makes the analysis of the algorithm that is based on the Bellmean optimality equation difficult is the presence of the maximum in this equation. Hence, TensorPlan removes this maximum. Accordingly, the policy induced by $\\theta^+$ is defined as any policy $\\pi_{\\theta^+}$ which in state $s$ and stage $h$ chooses any action $a\\in \\mathcal{A}$ which ensures that . \\[\\begin{align} v_h(s;\\theta^+) = r_a(s)+ \\langle P_a(s), v_{h+1}(\\cdot;\\theta^+) \\rangle\\,. \\label{eq:tpcons} \\end{align}\\] If there is no such action, \\(\\pi_{\\theta}\\) is free to choose any action. We say that local consistency holds at $(s,h,\\theta^+)$ when there exists an action $a\\in \\mathcal{A}$ such that \\eqref{eq:tpcons} holds. If there are multiple actions that satisfy \\eqref{eq:tpcons}, any of them will do: Choosing the maximizing action is not enforced. However, when \\(v^*\\) is realizable and \\(\\theta^+=\\theta^*\\), any action that satisfies \\eqref{eq:tpcons} will be a maximizing action and the policy induced will be optimal. The advantage of the relaxed notion of induced policy is that with this choice, TensorPlan will also be able to compete with any deterministic policy whose value-function is realizable. This expands the scope of TensorPlan: Perhaps the optimal value function is not realizable with the features handed to TensorPlan, but if there is any deterministic policy whose value-function is realizable with them, then TensorPlan will be guaranteed to produce almost as much as reward as that policy. In fact, it will produce nearly as much reward as the policy that achieves the best value. TensorPlan . To summarize, after generating a hypothesis \\(\\theta^+\\), TensorPlan will run a number of rollouts using the simulator so that for each state $s$ encountered TensorPlan first finds an action $a$ satisfying \\eqref{eq:tpcons}. If this succeeds, the rollout continues by TensorPlan getting a next state from the simulator at $(s,a,h)$ and $h$ is incremented. This continues up to $h=H$, which ends a rollout. TensorPlan will run \\(m\\) rollouts of this type and if all of them succeeds, TensorPlan stops and will use the parameter vector \\(\\theta^+\\) in the rest of the episode and the same policy \\(\\pi_{\\theta^+}\\) as used during the rollouts. If during the rollouts an inconsistency is detected, TensorPlan will decrease the hypothesis set \\(\\Theta\\) and continue with a next experiment. It remains to be seen why TensorPlan (1) stops with a bounded number of queries and (2) why it is sound. Boundedness . We start with boundedness. This is where the change of how policies are induced by parameters is used in a critical manner. Introduce the discriminants: . \\[\\begin{align*} \\Delta(s,a,h,\\theta) = r_a(s) = \\langle P_a(s)\\phi_{h+1},\\theta \\rangle - \\phi_h(s)^\\top \\theta\\,. \\end{align*}\\] Note that \\(\\Delta(s,a,h,\\theta)\\) is just the difference between the right-hand and the left-hand side of \\eqref{eq:tpcons}, where we plugged in the definition $v_h$ and $v_{h+1}$ and we define . \\[P_a(s)\\phi_{h+1} = \\sum_{s'\\in \\mathcal{S}} P_a(s,s') \\phi_{h+1}(s')\\,;\\] thus \\(P_a(s)\\phi_{h+1}\\) is the “expected next feature vector” given $(s,a)$. Then, by definition, local consistency holds for $(s,h,\\theta)$ if and only if there exists some action $a\\in \\mathcal{A}$ such that $\\Delta(s,a,h,\\theta)=0$. Exploiting that the product of numbers is zero if and only if some of them is zero, we see that local consistency is equivalent to . \\[\\begin{align} \\prod_{a\\in \\mathcal{A}} \\Delta(s,a,h,\\theta) = 0\\,. \\label{eq:diprod} \\end{align}\\] The reason this purely algebraic reformulation of local consistency is helpful is because the product of the discriminants can be see as a linear function of the $A$-fold tensor product of \\((1,\\theta^\\top)^\\top\\). To see why this holds, it will be useful to introduce some extra notation: For a real $r$ and a finite-dimensional vector $u$, we will denote by \\(\\overline{ r u}\\) the vector \\((r,u^\\top)^\\top\\) (i.e., adding $r$ to the first position and shifting down all other entries in $u$). With this notation, we can write the discriminants as an inner product: . \\[\\begin{align*} \\Delta(s,a,h,\\theta) = \\langle \\overline{r_a(s)\\, (P_a(s)\\phi_{h+1}-\\phi_h(s))}, \\overline{1 \\, \\theta} \\rangle \\end{align*}\\] Now, recall that the tensor product \\(\\otimes\\) of vectors satisfies the following property: . \\[\\begin{align*} \\prod_a \\langle x_a, y_a \\rangle = \\langle \\otimes_a x_a, \\otimes_a y_a \\rangle\\,, \\end{align*}\\] where the inner product between two tensors is defined in the usual way, by overlaying them and then taking the sum of the products of the entries that are on the top of each other. Based on this identity, we see that \\eqref{eq:diprod}, and thus local consistency, is equivalent to . \\[\\begin{align*} \\langle \\underbrace{\\otimes_a \\overline{r_a(s)\\, (P_a(s)\\phi_{h+1}-\\phi_h(s))}}_{D(s,h)}, \\underbrace{\\otimes_a \\overline{1 \\, \\theta}}_{F(\\theta)} \\rangle = 0\\,. \\end{align*}\\] Note that while $F(\\theta)\\in \\mathbb{R}^{(d+1)^A}$ is a nonlinear function of \\(\\theta\\), the above equation is linear in \\(F(\\theta)\\). Imagine for a moment that the data \\(D(s,h)\\) above can be obtained with no errors and assume that \\(v^*\\) is realizable. Let $k = (d+1)^A$. We can think of both \\(D(s,h)\\) and \\(F(\\theta)\\) taking values in \\(\\mathbb{R}^k\\) (this corresponds to “flattening” these tensors). TensorPlan can be seen as an algorithm that generates a sequence $(\\theta_1,x_1), (\\theta_2,x_2), \\dots$ such that \\(\\theta_i\\in \\mathbb{R}^d\\) is the \\(i\\)th hypothesis that TensorPlan chooses, \\(x_i\\in \\mathbb{R}^k\\) is the \\(i\\)th data of the form \\(D(s,h)\\) with some \\((s,h)\\) where TensorPlan detects an inconsistency. When inconsistency is detected, the hypothesis set is shrunk: . \\[\\Theta_{i+1} = \\Theta_i \\cap \\{ \\theta\\,:\\, F(\\theta)^\\top x_i=0 \\},\\] and \\(\\theta_{i+1}\\) is chosen in \\(\\Theta_{i+1}\\) by \\eqref{eq:optplanning}. Together with \\(\\Theta_1 = B_2^d(B)\\) (the \\(\\ell^2\\) ball of radius \\(B\\) in \\(\\mathbb{R}^d\\)), we have that for $i&gt;1$, . \\[\\Theta_i = \\{ \\theta\\in B_2^d(B)\\,:\\, F(\\theta)^\\top x_1 = 0, \\dots, F(\\theta)^\\top x_{i-1}=0 \\}.\\] Let \\(f_i = F(\\theta_i)\\). By its construction, for any \\(i\\ge 1\\), \\(\\theta_i\\in \\Theta_i\\) and hence \\(f_i\\) is orthogonal to \\(x_1,\\dots,x_{i-1}\\). Also by its construction, \\(x_i\\) is not orthogonal to \\(f_i\\). Because of this, \\(x_i\\) cannot lie in the span of \\(x_1,\\dots,x_{i-1}\\) (if it did, it would be orthogonal to \\(f_i\\)). Hence, the vectors \\(x_1,x_2,\\dots\\) are linearly independent. As there are at most \\(k\\) linearly independent vectors in \\(\\mathbb{R}^k\\), Tensorplan will generate at most \\(k\\) of these data vectors (in fact, for TensorPlan, this is \\(k-1\\), can you explain why?). This means that after at most \\(k\\) “contradictions” to local consistency, TensorPlan will cease to detect more inconsistencies and thus it will stop. Soundness . It remains to be seen that TensorPlan is sound. Let \\(\\theta^+\\) be the parameter vector that TensorPlan generated when it stops. This means that during the \\(m\\) rollouts, TensorPlan did not detect any inconsistencies. Take a trajectory \\(S_0^{(i)},A_0^{(i)},\\dots,S_{H-1}^{(i)},A_{H-1}^{(i)},S_H^{(i)}\\) generated during the \\(i\\)th rollout of \\(m\\) rollouts. Since there is no inconsistency along it, for any \\(0\\le t \\le H-1\\) we have . \\[\\begin{align} r_{A_t^{(i)}}(S_t^{(i)}) = v_t(S_t^{(i)};\\theta^+)-\\langle P_{A_t^{(i)}}(S_t^{(i)}), v_{t+1}(\\cdot;\\theta^+) \\rangle\\,. \\label{eq:constr} \\end{align}\\] Hence, with probability \\(1-\\zeta\\), . \\[\\begin{align*} v_0^{\\pi_{\\theta^+}}(s_0) &amp; \\ge \\frac1m \\sum_{i=1}^m\\sum_{t=0}^{t-1} r_{A_t^{(i)}}(S_t^{(i)}) - H \\sqrt{ \\frac{\\log(1/\\zeta)}{2m}} \\\\ &amp; = \\frac1m \\sum_{i=1}^m\\sum_{t=0}^{t-1} v_t(S_t^{(i)};\\theta^+)-\\langle P_{A_t^{(i)}}(S_t^{(i)}), v_{t+1}(\\cdot;\\theta^+)\\rangle - H \\sqrt{ \\frac{\\log(2/\\zeta)}{2m}} \\\\ &amp; \\ge \\frac1m \\sum_{i=1}^m\\sum_{t=0}^{t-1} v_t(S_t^{(i)};\\theta^+)- v_{t+1}(S_{t+1}^{(i)};\\theta^+) - (H+2B) \\sqrt{ \\frac{\\log(2/\\zeta)}{2m}} \\\\ &amp; = v_0(s_0;\\theta^+) - (H+2B) \\sqrt{ \\frac{\\log(2/\\zeta)}{2m}}\\,, \\end{align*}\\] where the first inequality is by Hoeffding’s inequality and uses that rewards are bounded in \\([0,1]\\), the equality after it uses \\eqref{eq:constr}, the second inequality is again by Hoeffding’s inequality and uses that . \\[\\begin{align*} \\langle P_{A_t^{(i)}} (S_t^{(i)}), v_{t+1}(\\cdot;\\theta^+)\\rangle = \\mathbb{E} [ v_{t+1}(S_{t+1}^{(i)};\\theta^+) | S_t^{(i)},A_t^{(i)}] \\end{align*}\\] and that \\(v_t\\) is bounded between \\([-B,B]\\) (note that we could truncate \\(v_t\\) to \\([0,H]\\) to replace \\(H+2B\\) above by \\(2H\\)), while the last equality uses that \\(v_H(\\cdot;\\theta^+)=\\boldsymbol{0}\\) by definition and that \\(S_0^{(i)}=s_0\\) by definition. Setting \\(m\\) high enough (\\(m=\\tilde O((H+B)^2/\\delta^2)\\)) we can guarantee . \\[v_0^{\\pi_{\\theta^+}}(s_0) \\ge v_0(s_0;\\theta^+)-\\delta.\\] We now argue that this implies soundness. Letting \\(\\Theta^\\circ \\subset B_2^d(B)\\) be the set of \\(B\\)-bounded parameter vectors \\(\\theta\\) such that for some deterministic policy \\(\\pi\\), \\(v^\\pi = \\Phi \\theta\\). By the definition of \\(D\\) and \\(F\\), for any \\(i\\ge 1\\), \\(\\Theta^\\circ \\subset \\Theta_{i}\\) (no correct hypothesis is ever eliminated). It also follows that at any stage of the process, . \\[v_0(s_0;\\theta^+)\\ge \\max_{\\theta\\in \\Theta^\\circ} v^{\\pi_{\\theta}}_0(s_0).\\] Hence, when TensorPlan stops with parameter \\(\\theta^+\\), with high probability, . \\[v^{\\pi_{\\theta^+}}_0(s_0)\\ge v_0(s_0;\\theta^+)-\\delta \\ge \\max_{\\theta\\in \\Theta^\\circ} v^{\\pi_{\\theta}}_0(s_0)-\\delta\\,.\\] In particular, if \\(v^*\\) is \\(B\\)-realizable, \\(v^{\\pi_{\\theta^+}}_0(s_0) \\ge v^*_0(s_0)-\\delta\\). Thus, after stopping, for the rest of the episode, TensorPlan can safely use the policy induced by \\(\\theta^+\\). Summary . So far we have seen that if somehow TensorPlan would be able to get \\(\\Delta(s,a,h,\\theta)\\) with no errors, (1) it would stop after refining its hypothesis set at most \\(k\\) times and (2) when it stops, with high probability it would return with a parameter vector that induces a policy with high value. Regarding the number of queries used, if obtaining \\(\\Delta(s,a,h,\\theta)\\) is counted as a single query, TensorPlan would need at most \\(maH k =maH (d+1)^A\\) queries (\\(m\\) rollouts, for each of the \\(H\\) states in the rollout, \\(A\\) queries are needed). It remains to be seen how to adjust this argument to the case when \\(\\Delta(s,a,h,\\theta)\\) need to be estimated based on interactions with a stochastic simulator. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec11/#tensorplan-an-optimistic-planner",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec11/#tensorplan-an-optimistic-planner"
  },"209": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "Notes",
    "content": ". | It is not known whether TensorPlan can be computationally efficiently implemented. I suspect it cannot. This is because \\(\\Theta_i\\) is specified with a number of highly nonlinear constraints (in the parameter vector). | The essence of the construction here is lifting the problem into a higher-dimensional linear space. This is a standard technique in machine learning but in a very different context when data is mapped to a higher dimensional space to strengthen the power of linear predictors. The once popular RKHS methods take this to the extreme. Note that here, in contrast to this classic lifting procedure, the parameter vector is mapped through a nonlinear function to a higher dimensional space and the purpose is to simply have a clear grasp on why learning stops. | We call \\(\\Delta\\) here the discriminant function because what is important about it is that it discriminates between “good” and “bad” cases and it does it by using the special value of zero. Readers familiar with the RL literature will note, however, that \\(\\Delta\\) is nothing but, what is known as the “temporal difference error” (under some fixed action). | It is curious that the algorithm builds up a data-bank of critical data that it uses to restrain the set of parameter vectors and that it is quite selective in adding new data here. That is, TensorPlan may generate a lot more data then goes on the list $x_1,x_2,\\dots$. If we wanted to be philosophical and would not mind antropomorphising algorithms, we could say that TensorPlan remembers what it is “surprised by”. This is very much unlike other algorithms, like LSVI-$G$, which may generate a lot of redundant data. The other difference is that TensorPlan uses the data to generate a hypothesis set. The choice of the parameter vector from this set is dictated by the optimization (reward maximization) problem solved by TensorPlan. | There are quite a few examples of optimistic algorithms in planning; there is a considerable literature of using optimisim in tree search. However, classics, such as the \\(A^*\\) algorithm can also be seen as an optimistic algorithm (at least when used with an “admissible heuristic”, which is just a way of saying that \\(A^*\\) uses an optimistic estimate of the values). The \\(LAO^*\\) algorithm is another example. However, the real “homeland” of optimistic algorithms in online learning, a topic that will be covered later in the course. | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec11/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec11/#notes"
  },"210": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "Bibliographical notes",
    "content": "This lecture is entirely based on the paper . | Weisz, Gellert, Philip Amortila, Barnabás Janzer, Yasin Abbasi-Yadkori, Nan Jiang, and Csaba Szepesvári. 2021. “On Query-Efficient Planning in MDPs under Linear Realizability of the Optimal State-Value Function.” | . available on arXiv. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec11/#bibliographical-notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec11/#bibliographical-notes"
  },"211": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "content": "PDF Version . In the last lecture we saw that under \\(q^*\\) linear realizability, query-efficient fixed-horizon online planning with a constant suboptimality gap is intractable provided that there is no limit on the number of actions. In particular, the MDPs that were used to show intractability use $e^{\\Theta(d)}$ actions, where $d$ is the dimension of the feature-map that realizes the optimal action-value function. At the end of the lecture, we also noted that intractabality also holds for undiscounted infinite horizon problems under \\(v^*\\) linear realizability in the regime when the number of actions scales linearly with $d$. In this lecture we further dissect \\(v^*\\) realizability, but return to the fixed horizon setting and we will consider the case when the number of actions is fixed. As it turns out, in this case, query-efficient online planning is possible. Before giving the details of this result, we need to firm up some and refine other definitions. First, \\(v^*\\) realizability under a feature map \\(\\phi=(\\phi_h)_{0\\le h \\le H-1}\\) in the $H$-horizon setting means that . \\[\\begin{align} \\inf_{\\theta\\in \\mathbb{R}^d} \\max_{0\\le h \\le H-1}\\|\\Phi_h \\theta - v^*_{h} \\|_\\infty = 0\\,, \\label{eq:vsrealizability} \\end{align}\\] where \\(v^*_h\\) is the optimal-value function when \\(H-h\\) steps are left (in particular, \\(v^*_H=\\boldsymbol{0}\\)). Again, this uses the indexing introduced in the previous lecture. In what follows, without the loss of generality we assume that the feature map is such that all the feature-vectors lie within the a ($2$-norm) ball of radius one. When realizability holds with a parameter vector bounded in $2$-norm by $B$, we say that \\(v^*\\) is $B$-realizable under the feature map $\\phi$. We also slightly modify the interaction protocol between the planner and the simulator, as shown on the figure below. The main new features are introducing stages, and restricting the planners to access states and features only through local calls to the simulator. Illustration of the interaction protocol between the planner and the simulator. Because in fixed-horizon problems the stage index influences what actions should be taken, the planner is called with an initial state \\(s_0\\) and a stage index \\(h\\). For defining the policy induced the planner, it is assumed that the planner is first called with \\(h=0\\) at some state, then it is called with \\(h=1\\) with a state obtained following a transition by taking the action returned by the planner, etc. While interacting with the simulator, the planner is restricted to use only states that it has encountered before. Also, the planner can feed a stage index to the simulator, to get the features of the next state corresponding to the incremented input stage index. There is no other access to the features. Note also that just like in the previous lecture, we allow the MDPs to generate random rewards. In this setting a \\(\\delta\\)-sound planner is one which, under the above protocol, induces a policy of the MDP whose simulator it interacts with which is at most \\(\\delta\\)-suboptimal. Theorem (query-efficient planning under \\(v^*\\)-realizability): For any integers $A,H&gt;0$ and reals $B,\\delta&gt;0$, there exists an online planner $\\mathcal{P}$ with the following properties: . | The planner $\\mathcal{P}$ is \\(\\delta\\)-sound for the $H$-horizon planning problem and the class of MDP-feature-map pairs $(M,\\phi)$ such that $v^*$ is $B$-realizable under $\\phi$ and $M$ has at most $A$ actions and its rewards are bounded in $[0,1]$; | The number of queries used by the planner in each of its call is at most | . \\[\\begin{align*} \\text{poly}\\left( \\left(\\frac{dH}{\\delta}\\right)^A, B \\right) \\end{align*}\\] . Note that for $A&gt;0$ fixed the query-cost is polynomial in $d,H,1/\\delta$ and $B$. It remains to be seen whether this bound can be improved. However, this is somewhat of a theoretical question as under \\(v^*\\)-realizability, even if the coefficients $\\theta\\in \\mathbb{R}^d$ that realize \\(v^*\\) are known, in the lack of extra information, one needs to perform \\(\\Theta(A)\\) simulation calls to be able to get good approximations to the action-value function \\(q^*\\), which seems necessary for inducing a good policy. Hence, the query cost must scale at least linearly with \\(A\\), hence, no algorithm is expected to be even query-efficient when the number of actions is large. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec11/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec11/"
  },"212": {
    "doc": "12. TensorPlan and eluder sequences",
    "title": "12. TensorPlan and eluder sequences",
    "content": "PDF Version . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec12/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec12/"
  },"213": {
    "doc": "13. From API to Politex",
    "title": "Politex",
    "content": "Politex comes from Policy Iteration with Expert Advice. Assume that one is given a featurized MDP \\((M,\\phi)\\) with state-action feature-map \\(\\phi\\) and access to a simulator, and a $G$-optimal design \\(\\mathcal{C}\\subset \\mathcal{S}\\times\\mathcal{A}\\) for \\(\\phi\\). Politex generates a sequence of policies \\(\\pi_0,\\pi_1,\\dots\\) such that for \\(k\\ge 1\\), . \\[\\pi_k(a|s) \\propto \\exp\\left( \\eta \\bar q_{k-1}(s,a)\\right)\\,,\\] where . \\[\\bar q_{k} = \\hat q_0 + \\dots + \\hat q_j,\\] with . \\[\\hat q_j = \\Pi \\Phi \\hat \\theta_j,\\] where for \\(j\\ge 0\\), \\(\\hat\\theta_j\\) is the parameter vector obtained by running the least-squares policy evaluation algorithm based on G-optimal design (LSPE-G) to evaluate policy \\(\\pi_j\\) (see this lecture). In particular, recall that this algorithm rolls out policy \\(\\pi_j\\) from the points of a G-optimal design to produce \\(m\\) independent trajectories of length \\(H\\) each, calculates the average return for each of these design points and then solves the (weighted) least-squares regression problem where the features are used to regress on the obtained values. Above, \\(\\Pi : \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{S}}\\) truncates its argument to the \\([0,1/(1-\\gamma)]\\) interval: . \\[(\\Pi q)(s,a) = \\max(\\min( q(s,a), 1/(1-\\gamma)), 0), \\qquad (s,a) \\in \\mathcal{S}\\times \\mathcal{A}\\,.\\] Note that to calculate \\(\\pi_k(a\\vert s)\\), one does need to calculate \\(E_k(s,a)=\\exp\\left( \\eta \\Pi [ \\phi(s,a)^\\top \\bar \\theta_{k-1} ] \\right)\\) and then compute \\(\\pi_k(a\\vert s) = E_k(s,a)/\\sum_{a'} E_k(s,a')\\). Unlike in policy iteration, the policy returned by Politex after $k$ iterations is either the “mixture policy” . \\[\\bar \\pi_k = \\frac{1}{k} (\\pi_0+\\dots+\\pi_{k-1})\\,,\\] or the policy which gives the best value with respect to the start state, or start distribution. For simplicity, let us just consider the case when $\\bar \\pi_k$ is used as the output. The meaning of a mixture policy is simply that one of the $k$ policies is selected uniformly at random and then the selected policy is followed for the rest of time. Homework 3 gives precise definitions and asks you to prove that the value function of $\\bar \\pi_k$ is just the mean of the value functions of the constituent policies: . \\[\\begin{align} v^{\\bar \\pi_k} = \\frac1n \\left(v^{\\pi_0}+\\dots+v^{\\pi_{k-1}}\\right)\\,. \\label{eq:avgpol} \\end{align}\\] We now argue that the dependence on the approximation error of the suboptimality gap of $\\bar \\pi_k$ only scales with $1/(1-\\gamma)$, unlike the case of approximate policy iteration. For this, recall that by the value difference identity . \\[v^{\\pi^*} - v^{\\pi_j} = (I-\\gamma P_{\\pi^*})^{-1} \\left[T_{\\pi^*} v^{\\pi_j} - v^{\\pi_j} \\right]\\,.\\] Summing up, dividing by $k$, and using \\eqref{eq:avgpol} gives . \\[v^{\\pi^*} - v^{\\bar \\pi_k} = \\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\sum_{j=0}^{k-1} T_{\\pi^*} v^{\\pi_j} - v^{\\pi_j}\\,.\\] Now, \\(T_{\\pi^*} v^{\\pi_j} = M_{\\pi^*} (r+\\gamma P v^{\\pi_j}) = M_{\\pi^*} q^{\\pi_j}\\). Also, \\(v^{\\pi_j} = M_{\\pi_j} q^{\\pi_j}\\). Let \\(\\hat q_j = \\Pi \\Phi \\hat \\theta_j\\). Elementary algebra then gives . \\[\\begin{align*} v^{\\pi^*} - v^{\\bar \\pi_k} &amp; = \\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\sum_{j=0}^{k-1} M_{\\pi^*} q^{\\pi_j} - M_{\\pi_j} q^{\\pi_j}\\\\ &amp; = \\frac1k(I-\\gamma P_{\\pi^*})^{-1} \\underbrace{ \\sum_{j=0}^{k-1} M_{\\pi^*} \\hat q_j - M_{\\pi_j} \\hat q_j}_{T_1} + \\underbrace{\\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\sum_{j=0}^{k-1} ( M_{\\pi^*} - M_{\\pi_j} )( q^{\\pi_j}-\\hat q_j)}_{T_2} \\,. \\end{align*}\\] We see that the approximation errors $\\varepsilon_j = q^{\\pi_j}-\\hat q_j$ appear only in term $T_2$. In particular, taking pointwise absolute values, using the triangle inequality, we get that . \\[\\|T_2\\|_\\infty \\le \\frac{2}{1-\\gamma} \\max_{0\\le j \\le k-1}\\| \\varepsilon_j\\|_\\infty\\,,\\] which shows the promised dependence. It remains to show that \\(\\|T_1\\|_\\infty\\) above is also under control. However, this is left to the next lecture. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec13/#politex",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec13/#politex"
  },"214": {
    "doc": "13. From API to Politex",
    "title": "Notes",
    "content": "State aggregation and extrapolation friendliness . The $\\sqrt{d}$ in our results comes from controlling the extrapolation errors of linear prediction. In the case of state-aggregretion, however, this extra \\(\\sqrt{d}\\) error amplification is completely avoided: Clearly, if we measure a function with a precision \\(\\varepsilon\\) and there is at least one measurement per part, then by using the value measured at each part (at an arbitrary state there) over the whole part, the worst-case error is bounded by \\(\\varepsilon\\). Weighted least-squares in this context just takes the weighted average of the responses over each part and uses this as the prediction, so it also avoids amplifying approximation errors. In this case, our analysis of extrapolation errors is clearly conservative. The extrapolation error was controlled in two steps: In our first lemma, for \\(\\rho\\) weighted least-squares we reduced this problem to that of controlling \\(g(\\rho)=\\max_{z\\in \\mathcal{Z}} \\| \\phi(z) \\|_{G_{\\rho}^{-1}}\\) where \\(G_{\\rho}\\) is the moment matrix for \\(\\rho\\). In fact, the proof of this lemma is the culprit: By carefully inspecting the proof, we can see that the application of Jensen’s inequality introduces an unnecessary term: For the case of state aggregation (orthonormed feature matrix), . \\[\\sum_{z' \\in C} \\varrho(z') |\\phi(z')^\\top G_\\varrho^{-1} \\phi(z')| = 1\\,\\] as long as the design \\(\\rho\\) is such that it chooses any group exactly once. Thus, the case of state-aggregation shows that some feature-maps are more extrapolation friendly than others. Also, note that the Kiefer-Wolfowitz theorem, of course, still gives that \\(\\sqrt{d}\\) is the smallest value that we can get for \\(g\\) when optimizing for \\(\\rho\\). It is a fascinating question of how extrapolation errors behave for various feature-maps. Least-squares value iteration (LSVI) . In homework 2, Question 3 was concerned with least-squares value iteration. The algorithm concerned (call it LSVI-G) uses a random approximation of the Bellman operator, based on a G-optimal design (and action-value functions). The problem was to show a result similar to what holds for LSPI-G holds for LSVI-G, as well. That is, for any MDP feature-map pair $(M,\\phi)$ and any $\\varepsilon’&gt;0$ excess suboptimality target, with a total runtime of . \\[\\text{poly}\\left( d, \\frac{1}{1-\\gamma}, A, \\frac{1}{\\varepsilon'} \\right)\\,,\\] least-squares policy iteration with $G$-optimal design (LSPI-G) can produce a policy $\\pi$ such that the suboptimality gap $\\delta$ of $\\pi$ satisfies . \\[\\begin{align} \\delta \\le \\frac{4(1+\\sqrt{d})}{(1-\\gamma)^{\\color{red} 2}} \\varepsilon_\\text{BOO} + \\varepsilon'\\,. \\label{eq:lsviup} \\end{align}\\] Thus, the dependence on the horizon of the approximation error is similar to the one that was obtained for LSPI. Note that the definition of \\(\\varepsilon_\\text{BOO}\\) is different from what we have used in analyzing LSPI: . \\[\\varepsilon_{\\text{BOO}} := \\sup_{\\theta}\\inf_{\\theta'} \\| \\Phi \\theta' - T \\Pi \\Phi \\theta \\|_\\infty\\,.\\] Above, \\(T\\) is the Bellman optimality oerator for action-value functions and $\\Pi$ is defined so that for \\(f:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}\\), \\(\\Pi f\\) is also a $\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$ function which is obtained from $f$ by truncating for each input $(s,a)$ the value $f(s,a)$ to $[0,1/(1-\\gamma)]$: $(\\Pi(f))(s,a) = \\max(\\min( f(s,a), 1/(1-\\gamma) ), 0)$. In $\\varepsilon_{\\text{BOO}}$, “BOO” stands for “Bellman-optimality operator” in reference to the appearance of $T$ in the definition. In general, the error measures \\(\\varepsilon\\) used in LSPI and \\(\\varepsilon_{\\text{BOO}}\\) are incomparable. The latter quantity measures a “one-step error”, while \\(\\varepsilon\\) is concerned with approximating functions defined over an infinite-horizon. Linear MDPs . Call an MDP linear if both the reward function and the next state distributions for each state lie in the span of the features: \\(r = \\Phi \\theta_r\\) with some $\\theta_r\\in \\mathbb{R}^d$ and $P$, as an $\\mathrm{S}\\mathrm{A}\\times \\mathrm{S}$ matrix takes the form \\(P = \\Phi W\\) with some \\(W\\in \\mathbb{R}^{d\\times \\mathrm{S}}\\). Clearly, this is a notion that captures how well the “dynamics” (including the reward) of the MDP can be “compressed”. When an MDP is linear, \\(\\varepsilon_{\\text{BOO}}=0\\). We also have in this case that $\\varepsilon=0$. More generally, defining \\(\\zeta_r = \\inf_{\\theta}\\| \\Phi \\theta_r - r \\|_\\infty\\) and \\(\\zeta_P=\\inf_W \\|\\Phi W - P \\|_\\infty\\), it is not hard to see that \\(\\varepsilon_{\\text{BOO}}\\le \\zeta_r + \\gamma \\zeta_P/(1-\\gamma)\\) and \\(\\varepsilon\\le \\zeta_r + \\gamma \\zeta_P/(1-\\gamma)\\), which shows that both policy iteration (and its soft versions) and value iteration are “valid” approaches, though, by ignoring the fact that we are comparing upper bounds, this also shows that value iteration may have an edge over policy iteration when the MDP itself is compressible. This should not be too surprising given that value-iteration is “more direct” in aiming to calculate \\(q^*\\). Yet, they may exist cases when the action-value functions are compressible, while the dynamics is not. Stationary points of a policy search objective . Let \\(J(\\pi) = \\mu v^\\pi\\). A stationary point of \\(J\\) with respect to some set of memoryless policies \\(\\Pi\\) is any \\(\\pi\\in \\Pi\\) such that . \\[\\langle \\nabla J(\\pi), \\pi'- \\pi \\rangle \\le 0\\,.\\] It is known that if $\\phi$ are state-aggregation features then any stationary point \\(\\pi\\) of \\(J\\) satisfies . \\[\\mu v^\\pi \\ge \\mu v^* - \\frac{4\\varepsilon_{\\text{apx}}}{1-\\gamma}\\,,\\] where $\\varepsilon_{\\text{apx}}$ is defines as the worst-case error of approximation action-value functions of $\\phi$-measurable policies with the features (the same constant as used in the analysis of approximate policy iteration). Soft-policy iteration with Averaging . Politex can be seen as a “soft” version of policy iteration with averaging. The softness is controlled by $\\eta$: When $\\eta\\to \\infty$, Politex uses a greedy policy w.r.t. to an average of all previous \\(Q\\)-functions. Notice that in this case if Politex were to use a greedy policy w.r.t. the last \\(Q\\)-function, then it would reduce exactly to LSPI-G. As we have seen, in LSPI-G the approximation error can get quadratically amplified with the horizon $1/(1-\\gamma)$. Thus, one way to avoid this quadratic amplification is to stay soft with averaging. As we shall see in the next lecture, the price of this is a relatively slower convergence to a target suboptimality excess value. Nevertheless, the promise is that the algorithm will still stay polynomial in all the relevant quantities. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec13/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec13/#notes"
  },"215": {
    "doc": "13. From API to Politex",
    "title": "References",
    "content": "Politex was introduced in the paper . | POLITEX: Regret Bounds for Policy Iteration using Expert Prediction. Abbasi-Yadkori, Y.; Bartlett, P.; Bhatia, K.; Lazic, N.; Szepesvári, C.; and Weisz, G. In ICML, pages 3692–3702, May 2019. pdf | . However, as this paper also notes, the basic idea goes back to the MDP-E algorithm by Even-Dar et al: . | Even-Dar, E., Kakade, S. M., and Mansour, Y. Online Markov decision processes. Mathematics of Operations Research, 34(3):726–736, 2009. | . This algorithm considered a tabular MDP with nonstationary rewards – a completely different setting. Nevertheless, this paper introduces the basic argument presented above. The Politex paper notices that the argument can be extended to the case of function approximation. In particular, it also notes the nature of the function approximator is irrelevant as long as the approximation and estimation errors can be tightly controlled. The Politex paper presented an analysis for online RL and average reward MDPs. Both add significant complications. The argument shown here is therefore a simpler version. Connecting Politex to LSPE-G in the discounted setting is trivial, but has not been presented before in the literature. The first paper to use the error decomposition shown here together with function approximation is . | Abbasi-Yadkori, Y., Lazic, N., and Szepesvári, C. Modelfree linear quadratic control via reduction to expert prediction. In AISTATS, 2019. | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec13/#references",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec13/#references"
  },"216": {
    "doc": "13. From API to Politex",
    "title": "13. From API to Politex",
    "content": "PDF Version . In the lecture on approximate policy iteration, we proved that for any MDP feature-map pair $(M,\\phi)$ and any $\\varepsilon’&gt;0$ excess suboptimality target, with a total runtime of . \\[\\text{poly}\\left( d, \\frac{1}{1-\\gamma}, A, \\frac{1}{\\varepsilon'} \\right)\\,,\\] least-squares policy iteration with $G$-optimal design (LSPI-G) can produce a policy $\\pi$ such that the suboptimality gap $\\delta$ of $\\pi$ satisfies . \\[\\begin{align} \\delta \\le \\frac{2(1+\\sqrt{d})}{(1-\\gamma)^{\\color{red} 2}} \\varepsilon + \\varepsilon'\\,, \\label{eq:lspiup} \\end{align}\\] where \\(\\varepsilon\\) is the worst-case error with which the $d$-dimensional features can approximate the action-value functions of memoryless policies of the MDP $M$. In fact, the result continues to hold if we restrict the memoryless policies to those that are $\\phi$-measurable in the sense that the probability assigned by such a policy to taking some action $a$ in some state $s$ depends only on \\(\\phi(s,\\cdot)\\). Denote the set of such policies by \\(\\Pi_\\phi\\). Then, for an MDP $M$ and associated feature-map $\\phi$, let . \\[\\tilde\\varepsilon(M,\\phi) = \\sup_{\\pi \\in \\Pi_\\phi}\\inf_{\\theta} \\|\\Phi \\theta - q^\\pi\\|_\\infty\\,.\\] Checking the proof, noticing that LSPI produces \\(\\phi\\)-measurable policies only, it follows that provided the first policy it uses is also \\(\\phi\\)-measurable, $\\varepsilon$ in \\eqref{eq:lspiup} can be replaced by \\(\\tilde \\varepsilon(M,\\phi)\\). Earlier, we also proved that the amplification of $\\varepsilon$ by the $\\sqrt{d}$-factor is unavoidable by any efficient planner. However, this leaves open the question of whether the amplification by a polynomial power of $1/(1-\\gamma)$ is necessary, and whether in particular, the quadratic dependence is necessary? Our first result, which is given without proof, shows that in the case of LSPI this amplification is real and the quadratic dependence cannot be improved. Theorem (LSPI error amplification lower bound): The quadratic dependence in \\eqref{eq:lspiup} is tight: There exists a constant $c&gt;0$ such that for every $0\\le \\gamma&lt;1$ and every \\(\\varepsilon&gt;0\\) there exists a featurized MDP \\((M,\\phi)\\), a policy \\(\\pi\\) of the MDP, a distribution \\(\\mu\\) over the states such that LSPI when it is allowed infinitely many rollouts of infinite length produces a sequence of policies \\(\\pi_0=\\pi,\\pi_1,\\dots\\) such that . \\[\\inf_{k\\ge 1} \\mu (v^*-v^{\\pi_k}) \\ge \\frac{c\\tilde\\varepsilon(M,\\phi)}{(1-\\gamma)^2}\\,.\\] . The result of the theorem holds even when LSPI is used with state-aggregation. Intuitively, state-aggregation means that states are groups into a number of groups and states belonging to the same group are treated identically when it comes to representing value functions. This, value-functions based on state-aggregation are constant over any group. When we are concerned with state-value functions, aggregating the states based on a partitioning of the states \\(\\mathcal{S}\\) into the groups \\(\\{\\mathcal{S}_i\\}_{1\\le i \\le d}\\) (i.e., \\(\\mathcal{S}_i\\subset \\mathcal{S}\\) and all the subsets are disjoint from each other), a feature-map that allows to represent these piecewise constant functions is . \\[\\phi_i(s) = \\mathbb{I}(s\\in \\mathcal{S}_i)\\,, \\qquad i\\in [d]\\,,\\] where $\\mathbb{I}$ is the indicator function that takes the value of one when its argument (a logical expression) is true, and is zero otherwise. In other words, \\(\\phi: \\mathcal{S} \\to \\{ e_1,\\dots,e_d\\}\\). Any feature map of this form defines a partitioning of the state-space and thus corresponds to the state-aggregation. Note that the piecewise constant functions can also be represented if we rotate all the features by the same rotation. The only important aspect here is that the features of different states are either identical, or orthogonal to each other, making the rows of the feature matrix an orthonormal system. For approximating action-value functions, state-aggregation uses the same partitioning of states regardless of the identity of the actions: In effect, for each action, one uses the feature map from above, but with a private parameter vector. This effectively amounts to stacking $\\phi(s)$ \\(\\mathrm{A}\\)-times, to get one copy of it for each action $a\\in \\mathcal{A}$. Note that for state-aggregation, there is no $\\sqrt{d}$ amplification of the approximation errors: State-aggregation is extrapolation friendly, as will be explained at the end of the lecture. Returning to the result, an inspection of the actual proof reveals that in this case LSPI leads to a sequence of policies that alternate between the initial policy and $\\pi_1$. “Convergence” is fast, yet, the guarantee is far from satisfactory. In particular, in the same example, an alternate algorithm, which we will cover next can reduce the quadratic dependence on the horizon to a linear dependence. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec13/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec13/"
  },"217": {
    "doc": "14. Politex",
    "title": "Online linear optimization",
    "content": "As it happens, the problem of controlling terms of this type is the central problem studied in a subfield of learning theory, online learning. In particular, in online linear optimization, the following problem is studied: . An adversary and a learner are playing a zero-sum minimax game in $k$ discrete rounds, taking actions in an alternating manner. In round $j$ ($0\\le j \\le k-1$), first, the learner needs to choose a vector \\(x_j\\in \\mathcal{X}\\subset \\mathbb{R}^d\\). Then, the adversary chooses a vector, \\(y_j \\in \\mathcal{Y}\\subset \\mathbb{R}^d\\). Before its choice, the adversary learns about all previous choices of the learner, and the learner also learns about all previous choices of the adversary. They also remember their own choices. For simplicity, let us constraint the adversary and the learner to be deterministic. The payoff to the adversary at the end of the $k$ rounds is . \\[\\begin{align} R_k = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} \\langle x, y_j \\rangle - \\langle x_j,y_j \\rangle\\,. \\label{eq:regretdefolo} \\end{align}\\] In particular, the adversary’s goal is maximize this, while the learner’s goal is to minimize this (the game is zero-sum). Both the adversary and the learner are given $k$ and the sets $\\mathcal{X},\\mathcal{Y}$. Letting $L$ to denote the learner’s strategy (a sequence of maps of histories to $\\mathcal{X}$) and $A$ to denote the adversary’s strategy (a sequence of maps of histories to $\\mathcal{Y}$), the above quantity depends on $L$ and $A$: $R_k = R_K(A,L)$. Taking the perspective of the learner, the quantity defined in \\eqref{eq:regretdefolo} is called the learner’s regret. Denote the minimax value of the game by \\(R_k^*\\): \\(R_k^* = \\inf_L \\sup_A R_k(A,L)\\). Thus, this only depends on \\(k\\), \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). The dependence is suppressed when it is clear from the context. The central question then is how $R_k^*$ depends on $k$ and also on $\\mathcal{X}$ and $\\mathcal{Y}$. In online linear optimization both sets $\\mathcal{X}$ and $\\mathcal{Y}$ are convex. Connecting these games to our problem, we can see that \\(T_1(s)\\) in \\eqref{eq:t1s} matches the regret definition in \\eqref{eq:regretdefolo} if we let \\(d=\\mathrm{A}\\), \\(\\mathcal{X} = \\mathcal{M}_1(\\mathrm{A}) = \\{ p\\in [0,1]^{\\mathrm{A}} \\,:\\, \\sum_a p_a = 1 \\}\\) be the \\(\\mathrm{A}-1\\) simplex of \\(\\mathbb{R}^{\\mathrm{A}}\\) and \\(\\mathcal{Y} = [0,1/(1-\\gamma)]^{\\mathrm{A}}\\). Furthermore, \\(\\pi_j(s,\\cdot)\\) needs to be chosen first, which is followed by the choice of \\(\\hat q_j(s,\\cdot)\\). While \\(\\hat q_j(s,\\cdot)\\) will not be chosen in an adversarial fashion, a bound $B$ on the regret against arbitrary choices will also serve as a bound for the specific choice we will need to make for \\(\\hat q_j(s,\\cdot)\\). ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec14/#online-linear-optimization",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec14/#online-linear-optimization"
  },"218": {
    "doc": "14. Politex",
    "title": "Mirror descent",
    "content": "Mirror descent (MD) is an algorithm that originates in optimization theory. In the context of online linear optimization, MD is a strategy for the learner which is known to guarantee near minimax regret for the learner under a wide range of circumstances. To align with the large body of literature on online linear optimization, it will be beneficial to switch signs. Thus, in what follows we assume that the learner will aim at minimizing \\(\\langle x,y \\rangle\\) by its choice \\(x\\in \\mathcal{X}\\) and the adversary will aim at maximizing the same expression over its choice \\(y\\in \\mathcal{Y}\\). This means that we also redefine the regret to . \\[\\begin{align} R_k &amp; = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} \\langle x_j, y_j \\rangle - \\langle x,y_j \\rangle \\nonumber \\\\ &amp; = \\sum_{j=0}^{k-1} \\langle x_j, y_j \\rangle - \\min_{x\\in \\mathcal{X}} \\sum_{j=0}^{k-1}\\langle x,y_j \\rangle\\,. \\label{eq:regretdefololosses} \\end{align}\\] Everything else remains the same: The game is zero-sum, minimax, the regret is the payoff for the adversary and the negative regret is the payoff of the learner. This version is called a loss-game. The reason to prefer the loss game is because most of optimization theory is written for minimizing convex functions rather than for maximizing concave functions. However, clearly, this is an arbitrary choice. The second form of the regret shows that the player’s goal is to compete with the best single decision from \\(\\mathcal{X}\\) but chosen given the hindsight of knowing all the choices of the adversary. That is, the learner’s goal is to keep its cumulative loss \\(\\sum_{j=0}^{k-1} \\langle x_j, y_j \\rangle\\) close to, or even below the best cumulative loss in hindsight, \\(\\min_{x\\in \\mathcal{X}} \\sum_{j=0}^{k-1}\\langle x,y_j \\rangle\\). (With this, $T_1(s)$ matches \\(R_k\\) when we change \\(\\mathcal{Y} = [-1/(1-\\gamma),0]^{\\mathrm{A}}\\).) . MD is recursively defined and in its simplest form it has two design parameters. The first is an extended real-valued convex function \\(F: \\mathbb{R}^d \\to \\bar {\\mathbb{R}}\\), called the “regularizer”, while the second is a stepsize, or learning rate parameter $\\eta&gt;0$. (The extended reals is just \\(\\mathbb{R}\\) together with \\(+\\infty,-\\infty\\) and an appropriate extension of basic arithmetic. By allowing convex functions to take the value \\(+\\infty\\) allows to merge “constraints” with objectives in a seamless fashion. The value $-\\infty$ is added because sometimes we have to work with negated extended real-valued convex functions.) . The specification of MD is as follows: In round $0$, $x_0\\in \\mathcal{X}$ is picked to minimize \\(F\\): . \\[x_0 = \\arg\\min_{x\\in \\mathcal{X}} F(x)\\,.\\] In what follows, we assume that all the minimizers that we need in the definition of MD do exist. In the specific case that we need, \\(\\mathcal{X}\\) is the \\(d-1\\) simplex, which is a closed convex set, and since convex functions are also continuous, the minimizers that we will need are guaranteed to exist. Then, in round \\(j&gt;0\\), MD chooses \\(x_j\\) as follows: . \\[\\begin{equation} \\begin{split} x_j &amp; = \\arg\\min_{x\\in \\mathcal{X}}\\,\\,\\eta \\langle x, y_{j-1} \\rangle + D_F(x,x_{j-1}) \\\\ \\end{split} \\label{eq:mddef} \\end{equation}\\] Here, . \\[D_F(x,x') = F(x)-(F(x')+\\langle \\nabla F(x'), x-x'\\rangle)\\] is the remainder term in the first-order Taylor-series expansion of the value of $F$ at $x$ when the expansion is carried out at \\(x'\\) and, for simplicity, we assume that \\(F\\) is differentiable on the interior of its domain \\(\\text{dom}(F) = \\{ x\\in \\mathbb{R}\\,:\\, F(x)&lt;+\\infty \\}\\). Since for any convex function and any linear approximation of it stays below the graph of the convex function, we immediately get that \\(D_F\\) is nonnegative valued. For an illustration see the figure on the right, which shows a convex function, the first-order Taylor approximation of the function at some point. One should think of \\(F\\) is a “nonlinear distance inducing function”; above \\(D_F(x,x')\\) can be thought of penalty imposed on deviating from \\(x'\\). However, \\(D_F\\) is more often than not is not a distance, i.e., often it is not even symmetric. Because of this, we can’t really call $D_F$ a distance. Hence, it is called a divergence. In particular, \\(D_F(x,x')\\) is called the Bregman divergence of \\(x\\) from \\(x'\\). In the definition of the MD update rule, we tacitly assumed that \\(D_F(x,x_{j-1})\\) is well-defined. This requires that \\(F\\) should be differentiable at \\(x_{j-1}\\), which one needs to check when applying MD. In our specific case, this will hold, again. The idea of the MD update rule is to (1) allow the learner to react to the last loss \\(y_{j-1}\\) vector chosen by the adversary, while also (2) limiting how much \\(x_j\\) can depart from \\(x_{j-1}\\), thus, effectively stabilizing the algorithm, the tradeoff governed by the choice of $\\eta&gt;0$. (Separating \\(\\eta\\) from \\(F\\) only makes sense because there are some standard choices for \\(F\\), but \\(\\eta\\) is really just a scale parameter for \\(F\\)). In particular, the larger the value of \\(\\eta\\) is, the less “data-sensitive” MD will be (here, \\(y_0,\\dots,y_{k-1}\\) constitute the data), and vice versa, the smaller \\(\\eta\\) is, the more data-sensitive MD will be. Where is the mirror? . Under some technical conditions on \\(F\\), the update rule \\eqref{eq:mddef} has a two step-implementation: . \\[\\begin{align} \\tilde x_j &amp; = (\\nabla F)^{-1} ( \\nabla F (x_{j-1}) - \\eta y_{j-1} )\\,,\\label{eq:mds1}\\\\ x_j &amp;= \\arg\\min_{x\\in \\mathcal{X}} D_F(x,\\tilde x_j)\\,.\\label{eq:mds2} \\end{align}\\] The first equation above explains the name: To obtain \\(\\tilde x_j\\), one first transforms \\(x_{j-1}\\) using \\(\\nabla F: \\text{dom}(\\nabla F ) \\to \\mathbb{R}^d\\) to the “mirror” (dual) space where “gradients”/”slopes live”, where one then adds to the result \\(-\\eta y_{j-1}\\), which can be seen as a “gradient step” (interpreting \\(y_{j-1}\\) as the gradient of some loss). Finally, the result is then mapped back to the original (primal) space using the inverse of \\(\\nabla F\\). The second step of the update takes the resulting point \\(\\tilde x_j\\) and “projects” it to \\(\\mathcal{X}\\) in a way that respects the “geometry induced by \\(F\\)” on the space \\(\\mathbb{R}^d\\). The use of complex terminology, like “primal” and “dual” spaces, which happen to be the same old Euclidean space, \\(\\mathbb{R}^d\\), probably sounds like an overkill. Indeed, in the simple case we consider when these spaces are identical it is. The distinction would become important when working with infinite dimensional spaces, which we leave to others for now. Besides helping with understanding the terminology, the two-step update shown can also be useful for computation. In fact, this will be the case in the special case that we need. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec14/#mirror-descent",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec14/#mirror-descent"
  },"219": {
    "doc": "14. Politex",
    "title": "Mirror descent on the simplex",
    "content": "We have seen that in the special case we need, . \\[\\begin{align*} \\mathcal{X} &amp;= \\mathcal{P}_{d-1}:=\\{ p\\in [0,1]^{d}\\,:\\, \\sum_a p_a = 1 \\}\\,, \\\\ \\mathcal{Y} &amp;= [-1/(1-\\gamma),0]^d\\,, \\text{and} \\\\ d &amp;= \\mathrm{A}\\,. \\end{align*}\\] To use MD we need to specify the regularizer \\(F\\) and the learning rate. For the former, we choose . \\[F(x) = \\sum_i x_i \\log(x_i) - x_i\\,,\\] which is known as the unnormalized negentropy function. Note that \\(F\\) takes on finite values when \\(x\\in [0,\\infty]^d\\) (since \\(\\lim_{x\\to 0+} x \\log(x)=0\\), we set \\(x_i \\log(x_i)=0\\) whenever $x_i=0$). Outside of this quadrant, we define the value of \\(F\\) to be \\(+\\infty\\). The plot of \\(x\\log(x)-x\\) for \\(x\\ge 0\\) is shown on the right. It is not hard to verify that \\(F\\) is convex: First, \\(\\text{dom}(F) = [0,\\infty]^d\\) is convex. Taking the first derivative, we find that for any \\(x\\in (0,\\infty)^d\\), . \\[\\nabla F(x) = \\log(x)\\,,\\] where \\(\\log\\) is applied componentwise. Taking the derivative again, we find that for \\(x\\in (0,\\infty)^d\\), . \\[\\nabla^2 F(x) = \\text{diag}(1/x)\\,,\\] i.e., the matrix whose $(i,i)$th diagonal entry is \\(1/x_i\\). Clearly, this is a positive definite matrix, which suffices to verify that \\(F\\) is a convex function. The Bregman divergence induced by \\(F\\) is . \\[\\begin{align*} D_F(x,x') &amp; = \\langle \\boldsymbol{1}, x \\log(x) - x - x' \\log(x')+x'\\rangle - \\langle \\log(x'), x-x'\\rangle \\\\ &amp; = \\langle \\boldsymbol{1}, x \\log(x/x') - x +x'\\rangle \\,, \\end{align*}\\] where again we use an “intuitive” notation when operations are first applied componentwise (i.e., $x \\log(x)$ denotes a vector whose $i$th component is $x_i \\log(x_i)$). Note that the domain of $D_F$ is \\([0,\\infty)^d \\times (0,\\infty)^d\\). If both $x$ and $x’$ lie in the $d-1$-simplex, $D_F$ becomes the well-known relative entropy, or Kullback-Leibler (KL) divergence. It is not hard to verify that $x_j$ can be obtained as shown in \\eqref{eq:mds1}-\\eqref{eq:mds2} and in particular this two-step update takes the form . \\[\\begin{align*} \\tilde x_{j,i} &amp;= x_{j-1,i} \\exp(-\\eta y_{j-1,i})\\,, \\qquad x_{j,i} = \\frac{\\tilde x_{j,i}}{ \\sum_{i'} \\tilde x_{j,i'}}\\,, \\quad i\\in [d]\\,. \\end{align*}\\] Unrolling the recursion, we can also that this is the same as . \\[\\begin{equation} \\tilde x_{j,i} = \\exp(-\\eta (y_{0,i}+\\dots + y_{j-1,i}))\\,, \\qquad x_{j,i} = \\frac{\\tilde x_{j,i}}{ \\sum_{i'} \\tilde x_{j,i'}}\\,, \\quad i\\in [d]\\,. \\label{eq:mdunrolled} \\end{equation}\\] Based on this, it is obvious that MD can be efficiently implemented with this choice of $F$. As far as the regret is concerned, the following theorem holds: . Theorem (MD with negentropy on the simplex): Let \\(\\mathcal{X}= \\mathcal{P}_{d-1}\\) amd \\(\\mathcal{Y} = [0,1]^d\\). Then, no matter the adversary, a learner using MD with . \\[\\eta = \\sqrt{ \\frac{2\\log(d)}{k}}\\] is guaranteed that its regret \\(R_k\\) in \\(k\\) rounds is at most . \\[R_k \\le \\sqrt{2k \\log(d)}\\,.\\] . When the adversary plays in \\(\\mathcal{Y} = [a,b]^d\\) with \\(a&lt;b\\), we can use MD on the transformed sequence \\(\\tilde y_j = (y_j-b \\boldsymbol{1})/(b-a) \\in [0,1]^d\\). Then, for any $x\\in \\mathcal{X}$, . \\[\\begin{align*} R_k(x) &amp; := \\sum_{j=0}^{k-1} \\langle x_j-x, y_j \\rangle \\\\ &amp; = \\sum_{j=0}^{k-1} \\langle x_j-x, (b-a)\\tilde y_j+b \\boldsymbol{1} \\rangle \\\\ &amp; = (b-a)\\sum_{j=0}^{k-1} \\langle x_j-x, \\tilde y_j \\rangle \\\\ &amp; \\le (b-a) \\sqrt{2k \\log(d)}\\,, \\end{align*}\\] where the third equality used that $\\langle x_j,\\boldsymbol{1}\\rangle = \\langle x, \\boldsymbol{1} \\rangle = 1$. Taking the maximum over $x\\in \\mathcal{X}$ gives that . \\[\\begin{align} R_k \\le (b-a) \\sqrt{2k \\log(d)}\\,. \\label{eq:mdrbscaled} \\end{align}\\] By the update rule in \\eqref{eq:mdunrolled}, . \\[\\begin{align*} \\tilde x_{j,i} = \\exp(-\\eta (\\tilde y_{0,i}+\\dots + \\tilde y_{j-1,i})) = \\exp(-\\eta/(b-a) (y_{0,i}+\\dots + y_{j-1,i}-j b) )\\,, \\qquad i\\in [d]\\,. \\end{align*}\\] Note that the “shift” by $-jb$ cancels out in the normalization step. Hence, MD in this case takes the form . \\[\\begin{equation} \\begin{split} \\tilde x_{j,i} &amp;= \\exp(-\\eta/(b-a) (y_{0,i}+\\dots + y_{j-1,i}))\\,, \\qquad x_{j,i} = \\frac{\\tilde x_{j,i}}{ \\sum_{i'} \\tilde x_{j,i'}}\\,, \\quad i\\in [d]\\,, \\label{eq:mdunrolledscaled} \\end{split} \\end{equation}\\] which is the same as before, except that the learning rate is scaled by $1/(b-a)$. In particular, in this case one can set . \\[\\begin{align} \\eta = \\frac{1}{b-a} \\sqrt{\\frac{2\\log(d)}{k}}\\,. \\label{eq:etascaled} \\end{align}\\] and use update rule \\eqref{eq:mdunrolled}. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec14/#mirror-descent-on-the-simplex",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec14/#mirror-descent-on-the-simplex"
  },"220": {
    "doc": "14. Politex",
    "title": "MD applied to MDP planning",
    "content": "As agreed, \\(T_1(s)\\) from \\eqref{eq:t1s} takes the form of a $k$-round regret against \\(\\pi^*(s,\\cdot)\\) in online linear optimization on the simplex with losses in \\([-1/(1-\\gamma),0]^{\\mathrm{A}}\\). This suggest to use MD in a state-by-state manner to control \\(T_1(s)\\). Using \\eqref{eq:mdunrolled} and \\eqref{eq:etascaled} gives . \\[E_j(s,a) = \\exp(\\eta (\\hat q_0(s,a) +\\dots + \\hat q_{j-1}(s,a)))\\,, \\qquad \\pi_j(a|s) = \\frac{E_j(s,a)}{ \\sum_{a'} E_j(s,a')}\\,, \\quad a\\in \\mathcal{A}\\] to be used with . \\[\\eta = (1-\\gamma) \\sqrt{\\frac{2\\log(\\mathrm{A})}{k}}\\,.\\] Note that this is the update used by Politex. Then, \\eqref{eq:mdrbscaled} gives that simultaneously for all $s\\in \\mathcal{S}$, . \\[\\begin{align} |T_1(s)| \\le \\frac{1}{1-\\gamma} \\sqrt{2k \\log(\\mathrm{A})}\\,. \\label{eq:t1sbound} \\end{align}\\] Putting things together, we get the following result: . Theorem (Politex suboptimality gap bound): Pick a featurized MDP $(M,\\phi)$ with a full rank feature-map $\\varphi: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}^d$ and let $K,m,H\\ge 1$. Assume that B2\\(_{\\varepsilon}\\) holds for $(M,\\phi)$ and the rewards in $M$ are in the $[0,1]$ interval. For $0\\le \\zeta&lt;1$, define . \\[\\kappa(\\zeta) = \\varepsilon (1 + \\sqrt{d}) + \\sqrt{d} \\left(\\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log( d(d+1)K / \\zeta)}{2m}}\\right)\\,,\\] Then, in $K$ iterations, Politex produces a mixed policy $\\bar \\pi_K$ such that with probability $1-\\zeta$, the suboptimality gap $\\delta$ of $\\bar \\pi_K$ satisfies . \\[\\begin{align*} \\delta \\le \\frac{1}{(1-\\gamma)^2}\\sqrt{\\frac{2\\log(\\mathrm{A})}{K}}+\\frac{2 \\kappa(\\zeta) }{1-\\gamma} \\,. \\end{align*}\\] In particular, for any $\\varepsilon’&gt;0$, choosing $K,H,m$ so that . \\[\\begin{align*} K &amp; \\ge \\frac{32 \\log(A)}{ (1-\\gamma)^4 (\\varepsilon')^2}\\,, \\\\ H &amp; \\ge H_{\\gamma,(1-\\gamma)\\varepsilon'/(8\\sqrt{d})} \\qquad \\text{and} \\\\ m &amp; \\ge \\frac{32 d}{(1-\\gamma)^4 (\\varepsilon')^2} \\log( (d+1)^2 K /\\zeta )\\,, \\end{align*}\\] policy $\\pi_K$ is $\\delta$-optimal with . \\[\\begin{align*} \\delta \\le \\frac{2(1 + \\sqrt{d})}{1-\\gamma}\\, \\varepsilon + \\varepsilon'\\,, \\end{align*}\\] while the total computation cost is $\\text{poly}(\\frac{1}{1-\\gamma},d,\\mathrm{A},\\frac{1}{(\\varepsilon’)^2},\\log(1/\\zeta))$. Note that as compared to the result of LSPI with G-optimal design, the amplification of the approximation error $\\varepsilon$ is reduced by a factor of $1/(1-\\gamma)$, as it was promised. The price is that now the number of iterations $K$, is a polynomial of $\\frac{1}{(1-\\gamma)\\varepsilon’}$, whereas before it was logarithmic. This suggest that perhaps a higher learning rate can help initially to speed up convergence to get the best of both words. Proof: As in the proof of the suboptimality gap for LSPI, we get that for any $0\\le \\zeta \\le 1$, with probability at least $1-\\zeta$, for any $0 \\le k \\le K-1$, . \\[\\begin{align*} \\| q^{\\pi_k} - \\hat q_k \\|_\\infty =\\| q^{\\pi_k} - \\Pi \\Phi \\hat \\theta_k \\|_\\infty \\le \\| q^{\\pi_k} - \\Phi \\hat \\theta_k \\|_\\infty &amp;\\leq \\kappa(\\zeta)\\,, \\end{align*}\\] where the first inequality uses that \\(q_{\\pi_k}\\) takes values in $[0,1]$. On the event when the above inequalities hold, by \\eqref{eq:polsubgapgen} and \\eqref{eq:t1sbound}, . \\[\\begin{align*} \\delta \\le \\frac{1}{(1-\\gamma)^2}\\sqrt{\\frac{2\\log(\\mathrm{A})}{K}}+\\frac{2 \\kappa(\\zeta) }{1-\\gamma} \\,. \\end{align*}\\] The details of this calculation are left to the reader. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec14/#md-applied-to-mdp-planning",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec14/#md-applied-to-mdp-planning"
  },"221": {
    "doc": "14. Politex",
    "title": "Notes",
    "content": "Online convex optimization, online learning . Online linear optimization is a special case of online convex/concave optimization, where the learner chooses elements of some nonempty convex set \\(\\mathcal{X}\\subset \\mathbb{R}^d\\) and the adversary needs to choose an element of a nonempty set \\(\\mathcal{Y}\\) of concave functions over \\(\\mathcal{X}\\): \\(\\mathcal{Y} \\subset \\{ f: \\mathcal{X} \\to \\mathbb{R}\\,:\\, f \\text{ is concave} \\}\\). Then, the definition of regret is changed to . \\[\\begin{align} R_k = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} y_j(x) - y_j(x_j) \\,, \\label{eq:regretdefoco1} \\end{align}\\] where as before \\(x_j\\in \\mathcal{X}\\) is the choice of the learner for round \\(j\\) and \\(y_j\\in \\mathcal{Y}\\) is the choice of the adversary for the same round. Identifying any vector \\(u\\) of \\(\\mathbb{R}^d\\) with the linear map \\(x \\mapsto \\langle x, u \\rangle\\), we see that online linear optimization is a special case of this problem. Of course, by negating all functions in \\(\\mathcal{Y}\\) (i.e., letting \\(\\tilde {\\mathcal{Y}} = \\{ - y \\,:\\, y\\in \\mathcal{Y} \\}\\)) and redefining the regret to . \\[\\begin{align} R_k = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} \\tilde y_j(x_j)- \\tilde y_j(x) \\, \\label{eq:regretdefoco} \\end{align}\\] we get a definition that is used in the literature, which prefers the convex case to the concave. Here, the interpretation is that \\(\\tilde y_j\\in \\tilde {\\mathcal{Y}}\\) is a “loss function” chosen by the adversary in round \\(j\\). The standard function notation (\\(y_j\\) is applied to \\(x\\)) injects unwarranted asymmetry in the notation. After all, from the perspective of the learner, they need to choose a value in \\(\\mathcal{X}\\) that works for the various functions in \\(\\mathcal{Y}\\). Thus, we can consider any element of \\(\\mathcal{X}\\) as a function that maps elements of \\(\\mathcal{Y}\\) to reals through \\(y \\mapsto y(x)\\). Whether \\(\\mathcal{Y}\\) has functions in them or \\(\\mathcal{X}\\) has functions in them does not matter that much; it is the interconnection between \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) that matters more. For this reason, one can study online learning when \\(y(x)\\) above is replaced by \\(b(x,y)\\), where \\(b: \\mathcal{X}\\times \\mathcal{Y} \\to \\mathbb{R}\\) is a specific map that assigns payoffs to every pair of points in \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). When the map is fixed, one can spare an extra symbol by just using \\([x,y]\\) in place of \\(b(x,y)\\), which makes things almost a full circle given that we started with the linear case when \\([x,y] = \\langle x,y \\rangle\\). Truncation or no truncation? . We introduced truncation to simplify the analysis. The proof can be made to go through even without it, with a mild increase of the suboptimality gap (or runtime). The advantage of removing the projection is that without projection, \\(\\hat q_0 + \\dots + \\hat q_{j-1} = \\Phi (\\hat \\theta_0 + \\dots + \\hat \\theta_{j-1})\\), which leads to a practically significant reduction of the runtime. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec14/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec14/#notes"
  },"222": {
    "doc": "14. Politex",
    "title": "14. Politex",
    "content": "PDF Version . The following lemma can be extracted from the calculations found at the end of the last lecture: . Lemma (Mixture policy suboptimality): Fix an MDP $M$. For any sequence \\(\\pi_0,\\dots,\\pi_{k-1}\\) of policies, any sequence $\\hat q_0,\\dots,\\hat q_{k-1}: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}$ of functions, and any policy \\(\\pi^*\\), the mixture policy $\\bar \\pi_k = 1/k(\\pi_0+\\dots+\\pi_{k-1})$ satisfies . \\[\\begin{align} v^{\\pi^*} - v^{\\bar \\pi_k} &amp; \\le \\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\underbrace{ \\sum_{j=0}^{k-1} M_{\\pi^*} \\hat q_j - M_{\\pi_j} \\hat q_j}_{T_1} + \\frac{2 \\max_{0\\le j \\le k-1}\\| q^{\\pi_j}-\\hat q_j\\|_\\infty }{1-\\gamma} \\,. \\label{eq:polsubgapgen} \\end{align}\\] . In particular, the only restriction is on policy $\\pi^*$ so far and that is that it has to be a memoryless policy. To control the suboptimality of the mixture policy, one just needs to control the action-value approximation errors \\(\\| q^{\\pi_j}-\\hat q_j\\|_\\infty\\) and the term $T_1$ and for this we are free to choose the policies \\(\\pi_0,\\dots,\\pi_{k-1}\\) in any way we want them to be chosen. To help with this choice, let us now inspect \\(T_1(s)\\) for a fixed state $s$: . \\[\\begin{align} T_1(s) = \\sum_{j=0}^{k-1} \\langle \\pi^*(s,\\cdot),\\hat q_j(s,\\cdot)\\rangle - \\langle \\pi_j(s,\\cdot),\\hat q_j(s,\\cdot)\\rangle \\,, \\label{eq:t1s} \\end{align}\\] where, abusing notation, we use \\(\\pi(s,a)\\) for \\(\\pi(a|s)\\). Now, recall that \\(\\hat q_j\\) will be computed based on \\(\\pi_j\\) while \\(\\pi^*\\) is unknown. One must thus wonder whether it is possible to control this term? . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec14/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec14/"
  },"223": {
    "doc": "15. From policy search to policy gradients",
    "title": "Policy search",
    "content": "A reasonable goal then is to ask for a planner that competes with the best policy within the parameterized family, or the $\\varepsilon$-best policy for some positive $\\varepsilon$. Since there may not be a parameter $\\theta$ such that $v^{\\pi_\\theta}\\ge v^{\\pi_{\\theta’}}-\\varepsilon\\boldsymbol{1}$ for any $\\theta’\\in \\mathbb{R}^d$, we simplify the problem by requiring that the policy computed is nearly best when started from some initial distribution $\\mu \\in \\mathcal{M}_1(\\mathcal{S})$. Defining $J: \\text{ML} \\to \\mathbb{R}$ as . \\[J(\\pi) = \\mu v^{\\pi} (=\\sum_{s\\in \\mathcal{S}}\\mu(s)v^{\\pi}(s)),\\] the policy search problem is to find a parameter $\\theta\\in \\mathbb{R}^d$ such that . \\[\\begin{align*} J(\\pi_{\\theta}) = \\max_{\\theta'} J(\\pi_{\\theta'})\\,. \\end{align*}\\] The approximation version of the problem asks for finding $\\theta’\\in \\mathbb{R}^d$ such that . \\[\\begin{align*} J(\\pi_{\\theta}) \\ge \\max_{\\theta'} J(\\pi_{\\theta'}) - \\varepsilon\\,. \\end{align*}\\] The formal problem definition then is as follows: a planning algorithm is given the MDP $M$ and a policy parameterization $(\\pi_\\theta)_{\\theta}$ and we are asking for an algorithm that returns the solution to the policy search problem in time polynomial in the number of actions $\\mathrm{A}$ and the number of parameters $d$ that describes the policy. An even simpler problem is when the MDP has finitely many states, and the algorithm needs to run in polynomial time in $\\mathrm{S}$, $\\mathrm{A}$ and $d$. In this case, it is clearly advantageous for the algorithm if it is given the exact description of the MDP (as described in Lecture 3) Sadly, even this mild version of policy search is intractable. Theorem (Policy search hardness): Unless $\\text{P}=\\text{NP}$, there is no polynomial time algorithm for the finite policy search problem even when the policy space is restricted to the constant policies and the MDPs are restricted to be deterministic with binary rewards. The constant policies are those that assign the same probability distribution to each state. This is a special case of state aggregation when all the states are aggregated into a single class. As the policy does not depend on the state, the problem is also known as the blind policy search problem. Note that the result holds regardless of the representation used to express the set of constant policies. Proof: Let $\\mathcal{S} = \\mathcal{A}=[n]$. The dynamics is deterministic: The next state is $a$ if action $a\\in \\mathcal{A}$ is taken regardless of the state. A policy is simply a probability distribution \\(\\pi \\in \\mathcal{M}_1([n])\\) over the action space, which we shall view as a column vector taking values in $[0,1]^n$. The transition matrix of $\\pi$ is $P_{\\pi}(s,s’) = \\pi(s’)$, or, in matrix form, $P_\\pi = \\boldsymbol{1} \\pi^\\top$. Clearly, $P_\\pi^2 = \\boldsymbol{1} \\pi^\\top \\boldsymbol{1} \\pi^\\top = P_\\pi$ (i.e., $P_\\pi$ is idempotent). Thus, $P_\\pi^t = \\boldsymbol{1}\\pi^\\top$ for any $t&gt;0$ and hence . \\[\\begin{align*} J(\\pi) &amp; = \\mu (r_\\pi + \\sum_{t\\ge 1} \\gamma^t P_\\pi^t r_\\pi) = \\mu \\left(I + \\frac{\\gamma}{1-\\gamma} \\boldsymbol{1} \\pi^\\top \\right)r_\\pi\\,. \\end{align*}\\] Defining $R_{s,a} = r_a(s)$ so that $R\\in [0,1]^{n\\times n}$, we have $r_\\pi = R\\pi$. Plugging this in into the previous displayed equation and using that $\\mu \\boldsymbol{1}=1$, we get . \\[\\begin{align*} J(\\pi) &amp; = \\mu R \\pi + \\frac{\\gamma}{1-\\gamma} \\pi^\\top R \\pi\\,. \\end{align*}\\] Thus we see that the policy search problem is equivalent to maximizing the quadratic expression in the previous display over the probability simplex. Since there is no restriction on $R$, one may at this point conjecture that this will be hard to do. That this is indeed the case can be shown by a reduction to the maximum independent set problem, which asks for checking whether the independence number of a graph is above a threshold and which is known to be NP-hard even for $3$-regular graphs (i.e., graphs where every vertex has exactly three neighbours). Here, the independence number of a graph is defined as follows: We are given a simple graph $G=(V,E)$ (i.e., there are no self-loops, no double edges, and the graph is undirected). An independent set in $G$ is a neighbour-free subset of vertices. The independence number of $G$ is defined as . \\[\\begin{align*} \\alpha(G) = \\max \\{ |V'| \\,:\\, V'\\subset \\text{ independent in } G \\}\\,. \\end{align*}\\] Quadratic optimization has close ties to the maximum independent set problem: . Lemma (Motzkin-Strauss ‘65): Let \\(G\\in \\{0,1\\}^n\\) be the vertex-vertex adjacency matrix of simple graph (i.e., $G_{ij}=1$ if and only if $(i,j)$ is an edge of the graph). Then, for \\(I\\in \\{0,1\\}^{n\\times n}\\) the $n\\times n$ identity matrix, . \\[\\begin{align*} \\frac{1}{\\alpha(G)} = \\min_{y\\in \\mathcal{M}_1([n])} y^\\top (G+I) y\\,. \\end{align*}\\] . We now show that if there is an algorithm that solves policy search in polynomial time then it can also be used to solve the maximum independent set problem for simple, $3$-regular graphs. For this pick a $3$-regular graph $G$ with $n$ vertices. Define the MDP as above with $n$ states and actions and the rewards chosen so that $R = E-(I+G)$ where $G$ is the vertex-vertex adjacency matrix of the graph and $E$ is the all-ones matrix: $E = \\boldsymbol{1} \\boldsymbol{1}^\\top$. We add $E$ so that the rewards are in the $[0,1]$ interval and in fact are binary as required. Choose $\\mu$ as the uniform distribution over the states. Note that $\\boldsymbol{1}^\\top (I+G) = 4 \\boldsymbol{1}^\\top$ because the graph is $3$-regular. Then, for $\\pi \\in \\mathcal{M}_1(\\mathcal{A})$, . \\[\\begin{align*} J(\\pi) &amp; = \\frac{1}{1-\\gamma}- \\mu(E+I+G) \\pi - \\frac{\\gamma}{1-\\gamma} \\pi^\\top (E+I+G) \\pi \\\\ &amp; = \\frac{1}{1-\\gamma}- \\frac{1}{n} \\boldsymbol{1}^\\top (I+G) \\pi - \\frac{\\gamma}{1-\\gamma} \\pi^\\top (I+G) \\pi \\\\ &amp; = \\frac{1}{1-\\gamma}- \\frac{4}{n} - \\frac{\\gamma}{1-\\gamma} \\pi^\\top (I+G) \\pi\\,. \\end{align*}\\] Hence, \\(\\begin{align*} \\max_{\\pi \\in \\mathcal{M}_1([n]} J(\\pi) &amp; = \\frac{1}{1-\\gamma}- \\frac{4}{n} - \\frac{\\gamma}{1-\\gamma} \\frac{1}{\\alpha(G)} \\ge \\frac{1}{1-\\gamma}- \\frac{4}{n} - \\frac{\\gamma}{1-\\gamma} \\frac{1}{m} \\end{align*}\\) holds if and only if $\\alpha(G)\\ge m$. Thus, the decision problem of deciding that $J(\\pi)\\ge a$ is at least as hard as the maximum independent set problem. As noted, this is an NP-hard problem, hence the result follows. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec15/#policy-search",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec15/#policy-search"
  },"224": {
    "doc": "15. From policy search to policy gradients",
    "title": "Potential remedy: Local search",
    "content": "Based on the theorem just proved it is not very likely that we can find computationally efficient planners to compete with the best policy in a restricted policy class, even if the class looks quite benign. This motivates aiming at some more modest goal, one possibility of which is to compute local maxima of the map $J:\\pi \\mapsto \\mu v^{\\pi}$. Let \\(\\Pi = \\{ \\pi_\\theta \\,:\\, \\theta\\in \\mathbb{R}^d \\} \\subset [0,1]^{\\mathcal{S}\\times\\mathcal{A}}\\) be the set of policies that can represented; we view these now as “large vectors”. Then, in this approach we aim to identify \\(\\pi^*\\in \\Pi\\) (and its parameters) so that for any $\\pi’\\in \\Pi$ and small enough $\\delta&gt;0$ so that \\(\\pi^*+\\delta (\\pi'-\\pi^*)\\in \\Pi\\), \\(J(\\pi^*+\\delta (\\pi'-\\pi^*))\\le J(\\pi^*)\\). For $\\delta$ small, \\(J(\\pi^*+\\delta (\\pi'-\\pi^*))\\approx J(\\pi^*) + \\delta \\langle J'(\\pi^*), \\pi'- \\pi^* \\rangle\\). Plugging this in into the previous inequality, reordering and dividing by $\\delta&gt;0$ gives . \\[\\begin{align} \\langle J'(\\pi^*), \\pi'- \\pi^* \\rangle \\le 0\\,, \\qquad \\pi' \\in \\Pi\\,. \\label{eq:stp} \\end{align}\\] Here, $J’(\\pi)$ denotes the derivative of $J$. What remains to be seen is whether (1) relaxing the goal to computing \\(\\pi^*\\) helps with the computation (and when) and (2) whether we can get some guarantees for how well $\\pi^*$ satisfying \\eqref{eq:stp} will do compared to \\(J^* = \\max_{\\pi\\in \\Pi} J(\\pi)\\), that is obtaining some approximation guarantees. For the latter we seek for some function $\\varepsilon$ of the MDP $M$ and $\\Pi$ (or $\\varphi$, when $\\Pi$ is based on some featuremap) so that . \\[\\begin{align*} J(\\pi^*) \\ge J^* - \\varepsilon(M,\\Pi) \\end{align*}\\] As to the computational approaches, we will consider a simple approach based on (approximately) following the gradient of $\\theta \\mapsto J(\\pi_\\theta)$. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec15/#potential-remedy-local-search",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec15/#potential-remedy-local-search"
  },"225": {
    "doc": "15. From policy search to policy gradients",
    "title": "Notes",
    "content": "Access models . The reader may be wondering about what is the appropriate “access model” when $\\pi_\\theta$ is not restricted to the form given in \\eqref{eq:boltzmannpp}. There are many possibilities. One is to develop planners for specific parametric forms. A more general approach is to let the planner access \\(\\pi_{\\theta}(\\cdot\\vert s)\\) and $\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(\\cdot \\vert s)$ for any $s$ it has encountered and any value of $\\theta\\in \\mathbb{R}^d$ it chooses. This is akin to the first-order black-box oracle model familiar from optimization theory. From function approximation to POMDPs . The hardness result for policy search is taken from a paper of Vlassis, Littman and Barber, who actually were interested in the computational complexity of planning in partially observable Markov Decision Problems (POMDPs). It is in fact an important observation that with function approximation, planning in MDPs becomes a special case of planning in POMDPs: In particular, if policies are restricted to depend on the states through a feature-map $\\varphi:\\mathcal{S}\\to \\mathbb{R}^d$ (any two states with identical features will get the same action distribution assigned to them), then planning to achieve high reward with this restricted class is almost the same as planning to achieve high reward in a partially observable MDP where the observation function is $\\varphi$. Planners for the former problem could still have some advantage though if they can also access the states: In particular, an online planner which is given a feature-map to help its search but is also given access to the states is in fact not restricted to return actions whose distribution follows a policy from the feature-restricted class of policies. In machine learning, in the analogue problem of competing with a best predictor within a class but using predictors that do not respect the restrictions put on the competitors are called improper and it is known that improper learning is often more powerful than proper learning. However, when it comes to learning online or in a batch fashion then feature-restricted learning and learning in POMDPs become exact analogs. Finally, we note in passing that Vlassis et al. (2012) also add an argument that shows that it is not likely that policy search is in NP. Open problem: Hardness of approximate policy search . Provided that from an approximate solution to the Motzkin-Straus problem one can efficiently extract an approximate solution to the maximum independent set problem, it follows that the approximate version of policy search is also NP-hard. In particular, it is not hard to see with the same construction that if one has an efficient method to find a policy with $J(\\pi) \\ge \\max_\\pi J_\\pi - \\varepsilon$ then this gives an efficient method to find an independent set of size $c\\alpha(G)$ for the said $3$-regular graphs where . \\[c = \\frac{1}{1 + \\frac{1-\\gamma}{\\gamma} \\varepsilon \\alpha(G)} \\ge \\frac{1}{1+ \\frac{1-\\gamma}{\\gamma} \\varepsilon n} \\ge 94/95 \\,,\\] where the last inequality follows if $\\varepsilon\\le 0.5$, $\\gamma\\ge 0.5$ and $ H:=\\frac{1}{1-\\gamma} \\ge \\frac{n}{95/94-1} = 94 n$ holds. Now, it is known that, unless P=NP, there is no polynomial time approximation algorithm for the maximal independent set problem with approximation factor $c=94/95$ or better. Hence, we get that, unless P=NP, there is no polynomial time approximation algorithm for the policy search problem for any fixed $0\\le \\epsilon\\le 0.5$ provided the planning horizon is scaled with $n$ so that $H = \\mathrm{const} n$. (This is somewhat unsatisfactory given that the range of the optimal values is $1/(1-\\gamma)$: It would be more natural to scale $\\epsilon$ with $1/(1-\\gamma)$, i.e., consider relative errors as in complexity theory.) Also, it remains an open problem to get a hardness result for a “constant” $\\gamma$ (independent of $n$). The above is still dependent on whether an approximate solution to the maximum independent set problem can be extracted from an approximate solution to the Motzkin-Straus optimization problem. Dealing with large action spaces . A common reason to consider policy search is because working with a restricted parametric family of policies holds the promise of decoupling the computational cost of learning and planning from the cardinality of the action-space. Indeed, with action-value functions, one usually needs an efficient way of computing greedy actions (with respect to some fixed action-value function). Computing $\\arg\\max_{a\\in \\mathcal{A}} q(s,a)$ in the lack of extra structure of the action-space and the function $q(s,\\cdot)$ takes linear time in the size of $\\mathcal{A}$, which is highly problematic unless $\\mathcal{A}$ has a small cardinality. In many applications of practical interest this is not the case: The action space can be “combinatorially sized”, or even a subset of some (potentially multidimensional) continuous space. If sampling from $\\pi_{\\theta}(\\cdot\\vert s)$ can be done efficiently, one may then potentially avoid the above expensive calculation. Thus, policy search is often proposed as a remedy to extend algorithms to work with large action spaces. Of course, this only applies if the sampling problem can indeed be efficiently implemented, which adds an extra restriction on the policy representation. Nevertheless, there are a number of options to achieve this: One can use for example an implicit representation (perhaps in conjunction with a direct one that uses probabilities/densities) for the policy. For example, the policy may be “represented” as a map $f_\\theta: \\mathcal{S} \\times \\mathcal{R} \\to \\mathcal{A}$ so that sampling from $\\pi_\\theta(\\cdot\\vert s)$ is accomplished by drawing a sample $R\\sim P$ from a fixed distribution over the set $\\mathcal{R}$ and then returning $f(s,R)\\in \\mathcal{A}$. Clearly, this is efficient as long as $f_\\theta$ can be efficiently evaluated at any of its inputs and the random value $R$ can be efficiently produced. If $f_\\theta$ is sufficiently flexible, one can in fact choose a very simple distribution for $P$, such as the standard normal distribution, or the uniform distribution. Note that when $\\mathcal{A}$ is continuous and the policies are deterministic is a special case: The key is still to be able to efficiently produce a sample from $\\pi_\\theta(\\cdot\\vert s)$, just in this case this means a deterministic computation. The catch is that one may also still need the derivatives of $\\pi_{\\theta}(\\cdot\\vert s)$ with respect to the parameter $\\theta$ and with an implicit representation as described above, it is unclear whether these derivatives can be efficiently obtained. As it turns out, this can be arranged if $f_{\\theta}(\\cdot\\vert s)$ is made of composition of elementary (invertible, differentiable) transformations with this property (by the chain rule). This observation is the basis of various approaches to “neural” density estimation (e.g., Tabak and Vanden-Eijnden, 2010, Rezende, Mohamed, 2015, or Jaini et al. 2019). ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec15/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec15/#notes"
  },"226": {
    "doc": "15. From policy search to policy gradients",
    "title": "References",
    "content": ". | Vlassis, Nikos, Michael L. Littman, and David Barber. 2012. “On the Computational Complexity of Stochastic Controller Optimization in POMDPs.” ACM Trans. Comput. Theory, 12, 4 (4): 1–8. | Esteban G. Tabak. Eric Vanden-Eijnden. “Density estimation by dual ascent of the log-likelihood.” Commun. Math. Sci. 8 (1) 217 - 233, March 2010. | Rezende, Danilo Jimenez, and Shakir Mohamed. 2015. “Variational Inference with Normalizing Flows” link. | Rezende, D. J., and S. Mohamed. 2014. “Stochastic Backpropagation and Approximate Inference in Deep Generative Models.” ICML. link. | Jaini, Priyank, Kira A. Selby, and Yaoliang Yu. 2019. “Sum-of-Squares Polynomial Flow.” In Proceedings of the 36th International Conference on Machine Learning, edited by Kamalika Chaudhuri and Ruslan Salakhutdinov, 97:3009–18. Proceedings of Machine Learning Research. PMLR. | Arora, Sanjeev, and Boaz Barak. 2009. Computational Complexity. A Modern Approach. Cambridge: Cambridge University Press. | . The hardness of the maximum independent set problem is a classic result; see, e.g., Theorem 2.15 in the book of Arora and Barak (2009) above, though this proof does not show that the hardness also applies to the case of 3-regular graphs. Below is the paper that shows that approximating the maximum independent set size within a factor of $94/95=0.9894\\dots$ is NP-hard even for $3$-regular graphs. The precise statement is in the main theorem statement on page 29 (this is the first, unnumbered and unnamed theorem on pdf page 3). In particular, the 2nd bullet point has this bound, specifically the hardness kicks in for approximation factors at least as large as $94/95$. I am very grateful for Zachary Friggstad who pointed me to this paper. | Miroslav Chlebík, Janka Chlebíková: Inapproximability Results for Bounded Variants of Optimization Problems. FCT 2003: 27-38 DBLP page | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec15/#references",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec15/#references"
  },"227": {
    "doc": "15. From policy search to policy gradients",
    "title": "15. From policy search to policy gradients",
    "content": "PDF Version . In the previous lectures we attempted to reduce the complexity of planning by assuming that value functions over the large state-action spaces can be compactly represented with a few parameters. While value-functions are an indispensable component of poly-time MDP planners (see Lectures 3 and 4), it is far from clear whether they should also be given priority when working with larger MDPs. Indeed, perhaps it is more natural to consider sets of policies with a compact description. Formally, in this problem setting the planner will be given a black-box simulation access to a (say, $\\gamma$-discounted) MDP $M=(\\mathcal{S},\\mathcal{A},P,r)$ as before, but the interface also provides access to a parameterized family of policies over $(\\mathcal{S},\\mathcal{A})$, \\(\\pi = (\\pi_\\theta)_{\\theta\\in \\mathbb{R}^d}\\), where for any fixed parameter $\\theta\\in \\mathbb{R}^d$, $\\pi_\\theta$ is a memoryless stochastic policy: $\\pi_\\theta:\\mathcal{S} \\to \\mathcal{M}_1(\\mathcal{A})$. For example, $\\pi_\\theta$ could be such that for some feature-map $\\varphi: \\mathcal{S}\\times \\mathcal{A} \\to \\mathcal{R}^d$, . \\[\\begin{align} \\pi_\\theta(a|s) = \\frac{\\exp( \\theta^\\top \\varphi(s,a))}{\\sum_{a'} \\exp(\\theta^\\top \\varphi(s,a'))}\\,, \\qquad (s,a)\\in \\mathcal{S}\\times \\mathcal{A}\\,. \\label{eq:boltzmannpp} \\end{align}\\] In this case “access” to $\\pi_\\theta$ means access to $\\varphi$, which can be either global (i.e., the planner is given the “whole” of $\\varphi$ and can run any preprocessing on it), or local (i.e., $\\varphi(s’,a)$ is returned by the simulator for the “next states” $s’\\in \\mathcal{S}$ and for all actions $a$). Of course, the exponential function can be replaced with other functions, or, one can just use a neural network to output “scores”, which are turned into probabilities in some way. Dispensing with stochastic policies, a narrower class is the class of policies that are greedy with respect to action-value functions that belong to some parametric class. One special case that is worthy of attention due to its simplicity is the case when $\\mathcal{S}$ is partitioned into $m$ (disjoint) subsets $\\mathcal{S}_1,\\dots,\\mathcal{S}_m$ and for $i\\in [m]$, we have $\\mathrm{A}$ basis functions defined as follows: . \\[\\begin{align} \\varphi_{i,a'}(s,a) = \\mathbb{I}( s\\in \\mathcal{S}_i, a= a' )\\,, \\qquad s\\in \\mathcal{S}, a,a'\\in \\mathcal{A}, i\\in [m]\\,. \\label{eq:stateagg} \\end{align}\\] Here, to minimize clutter, we allow the basis functions to be indexed by pairs and identified $\\mathcal{A}$ with ${ 1,\\dots,\\mathrm{A}}$, as usual. Then, the policies are given by $\\theta = (\\theta_1,\\dots,\\theta_m)$, the collection of $m$ probability vectors $\\theta_1,\\dots,\\theta_m\\in \\mathcal{M}_1(\\mathcal{A})$: . \\[\\begin{align} \\pi_\\theta(a|s) = \\sum_{i=1}^m \\sum_{a'} \\varphi_{i,a'}\\theta_{i,a'}\\,. \\label{eq:directpp} \\end{align}\\] Note that because of the special choice of $\\varphi$, $\\pi_{\\theta}(a|s) = \\theta_{i,a}$ for the unique index $i\\in [m]$ such that $s\\in \\mathcal{S}_i$. This is known as state-aggregretion: States belonging to the same group give rise to the same probability distribution over the actions. We say that the featuremap $\\varphi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ is of the state-aggregation type if it takes the form \\eqref{eq:stateagg} with an appropriate reindexing of the basis functions. Fix now a state-aggregation type featuremap. We can consider both the direct parameterization of policies given in \\eqref{eq:directpp}, or the “Boltzmann” parameterization given in \\eqref{eq:boltzmannpp}. As it is easy to see the set of possible policies that can be expressed with the two parameterizations are nearly identical. Letting $\\Pi_{\\text{direct}}$ be the set of policies that can be expressed using $\\varphi$ and the direct parameterization and letting $\\Pi_{\\text{Boltzmann}}$ be the set of policies that can be expressed using $\\varphi$ but with the Boltzmann parameterization, first note that \\(\\Pi_{\\text{direct}},\\Pi_{\\text{Boltzmann}} \\subset \\mathcal{M}_1(\\mathcal{A})^{\\mathcal{S}} \\subset ([0,1]^{\\mathrm{A}})^{\\mathrm{S}}\\), and if we take the closure, $\\text{clo}(\\Pi_{\\text{Boltzmann}})$ of $\\Pi_{\\text{Boltzmann}}$ then we can notice that . \\[\\text{clo}(\\Pi_{\\text{Boltzmann}}) = \\Pi_{\\text{direct}}\\,.\\] In particular, the Boltzmann policies cannot express point-mass distributions with finite parameters, but letting the parameter vectors grow without bound, any policy that can be expressed with the direct parameterization can also be expressed by the Boltzmann parameterization. There are many other possible parameterizations, as also mentioned earlier. The important point to notice is that while the parameterization is necessary so that the algorithms can work with a compressed representation, different representations may describe an identical set of policies. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec15/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec15/"
  },"228": {
    "doc": "16. Policy gradients",
    "title": "The policy gradient theorem",
    "content": "Fix an MDP $M=(\\mathcal{S},\\mathcal{A},P,r)$ and a discount factor $0\\le \\gamma &lt;1$. Continuing from the last lecture for $\\theta\\in \\mathbb{R}^d$ let $\\pi_\\theta$ be a stochastic policy: $\\pi_\\theta:\\mathcal{S}\\to \\mathcal{M}_1(\\mathcal{A})$. Further, fix a distribution $\\mu \\in \\mathcal{M}_1(\\mathcal{S})$ over the states and for a policy $\\pi:\\mathcal{S}\\to \\mathcal{M}_1(\\mathcal{A})$ let . \\[J(\\pi) = \\mu v^\\pi\\] denote the expected value of using policy $\\pi$ in $M$ from an initial state randomly chosen from $\\mu$. The policy gradient theorem gives sufficient conditions under which the map $\\theta \\mapsto J(\\pi_\\theta)$ is differentiable at some parameter $\\theta=\\theta_0$ and gives a “simple” expression for the gradient as a function of $\\frac{d}{dx} M_{\\pi_x} q^{\\pi_{\\theta_0}}$. Just to demistify this, for finite (or discrete) action spaces, for a memoryless policy $\\pi$ and function $q:\\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}} \\to \\mathbb{R}$, $M_{\\pi} q$ is a function mapping states to reals defined via . \\[(M_{\\pi} q)(s) = \\sum_a \\pi(a\\vert s) q(s,a)\\,.\\] Hence, the derivative, $\\frac{d}{dx} M_{\\pi_x} q$ is actually quite simple. It is a function mapping states to $d$ dimensional vectors which satisfies . \\[\\frac{d}{dx} (M_{\\pi_x} q)(s) = \\sum_a \\frac{d}{dx}\\pi_x(a\\vert s) q(s,a)\\,.\\] The theorem we give though is not limited to this case and also applies to when the action space is infinite and even when the policy is deterministic. For the theorem statement, recall that for a policy $\\pi$ we used $\\tilde \\nu_\\mu^\\pi$ to denote its discounted state occupancy measure. Also, for a function $f$, we use $f’$ to denote its derivative. Theorem (Policy Gradient Theorem): Fix an MDP $(\\mathcal{S},\\mathcal{A},P,r)$. For $x\\in \\mathbb{R}^d$, define the maps $f_\\pi: x\\mapsto \\tilde\\nu_\\mu^\\pi M_{\\pi_x} q^\\pi$ and $g_\\pi: x \\mapsto \\tilde \\nu_\\mu^{\\pi_x} v^\\pi$. Fix $\\theta_0\\in \\mathbb{R}^d$. Assume that at least one of the following two conditions is met: . | $\\theta \\mapsto f_{\\pi_\\theta}’(\\theta_0)$ exists and is continuous in a neighborhood of $\\theta_0$ and $g_{\\pi_{\\theta_0}}’(\\theta_0)$ exists; | $\\theta \\mapsto g_{\\pi_\\theta}’(\\theta_0)$ exists and is continuous in a neighborhood of $\\theta_0$ and $f_{\\pi_{\\theta_0}}’(\\theta_0)$ exists; | . Then, $x\\mapsto J(\\pi_x)$ is differentiable at $x=\\theta_0$ and . \\[\\begin{align} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} &amp;= \\frac{d}{dx} \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} M_{\\pi_x} q^{\\pi_{\\theta_0}}|_{x=\\theta_0} = \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} \\frac{d}{dx} M_{\\pi_x} q^{\\pi_{\\theta_0}}|_{x=\\theta_0}\\,, \\label{eq:pgt} \\end{align}\\] where the last equality holds if $\\mathcal{S}$ is finite. For the second expression, we treat $\\frac{d}{dx} M_{\\pi_x} q^{\\pi_{\\theta_0}}$ as an $\\mathrm{S}\\times d$ matrix. Note that this fits well with our convention of treating functions as “column vectors” (hence $M_{\\pi_x}) q^{\\pi_{\\theta_0}}$ is a vector of dimension $\\mathrm{S}$) and with the standard convention that a “vector derivative” creates “row vectors”. Above, the second expression where we moved the derivative with respect to the parameter inside the expression will only be valid in infinite state spaces when some additional regularity assumption is met. One such assumption is that \\(s\\mapsto\\|\\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s)|_{x=\\theta_0}\\|\\) is $\\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}}$-integrable. In words, the theorem shows that the derivative of the performance of a policy can be obtained by integrating a simple derivative that involves the action-value function of the policy. Of the two conditions of the theorem, the first condition is the one that is generally easier to verify. In particular, the condition on the continuous differentiability of $x\\mapsto f_{\\pi_x}$ at $x=\\theta_0$ is usually easy to verify. To show the differentiability of $x\\mapsto g_{\\pi_x}$ at $x=\\theta_0$ just recall that if the partial derivatives of a function exist and are continuous the function is differentiable. Then recall that $\\tilde \\nu_\\mu^{\\pi_x} v = \\sum_{t=0}^\\infty \\gamma^t \\nu P_{\\pi_x}^t v$ and hence its differentiability with respect to (say) $x_1$ follows if $x\\mapsto \\nu M_{\\pi_x} P v$ is continuously differentiable at $x=\\theta_0$. In effect, for finite state-action spaces, differentiability at $\\theta_0$ follows (and the conditions of the theorem are satisfied) as long as for any $(s,a)$ state-action pair, the maps $x\\mapsto \\pi_x(a|s)$ have continuous partial derivatives at $x=\\theta_0$. Proof: The proof is based on a short calculation that starts with writing the value difference identity for $v^{\\pi_x}-v^{\\pi_{\\theta_0}}$, multiplied from the right by $\\mu$, taking derivatives and then letting $x=\\theta_0$. The details are as follows: Recall from Calculus 101 the following result: Assume that $f = f(u,v)$ satisfies at least one of the two conditions: . | \\(z\\mapsto \\frac{\\partial}{\\partial v} f(z,x)\\) exists and is continuous in a neighborhood of $z=x$ and \\(\\frac{\\partial}{\\partial u} f(u,x)\\vert_{u=x}\\) exists; | \\(z\\mapsto \\frac{\\partial}{\\partial u} f(x,z)\\) exists and is continuous in a neighborhood of $z=x$ and \\(\\frac{\\partial}{\\partial v} f(x,v)\\vert_{v=x}\\) exists. | . Then $z \\mapsto f(z,z)$ is differentiable at $z=x$ and . \\[\\begin{align} \\frac{d}{dx} f(x,x) = \\frac{\\partial}{\\partial u} f(x,x) + \\frac{\\partial}{\\partial v} f(x,x)\\,. \\label{eq:c101} \\end{align}\\] Let $\\pi’,\\pi$ be two memoryless policies. By the value difference identity, . \\[\\begin{align*} v^{\\pi'}- v^{\\pi} &amp; = (I-\\gamma P_{\\pi'})^{-1} [ T_{\\pi'} v^\\pi - v^\\pi] \\\\ &amp; = (I-\\gamma P_{\\pi'})^{-1} [ M_{\\pi'} q^\\pi - v^\\pi]\\,, \\end{align*}\\] where the last equality just used that that $T_{\\pi’} v^\\pi = M_{\\pi’} (r+\\gamma P v^\\pi) = M_{\\pi’} q^{\\pi}$. Now let $\\pi’ = \\pi_x$ and $\\pi = \\pi_{\\theta_0}$ and multiply the value difference identity from the left by $\\mu$ to get . \\[\\begin{align} \\mu( v^{\\pi_x}- v^{\\pi_{\\theta_0}}) = \\tilde \\nu_\\mu^{\\pi_x} [ M_{\\pi_x}q^{\\pi_{\\theta_0}} - v^{\\pi_{\\theta_0}}]\\,. \\label{eq:vdi2} \\end{align}\\] Now, focusing on the first term on the right-hand-side, let . \\[\\begin{align} f(u,v) = \\tilde \\nu_\\mu^{\\pi_u} M_{\\pi_v}q^{\\pi_{\\theta_0}}\\,. \\label{eq:fdef} \\end{align}\\] Provided that $f$ is sufficiently regular in a neighborhood of $(x,x)$ (to be discussed later), \\eqref{eq:c101} gives that . \\[\\begin{align*} \\frac{d}{dx} f(x,x) = \\frac{d}{du} \\tilde \\nu_\\mu^{\\pi_u} M_{\\pi_x}q^{\\pi_{\\theta_0}}|_{u=x} + \\frac{d}{dv} \\tilde \\nu_\\mu^{\\pi_x} M_{\\pi_v}q^{\\pi_{\\theta_0}}|_{v=x} \\end{align*}\\] Taking the derivative of both sides of \\eqref{eq:vdi2} with respect to $x$ and using the above display we get . \\[\\begin{align*} \\frac{d}{dx} J(x) = \\frac{d}{dx} \\mu( v^{\\pi_x}- v^{\\pi_{\\theta_0}}) = \\frac{d}{du} \\tilde \\nu_\\mu^{\\pi_u} M_{\\pi_x}q^{\\pi_{\\theta_0}}|_{u=x} + \\frac{d}{dv} \\tilde \\nu_\\mu^{\\pi_x} M_{\\pi_v}q^{\\pi_{\\theta_0}}|_{v=x} + \\frac{d}{dx} \\tilde \\nu_\\mu^{\\pi_x} v^{\\pi_{\\theta_0}}\\,. \\end{align*}\\] Now let $x = \\theta_0$. Then, $M_{\\pi_x}q^{\\pi_{\\theta_0}} = M_{\\pi_{\\theta_0}}q^{\\pi_{\\theta_0}} = v^{\\pi_{\\theta_0}}$. Hence, the first and the third term of the above display cancel each other and we get . \\[\\begin{align*} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} = \\frac{d}{dv} \\tilde \\nu_\\mu^{\\pi_{\\theta_0}} M_{\\pi_v} q^{\\pi_{\\theta_0}}|_{v=\\theta_0}\\,. \\end{align*}\\] Finally, the conditions to apply \\eqref{eq:c101} to our $f$ in \\eqref{eq:fdef} are met by our assumption on $f_\\pi$ and $g_\\pi$, finishing the proof. \\(\\qquad \\blacksquare\\) . When the action-space is discrete and $\\pi_\\theta$ are stochastic policies, we can further manipulate the expression we obtained. In particular, in this case . \\[\\begin{align*} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\pi_x(a\\vert s) q^{\\pi_{\\theta_0}}(s,a) \\end{align*}\\] and thus, for finite $\\mathcal{A}$, . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\frac{d}{dx} \\pi_x(a\\vert s) q^{\\pi_{\\theta_0}}(s,a)\\,. \\label{eq:basicpg} \\end{align}\\] While this can be used as the basis of evaluating (or approximating) gradient, it may be worthwhile to point out an alternate form which is available when $\\pi_x(a\\vert s)&gt;0$. In this case, using the chain rule we get . \\[\\begin{align*} \\frac{d}{dx} \\log \\pi_x(a\\vert s) = \\frac{\\frac{d}{dx} \\pi_x(a\\vert s)}{\\pi_x(a\\vert s)}\\,. \\end{align*}\\] Using this in \\eqref{eq:basicpg} we get . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) q^{\\pi_{\\theta_0}}(s,a)\\,, \\label{eq:basicpga} \\end{align}\\] which has the pleasant property that it takes the form of an expected value over the actions of the score function of the policy map correlated with the action-value function. Before moving on it is worth pointing out that an equivalent expression is obtained if $q^{\\pi_{\\theta_0}}(s,a)$ above is shifted by an arbitrary constant which may depend on $\\theta_0$ or $s$ but not $a$. Indeed, since $\\sum_a \\pi_x(a\\vert s) b(s,\\theta_0) = b(s,\\theta_0)$, differentiating both sides with respect to $x$ gives $\\sum_a \\frac{d}{dx}\\pi_x(a\\vert a) b(s,\\theta_0)=0$. Hence, we also have . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) (q^{\\pi_{\\theta_0}}(s,a)-b(s,\\theta_0))\\,. \\label{eq:basicpga2} \\end{align}\\] This may have significance when using simulation to evaluate derivatives: One may attempt to use an appropriate “bias” term to reduce the variance of the estimate of the gradient. Before discussing simulation any further, it may be also worthwhile to discuss what happens when the action-space is infinite. For countable infinite action spaces, the only difference is that \\eqref{eq:basicpg} may not always hold. An easy sufficient condition for this to hold is that \\(\\sum_{a} \\|\\frac{d}{dx} \\pi_x(a\\vert s)\\|\\, |q^{\\pi_{\\theta_0}}(s,a)|\\) is summable, or equivalently, \\(\\|\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\|\\, |q^{\\pi_{\\theta_0}}(s,a)|\\) is $\\pi_x(\\cdot\\vert s)$-summable/integrable. For uncountably infinite action spaces, this argument works with the minimal necessary changes. In the most general case, $\\pi_\\theta(\\cdot\\vert s)$ is a probability measure over $\\mathcal{A}$ and its derivative is a vector-valued measure. The formulae derived above (e.g., \\eqref{eq:basicpga2}) remain valid if we replace the sum with an integral when $\\pi_\\theta(\\cdot\\vert s)$ is given in the form of a density with respect to some fixed measure $\\lambda$ over $\\mathcal{A}$: . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\int_{\\mathcal{A}} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) (q^{\\pi_{\\theta_0}}(s,a)-b(s,\\theta_0)) \\lambda(da)\\,. \\label{eq:basicpga3} \\end{align}\\] In fact, this is a strictly more general form: \\eqref{eq:basicpga2} is a special case of \\eqref{eq:basicpga3} when $\\lambda$ is set to the counting measure over $\\mathcal{A}$. In the special case when \\(\\pi_{\\theta}(\\cdot\\vert s) = \\delta_{f_{\\theta}(s)}(\\cdot)\\) (a Dirac at $f_{\\theta}(s)$), in words, when we have a deterministic policy map and $f$ is differentiable with respect to $\\theta$, it is better to start from the formula given in the theorem. Indeed, in this case, . \\[\\begin{align*} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = q^{\\pi_{\\theta_0}}(s,f_\\theta(s)) \\end{align*}\\] and hence . \\[\\begin{align*} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\frac{d}{dx} q^{\\pi_{\\theta_0}}(s,f_x(s)) \\end{align*}\\] and thus, if either $\\mathcal{S}$ is finite or an appropriate regularity condition holds, . \\[\\begin{align*} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} &amp;= \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} \\frac{d}{dx} q^{\\pi_{\\theta_0}}(\\cdot,f_x(\\cdot))|_{x=\\theta_0}\\,. \\end{align*}\\] If $a\\mapsto q^{\\pi_{\\theta_0}}(s,a)$ is differentiable and $x\\mapsto f_x(s)$ is also differentiable at $x=\\theta_0$ for every $s$ then . \\[\\begin{align*} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} &amp;= \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} \\frac{\\partial}{\\partial a} q^{\\pi_{\\theta_0}}(\\cdot,f_{\\theta_0}(\\cdot)) \\frac{d}{dx} f_x(\\cdot)|_{x=\\theta_0}\\,, \\end{align*}\\] which is known as the “deterministic policy gradient formula”. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec16/#the-policy-gradient-theorem",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec16/#the-policy-gradient-theorem"
  },"229": {
    "doc": "16. Policy gradients",
    "title": "Gradient methods",
    "content": "The idea of gradient methods is to make small steps in the parameter space in the direction of the gradient of an objective function that is to be maximized. In the context of policy search, this works as follows: If $x_i\\in \\mathbb{R}^d$ denotes the parameter vector in round $i$, . \\[x_{i+1} = x_i + \\alpha_i \\nabla_x J(\\pi_x)|_{x=x_i}\\,,\\] where for $f$ differentiable, $\\nabla_x f = (\\frac{d}{dx} f)^\\top$ is the “gradient” (transpose of derivative). Above, $\\alpha_i$ is a positive tuning parameter, called the “stepsize” of the update. The idea is that the “gradient” points in the direction where the function is expected to grow. Indeed, since by definition, . \\[f(x') =f(x) + f'(x) (x'-x) + o(\\|x'-x\\|)\\] if $x’ = x+ \\delta (f’(x))^\\top$, . \\[f( x+ \\delta (f'(x))^\\top ) = f(x)+\\delta \\| f'(x)\\|_2^2 + o(|\\delta|)\\,,\\] or . \\[\\frac{f( x+ \\delta (f'(x))^\\top ) - f(x)}{\\delta} = \\| f'(x)\\|_2^2 + o(1)\\,,\\] For any $\\delta$ sufficiently small so that the $o(1)$ term (in absolute value) is below \\(\\| f'(x)\\|_2^2\\), we see that the right-hand side is positive, hence so is the left-hand side, as claimed. This simple observation is the basis of a huge number of algorithmic variants. In the lack of extra structure the best we can hope from a gradient method is that it will end up in the vicinity of a stationary point. In the presence of extra structure (.e.g, concave function to be maximized), convergence to a global maximum can be guaranteed. In all cases the key to the success of gradient methods is the appropriate choice of the stepsizes; these choices are based on a refinement of the above simple argument that shows that moving towards the direction of the gradient helps. There are also ways of “speeding up” convergence; these “acceleration methods” use a refined iteration (two iterates updated simultaneously) and can greatly speed up convergence. As there are many excellent texts that describe various aspects of gradient methods which cover these ideas, we will not delve into them any further, but I will rather give some pointers to this literature in the endnotes. The elephant in the room here is that the gradient of $J$ is not readily available. The next best thing then is to attempt to build an estimate $G$ of $\\nabla_x J(\\pi_x)$. In the planning setting, the question is whether one can get reasonable estimates of this gradient using a simulator. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec16/#gradient-methods",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec16/#gradient-methods"
  },"230": {
    "doc": "16. Policy gradients",
    "title": "Gradient estimation",
    "content": "Generally speaking there are two types of errors when construction an estimate of the gradient: The one that is purely random, and the one that is not. Defining $g(x) = \\mathbb{E}[G]$, $b(x)=\\nabla_x J(\\pi_x) - g(x)$ measures the “bias” of the gradient estimate, while $G-g(x)$ is the noise. Gradient methods with decreasing (or small) stepsizes naturally “average out” the noise. The version of gradient methods that are able to do this are called stochastic gradient methods. Naturally, these methods are slower when the noise is larger and in general cannot converge faster than how fast the noise averages out. In particular, in persistent noise (i.e., noise with nonvanishing variance), the best rate available for stochastic gradient methods is $O(1/\\sqrt{t})$. While this can be slower than what can be achieved without noise, if the iteration cost is polynomial in the relevant quantities, the total cost of achieving an $\\varepsilon&gt;0$ stationary point can be bounded by a polynomial in these quantities and $1/\\varepsilon^2$. When the gradient estimates are biased, the bias will in general put a limit on how close a gradient method can get to a stationary point. While generally a zero bias is preferred to a nonzero bias, a nonzero bias which is positively aligned with the gradient ($\\langle b(x),\\nabla_x J(\\pi_x) \\rangle\\ge 0$) does not hurt (again, for small stepsizes). When there is no way to guarantee that the bias is positively aligned with the gradient, one may get back into control by making sure that the magnitude of the bias is small relative to the magnitude of the gradient. The next question is of course, how to estimate the gradient. For this many approaches have been proposed in the literature. When a simulator is available, as in our case, a straightforward approach is to start from the policy gradient theorem. Indeed, under mild regularity conditions (e.g., if there are finitely many states) \\eqref{eq:pgt} together with \\eqref{eq:basicpga3} gives . \\[\\begin{align} \\frac{d}{dx} J(\\pi_x) = \\int_{\\mathcal{S}} \\tilde \\nu_\\mu^{\\pi_x}(ds) \\int_{\\mathcal{A}} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) (q^{\\pi_{x}}(s,a)-b(s,x)) \\lambda(da)\\,. \\end{align}\\] Now note that $(1-\\gamma)\\tilde \\nu_\\mu^{\\pi_x}$ is a probability measure over $\\mathcal{S}$. Let $S_0,A_0,S_1,A_1,\\dots$ be an infinite sequence of state-action pairs obtained by simulating policy $\\pi_x$ starting from $S_0\\sim \\mu$. In particular, $A_t \\sim \\pi_x(\\cdot|S_t)$ and $S_{t+1}\\sim P_{A_t}(S_t)$ for any $t\\ge 0$. In addition, define $T_1,T_2$ to be independent of each other and from the trajectory $S_0,A_0,S_1,A_1,\\dots$ and have a geometric distribution with parameter $1-\\gamma$. Then, . \\[G = \\frac{1}{1-\\gamma} \\frac{d}{dx} \\log \\pi_x(A_{T_1}\\vert S_{T_1}) \\left(\\sum_{t=0}^{T_2-1} r_{A_{T_1+t}}(S_{T_1+t}) -b(S_{T_1},x)\\right)\\] is an unbiased estimate of $\\frac{d}{dx} J(\\pi_x)$: . \\[\\mathbb{E}[G] = \\frac{d}{dx} J(\\pi_x)\\,.\\] The argument to show this has partially be given earlier in Lecture 8. One can also show that $G$ has a finite covariance matrix, as well as that the expected effort to obtain $G$ is $O(\\frac{1}{1-\\gamma})$. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec16/#gradient-estimation",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec16/#gradient-estimation"
  },"231": {
    "doc": "16. Policy gradients",
    "title": "Vanilla policy gradients (PG) with some special policy classes",
    "content": "Given the hardness result presented in the previous lecture, there is no hope that gradient methods or any other method will find the global optima of the objective function in policy search in a policy-class agnostic manner. To guarantee computational efficiency, one then . | either needs to give up on convergence to a global optima, or | give up on generality, i.e., give up on that the method should work for any policy class and/or policy parameterization. | . Gradient ascent to find a good policy (“vanilla policy gradients”) is one possible approach to take even if it faces these restrictions. In fact, gradient ascent in some cases will find a globally optimal policy. In particular, it has been long known that with small enough stepsizes gradient ascent converges at a reasonable speed to a global optimum provided that two conditions hold: . | The objective function $f$ is smooth (its derivative is Lipschitz continuous); | The objective function is gradient dominated, i.e., with some constants $c&gt;0$, $p \\ge 1$, $f$ satisfies \\(\\sup_x f(x)-f(x')\\le c \\| f'(x') \\|_2^p\\) for any $x’\\in \\mathbb{R}^d$. | . An example when both of these conditions are met is the direct policy parameterization, which does not allow any compression and is thus not helpful per se, but can serve as a test-case to see how far policy gradient (PG) methods can be pushed. In this case, the parameter vector $\\theta$ is $\\mathrm{S}\\mathrm{A}$ dimensional. By allowing “two-dimensional index”, $\\pi_{\\theta}(a\\vert s)=\\theta_{s,a}$, that is, the parameters encode the action selection probabilities in a direct manner. In this case, since the components of $\\theta$ represent probabilities, they need to be nonnegative and the appropriate components needs to sum to one. Hence, $\\theta\\in \\Theta$ for an appropriate set $\\Theta \\subset [0,1]^{\\mathrm{S}\\mathrm{A}}$. Accordingly, one needs to change gradient ascent. This is done as follows: When a proposed update moves the parameter vector outside of $\\Theta$, the proposed updated parameter vector is “back-projected” to $\\Theta$. For the projection there are a number of reasonable options, such as choosing the point within $\\Theta$ which is closest to the proposed point in the standard Euclidean distance. With this modification, gradient ascent can be shown to converge at a reasonable speed in this case. This parallels the methods that were developed for the tabular case (policy iteration, value iteration). In fact, the algorithm can be seen as a “smoother”, incremental version of policy iteration, which gradually adjusts the probabilities assigned to the individual actions. Using $\\pi_i$ to denote the $i$th policy, from the policy gradient theorem one gets . \\[\\tilde \\pi_{i+1}(a\\vert s ) = \\pi_i(a\\vert s) + \\alpha_i \\tilde \\nu_\\mu^{\\pi_i}(s) q^{\\pi_i}(s,a) \\,,\\] and . \\[\\pi_{i+1}(\\cdot\\vert s) = \\arg\\min_{p\\in \\mathcal{M}_1(\\mathcal{A})} \\| p - \\pi_{i+1}(\\cdot\\vert s) \\|_2, \\qquad s\\in \\mathcal{S}\\,.\\] Thus, the probability of an action in a state is increased in proportion to the value of that state. That the action-value of action $a$ at state $s$ is multiplied with the discounted occupancy at $s$ induced by using policy $\\pi_i$ started from $\\mu$ is a bit of a surprise. In particular, if a state is inaccessible under policy $\\pi_i$, the corresponding probabilities will not be updated. In fact, because this, the above iteration may get stuck at a suboptimal policy. The reader is invited to construct an example when this happens. To prevent this, it turns out to be sufficient if there is a constant $C&gt;0$ such that it holds that . \\[\\begin{align} \\tilde \\nu_\\mu^{\\pi^*}(s)\\ge C \\mu(s)\\,, \\qquad \\text{for all } s\\in \\mathcal{S}\\,, \\label{eq:exploinit} \\end{align}\\] where \\(\\pi^*\\) is an optimal policy. Since $\\mu$ appears on both sides and \\(\\pi^*\\) is unknown, this condition does not look to helpful. However, if one chooses $\\mu$ to be positive everywhere, the condition is clearly met. In any case, when \\eqref{eq:exploinit} holds, gradient dominance and smoothness can be both verified, which in turn implies that the above update will converge at a geometric speed, the geometric speed involves an instance dependent constant which has no polynomial bound in terms of $H_\\gamma = 1/(1-\\gamma)$ and the size of the state-action space. Needless to say this is quite unattractive. Policy gradient methods can be sensitive to how policies are parameterized. For illustration, consider still the “tabular case”, just now change the way the memoryless policies are represented. One possibility is to use the Boltzmann, also known as the softmax representation. In this case $\\theta\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ and . \\[\\begin{align*} \\pi_{\\theta}(a\\vert s) = \\frac{\\exp( \\theta_{s,a})}{ \\sum_{a'} \\exp( \\theta_{s,a'})}\\,, \\qquad (s,a)\\in \\mathcal{S}\\times\\mathcal{A}\\,. \\end{align*}\\] A straightforward calculation gives . \\[\\begin{align*} \\frac{\\partial}{\\partial \\theta_{s,a}} \\log \\pi_\\theta( a'\\vert s' ) = \\mathbb{I}(s=s',a=a') - \\pi_{\\theta}(a\\vert s) \\mathbb{I}(s=s') \\end{align*}\\] and hence . \\[\\begin{align*} \\frac{\\partial}{\\partial\\theta_{(s,a)}} J(\\pi_{\\theta}) &amp; = \\sum_{s'} \\tilde \\nu_\\mu^{\\pi_\\theta}(s') \\sum_{a'}\\pi_\\theta(a'\\vert s) \\frac{\\partial}{\\partial \\theta_{s,a}} \\log \\pi_\\theta( a'\\vert s' ) q^{\\pi_\\theta}(s',a')\\\\ &amp; = \\nu_\\mu^{\\pi_\\theta}(s,a) \\left( q^{\\pi_\\theta}(s,a)-v^{\\pi_\\theta}(s) \\right)\\,, \\end{align*}\\] where recall that $\\nu_\\mu^{\\pi}$ is the discounted state-occupancy measure over the state-action pairs of policy $\\pi$ when the initial state distribution is $\\mu$. The difference in the bracket on the right-hand side is known as the advantage of action $a$ and, accordingly, the function . \\[\\mathfrak{a}^{\\pi} = q^\\pi-v^\\pi\\,,\\] which is a function mapping state-action pairs to reals, is called the advantage function underlying policy $\\pi$. To justify the terminology, note that policy iteration can be seen as choosing in each state the action that maximizes the “advantage”. Thus, we expect that we get a better policy if the “probability mass” in the action distribution is shifted towards actions with a larger advantage. Note though that advantages (as defined above) can also be negative and in fact if $\\pi$ is optimal, all actions have nonnegative advantages only. The gradient ascent rule prescribes that . \\[\\theta_{i+1} = \\theta_i + \\alpha_i \\nu_\\mu^{\\pi_{\\theta_i}} \\circ \\mathfrak{a}^{\\pi_{\\theta_i}}\\,,\\] where $\\circ$ denotes componentwise product. While this is similar to the previous update, now the meaning of parameters is quite different. In fact, just because a parameter is increased does not necessarily mean that the probability of the corresponding action is increased: This will only happen if the increase of this parameter exceeds that of the other parameters “at the same state”. By slightly abusing notation with defining $\\pi_i = \\pi_{\\theta_i}$, we have . \\[\\begin{align} \\pi_{i+1}(a\\vert s) \\propto \\pi_i(a\\vert s) \\exp( \\alpha_i \\nu_\\mu^{\\pi_i}(s,a) \\mathfrak{a}^{\\pi_i}(s,a))\\,. \\label{eq:softmaxtab} \\end{align}\\] Just like in the previous update rule, we also see the occupancy measure “weighting” the update. This is again not necessarily helpful and if anything, again, speaks to the arbitrariness of gradient methods. And while this does not entirely stop policy gradient to find an optimal policy, and again, one can even show that the speed is geometric, though, as before, the algorithm altogether fails to run in polynomial time in the relevant quantities. For this theorem which we give without proof recall that $H_\\gamma = 1/(1-\\gamma)$. Theorem (PG is slow with Boltzmann policies): There exists universal constants $\\gamma_0,c,C&gt;0$ such that for any $\\gamma_0&lt;\\gamma&lt;1$, if $\\mathrm{S}&gt;C H_\\gamma^6$ then one can find a discounted MDP with $\\mathrm{S}$ states and $3$ actions, setting $\\mu$ to be the uniform distribution and initializing the parameters so that $\\pi_0$ is the uniform random policy, softmax PG with a constant stepsize of $\\alpha&gt;0$ takes at least . \\[\\frac{c}{\\alpha} \\mathrm{S}^{2^{\\Omega({H_\\gamma})}}\\] iterations. As one expects that without any compression, the chosen planner should behave reasonably, this rules out the “vanilla” version of policy gradient. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec16/#vanilla-policy-gradients-pg-with-some-special-policy-classes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec16/#vanilla-policy-gradients-pg-with-some-special-policy-classes"
  },"232": {
    "doc": "16. Policy gradients",
    "title": "Natural policy gradient (NPG) methods",
    "content": "In fact, a quite unsatisfactory property of gradient ascent that the speed at which it converges can greatly depend on the parameterization used. Thus, for the same policy class, there are many possible “gradient directions”, depending on the parameterization chosen. What is a gradient direction for one parameterization is not necessarily a gradient direction for another one. But what is common about these directions that an infinitesimal step along them is guaranteed increase the objective. One can in fact take a direction obtained with a parameterization and look at what direction it gives with another parameterizations. To get some order, consider transforming all these directions into the space that corresponds to the direct parameterization. It is not hard to see that all possible directions that are within 90 degrees of the gradient direction with this parameterization can be obtained by considering an appropriate parameterization. More generally, regardless of parameterization, all directions within 90 degrees of the gradient direction are ascent directions. This motivates changing the stepsize $\\alpha_i$ from a scalar to a matrix $A_i$. Clearly, to keep the angle between the original gradient direction $g$ and the transformed direction $A_i g$ below 90 degrees, $g^\\top A_i g\\ge 0$ has to hold. For $A_i$ symmetric, this restricts the set of matrix “stepsizes” to the set of positive definite matrices (still, a large set). There are many ways to choose a matrix stepsize. Newton’s method is to choose it so that the direction is the “best” if the function is replaced by its local quadratic approximation. This provably helps to reduce the number of iterations when the objective function is “ill-conditioned”, though all matrix stepsize methods incur additional cost per each iteration, which will often offset the gains. Another idea, which comes from statistical problems where one often works with distributions is to find the direction of update which coincides with the direction one would obtain if one used the steepest descent direction directly in the space of distributions where distances are measured with respect to relative entropy. In some cases, this approach, which was coined the “natural gradient” approach, has been shown to give better results, though the evidence is purely empirical. As it turns out, the matrix stepsize to be used with this approach is the (pseudo)inverse of the so-called Fisher information matrix. In our context, for every state, we have distributions over the actions. Fixing a state $s$, the Fisher information matrix becomes . \\[F_x(s) = \\frac{d}{dx} \\log \\pi_x(\\cdot\\vert s)\\, \\frac{d}{dx} \\log \\pi_x(\\cdot\\vert s)^\\top\\,.\\] To get the “information rate” over the states, one can sum these matrices up, weighted by the discounted state occupancy measure underlying $\\mu$ and $\\pi_x$ to get . \\[F(x) := \\nu_\\mu^{\\pi_x} F_x \\,.\\] The update rule then takes the form . \\[x_{i+1} = x_i + \\alpha_i F(x_i)^{\\dagger} \\nabla_x J(\\pi_x)\\,,\\] where for a square matrix $A$, $A^{\\dagger}$ denotes the pseudoinverse of $A$. Interestingly, the update direction can be obtained without calculating $F$ and inverting it: . Proposition: We have . \\[\\begin{align*} (1-\\gamma) F(x)^{\\dagger} \\nabla_x J(\\pi_x) = \\arg\\min_{w\\in \\mathbb{R}^d} \\nu_\\mu^{\\pi_x} \\left( w^\\top \\nabla_x \\log \\pi_x(\\cdot\\vert \\cdot)- \\mathfrak{a}^{\\pi_x}\\right)^2\\,, \\end{align*}\\] where $\\mathfrak{a}^{\\pi_x} =q^{\\pi_x}-v^{\\pi_x}$ and $\\arg\\min$ chooses the minimum \\(\\|\\cdot\\|_2\\)-norm solution if multiple minimizers exist. Proof: Just recall the formula that gives the solution to a least-squares problem. The details are left to the reader. \\(\\qquad \\blacksquare\\) . As an example of how things look like consider the case when $\\pi_x$ takes the form of a Boltzmann policy: . \\[\\pi_x(a\\vert s) \\propto \\exp(x^\\top \\phi(s,a))\\,,\\] where $\\phi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ is a feature-map. Then, assuming that there are finitely many actions, . \\[\\nabla_x \\log \\pi_x(a\\vert s) = \\underbrace{\\phi(s,a)- \\sum_{a'} \\pi_x(a'\\vert s) \\phi(s,a')}_{\\psi_x(s,a)}\\,.\\] Then, the natural policy gradient update takes the form . \\[x_{i+1} = x_i + \\alpha_i w_i\\,,\\] where . \\[w_i = \\arg\\min_{w\\in \\mathbb{R}^d} \\nu_\\mu^{\\pi_x} \\left( w^\\top \\psi_x - \\mathfrak{a}^{\\pi_{x_i}}\\right)^2\\] In the tabular case $(d=\\mathrm{S}\\mathrm{A}$, no compression), . \\[w_i(s,a) = \\mathfrak{a}^{\\pi_{x_i}}(s,a)\\] and thus . \\[\\pi_{i+1}(a\\vert s) \\propto \\pi_i(a\\vert s) \\exp( \\alpha_i \\mathfrak{a}^{\\pi_i}(s,a) ) = \\pi_i(a\\vert s) \\exp( \\alpha_i q^{\\pi_i}(s,a) )\\,.\\] Note that this update rule eliminates the term $\\nu_\\mu^{\\pi_i}(s,a)$ term that we have previously seen (cf. \\eqref{eq:softmaxtab}). NPG is known to enjoy a reasonable speed of convergence, which gives altogether polynomial planning time. This is promising. No similar results are available for the nontabular case. Note that if we (arbitrarily) change the definition of $w_i$ by replacing $\\psi_x$ above with $\\phi$ and $a^{\\pi_x}$ with $q^{\\pi_x}$, we get what has been called in the literature Q-NPG: . \\[w_i = \\arg\\min_{w\\in \\mathbb{R}^d} \\nu_\\mu^{\\pi_x} \\left( w^\\top \\phi - q^{\\pi_x}\\right)^2\\,.\\] Note that the only difference between Q-NPG and Politex is that in Politex one uses . \\[w_i = \\arg\\min_{w\\in \\mathbb{R}^d} \\hat \\nu \\left( w^\\top \\phi - q^{\\pi_x}\\right)^2\\,,\\] where $ \\hat \\nu$ is the measure obtained from solving the G-optimal design problem. The price of not using $\\hat \\nu$ but using $\\nu_\\mu^{\\pi_x}$ in Q-NPG is that the approximation error in Q-NPG becomes . \\[\\frac{C\\varepsilon}{(1-\\gamma)^{1.5}}\\] where . \\[C = \\left\\| \\frac{d\\tilde\\nu_\\mu^{\\pi^*} }{d\\mu} \\right\\|_\\infty\\] gives a bound on how much the distribution $\\mu$ differs from that of obtained when the optimal policy $\\pi^*$ is followed from $\\mu$. As was argued before, it is necessary that $C$ is finite for policy gradient methods not to “get stuck” at local optima. However, $C$ can be arbitrarily large even for finite state-action MDPs; an in fact it is the presence of $C$ that makes the policy gradient with the direct parameterization a slow algorithm. In contrast, the same quantity in Politex is . \\[\\frac{\\sqrt{d}\\varepsilon}{1-\\gamma}\\,.\\] Not only the uncontrolled constant $C$ is removed, but the dependence on the planning horizon is also improved. Other than these differences, the results available for Q-NPG are similar to that of Politex and in fact the proof technique to obtain the results is also the same. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec16/#natural-policy-gradient-npg-methods",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec16/#natural-policy-gradient-npg-methods"
  },"233": {
    "doc": "16. Policy gradients",
    "title": "The proof of the Calculus 101 result",
    "content": "For completeness, here is the proof of \\eqref{eq:c101}. For the proof recall that for a function $g:\\mathbb{R}^d \\to \\mathbb{R}$, $\\frac{d}{dx} g(x_0)$ is the unique linear operator (row vector, in the Euclidean case) that satisfies . \\[\\begin{align*} g(x)=g(x_0)+\\frac{d}{dx} g(x_0) (x-x_0) + o( \\|x-x_0\\|) \\text{ as } x\\to x_0\\,. \\end{align*}\\] Hence, it suffices to show that . \\[\\begin{align*} f(x',x') = f(x,x) + \\left( \\frac{\\partial}{\\partial u} f(u,x)\\vert_{u=x} + \\frac{\\partial}{\\partial v} f(x,v)\\vert_{v=x} \\right) (x'-x) + o( \\|x'-x\\|)\\,. \\end{align*}\\] To minimize clutter we will write $\\frac{\\partial}{\\partial u} f(x’,x)$ for $\\frac{\\partial}{\\partial u} f(u,x)\\vert_{u=x’}$ (and similarly we write $\\frac{\\partial}{\\partial v} f(x,x’)$ for $\\frac{\\partial}{\\partial v} f(x,v)\\vert_{v=x’}$). By definition we have . \\[\\begin{align*} f(x',x') = f(x',x) + \\frac{\\partial}{\\partial v} f(x',x) (x'-x) + o( \\| x'-x\\| ) \\end{align*}\\] and . \\[\\begin{align*} f(x',x) = f(x,x) + \\frac{\\partial}{\\partial u} f(x,x) (x'-x) + o( \\| x'-x\\| )\\,. \\end{align*}\\] Putting these together we get . \\[\\begin{align*} f(x',x') &amp; = f(x,x) + \\left( \\frac{\\partial}{\\partial v} f(x',x) + \\frac{\\partial}{\\partial u} f(x,x) \\right) (x'-x) + o( \\|x'-x\\|) \\\\ &amp; = f(x,x) + \\left( \\frac{\\partial}{\\partial v} f(x,x) + \\frac{\\partial}{\\partial u} f(x,x) \\right) (x'-x) \\\\ &amp; \\qquad\\qquad\\,\\,+ \\left( \\frac{\\partial}{\\partial v} f(x',x) - \\frac{\\partial}{\\partial v} f(x,x)\\right) (x'-x) + o( \\|x'-x\\|) \\\\ &amp; = f(x,x) + \\left( \\frac{\\partial}{\\partial v} f(x,x) + \\frac{\\partial}{\\partial u} f(x,x) \\right) (x'-x) + o( \\|x'-x\\|) \\,. \\end{align*}\\] where the last equality follows if $\\frac{\\partial}{\\partial v} f(x’,x) - \\frac{\\partial}{\\partial v} f(x,x) = o(1)$ as $x’\\to x$, i.e., if $x’\\mapsto \\frac{\\partial}{\\partial v} f(x’,x)$ is continuous at $x’=x$. That the result also holds under the assumption that $x’\\mapsto \\frac{\\partial}{\\partial u} f(x,x’)$ is continuous at $x’=x$ follows from a symmetric argument. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec16/#the-proof-of-the-calculus-101-result",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec16/#the-proof-of-the-calculus-101-result"
  },"234": {
    "doc": "16. Policy gradients",
    "title": "Summary",
    "content": "While policy gradient methods remain extremely popular and the idea of directly searching in the set of policies is attractive, at the moment it appears that they not only lack theoretical support, but the theoretical results suggest that it is hard to find any setting where policy gradient methods would be provably competitive with alternatives. At minimum, they need careful choices of policy parameterizations and even in that case the update rule may need to be changed to guarantee efficiency and effectiveness, as we have seen above. As an approach to algorithm design their main advantage is their generality and a strong support through various software libraries. Compared to vanilla “dynamic programming” methods they make generally smaller, more incremental changes to the policies, which seems useful. However, this is also achieved by methods like Politex, which is derived using a “bound minimization” approach. While this may seem more ad hoc than following gradients, in fact, one may argue that following gradients is more ad hoc as it fails to guarantee good performance. However, perhaps the most important point here is that one should not care too much about how a method is derived, or what “interpretation” it may have (is Politex a gradient algorithm? does this matter?). What matters is the outcome: In this case how the methods perform. It is thus wise to learn about all possible ways of designing algorithms, especially since there is much room for improving the performance of current algorithms. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec16/#summary",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec16/#summary"
  },"235": {
    "doc": "16. Policy gradients",
    "title": "Notes",
    "content": "Philip Thomas (2014, see citation below) takes a careful look at the claims surrounding natural gradient descent. One claim that is often heard is that natural gradient descent will speed up convergence. This is usually back up by giving a demonstration (e.g., Kakade, 2002, or Amari, 1998). However, it is far from clear whether this speedup will necessarily happen. As it turns out, this is far from being true. In fact, natural policy gradient can cause divergence even where following the normal gradient is guaranteed to converge to a global optimum. An example of this is given in Section 6.5 of the paper of Thomas (2014). ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec16/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec16/#notes"
  },"236": {
    "doc": "16. Policy gradients",
    "title": "References",
    "content": ". | Amari, S. Natural gradient works efficiently in learning.Neural Computation, 10:251–276, 1998. | Kakade, S. A natural policy gradient. In Advances in Neural Information Processing Systems, volume 14, pp.1531–1538, 2002. | Bagnell, J. A. and Schneider, J. Covariant policy search. In Proceedings of the International Joint Conference on Artificial Intelligence, pp. 1019–1024, 2003. | Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (1999). Policy gradient methods for reinforce-ment learning with function approximation. In Neural Information Processing Systems 12, pages 1057–1063. | Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. “Deterministic Policy Gradient Algorithms.” In ICML. http://hal.inria.fr/hal-00938992/. | Bhandari, Jalaj, and Daniel Russo. 2019. “Global Optimality Guarantees For Policy Gradient Methods,” June. https://arxiv.org/abs/1906.01786v1. | Agarwal, Alekh, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. 2019. “On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1908.00261. | Mei, Jincheng, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. 2020. “On the Global Convergence Rates of Softmax Policy Gradient Methods.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2005.06392. | Zhang, Junyu, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. 2020. “Variational Policy Gradient Method for Reinforcement Learning with General Utilities.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2007.02151. | Bhandari, Jalaj, and Daniel Russo. 2020. “A Note on the Linear Convergence of Policy Gradient Methods.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2007.11120. | Chung, Wesley, Valentin Thomas, Marlos C. Machado, and Nicolas Le Roux. 2020. “Beyond Variance Reduction: Understanding the True Impact of Baselines on Policy Optimization.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2008.13773. | Li, Gen, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. 2021. “Softmax Policy Gradient Methods Can Take Exponential Time to Converge.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2102.11270. | Thomas, Philip S. “GeNGA: A Generalization of Natural Gradient Ascent with Positive and Negative Convergence Results.” ICML 2014. http://proceedings.mlr.press/v32/thomasb14.pdf. | . The paper to read about natural gradient methods: . | Martens, James. 2014. “New Insights and Perspectives on the Natural Gradient Method,” December. https://arxiv.org/abs/1412.1193v9. Last update: September, 2020. | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec16/#references",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec16/#references"
  },"237": {
    "doc": "16. Policy gradients",
    "title": "16. Policy gradients",
    "content": "PDF Version . In this last lecture on planning, we look at policy search through the lens of applying gradient ascent. We start by proving the so-called policy gradient theorem which is then shown to give rise to an efficient way of constructing noisy, but unbiased gradient estimates in the presence of a simulator. We discuss at a high level the ideas underlying gradient ascent and stochastic gradient ascent methods (as opposed to more common case in machine learning where the goal is to minimize a loss, or objective function, we are maximizing rewards, hence ascending on the objective rather than descending). We then find out about the limitations of policy gradient even in the presence of “perfect representation” (unrestricted policy classes, tabular case) and perfect gradient information, which motivates the introduction of a variant known as “natural policy gradients” (NPG). We then uncover a close relationship between this method and Politex. The lecture concludes with comparing results for NPG and Politex. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec16/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec16/"
  },"238": {
    "doc": "2. The Fundamental Theorem",
    "title": "Introduction",
    "content": "A Markov decision Process (MDP) is a 5-tuple $M = (\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$, where $\\mathcal{S}$ represents the state space, $\\mathcal{A}$ represents the action space, $P = (P_a(s))_{s,a}$ collects the next state distributions for each state-action pair (to represent the transition dynamics), \\(r= (r_a(s))_{s,a}\\) gives the immediate rewards incurred for taking a given action in a given state, and $0 \\leq \\gamma &lt; 1$ is the discount factor. As said before, for simplicity, the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$ are finite. A policy \\(\\pi = (\\pi_t)_{t \\geq 0}\\) is an infinite long sequence where for each $t\\ge 0$, \\(\\pi_t: (\\mathcal{S} \\times \\mathcal{A})^{t-1} \\times \\mathcal{S} \\rightarrow \\mathcal{M}_1(\\mathcal{A})\\) assigns a probability distribution to histories of length $t$. (For $\\rho\\ge 0$ we use $\\mathcal{M}_\\rho(X)$ to denote the set of nonnegative measures $\\mu$ over $X$ that satisfy $\\mu(X)=\\rho$.) Following a policy in an MDP means that the distribution of the actions in each time step $t\\ge 0$ will follow what is prescribed by the policy for whatever the history is at that time. When a policy is used in an MDP, the interconnection of the policy and the MDP, together with a start-state distribution, results in a distribution $\\mathbb{P}$ such that for the infinite long sequence of state-action pairs $S_0,A_0,S_1,A_1,\\dots$, $S_0 \\sim \\mu(\\cdot), A_t \\sim \\pi_t(\\cdot | H_t)$, and $S_{t+1} \\sim P_{A_t}(S_t)$ for all $t \\geq 0$ where $H_t = (S_0,A_0,\\dots,S_{t-1},A_{t-1},S_t)$ is the history at time step $t$. This closed loop interaction (or interconnection) of the policy and the MDP is shown in the figure below. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec2/#introduction",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec2/#introduction"
  },"239": {
    "doc": "2. The Fundamental Theorem",
    "title": "Probabilities over Trajectories",
    "content": "One loose end from the previous lecture was the existence of the probability measures \\(\\mathbb{P}_\\mu^\\pi\\). For this, we have the following result: . Theorem (existence theorem): Fix a finite MDP $M$ with state space $\\mathcal{S}$ and action space $\\mathcal{A}$. Then there exists a measurable space $(\\Omega,\\mathcal{F})$ and a sequence of random elements $S_0, A_0, S_1, A_1, \\ldots$ over this space, $S_t\\in \\mathcal{S}$, $A_t\\in \\mathcal{A}$ for $t\\ge 0$, such that for any policy \\(\\pi = (\\pi_t)_{t \\geq 0}\\) of the MDP $M$ and any probability measure \\(\\mu \\in \\mathcal{M}_1(\\mathcal{S})\\) over $\\mathcal{S}$, there exists a probability measure $\\mathbb{P}(=\\mathbb{P}_{\\mu}^{\\pi})$ over $(\\Omega,\\mathcal{F})$ satisfying the following properties: . | $\\mathbb{P}(S_0 = s) = \\mu(s)$ for all $s \\in \\mathcal{S}$, | $\\mathbb{P}(A_t = a | H_t) = \\pi_t(a | H_t)$ for all $a \\in \\mathcal{A}, t \\geq 0$, and | $\\mathbb{P}(S_{t+1} = s’ | H_t, A_t) = P_{A_t}(S_t, s’)$ for all $s’ \\in \\mathcal{S}$. | . Furthermore, uniqueness holds in the following sense: if \\((\\tilde{\\Omega},\\tilde{\\mathcal{F}})\\) together with \\(\\tilde S_0,\\tilde A_0,\\tilde S_1,\\tilde A_1,\\dots\\) also satisfy the conditions of the definition with \\({\\tilde{\\mathbb{P}}}_{\\mu}^{\\pi}\\) denoting the associated probability measures for specific choices of $(\\pi,\\mu)$ then for any $\\pi$, $\\mu$, the joint distribution of $S_0,A_0,S_1,A_1,\\dots$ under \\(\\mathbb{P}_\\mu^{\\pi}\\) and that of \\(\\tilde S_0,\\tilde A_0,\\tilde S_1,\\tilde A_1,\\dots\\) under \\(\\tilde{\\mathbb{P}}_{\\mu}^{\\pi}\\) are identical. Proof: Use the Ionescu-Tulcea theorem (Theorem 3.3 in the “bandit book”, though the theorem statement there is weaker in that the uniqueness property is left out). \\(\\qquad\\blacksquare\\) . Property 3 above is known as the Markov property and is how MDPs derive their name. Note that implicit in the statement of this result is that $\\mathcal{S}$ and $\\mathcal{A}$ are endowed with the discrete $\\sigma$-algebra. This is because we want both ${S_t = s}$ and ${A_t = a}$ to be events for any $s\\in \\mathcal{S}$ and $a\\in \\mathcal{A}$ (these appear in the conditions underlying properties 1-3). Note that the result does not point to any singular measurable space. Indeed, there are many ways to choose $(\\Omega,\\mathcal{F})$. However, as long as we are only concerned with properties of the distributions of state-action trajectories, thanks to the uniqueness part of the theorem, no ambiguity will arise from this. As a result, in general, we will not care about the choice of $(\\Omega,\\mathcal{F})$: Any choice as given in the theorem will work. However, for some proofs, it will be convenient to choose $(\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}$, the set of infinite long trajectories as $\\Omega$, while setting $S_t((s_0,a_0,s_1,a_1,\\dots)) = s_t$, $A_t((s_0,a_0,s_1,a_1,\\dots)) = a_t$ ($t\\ge 0$) and choosing $\\mathcal{F}=(2^{\\mathcal{S}\\times\\mathcal{A}})^{\\otimes \\mathbb{N}}$, which the smallest $\\sigma$ algebra that makes $(S_t,A_t)$ measurable for any $t\\ge 0$. We will call the resulting probability space the canonical probability space underlying the MDP. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec2/#probabilities-over-trajectories",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec2/#probabilities-over-trajectories"
  },"240": {
    "doc": "2. The Fundamental Theorem",
    "title": "Optimality and Some Notation",
    "content": "As usual, we use $\\mathbb{E}$ to denote the expectation operator underlying a probability measure $\\mathbb{P}$. When the dependence on $\\mu$ or $\\pi$ is important, we use $\\mathbb{E}_\\mu^\\pi$. We may drop any of these, when the dropped quantity is clear from the context. We will pay special attention to start state distributions concentrated on a single state. When this is state $s$, the distribution is denoted by $\\delta_s$: this is the well-known Dirac distribution with an atom at $s$. The reason we pay special attention to these is because these in a way form the basis of all start state distributions (and in fact quantities that depend linearly on start state distributions). We will use the shorthand \\(\\mathbb{P}_{s}^{\\pi}\\) for \\(\\mathbb{P}_{\\delta_s}^{\\pi}\\). Similarly, we use \\(\\mathbb{E}_{s}^{\\pi}\\) for \\(\\mathbb{E}_{\\delta_s}^{\\pi}\\). Define the return over a trajectory $\\tau = (S_0, A_0, S_1, A_1, \\ldots)$ as . \\[R = \\sum_{t=0}^{\\infty} \\gamma^t r_{A_t}(S_t).\\] The value function $v^\\pi$ of policy $\\pi$ maps states to values and in particular for a state $s\\in\\mathcal{S}$, $v^\\pi(s)$ is defined via $v^\\pi(s) = \\mathbb{E}_{s}^{\\pi}[R]$: This is the expected return under the distribution induced by the interconnection of policy $\\pi$ and the MDP when the start state is $s$. Note that $v^\\pi(s)$ is well-defined. This is because it is the expectation of a quantity that is a function of the trajectory $\\tau$; for an explanation see the end-notes. The standard goal in an MDP is to identify a policy that maximizes this value in every state. A policy achieving this is known as an optimal policy. Whether an optimal policy exists at all is not clear at this stage. In any case, if it exist, an optimal policy must satisfy \\(v^\\pi = v^*\\) where $v^*:\\mathcal{S} \\to \\mathbb{R}$ is defined by . \\[v^{*}(s) = \\sup_{\\pi} v^{\\pi}(s)\\,, \\qquad s\\in \\mathcal{S}\\,.\\] By the definition of the optimal value function, we have \\(v^\\pi(s) \\leq v^{*}(s)\\) for all $s \\in \\mathcal{S}$ and any policy $\\pi$. We also use $v^\\pi \\le v^*$ to express this. In general, $f \\le g$ for two functions $f,g$ that are defined over the same domain and take values (say) in the reals, if $f(z)\\le g(z)$ holds for all the possible elements $z$ of their common domain. We similarly define $f\\ge g$. We will also identify functions with vectors and allow vector-space operations on them. All vectors, unless otherwise stated, are column vectors. The symbol $\\boldsymbol{1}$ is defined as a vector of ones. The length of this vector can change depending on the context. In this lecture, it will be $\\mathrm{S}$-dimensional. This symbol will be very useful in a number of calculations. We start with a definition that uses it. Approximately optimal policies . Let $\\varepsilon&gt;0$. A policy $\\pi$ is said to be $\\varepsilon$-optimal if . \\[v^\\pi \\ge v^* - \\varepsilon \\boldsymbol{1}\\,.\\] Finding an $\\varepsilon$-optimal policy with a positive $\\varepsilon$ should intuitively be easier than finding an optimal policy. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec2/#optimality-and-some-notation",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec2/#optimality-and-some-notation"
  },"241": {
    "doc": "2. The Fundamental Theorem",
    "title": "Memoryless Policies (ML)",
    "content": "If optimal policies would need to remember the past of arbitrary length, it would be hopeless to search for efficient algorithms that can compute them as even describing them could take infinite time. Luckily, this is not the case. In finite MDPs, it will turn out to be sufficient to consider policies that use only the most recent state without losing optimality: this is the subject of the fundamental theorem of MDPs, which we will give shortly. We call the policies that take only the most recent state into account memoryless. Formally, a memoryless policy can be identified with a map from the states to probability distributions over the actions: \\(m: \\mathcal{S}\\to \\mathcal{M}_1(\\mathcal{A})\\). Given $m$, the memoryless policy, using our previous policy notation, is $\\pi_t(a|s_0,a_0,\\dots,s_{t-1},a_{t-1},s_t) = m(a|s_t)$, where we abuse notation by using $m(a|s_t)$ in place of $m(s_t)(a)$. Thus, as expected, the policy itself “forgets” the past and just uses the most recent state in assigning probabilities to the individual actions. Under a distribution induced by interconnecting a memoryless policy with an MDP, the sequence of state-action pairs forms a Markov chain. In what follows, by abusing notation further, when it comes to a memoryless policy, we will identify $\\pi$ with $m$ and will just write $\\pi: \\mathcal{S} \\to \\mathcal{M}_1(\\mathcal{A})$. For building up to the proof of the fundamental theorem, we start with the concept of discounted occupancy measures. (Discounted) Occupancy Measure . Given a start state distribution \\(\\mu \\in \\mathcal{M}_1(\\mathcal{S})\\) and a policy \\(\\pi\\), the (discounted) occupancy measure \\(\\nu_\\mu^\\pi \\in \\mathcal{M}_{1/(1-\\gamma)}(\\mathcal{S} \\times \\mathcal{A})\\) induced by \\(\\mu\\) and \\(\\pi\\) and the underlying MDP \\(M\\) is defined as . \\[\\nu_\\mu^\\pi(s, a) = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{P}_\\mu^\\pi (S_t = s, A_t = a).\\] Interestingly, the value function can be represented as an inner product between the immediate reward function $r$ and the occupancy measure $\\nu_\\mu^\\pi$: . \\[\\begin{align*} v^\\pi(\\mu) &amp;= \\mathbb{E}_\\mu^\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r_{A_t}(S_t) \\right] \\\\ &amp;= \\sum_{s, a} \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_\\mu^\\pi \\left[ r_{A_t}(S_t) \\mathbb{I}(S_t = s, A_t = a) \\right] \\\\ &amp;= \\sum_{s, a} r_{a}(s) \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_\\mu^\\pi \\left[ \\mathbb{I}(S_t = s, A_t = a) \\right] \\\\ &amp;= \\sum_{s, a} r_{a}(s) \\sum_{t=0}^\\infty \\gamma^t \\mathbb{P}_\\mu^\\pi(S_t = s, A_t = a) \\\\ &amp;= \\sum_{s, a} r_a(s) \\nu_\\mu^\\pi(s, a) \\\\ &amp;=: \\langle \\nu_\\mu^\\pi, r \\rangle, \\end{align*}\\] where $\\mathbb{I}(S_t = s, A_t = a)$ is the indicator of the event ${S_t=s,A_t=a}$, which gives the value of one when the event holds (i.e., $S_t = s$ and $A_t = a$), and gives zero otherwise. That the summation over $(s,a)$ can be moved outside of the expectation in the first equality follows because expectations are linear. That the infinite sum can be moved outside is more subtle: this follows from Lebesgue’s dominated convergence theorem. See, for example, Chapter 2 of Lattimore &amp; Szepesvári (2020). With the above equation, we see that the problem of maximizing the expected reward for a given initial distribution is the same as choosing a policy that “stirs” the occupancy measure to maximally align with the reward vector $r$. A better alignment will result in a higher value for the policy. This is depicted in the figure below. A key step in proving the sufficiency of memoryless policies for optimal control is the following result: . Theorem: For any policy $\\pi$ and a start state distribution $\\mu \\in \\mathcal{M}_1(\\mathcal{S})$, there exists a memoryless policy $\\pi’$ such that . \\[\\nu_\\mu^{\\pi'} = \\nu_\\mu^{\\pi}.\\] . Proof (hint): First define the occupancy measure over the state space \\(\\tilde{\\nu}_\\mu^\\pi(s) := \\sum_a \\nu_\\mu^\\pi(s, a)\\). Then show that the theorem statement holds for the policy $\\pi’$ defined as follows: . \\[\\pi'(a | s) = \\begin{cases} \\frac{\\nu_\\mu^\\pi(s, a)}{\\tilde{\\nu}_\\mu^\\pi(s)} &amp; \\text{if } \\tilde{\\nu}_\\mu^\\pi(s) \\neq 0 \\\\ \\pi_0(a) &amp; \\text{otherwise,} \\end{cases}\\] where \\(\\pi_0(a) \\in \\mathcal{M}_1(\\mathcal{A})\\) is an arbitrary distribution. To do this, expand $\\tilde \\nu_\\mu^\\pi$ using the definition of discounted occupancy measures and use algebra. \\[\\tag*{$\\blacksquare$}\\] Note that it is crucial that the memoryless policy obtained depends on the start state distribution: The reader should try to convince themselves that there are non-memoryless policies whose value function cannot be reproduced by a memoryless policy at every state. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec2/#memoryless-policies-ml",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec2/#memoryless-policies-ml"
  },"242": {
    "doc": "2. The Fundamental Theorem",
    "title": "Bellman Operators, Contractions",
    "content": "The last definitions and results that we need before stating the fundamental theorem concern what are known as Bellman operators. Fix a memoryless policy $\\pi$. Recall that $\\mathrm{S}$ is the cardinality (size) of $\\mathcal{S}$. First, define $r_\\pi(s) = \\sum_a \\pi(a|s) r_a(s)$ to be the expected reward under policy $\\pi$ for a given state $s$. Again, we overload the notation and let $r_\\pi \\in \\mathbb{R}^{\\mathrm{S}}$ denote a vector whose $s$th element \\((r_\\pi)_s = r_\\pi(s)\\). Similarly, we define $P_\\pi(s, s’) := \\sum_a \\pi(a|s) P_a(s, s’)$ and let $P_\\pi \\in [0, 1]^{\\mathrm{S} \\times \\mathrm{S}}$ denote the stochastic transition matrix where the element in the \\(s\\)th row and \\(s'\\)th column \\((P_\\pi)_{s, s'} = P_\\pi(s, s')\\). Note that each row of $P_\\pi$ sums to one: . \\[P_\\pi \\mathbf{1} = \\mathbf{1}\\,.\\] The Bellman/policy evaluation operator underlying $\\pi$, $T_\\pi: \\mathbb{R}^{\\mathrm{S}} \\rightarrow \\mathbb{R}^{\\mathrm{S}}$, is defined as . \\[\\begin{align*} T_\\pi v(s) &amp;= \\sum_a \\pi(a|s) \\left \\{r_a(s) + \\gamma \\sum_{s'} P_a(s, s') v(s') \\right \\} \\\\ &amp;= \\sum_a \\pi(a|s) \\left \\{r_a(s) + \\gamma \\langle P_a(s), v \\rangle \\right \\} \\end{align*}\\] or, in short, . \\[T_\\pi v = r_\\pi + \\gamma P_\\pi v,\\] where $v \\in \\mathbb{R}^{\\mathrm{S}}$. The Bellman operator performs a one-step lookahead (also called a Bellman lookahead) on the value function. We will use the notations $(T_\\pi(v))(s)$, $T_\\pi v(s)$, and \\((T_\\pi v)_s\\) interchangeably. $T_\\pi$ is also known as the policy evaluation operator for the policy $\\pi$. The Bellman optimality operator $T: \\mathbb{R}^{\\mathrm{S}} \\rightarrow \\mathbb{R}^{\\mathrm{S}}$ is defined as . \\[T v(s) = \\max_a \\{ r_a(s) + \\gamma \\langle P_a(s), v \\rangle \\}.\\] We use \\(\\|\\cdot\\|_\\infty\\) to denote the maximum-norm: \\(\\| v \\|_{\\infty} = \\max_i |v_i|\\). The maximum-norm is a “good friend” of the operators we just defined. This is because stochastic matrices, viewed as operators and “maximizing” are “good friends” of this norm. All this results in the following proposition: . Proposition ($\\gamma$-contraction of the Bellman Operators): Given any two vectors $u, v \\in \\mathbb{R}^{\\mathrm{S}}$ and any memoryless policy \\(\\pi\\), . | \\(\\|T_\\pi u - T_\\pi v\\|_\\infty \\leq \\gamma \\|u - v\\|_\\infty\\), and | \\(\\|T u - T v\\|_\\infty \\leq \\gamma \\|u - v\\|_\\infty\\). | . The proposition can be proved by elementary algebra and the complete proof can be found in Appendix A.2 of Szepesvári (2010). For action $a\\in \\mathcal{A}$, we will find it useful to also define the operator $T_a: \\mathbb{R}^{\\mathrm{S}} \\to \\mathbb{R}^{\\mathrm{S}}$ which matches $T_\\pi$ with the memoryless policy which in every state chooses action $a$. Of course, this operator, being a special case, satisfies the above contraction property as well. This can be seen as performing a one-step lookahead with a fixed action. From Banach’s fixed point theorem, we get the following corollary: . Proposition (Fixed-point iteration): Given any $u \\in \\mathbb{R}^{\\mathrm{S}}$ and any memoryless policy $\\pi$, . | \\(v^\\pi = \\lim_{k\\to\\infty} T_\\pi^k u\\) and in particular for any $k\\ge 0$, \\(\\| v^\\pi - T_\\pi^k u \\|_\\infty \\le \\gamma^k \\| u - v^\\pi \\|_\\infty\\) where \\(v^\\pi\\) is the unique vector/function that satisfies \\(T_\\pi v^\\pi = v^\\pi\\); | \\(v_\\infty=\\lim_{k\\to\\infty} T^k u\\) is well-defined and in particular for any $k\\ge 0$, \\(\\| v_\\infty - T^k u \\|_\\infty \\le \\gamma^k \\| u - v_\\infty \\|_\\infty\\). Furthermore, \\(v_\\infty\\) is the unique vector/function that satisfies \\(Tv_\\infty = v_\\infty\\). | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec2/#bellman-operators-contractions",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec2/#bellman-operators-contractions"
  },"243": {
    "doc": "2. The Fundamental Theorem",
    "title": "The Fundamental Theorem",
    "content": "Definition: A memoryless policy $\\pi$ is greedy w.r.t. to a value function $v: \\mathcal{S} \\rightarrow \\mathbb{R}$ if in every state $s\\in \\mathcal{S}$, with probability one $\\pi$ chooses actions that maximize $(T_a v)(s)=r_a(s) + \\gamma \\langle P_a(s), v \\rangle$. Note that there can be more than one action that maximizes the (one-step) Bellman lookahead $(T_a v)(s)$ at any given state (in case there are ties). In fact, ties can be extremely common: Just imagine “duplicating an action” in every state (i.e., the new action has the same associated transitions and rewards as the copied one). If the copied one was maximizing the Bellman lookahead at some state, the new action will do the same. Because we have finitely many actions, a maximizing action always exist. Thus, we can always “take” a greedy policy w.r.t. any $v\\in \\mathbb{R}^{\\mathrm{S}}$. Proposition (Characterizing greedyness): A memoryless policy $\\pi$ is greedy w.r.t. $v\\in \\mathbb{R}^{\\mathrm{S}}$ if and only if . \\[T_\\pi v = T v\\,.\\] . With this, we are ready to state what I call the Fundamental Theorem of MDPs: . Theorem (Fundamental Theorem of MDPs): The following hold true in any finite MDP: . | Any policy $\\pi$ that is greedy with respect to $v^*$ is optimal: \\(v^\\pi = v^*\\); | It holds that \\(v^* = T v^*\\). | . The equation $v=Tv$ is known as the Bellman optimality equation and the second part of the result can be stated in words by saying that the optimal value function satisfies the Bellman optimality equation. Also, our previous proposition on fixed-point iteration, where we already came across the Bellman optimality equation, foreshadows a way of approximately computing \\(v^*\\) that we will get back to after the proof. Proof: The proof would be easy if we only considered memoryless policies when defining $v^*$. In particular, letting $\\text{ML}$ stand for the set of memoryless policies of the given MDP, define . \\[\\tilde{v}^*(s) = \\sup_{\\pi \\in \\text{ML}} v^\\pi(s) \\quad \\text{for all } s \\in \\mathcal{S}\\,.\\] As we shall see soon, it is not hard to show the theorem just with \\(v^*\\) replaced everywhere with \\(\\tilde{v}^*\\). That is: . | Any policy $\\pi$ that is greedy with respect to $\\tilde{v}^*$ satisfies \\(v^\\pi = \\tilde{v}^*\\); | It holds that \\(\\tilde{v}^* = T \\tilde{v}^*\\). | . This is what we will show in Part 1 of the proof, while in Part 2 we will show that \\(\\tilde{v}^*=v^*\\). Clearly, the two parts together establish the desired result. Part 1: The idea of the proof is to first show that \\(\\begin{align} \\tilde{v}^*\\le T \\tilde{v}^* \\label{eq:suph} \\end{align}\\) and then show that for any greedy policy $\\pi$, \\(v^\\pi \\ge \\tilde{v}^*\\). The displayed equation follows by noticing that \\(v^\\pi \\le \\tilde{v}^*\\) holds for all memoryless policies $\\pi$ by definition. Applying $T_\\pi$ on both sides, using $v^\\pi = T_\\pi v^\\pi$, we get $v^\\pi \\le T_\\pi \\tilde{v}^*$. Taking the supremum of both sides over $\\pi$ and noticing that $T v = \\sup_{\\pi \\in \\text{ML}} T_\\pi v$ for any $v$, together with the definition of \\(\\tilde{v}^*\\) gives \\(\\eqref{eq:suph}\\). Now, take any memoryless policy $\\pi$ that is greedy w.r.t. \\(\\tilde{v}^*\\). Thus, \\(T_\\pi \\tilde{v}^* = T \\tilde{v}^*\\). Combined with \\(\\eqref{eq:suph}\\), we get . \\[\\begin{align} \\label{eq:start} T_\\pi \\tilde{v}^* \\ge \\tilde{v}^*\\,. \\end{align}\\] Applying $T_\\pi$ on both sides and noticing that $T_\\pi$ keeps the inequality intact (i.e., for any $u,v$ such that $u\\le v$ we get $T_\\pi u \\le T_\\pi v$), we get . \\[T_\\pi^2 \\tilde{v}^* \\ge T_\\pi \\tilde{v}^* \\ge \\tilde{v}^*\\,,\\] where the last inequality follows from \\(\\eqref{eq:start}\\). With the same reasoning we get that for any $k\\ge 0$, . \\[T_\\pi^k \\tilde{v}^* \\ge T_\\pi^{k-1} \\tilde{v}^* \\ge \\dots \\ge \\tilde{v}^*\\,,\\] Now, by our proposition, the fixed-point iteration $T_\\pi^k \\tilde{v}^*$ converges to $v^\\pi$. Hence, taking the limit above, we get . \\[v^\\pi \\ge \\tilde{v}^*.\\] This, together with \\(v^\\pi \\le \\tilde{v}^*\\) shows that \\(v^\\pi = \\tilde{v}^*\\). Finally, $T \\tilde{v}^* = T_\\pi \\tilde{v}^* = T_\\pi v^\\pi = v^\\pi = \\tilde{v}^*$. Part 2: It remains to be shown that \\(\\tilde{v}^* = v^*\\). Let $\\Pi$ be the set of all policies. Because $\\text{ML}\\subset \\Pi$, \\(\\tilde{v}^*\\le v^*\\). Thus, it remains to show that . \\[\\begin{align} \\label{eq:mlbigger} v^* \\le \\tilde{v}^*\\,. \\end{align}\\] To show this, we will use the theorem that guaranteed that for any state-distribution $\\mu$ and policy $\\pi$ (memoryless or not) we can find a memoryless policy, which we will call for now $\\text{ML}(\\pi)$, such that $\\nu_\\mu^\\pi = \\nu_\\mu^{\\text{ML}}$. Fix a state $s\\in \\mathcal{S}$. Applying this result with $\\mu = \\delta_s$, we get . \\[\\begin{align*} v^\\pi(s) &amp; = \\langle \\nu_s^\\pi, r \\rangle \\\\ &amp; = \\langle \\nu_s^{\\text{ML}(\\pi)}, r \\rangle \\\\ &amp; \\le \\sup_{\\pi'\\in \\text{ML}} \\langle \\nu_s^{\\pi'}, r \\rangle \\\\ &amp; = \\sup_{\\pi'\\in \\text{ML}} v^{\\pi'}(s) = \\tilde{v}^*(s)\\,. \\end{align*}\\] Taking the supremum of both sides over $\\pi$, we get \\(v^*(s)= \\sup_{\\pi\\in \\Pi} v^\\pi(s) \\le \\tilde{v}^*(s)\\). Since $s\\in \\mathcal{S}$ was arbitrary, we get \\(v^*\\le \\tilde{v}^*\\), finishing the proof. \\(\\qquad\\blacksquare\\) . A property that came up during the proof that we will repeatedly use is that $T_\\pi$ is monotone as an operator. The same holds for $T$. For the record, we state these as a proposition: . Proposition (monotonicity of Bellman operators): For any memoryless policy $\\pi$, $T_\\pi u \\le T_\\pi v$ holds for any $u,v\\in \\mathbb{R}^{\\mathrm{S}}$ such that $u\\le v$. The same also holds for $T$, the Bellman optimality operator. According to the Fundamental Theorem of MDPs, if we have access to the optimal value function \\(v^*\\), then we can find the optimal policy in an efficient and effective way. We just have to greedify it w.r.t. to the value function: (abusing the policy notation) \\(\\pi(s) = \\arg\\max_{a \\in \\mathcal{A}} \\{r_a(s) + \\gamma \\langle P_a(s), v^* \\rangle \\} \\quad \\forall s \\in \\mathcal{S}\\). Such a greedy policy can be found in $O(\\mathrm{S}^2 \\mathrm{A})$ time. Hence, if we can efficiently find the optimal value function, we will get an efficient way of computing an optimal policy. This is to be contrasted with the naive approach to finding an optimal policy, which is to enlist all the policies and compare their value functions to find a policy whose value function dominates the value functions of all the other policies. However, even if we restrict ourselves to just the set of deterministic policies, there are $\\Theta(\\mathrm{A}^{\\mathrm{S}})$ such policies and thus this can be a costly procedure. As it turns out, for finite MDPs, there is a way to calculate optimal policies in time that is polynomial in $\\mathrm{S}$, $\\mathrm{A}$, and $1/(1-\\gamma)$, avoiding the exponential growth of the naive approach with the size of the state space. Algorithms that can do this belong to the family of dynamic programming algorithms. For our purposes, we call any algorithm a dynamic programming algorithm that uses the idea of keeping track of value of states (that is, uses value functions) while doing its calculations. The Fundamental Theorem is somewhat surprising: how come that we can find policies whose value function dominates that of all other policies? In a way, the Fundamental Theorem tells us that the set of value functions of all policies in some MDP (as a set in $\\mathbb{R}^{\\mathrm{S}}$) is very special: It has a “vertex” which dominates all the other value functions. This is quite fascinating. Of course, the key was the Markov property as this gave us the tool to show the result that allowed us to switch from arbitrary policies to memoryless ones. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec2/#the-fundamental-theorem",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec2/#the-fundamental-theorem"
  },"244": {
    "doc": "2. The Fundamental Theorem",
    "title": "Value Iteration",
    "content": "By the Fundamental Theorem, \\(v^*\\) is the fixed point of $T$. By our earlier proposition, which built on the Banach’s fixed point theorem, the sequence \\(\\{T^k v\\}_{k\\ge 0}\\) converges to \\(v^*\\) at a geometric rate. In the context of MDPs, the process of repeatedly applying $T$ to some function is called value iteration. The initial function is usually taken to be the all-zero function, which we denote by $\\mathbf{0}$, but, of course, if there is a better initial guess on \\(v^*\\), that guess can also be used at initialization. The next result gives a bound on the number of iterations required to reach an $\\varepsilon$-neighborhood (in the max-norm sense) of $v^*$: . Theorem (Value Iteration): Consider an MDP with immediate rewards in the $[0,1]$ interval. Pick an arbitrary positive number $\\varepsilon&gt;0$. Let $v_0 = \\boldsymbol{0}$ and set . \\[v_{k+1} = T v_k \\quad \\text{for } k = 0, 1, 2, \\ldots\\] Then, for $k\\ge \\ln(1/(\\varepsilon(1-\\gamma))/\\ln(1/\\gamma)$, \\(\\|v_k -v^*\\|_\\infty \\le \\varepsilon\\). Before the proof recall that . \\[H_{\\gamma,\\varepsilon}:= \\frac{\\ln(1/(\\varepsilon(1-\\gamma)))}{1-\\gamma} \\ge \\frac{\\ln(1/(\\varepsilon(1-\\gamma)))}{\\ln(1/\\gamma)}\\,.\\] Thus, the effective horizon, $H_{\\gamma,\\varepsilon}$, whom we met in the first lecture, appeared again. Of course, this is no coincidence. Proof: By our assumptions on the rewards, $\\mathbf{0} \\le v^\\pi \\le \\frac{1}{1-\\gamma} \\mathbf{1}$ holds for any policy $\\pi$. Hence, \\(\\|v^*\\|_\\infty \\le \\frac{1}{1-\\gamma}\\) also holds. By our fixed-point iteration proposition, we get . \\[\\begin{align*} \\|v_k - v^*\\|_\\infty &amp;\\leq \\gamma^k \\|v^* - \\mathbf{0}\\|_\\infty = \\gamma^k \\|v^*\\|_\\infty \\leq \\frac{\\gamma^k}{1 - \\gamma} \\,. \\end{align*}\\] Solving for the smallest $k$ such that \\(\\gamma^k/(1-\\gamma)\\le \\varepsilon\\) gives the result. \\[\\tag*{$\\blacksquare$}\\] For fixed $\\gamma&lt;1$, note the mild dependence of the iteration complexity on the target accuracy $\\varepsilon$: we can expect with only a handful iterations to get in a small vicinity of $v^*$. Note also that the total computation cost is $O(\\mathrm{S}^2 \\mathrm{A}k)$ and the space required is at most $O(\\mathrm{S})$, all assuming each value takes up $O(1)$ memory and arithmetic and logic operations also require $O(1)$ time. Note that accuracy requirement was set up in the form of additive errors. If the value function \\(v^*\\) is of order $1/(1-\\gamma)$ (the maximum possible order), a relative accuracy of order $2$ means setting $\\epsilon=0.5/(1-\\gamma)$, making the iteration complexity to be $\\ln(2)/(1-\\gamma)$. However, for controlling the relative error, the more interesting case is when \\(v^*\\) takes on small values. Here, we see that the complexity may grow unbounded. Later, we will see that in a way this lack of fine-grained error control of value iteration will mean that value iteration is not ideal for calculating exactly optimal policies. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec2/#value-iteration",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec2/#value-iteration"
  },"245": {
    "doc": "2. The Fundamental Theorem",
    "title": "Notes",
    "content": "Value functions are well-defined . As noted in the text, value functions are well-defined despite that the probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ is not uniquely defined. In fact, for any $f: (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}} \\to \\mathbb{R}$ (measurable) function and for any $(\\Omega,\\mathcal{F},\\mathbb{P})$ and $(\\Omega’,\\mathcal{F}’,\\mathbb{P}’)$ probability spaces, as long as both $\\mathbb{P}$ and $\\mathbb{P}’$ satisfy the requirements postulated in the existence theorem, \\(\\int f(\\tau(\\omega)) \\mathbb{P}(d\\omega)=\\int f(\\tau(\\omega)) \\mathbb{P}'(d\\omega)\\), or, introducing $\\mathbb{E}$ ($\\mathbb{E}’$) to denote the expectation operator underlying $\\mathbb{P}$ (respectively, $\\mathbb{P}’$), \\(\\mathbb{E}[f(\\tau)]=\\mathbb{E}'[f(\\tau)]\\). It also follows that if we only need probabilities and expectations over trajectories, it suffices to choose $(\\Omega,\\mathcal{F},\\mathbb{P})$ as the canonical probability space induced by the state-action space of the MDP at hand. Other types of MDPs . The obvious question is what survives of all this in other types of MDPs, such as finite-horizon homogenous or inhomogeneous, with or without discounting, total cost (i.e. negative rewards only), or of course the average cost setting? The story is that the arguments can be usually made to work, but this is not entirely automatic. The subject is well-studied and we will give some references and hints later, perhaps even answer some of these questions. Infinite spaces anyone? . The first thing that changes when we switch to infinite spaces is that we cannot take the assumption that the immediate rewards are bounded for granted. This can cause quite a bit of trouble: $v^\\pi$ for some policies can be unbounded, and the same holds for $v^*$. Negative infinite values could be especially “hurtful”. (LQR control is the simplest example where this comes up.) . Another issue is that we cannot take the existence of greedy policies for granted. This happens already when the number of actions is infinite (what is the action that maximizes the reward $r_a(s)=1-1/a$ where $a&gt;0$?). Oftentimes compactness of the action space and continuity assumptions help with this, though, as much of what we will do will be approximate, approximate greedification should be sufficient for most of the time. From this perspective, that greedy actions may not exist is just annoyance. Finally, when either the state or action space is uncountably infinite, one has to be careful even with the definition of policies. Using a technical term from probability theory, a choice that makes thing work is to restrict policies to be probability kernels. Using this definition means that we need to put measurability structures over both the state and action spaces (this is only crucial when either respective set has a larger than countable cardinality). The main change here is that with policies defined this way, for any $U$ measurable subset of $\\mathcal{A}$, $h_t \\mapsto \\pi_t(U|h_t)$ must be measurable. This allows us then the use of the Ionescu-Tulcea theorem and at least the definitions can be made to work. The next difficulty in this case is that “greedification” may lead to outside of the set of these “measurable policies”, which could prevent the existence of optimal policies (again, if we are contend with approximate optimality, this difficulty disappears). There is a large literature concerned with these issues. From infinite trajectories to their finite prefixes . Since trajectories are allowed to be infinitely long, we have a nonconstructive result only for the existence of the probability measures induced by the interconnection of policies and MDPs. Oftentimes we need to check whether two probability measures over these infinitely long trajectories coincide. How can this be done? A general result from measure theory says that two measures agree, if they agree of a generator of the underlying $\\sigma$-algebra. A convenient generator system for the $\\sigma$-algebra over the trajectories (for the canonical probability space) is the system whose elements take the form . \\[\\{s_0\\} \\times \\{a_0\\} \\times \\dots\\times \\{ s_t \\} \\times \\mathcal{A} \\times (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}\\] and . \\[\\{s_0\\} \\times \\{a_0\\} \\times \\dots\\times \\{ s_t \\} \\times \\{a_t\\} \\times (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}\\] for some $s_0,a_0,\\dots,s_t,a_t,\\dots$. That is, if $\\mathbb{P}$ and $\\mathbb{P}’$ agree on the probabilities assigned to these sets, they agree everywehere. This makes things a full circle: what this result says is that we only need to check the probabilities assigned to finite prefixes of the infinitely long trajectories. Phew. Since the probabilities assigned to these finite prefixes are a function of $\\mu$, $P$ and $\\pi$ alone, it follows that there is a unique probability measure over the trajectory space $ (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}$ that satisfies the requirements postulated in the existence theorem. That is, the canonical probability space is uniquely defined. Optimization with (Discounted) Occupancy Measures . We learned that the value function can be represented as $v^\\pi(\\mu) = \\sum_{s, a} r_a(s) \\nu_\\mu^\\pi(s, a) = \\langle \\nu_\\mu^\\pi, r \\rangle$. Thus, maximizing the value function for a given initial distribution $\\mu$ is equivalent to maximizing the dot product between $\\nu_\\mu^\\pi$ and $r$. Next, we present a concrete example and point out some interesting results. To keep this example as simple as possible, we introduce some new notation. Let $\\mathcal{A}(s)$ represent the set of actions admissable to the state $s \\in \\mathcal{S}$. We now define the MDP. Let $\\mathcal{S} = \\{s_1, s_2 \\}$, $\\mathcal{A}(s_1) = \\{a_1, a_2 \\}$ and $\\mathcal{A}(s_2) = \\{a_3 \\}$. Also, let . \\[\\begin{align*} P_{a_1}(s_1, s_1) = 1, &amp;\\quad r_{a_1}(s_1) = 1 \\\\ P_{a_2}(s_1, s_2) = 1, &amp;\\quad r_{a_2}(s_1) = 1/2 \\\\ P_{a_3}(s_2, s_2) = 1, &amp;\\quad r_{a_3}(s_2) = 1/2. \\end{align*}\\] Our policy $\\pi$ can be parametrized by one parameter $p$ as . \\[\\begin{align*} \\pi(a_1|s_1) &amp;= p \\\\ \\pi(a_2|s_1) &amp;= 1 - p \\\\ \\pi(a_3|s_2) &amp;= 1. \\end{align*}\\] Finally, we assume $\\mu(s_1) = 1$. We explicitly write out $\\nu_\\mu^\\pi(s, a) = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{P}_\\mu^\\pi (S_t = s, A_t = a)$ for all state-action pairs. \\[\\begin{align*} \\nu_\\mu^\\pi(s_1, a_1) &amp;= \\sum_{t=0}^\\infty \\gamma^t p^{t+1} \\\\ &amp;= p\\sum_{t=0}^\\infty (\\gamma p)^t \\\\ &amp;= \\frac{p}{1-\\gamma p} \\\\ \\nu_\\mu^\\pi(s_1, a_2) &amp;= \\sum_{t=0}^\\infty \\gamma^t p^t (1-p) \\\\ &amp;= (1-p)\\sum_{t=0}^\\infty (\\gamma p)^t \\\\ &amp;= \\frac{1-p}{1-\\gamma p} \\\\ \\nu_\\mu^\\pi(s_2, a_3) &amp;= \\frac{1}{1-\\gamma} - \\frac{p}{1-\\gamma p} - \\frac{1-p}{1-\\gamma p} \\end{align*}\\] Recall, our goal is to maximize $\\sum_{s, a} r_a(s) \\nu_\\mu^\\pi(s, a)$. To do this we plug in the above quantities for $r_a(s)$ and $\\nu_\\mu^\\pi(s, a)$ . \\[\\begin{align*} \\sum_{s, a} r_a(s) \\nu_\\mu^\\pi(s, a) &amp;= \\frac{1-p}{1-\\gamma p} + \\frac{1}{2} \\left( \\frac{p}{1-\\gamma p} \\right) + \\frac{1}{2} \\left( \\frac{1}{1-\\gamma} - \\frac{p}{1-\\gamma p} - \\frac{1-p}{1-\\gamma p} \\right) \\\\ &amp;= \\frac{1}{2} \\left( \\frac{p}{1-\\gamma p} \\right) + \\frac{1}{2} \\left( \\frac{1}{1-\\gamma} \\right). \\end{align*}\\] Noting that the function on the right hand side is monotone increasing for $p \\in [0, 1]$, so we get that the above quantity is maximized for $p = 1$. Thus, the optimal policy is . \\[\\begin{align*} \\pi(a_1|s_1) &amp;= 1 \\\\ \\pi(a_2|s_1) &amp;= 0 \\\\ \\pi(a_3|s_2) &amp;= 1. \\end{align*}\\] Which, aligns with our intuition that action $a_1$ should always be selected in state $s_1$ since it produces larger reward. Notice how the set of occupancy measures . \\[\\left\\{ (t, (1-\\gamma t-t), 1/(1-\\gamma)-t-(1-\\gamma t-t)) : t \\in [0, 1/(1-\\gamma)] \\right\\}\\] is a convex set. This examples shows that optimizing in the space of occupancy measures could be a linear optimization while optimizing with a policy parametrization could be a non-linear optimization. Fundamental Theorem . I think I have seen Bertsekas and Shreve call the theorem I call fundamental also by the same name. However, this is not quite a standard name. Nevertheless, the result is important and many other things follow from it. In a way, this is the result that is at the heart of all the theory. I think it deserves this name. I have probably read the proof presented here somewhere, but this was a while ago and the source escapes me. In the RL literature people often start with memoryless policies and work with \\(\\tilde{v}^*\\) rather than with \\(v^*\\). The question whether \\(\\tilde{v}^*=v^*\\) is well-studied and understood, mostly in the control and operations research literature. The geometry of the space of value functions . An alternative way of seeing the fundamental theorem is as a result concerning the geometry of the space of value functions. Indeed, fix an MDP $M$ and let \\(\\mathcal{V} = \\{ v^\\pi \\,:\\, \\pi \\text{ is a policy of } M \\}\\), while let \\(\\mathcal{V}^{\\mathrm{DET}} = \\{ v^\\pi \\,:\\, \\pi \\text{ is a deterministic memoryless policy of } M \\}\\). The set $\\mathcal{V}$ is the set of all value functions of $M$. Both sets are subsets of $\\mathbb{R}^{\\mathcal{S}}$. Using terminology from multicriteria optimization, the optimal value function, \\(v^*\\), is the ideal point of $\\mathcal{V}$: \\(v^*(s) = \\sup \\{ v(s)\\,:\\, v\\in \\mathcal{V} \\}\\) for all $s\\in \\mathcal{S}$. Then, the fundamental theorem states that the ideal point of $\\mathcal{V}$ belongs to $\\mathcal{V}$: \\(v^* \\in \\mathcal{V}\\) and in fact \\(v^* \\in \\mathcal{V}^{\\mathrm{DET}}\\). However, more is known about $\\mathcal{V}$: . Theorem (existence theorem): Fix a finite MDP $M$. Then $\\mathcal{V} \\subset \\mathbb{R}^{\\mathcal{S}}$ is convex. Furthermore, any extreme point of $\\mathcal{V}$ belongs to $\\mathcal{V}^{\\mathrm{DET}}$. This result is due to Dadashi et al. (2019). Banach’s fixed point theorem . This theorem can be found in Appendix A.1 of my short RL book (Szepesvári, 2010). However, of course, it can be found in many places (the Wikipedia article is also OK). It is worthwhile to spend some time with this theorem to understand its conditions, going back to concepts like Cauchy-sequences (which should perhaps be called sequences with vanishing oscillations) and completeness of the set of real numbers. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec2/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec2/#notes"
  },"246": {
    "doc": "2. The Fundamental Theorem",
    "title": "References",
    "content": "The references mentioned before: . | Lattimore, T., &amp; Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press. | Szepesvári, C. (2010). Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence and machine learning, 4(1), 1-103. | . The next work (a book chpater) gives a concise yet relatively thorough introduction. The chapter also gives a proof of the fundamental theorem; through the sufficiency of Markov policies. This is done for the discounted and also for a number of alternate criteria. | Garcia, Frédérick, and Emmanuel Rachelson. 2013. “Markov Decision Processes.” In Markov Decision Processes in Artificial Intelligence, 1–38. Hoboken, NJ USA: John Wiley &amp; Sons, Inc. | . A summary of basic results for countable and Borel state-space, and Borel action spaces, with potentially unbounded (from below) reward functions can be found in the next (excellent) paper, which also gives a concise overview of the history of these results: . | Feinberg, Eugene A. 2011. Total Expected Discounted Reward MDPS: Existence of Optimal Policies. In Wiley Encyclopedia of Operations Research and Management Science. Hoboken, NJ, USA: John Wiley &amp; Sons, Inc. | . An argument showing the fundamental theorem for the finite-horizon case derived from a general result of David Blackwell can be found in a blog-post of Maxim Raginsky, who gives further pointers, most notable this. David Blackwell has contributed in numerous ways to the foundations of statistics, decision theory, probability theory, and many many other subjects and the importance of his work cannot be overstated. | Robert Dadashi, Adrien Ali Taïga, Nicolas Le Roux, Dale Schuurmans, Marc G. Bellemare. 2019. The Value Function Polytope in Reinforcement Learning. ICML. arXiv | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec2/#references",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec2/#references"
  },"247": {
    "doc": "2. The Fundamental Theorem",
    "title": "2. The Fundamental Theorem",
    "content": "PDF Version . We start by recapping the definition of MDPs and then firm up the loose ends from the previous lecture: why do the probability distributions \\(\\mathbb{P}_\\mu^\\pi\\) exist and how are they defined? We then continue with the introduction of what we call the Fundamental Theorem of Dynamic Programming and end with the discussion of value iteration. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec2/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec2/"
  },"248": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "Finding a Near-Optimal Policy using Value Iteration",
    "content": "In the previous lecture we found that the iterative computation that starts with some $v_0\\in \\mathbb{R}^{\\mathrm{S}}$ and then obtains $v_{k+1}$ using the “Bellman update” . \\[\\begin{align} v_{k+1} = T v_k \\label{eq:vi} \\end{align}\\] leads to a sequence \\(\\{v_k\\}_{k\\ge 0}\\) whose \\(k\\)th term approaches \\(v^*\\), the optimal value function, at a geometric rate: . \\[\\begin{align} \\| v_k - v^* \\|_\\infty \\le \\gamma^k \\| v_0 - v^* \\|_\\infty\\,. \\label{eq:vierror} \\end{align}\\] While this is reassuring, our primary goal is to obtain an optimal, or at least a near-optimal policy. Since any policy that is greedy with respect to (w.r.t) \\(v^*\\) is optimal, a natural idea is to stop the value iteration after some finite number of iteration steps and return a policy that is greedy w.r.t. the approximation of \\(v^*\\) that was just obtained. If we stop the process after the \\(k\\)th step, this defines a policy \\(\\pi_k\\) such that \\(\\pi_k\\) is greedy w.r.t. \\(v_k\\): \\(T_{\\pi_k} v_k = T v_k\\). The hope is that as \\(v_k\\) approaches \\(v^*\\), the policies \\(\\{\\pi_k\\}\\) will also get better in the sense that \\(\\|v^*-v^{\\pi_k}\\|_\\infty\\) decreases. The next theorem guarantees that this will indeed be the case. Theorem (Policy Error Bound): Let \\(v: \\mathcal{S} \\to \\mathbb{R}\\) be arbitrary and \\(\\pi\\) be the greedy policy w.r.t. \\(v\\): \\(T_\\pi v = T v\\). Then, . \\[v^\\pi \\geq v^* - \\frac{2 \\gamma \\|v^*-v\\|_\\infty}{1 - \\gamma} \\boldsymbol{1}.\\] . In words, the theorem states that the policy error (\\(\\|v^*-v^{\\pi}\\|_\\infty\\)) of a policy that is greedy with respect to a function \\(v\\) is controlled by the distance of $v$ to \\(v^*\\). This can also be seen as stating that the “greedy operator” $\\Gamma$, which maps functions $v\\in \\mathbb{R}^{\\mathcal{S}}$ to a policy that is greedy w.r.t. $v$, is continuous at $v=v^*$ when the “distance” $d(\\pi,\\pi’)$ between policies $\\pi,\\pi’$ is defined as the maximum norm distance between their value functions: \\(d(\\pi,\\pi') = \\| v^{\\pi}- v^{\\pi'}\\|_\\infty\\). Indeed, with the help of this notation, an alternative form of the theorem statement is that for any $v\\in \\mathbb{R}^{\\mathcal{S}}$, . \\[d( \\Gamma(v^*), \\Gamma(v) ) \\le \\frac{2\\gamma \\|v^*-v\\|_\\infty}{1-\\gamma}\\,.\\] In words, this can be described as that $v\\mapsto \\Gamma(v)$ is is “\\(2\\gamma/(1-\\gamma)\\)-smooth” at $v=v^*$ when the input space is equipped with the maximum norm distance and the output space is equipped with $d$. One can also show that this result is sharp in that the constant \\(2\\gamma/(1-\\gamma)\\) cannot be improved. The proof is an archetypical example of proofs of using contraction and monotonicity arguments to prove error bounds. We will see variations of this proof many times. Before the proof, let us introduce the notation $|x|$ for a vector $\\mathbb{R}^d$ to mean the componentwise absolute value of the vector: \\(|x|_i = |x_i|\\), \\(i\\in [d]\\). As a way of using this notation, note that for any memoryless policy $\\pi$, \\(\\begin{align} |P_\\pi x |\\le P_\\pi |x| \\le \\|x\\|_\\infty P_\\pi \\boldsymbol{1} = \\|x\\|_\\infty \\boldsymbol{1}\\,, \\label{eq:ppineb} \\end{align}\\) and hence \\(\\begin{align} \\|P_\\pi x \\|_\\infty \\le \\|x\\|_\\infty\\,. \\label{eq:stochmxne} \\end{align}\\) In Eq. \\(\\eqref{eq:ppineb}\\) the first inequality follows because $P_\\pi$ is monotone and \\(x\\le |x| \\le \\|x\\|_\\infty \\boldsymbol{1}\\). For the proof it will also be useful to recall that we also have . \\[\\begin{align} T_\\pi (v+c \\boldsymbol{1}) &amp;= T_\\pi v \\,\\, + c \\gamma \\boldsymbol{1}\\,, \\label{eq:tpiadd1} \\\\ T (v+c \\boldsymbol{1}) &amp;= T v \\,\\, + c \\gamma \\boldsymbol{1}\\,, \\label{eq:tadd1} \\end{align}\\] for any \\(v\\in \\mathbb{R}^{\\mathrm{S}}\\), \\(c\\in \\mathbb{R}\\) and memoryless policy \\(\\pi\\). These two identities follow just by the definitions of $T$ and $T_\\pi$, as the reader can easily verify them. Proof: Let \\(v,v^*,\\pi\\) be as in the theorem statement and let \\(\\varepsilon = \\|v^*-v\\|_\\infty\\). Let \\(\\delta = v^*-v^\\pi\\). The result follows by algebra once we prove that \\(\\|\\delta\\|_\\infty \\le \\gamma \\|\\delta\\|_\\infty + 2\\gamma \\varepsilon\\). Hence, we only need to prove this inequality. By our assumptions on \\(v\\) and \\(v^*\\), \\(-\\varepsilon\\boldsymbol{1}\\le v^*-v \\le \\varepsilon\\boldsymbol{1}\\). Now, . \\[\\begin{align*} \\delta &amp; = v^*-v^\\pi \\\\ &amp; = \\textcolor{red}{T} v^* - \\textcolor{red}{T_\\pi} v^\\pi &amp; \\text{(Fundamental Theorem, $T_\\pi v^\\pi = v^\\pi$)}\\\\ &amp; \\le T(v+\\textcolor{red}{\\varepsilon\\boldsymbol{1}})-T_\\pi v^\\pi &amp; \\text{($T$ monotone)}\\\\ &amp; = Tv-T_\\pi v^\\pi +\\textcolor{red}{\\gamma\\varepsilon\\boldsymbol{1}} &amp; \\text{(Eq. \\eqref{eq:tadd1})}\\\\ &amp; = \\textcolor{red}{T_\\pi} v-T_\\pi v^\\pi +\\gamma\\varepsilon\\boldsymbol{1} &amp; \\text{($\\pi$ def.)}\\\\ &amp; \\le T_\\pi(v^*+\\textcolor{red}{\\varepsilon\\boldsymbol{1}})-T_\\pi v^\\pi + \\gamma \\varepsilon \\boldsymbol{1} &amp; \\text{($T_\\pi$ monotone)}\\\\ &amp; = T_\\pi v^* - T_\\pi v^\\pi + \\textcolor{red}{2}\\gamma \\varepsilon\\boldsymbol{1} &amp; \\text{(Eq. \\eqref{eq:tpiadd1})}\\\\ &amp; = \\textcolor{red}{\\gamma P_\\pi}(v^*-v^\\pi)+2\\gamma \\varepsilon\\boldsymbol{1} &amp; \\text{($T_\\pi$ def.)}\\\\ &amp; = \\gamma P_\\pi \\textcolor{red}{\\delta}+2\\gamma \\varepsilon\\boldsymbol{1}\\,. &amp; \\text{($\\delta$ def.)} \\end{align*}\\] Taking the (pointwise) absolute value of both sides and using the triangle inequality, and then Eq. \\(\\eqref{eq:stochmxne}\\) we find that \\(\\begin{align*} |\\delta| \\le \\gamma \\|\\delta\\|_\\infty \\boldsymbol{1} + 2\\gamma \\varepsilon\\boldsymbol{1}\\,. \\end{align*}\\) The proof is finished by taking the maximum over the components, noting that \\(\\max_s |\\delta|_s = \\|\\delta\\|_\\infty\\). \\(\\qquad \\blacksquare\\) . An alternative way of finishing the proof is to note that from $\\delta \\le \\gamma P_\\pi \\delta + 2\\gamma \\varepsilon \\boldsymbol{1}$, by reordering and using that $(I-\\gamma P_\\pi)^{-1} = \\sum_{i\\ge 0} \\gamma^i P_\\pi^i$ is a monotone operator, $\\delta \\le 2\\gamma \\varepsilon \\sum_{i\\ge 0} \\gamma^i P_\\pi \\boldsymbol{1} = 2\\gamma \\varepsilon/(1-\\gamma) \\boldsymbol{1}$. Taking the max-norm of both sides, we get \\(\\|\\delta\\|_\\infty \\le 2\\gamma \\varepsilon/(1-\\gamma)\\). ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec3/#finding-a-near-optimal-policy-using-value-iteration",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec3/#finding-a-near-optimal-policy-using-value-iteration"
  },"249": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "Value Iteration as an Approximate Planning Algorithm",
    "content": ". From Eq. \\(\\eqref{eq:vierror}\\) we see for \\(k \\geq H_{\\gamma, \\varepsilon} = \\frac{\\ln(1 / (\\varepsilon (1 - \\gamma)))}{1 - \\gamma}\\), started with \\(v_0 =0\\), value iteration yields \\(v_k\\) such that \\(\\|v_k - v^*\\|_\\infty \\leq \\varepsilon\\) and consequently, for a policy \\(\\pi_k\\) that is greedy w.r.t. \\(v_k\\), \\(v^{\\pi_k} \\geq v^* - \\frac{2 \\gamma \\varepsilon}{1 - \\gamma} \\boldsymbol{1}\\). Now, for a fixed $\\delta&gt;0$ setting $\\varepsilon$ so that \\(\\delta = \\frac{2 \\gamma \\varepsilon}{1 - \\gamma}\\) holds, we see that after \\(k \\geq H_{\\gamma, \\frac{\\delta(1 - \\gamma)}{2\\gamma}}\\) iterations, we get a \\(\\delta\\)-optimal policy \\(\\pi_k\\): \\(v^{\\pi_k} \\geq v^* - \\delta \\boldsymbol{1}\\). Computing \\(v_{k+1}\\) using \\(\\eqref{eq:vi}\\) takes \\(O(\\mathrm{S}^2 \\mathrm{A})\\) elementary arithmetic (and logic) operations. Putting things together we get the following result: . Theorem (Runtime of Approximate Planning with Value Iteration): Fix a finite discounted MDP and a target accuracy $\\delta&gt;0$. Then, after . \\[O \\left(\\mathrm{S}^2 \\mathrm{A} H_{\\gamma, \\frac{\\delta(1 - \\gamma)}{2\\gamma}} \\right) = \\tilde O\\left( \\frac{\\mathrm{S}^2 \\mathrm{A} }{1 - \\gamma}\\, \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\\] elementary arithmetic operations, value iteration produces a policy $\\pi$ that is $\\delta$-optimal: $v^\\pi \\ge v^* - \\delta \\boldsymbol{1}$, where the $\\tilde{O}(\\cdot)$ result holds when $\\delta \\le 1/e$ is fixed and $\\tilde{O}(\\cdot)$ hides a $\\log(1/(1-\\gamma))$ term. Note that the number of operations needed depends very mildly on the target accuracy. However, accuracy here means an additive error. While the optimal value could be as high as \\(1/(1-\\gamma)\\), it can easily happen that the best value that can be achieved, \\(\\|v^*\\|_\\infty\\), is significantly smaller than \\(1/(1-\\gamma)\\). It may be for example that \\(\\|v^*\\|_\\infty = 0.01\\), in which case a guarantee with \\(\\delta = 0.5\\) is vacuous. By a careful inspection of \\(\\eqref{eq:vierror}\\) we can improve the previous result so that this problem is avoided: . Theorem (Runtime when Controlling for the Relative Error): Fix a finite discounted MDP and a target accuracy \\(\\delta_{\\text{rel}}&gt;0\\). Then, stopping value iteration after $k \\ge H_{\\gamma,\\frac{\\delta_{\\text{rel}}}{2\\gamma}}$ iterations, the policy $\\pi$ produced satisfies the relative error bound . \\[v^\\pi \\ge v^* - \\delta_{\\text{rel}} \\|v^*\\|_\\infty \\boldsymbol{1}\\,,\\] while the total number of elementary arithmetic operations is . \\[O \\left(\\mathrm{S}^2 \\mathrm{A} H_{\\gamma, \\frac{\\delta_{\\text{rel}}}{2\\gamma}} \\right) = \\tilde O\\left( \\frac{\\mathrm{S}^2 \\mathrm{A} }{1 - \\gamma}\\, \\ln\\left(\\frac{1}{\\delta_{\\text{rel}}}\\right)\\right)\\] where $\\tilde{O}(\\cdot)$ hides $\\log(1/(1-\\gamma))$. Notice that the runtime required to achieve a fixed relative accuracy appears to be the same as the runtime required to achieve the same level of absolute accuracy. In fact, the runtime slightly decreases. This should make sense: The worst-case for the fixed absolute accuracy is when \\(\\|v^*\\|_\\infty=1/(1-\\gamma)\\), and in this case the relative accuracy is significantly less demanding: With \\(\\delta_{\\text{rel}}=0.5\\), value iteration can stop after guaranteeing values of \\(0.5/(1-\\gamma)\\), which, as a value, is much smaller than \\(1/(1-\\gamma)-0.5\\), the target with the absolute accuracy level of \\(\\delta = 0.5\\). Note that the relative error bound is not without problems either: It is possible that for some states $s$, \\(v^*(s)-\\delta_{\\text{rel}} \\|v^*\\|_\\infty\\) is negative, a vacuous guarantee. A reasonable stopping criteria would be to stop when the policy that we read out satisfies . \\[v^{\\pi_k} \\ge (1-\\delta_{\\text{rel}}) v^*\\,.\\] Since \\(v^*\\) is not available, to arrive at a stopping condition that can be verified and which implies the above inequality, one can replace $v^*$ above with an upper bound on it, such as \\(v_k +\\gamma^k \\|v_k\\|_\\infty/(1-\\gamma^k) \\boldsymbol{1}\\). In this imagined procedure, in each iteration, one also needs to compute the value function of policy $\\pi_k$ to verify whether the stopping condition is met. If we do this much computation, we may as well replace $v_k$ with $v^{\\pi_k}$ in the update equation \\(\\eqref{eq:vi}\\) hoping that this will further speed up convergence. This results in what is known as policy iteration, which is the subject of the next lecture. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec3/#value-iteration-as-an-approximate-planning-algorithm",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec3/#value-iteration-as-an-approximate-planning-algorithm"
  },"250": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "The Computational Complexity of Planning in MDPs",
    "content": "Now that we have our first results for the computation of approximately optimal policies, it is time to ask whether the algorithm we discovered is doing unnecessary work. That is, what is the minimax computational cost of calculating an optimal, or approximately optimal policy? . To precisely formulate this problem, we need to specify the inputs and the outputs of the algorithms considered. The simplest setting is when the inputs to the algorithms are arrays, describing the transition probabilities and the rewards for each state action pair with some ordering of state-action pairs (and next states in the case of transition probabilities). The output, by the Fundamental Theorem, can be a memoryless policy, either deterministic or stochastic. To describe such a policy, the algorithm could write a table. Clearly, the runtime of the algorithm will be at least the size of the table that needs to be written, so the shorter the output, the better the runtime can be. To be nice with the algorithms, we should allow them to output deterministic policies. After all, the Fundamental Theorem also guarantees that we can always find a deterministic memoryless policy which is optimal. Further, greedy policies can also be chosen to be deterministic, so the value-iteration algorithm would also satisfy this requirement. The shortest specification for a deterministic policy is an array of the size of the state space that has \\(\\mathrm{S}\\) entries. Thus, the runtime of any algorithm that needs to “produce” a fully specified policy is at least \\(\\Omega(\\mathrm{S})\\). This is quite bad! As was noted before, \\(\\mathrm{S}\\), the number of states, in typical problems is expected to be gigantic. But by this easy argument we see that if we demand algorithms to produce fully specified policies then without any further help, they have to do as much work as the number of states. However, things are a bit even worse. In Homework 0, we have seen that no algorithm can find a given value in an array without looking at all entries of the array (curiously, we saw that if we allow randomized computation, that on expectation it is enough to check half of the entries). Based on this, it is not hard to show the following result: . Theorem (Computation Complexity of Planning in MDPs): . Let $0\\le \\delta &lt; \\gamma/(1-\\gamma)$. Any algorithm that is guaranteed to produce $\\delta$-optimal policies in any finite MDP described with tables, with a fixed discount factor $0\\le \\gamma &lt;1$ and rewards in the $[0,1]$ interval needs at least \\(\\Omega(\\mathrm{S}^2\\mathrm{A})\\) elementary arithmetic operations on some MDP with the above properties and whose state space is of size $\\mathrm{S}$ and action space is of size $\\mathrm{A}$. Proof sketch: We construct a family of MDPs such that no matter the algorithm, the algorithm will need to perform the said number of operations in at least one of the MDPs. One-third of the states is reserved for “heaven”, one-third is reserved for “hell” states. The remaining one-third set of states, call them $R$, is where the algorithms will need to make some nontrivial amount of work. The MDPs are going to be deterministic. In the tables given to the algorithms as input, we (conveniently for the algorithms) order the states so that the “hell” states come first, followed by the “heaven” states, followed by the states in $R$. In the “heaven” class, all states self-loop under all actions and give a reward of one. The optimal value of any of these states is $1/(1-\\gamma)$. In the “hell” class, states also self-loops under all actions but give a reward of zero. The optimal value of these states is $0$. For the remaining states, all actions except one lead to some hell state, while the chosen special action leads to some state in the heaven class. The optimal value of all states in set $R$ have a value of $\\gamma/(1-\\gamma)$ and the value of a policy that in a state in $R$ does not choose the special optimal action gets the value of $0$ in that state. It follows that any algorithm that is guaranteed to be $\\delta$ optimal needs to identify the unique optimal action at every state in $R$. In particular, for every state $s\\in R$ and action $a\\in \\mathcal{A}$, the algorithm needs to read $\\Omega(\\mathrm{S})$ entries of the transition probability vector $P_a(s)$ or it can’t find out whether $a$ leads to a state in the heaven class or the hell class: The probability vector $P_a(s)$ will have a single one at such an entry, either among the $\\mathrm{S}/3$ entries representing the hell, or the $\\mathrm{S}/3$ entries representing the heaven states. By the aforementioned homework problem, any algorithm that needs to find this “needle” requires to check $\\Omega(\\mathrm{S})$ entries. Since the number of states in $R$ is also $\\Omega(\\mathrm{S})$, we get that the algorithm needs to do $\\Omega( \\mathrm{S}\\times \\mathrm{A}) \\mathrm{S}) = \\Omega( \\mathrm{S}^2 \\mathrm{A})$ work. \\(\\qquad \\blacksquare\\) . We immediately see two differences between the lower bound and our previous upper bound(s): In the lower bound there is no dependence on $1/(1-\\gamma)$ (the effective horizon at a constant precision). Furthermore, there is no dependence on $1/\\delta$, the inverse accuracy. As it turns out, the dependence on $1/\\delta$ of value-iteration is superfluous and can be removed. The algorithm that achieves this is policy iteration, which was mentioned earlier. However, this result is saved for the next lecture. After this, the only remaining gap will be the order of the polynomials and the dependence on $1/(1-\\gamma)$, which is closely related to the said polynomial order. And of course, we save for later the most pressing issue that we need to somehow be able to avoid the situation when the runtime depends on the size of the state space (forgetting about the action space for a moment). By the lower bound just presented we already know that this will require changing the problem setting. Just how to do this will be the core question that we will keep returning to in the class. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec3/#the-computational-complexity-of-planning-in-mdps",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec3/#the-computational-complexity-of-planning-in-mdps"
  },"251": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "Notes",
    "content": "Value iteration . The idea of value iteration is probably due to Richard Bellman. Error bound for greedification . This theorem is due to Singh &amp; Yee, 1994. The example that shows that the result stated in the theorem is tight. Consider an MDP with two states, call them $A$ and $B$, two actions, and deterministic dynamics. Call the two actions $a$ and $b$. Regardless the state where it is used, action $a$ makes the next state transit to state $A$, while giving a reward of $2\\gamma \\epsilon$. Analogously, action $b$ makes the next state transit to state $B$, while giving a reward of $0$. The optimal values in both states are $2\\gamma \\varepsilon/(1-\\gamma)$. Let $v$ be so that \\(v(A) = v^*(A)-\\epsilon\\), while \\(v(B)=v^*(B)+\\epsilon\\). Thus, $v$ underestimates the value of $A$, while it overestimates the value of state $B$. It is not hard to see that the policy $\\pi$ that uses action $b$ regardless the state is greedy with respect to $v$ (actually, the action-values of the two actions tie at both states). The value function of this policy assigns the value of $0$ to both states, showing that the result stated in the theorem is indeed tight. Computational complexity lower bound . The last theorem is due to Chen and Wang (2017), but the construction is also (unsurprisingly) similar to one that appeared in an earlier paper that studied query complexity in the setting when the access to the MDP is provided by a simulation model. In fact, we will present this lower bound later in a lecture where we study batch RL. According to this result, the query-complexity (also known as sample-complexity) of finding a $\\delta$-optimal policy with constant probability in discounted MDPs accessible through a random access simulator, apart from logarithmic factors, is $SA H^3/\\delta^2$, where $H=1/(1-\\gamma)$. Representations matter . We already saw that in order to just clearly define the computational problems (which is necessary for being able to talk about lower bounds), we need to be clear about the inputs (and the outputs). The table representation of MDPs is far from being the only possibility. We just mentioned the “simulation model”. Here the algorithm “learns” about the MDP by issuing next state and reward queries to the simulator at some state-action pair $(s,a)$ of its choice to which the simulator responds with a random next state (drawn fresh) and the $r_a(s)$. Interestingly, this can provably reduce the number of queries compared to the table representation. Another alternative, which still keeps tables, is to give the algorithm a cumulative probability representation. In this representation, the states are identified with ${1,\\dots,\\mathrm{S}}$ as before but instead of giving the algorithm the tables $[P_a(s,1), \\dots, P_a(s,\\mathrm{S})]$ for fixed $(s,a)$, the algorithm is given . \\[[P_a(s,1), P_a(s,1)+P_a(s,2), \\dots, 1]\\] (the last entry could be saved, because it is always equal to one, but in the grand scheme of things, of course, this does not matter). Now, it is not hard to see that if the original probability vector had a single one and zeroes everywhere else, the “needle in the haystack problem” used in the lower bound, with the integral representation above, a clever algorithm can find the entry with the one with at most $O(\\log( \\mathrm{S})) $ queries. As it turns out, with this representation, the query complexity (number of queries required) of producing a good policy can indeed be reduced from the quadratic dependence on the size of the state-space to a log-linear dependence. Hence, we see that the input representation crucially matters. Chen and Wang (2017) also make this point and they discuss yet another, “tree” representation, which leads to a similar speedup. MDPs with short descriptions . The simulator model assumption addresses the problem that just reading the input may be the bottleneck. This is not the only possibility. One can imagine various classes of MDPs that have a short description, which may raise the hope that one can find out a good policy in them without touching each state-action pair. There are many examples of classes of MDPs that belong to this category. These include . | factored MDPs: The transition dynamics have a short, structured (factored) representation, and the same applies to the reward | parametric MDPs: The transition dynamics and the rewards have a short, parametric representation. Examples include linear-quadratic regulation (linear dynamics, quadratic reward, Euclidean state and action spaces, Gaussian noise in the transition dynamics), robotic systems, various operations research problems. | . For factored MDPs one is out of luck: In these, planning is provably “very hard” (computationally). For linear-quadratic regulation, on the other hand, planning is “easy”; once the data is read, all one has to do is to solve some algebraic equations, for which efficient solution methods have been worked out. Query vs. computational complexity . The key idea of the lower bound crucially hinges upon that good algorithms need to “learn” about their inputs: The number of arithmetic and logic operations of any algorithm is at least as large as the number of “read” operations it issues. The minimum number of required read operations to produce an input of some desired property is often called the problems query complexity and by the above reasoning we see that the computational complexity is lower bounded by the query complexity. As it happens, query complexity is much easier to bound than computational complexity in the sense that it is rare to see computational complexity lower bounds strictly larger than the query complexity (the exceptions to this come when a “compact” representation of the MDP is available, such as in the case of factored MDPs). At the heart of query complexity lower bounds is often the needle in the haystack problem. This seems to be generally true when the inputs are “deterministic”. When querying results in stochastic (random) outcomes, multiple queries may be necessary to “reject”, “reduce”, or “filter out” the noise and then new considerations appear. In any case, query complexity is a question about quickly determining the information crucial to arrive at a good decision early and is in a way about “learning”: Before a table is read, the algorithm does not know which MDP it faces. Hence, query complexity is essentially an “information” question and is also sometimes called information complexity and we can think of query complexity as the most basic information theory question. This is a bit different though than mainstream information theory, which is somehow tied up in dealing with reducing the effect of random responses (random “corruptions” of the clean information). Query complexity everywhere . Query complexity is widely studied in a number of communities which, sadly, are almost entirely disjoint. Information-theory, mentioned above is one of them, though as was noted, here the problems are often tied to studying the speed of gaining information in the presence of noise. Besides information theory, there is the whole field of information-based complexity, which has its own journal, multiple books and more. Also notable is the theory community that studies the complexity of evolutionary algorithms. Besides these, of course, query complexity made appearances in the optimization literature (with or without noise), operations research, and of course in the machine learning and statistics community. In particular, in the machine learning and statistics community, when the algorithm is just handed over noisy data, “the sample”, one can ask how large this sample needs to be to achieve some good outcome (e.g., good predictions on unseen data). This leads to the notion of sample complexity, which is the same as our query complexity except that the queries are of the “dull”, “passive” nature of “give me the next datapoint”. As opposed to this, “active learning” refers to the case when the algorithms themselves control some aspects of how the data is collected. Free lunches, needles and a bit of philosophy . Everyone after going to a few machine learning conferences or reading their first book, or blog posts would have heard about David Wolpert’s “no-free lunch theorems”. Yet, I find that to most people the exact nature (or significance) of these theorems remain elusive. Everyone heard that these theorem essentially state that “in the lack of bias, all algorithms are equal” (and therefore there is no free lunch), from which we should conclude that the only way to choose between algorithms is by introducing bias. But what does bias means? If one reads these results carefully (and the theory community of evolutionary computation made a good job of making them accessible) one finds that the results are nothing more that describing some corollaries that to find a needle in a haystack (the special entry in a long array), one needs to search the whole haystack (query almost all entries of the array). Believers of the power of data like to dismiss the significance of the no-free lunch result by claiming that it is ridiculous in that it assumes no structure at all. I find these arguments weak. The main problem is that they are evasive. The evasiveness comes from the reluctance to be clear about what we expect the algorithms to achieve. The claim is that once we are clear about this, that is, clear about the goals, or just the problem specification, we can always hunt for the “needle in the haystack” subproblems within the problem class. This is about figuring out the symmetries (as symmetry equals no structure) that sneakily appear in pretty much any reasonable problem we think of worth studying. The only problems that do not have “needle in the haystack” situations embedded into them are the ones that are not specified at all. What is the upshot of all this? In a way, the real problem is to be clear about what the problem we want to solve is. This is the problem that most theoreticians in my field struggle with every day. Just because this is hard, we cannot give up on this before even starting, or this will just lead to chaos. As we shall see in this class, how to specify the problem is also at the very heart of reinforcement learning theory research. We constantly experiment with various problem definitions, tweaking them in various ways, trying to separate hopelessly hard problems from the easy, but reasonably general ones. Theoreticians like to build a library of various problem settings that they can classify in various ways, including relating the problem settings to each other. While algorithm design is the constructive side of RL (and computer science, more generally), understanding the relationship between the various problem settings is just as equally important. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec3/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec3/#notes"
  },"252": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "References",
    "content": ". | Chen, Y., &amp; Wang, M. (2017). Lower bound on the computational complexity of discounted markov decision problems. arXiv preprint arXiv:1705.07312. [link] | Singh, S. P., &amp; Yee, R. C. (1994). An upper bound on the loss from approximate optimal-value functions. Machine Learning, 16(3), 227-233. [link] | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec3/#references",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec3/#references"
  },"253": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "3. Value Iteration and Our First Lower Bound",
    "content": "PDF Version . Last time, we discussed the Fundamental Theorem of Dynamic Programming, which then led to the efficient “value iteration” algorithm for finding the optimal value function. And then we could find the optimal policy by greedifying w.r.t. the optimal value function. In this lecture we will do two things: . | Elaborate more on the the properties of value iteration as a way of obtaining near-optimal policies; | Discuss the computational complexity of planning in finite MDPs. | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec3/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec3/"
  },"254": {
    "doc": "4. Policy Iteration",
    "title": "Policy Iteration",
    "content": "Policy iteration starts with an arbitrary deterministic (memoryless) policy \\(\\pi_0\\). Then, in step $k=0,1,2,\\dots$, the following computations are done: . | calculate \\(v^{\\pi_k}\\), and | obtain \\(\\pi_{k+1}\\), another deterministic memoryless policy, by “greedifying” w.r.t. \\(v^{\\pi_k}\\). | . How do we calculate $v^{\\pi_k}$? Recall that $v^{\\pi}$, for an arbitrary memoryless policy $\\pi$, is the fixed-point of the operator $T_\\pi$: $v^\\pi = T_\\pi v^\\pi$. Also, recall that $T_\\pi v = r_\\pi + \\gamma P_\\pi v$ for any $v\\in \\mathbb{R}^{\\mathcal{S}}$. Thus, $v^\\pi = T_\\pi v^\\pi$ is just a linear equation in $v^\\pi$, which we can solve explicitly. In the context of policy iteration from this we get . \\[\\begin{align} v^{\\pi_k} = (I - \\gamma P_{\\pi_k})^{-1} r_{\\pi_k}\\,. \\label{eq:vpiinv} \\end{align}\\] The careful reader will think of why the inverse of the matrix $I-\\gamma P_{\\pi_k}$ exist. There are many tools we have at this stage to argue that the above is well-defined. One approach is to note that $(I-A)^{-1} = \\sum_{i\\ge 0} A^i$ holds whenever all eigenvalues of the square matrix $A$ lie strictly within the unit circle on the complex plain (see homework 0). This is known as the von Neumann series expansion of $I-A$, but these big words just hide that at the heart of this is the elementary geometric series formula, $1/(1-x) = \\sum_{i\\ge 0} x^i$, which holds for all $|x|&lt;1$, as we have all learned in high school. Based on Eq. \\(\\eqref{eq:vpiinv}\\) we see that \\(v^{\\pi_k}\\) can be obtained with at most \\(O( \\mathrm{S}^3 )\\) (and in fact with at most \\(O( \\mathrm{S}^{2.373\\dots})\\) ) arithmetic and logic operations. In particular, the cost of computing $r_{\\pi_k}$ is $O(\\mathrm{S})$ (since $\\pi_k$ is deterministic), the cost of computing $P_{\\pi_k}$, with the table representation of the MDP and “random access” to the tables, is $O(\\mathrm{S}^2)$. Note that all these are independent of the number of actions. Computationally, the “greedification step” above just means to compute for each state $s\\in \\mathcal{S}$ an action that maximizes the one-step Bellman lookahead values w.r.t. $v^{\\pi_k}$. Writing this out, we see that we need to solve the maximization problem . \\[\\max_{a\\in \\mathcal{A}} r_a(s) + \\gamma \\langle P_a(s),v^{\\pi_k} \\rangle\\] and store the result as the action that will be selected by $\\pi_{k+1}$. Since we agreed that all these policies will be deterministic, we may remove a bit of the storage redundancy, if we allow the algorithm just to store the action chosen by $\\pi_{k+1}$ at every state (and eventually produce the output in this form), rather than requiring it to produce a probability vector for each state, which would have a lot of redundant zero entries in it. Correspondingly, we will further abuse notation and will allow deterministic memoryless policies to be identified with $\\mathcal{S} \\to \\mathcal{A}$ maps. Thus, $\\pi_{k+1}: \\mathcal{S} \\to \\mathcal{A}$. Given $v^{\\pi_k}$, a vector of length $\\mathrm{S}$, the cost of evaluating the argument of the maximum is $O(\\mathrm{S})$. Thus, the cost of computing the maximum is $O(\\mathrm{S}\\mathrm{A})$: This is where the number of actions appears (in these steps) in the runtime. Our main result will be a theorem that states that after $\\tilde O( \\mathrm{SA}/(1-\\gamma))$ iterations, the policy computed by policy iteration is necessarily optimal (and not only approximately optimal!). The proof of this result hinges up on two key observations: . | Policy iteration converges geometrically | After every $H_{\\gamma,1}$ iterations, it eliminates at least one suboptimal action at some state. | . The first result follows from comparing policy iteration with value iteration. We know that value iteration converges at a geometric rate regardless of its initialization. Hence, if we can prove that \\(\\| v^{\\pi_k}-v^* \\|_\\infty \\le \\| T^k v^{\\pi_0}-v^* \\|_\\infty\\) then we will be done. In the so-called “policy improvement lemma”, we will in fact prove a result that implies . \\[\\begin{align} T^k v^{\\pi_0} \\le v^{\\pi_{k}}\\,, \\qquad k=0,1,2,\\dots\\, \\label{eq:pilk} \\end{align}\\] which is stronger than the geometric convergence result. Lemma (Geometric Progress Lemma): Let $\\pi,\\pi’$ be memoryless policies such that $\\pi’$ is greedy w.r.t. $v^\\pi$. Then, . \\[\\begin{align*} v^\\pi \\le T v^{\\pi} \\le v^{\\pi'}\\,. \\end{align*}\\] . Proof: By definition, $T v^\\pi = T_{\\pi’} v^\\pi$. We also have $v^\\pi = T_\\pi v^\\pi \\le T v^\\pi$. Chaining these, we get . \\[\\begin{align} v^\\pi \\le T v^{\\pi} = T_{\\pi'} v^{\\pi}\\,. \\label{eq:pilemmabase} \\end{align}\\] We prove by induction on $i\\ge 1$ that . \\[\\begin{align} v^\\pi \\le T v^{\\pi} \\le T_{\\pi'}^i v^{\\pi}\\,. \\label{eq:pilemmainduction} \\end{align}\\] From this, the result will follow by taking $i\\to \\infty$ of both sides. The base case of induction $i=1$ has just been established. For the general case, assume that the required inequality holds for $i\\ge 1$. We show that it also holds for $i+1$. For this, apply $T_{\\pi’}$ on both sides of Eq. \\(\\eqref{eq:pilemmainduction}\\). Since $T_{\\pi’}$ is monotone, we get . \\[\\begin{align*} T_{\\pi'} v^\\pi \\le T_{\\pi'}^{i+1} v^{\\pi}\\,. \\end{align*}\\] Chaining this with Eq. \\(\\eqref{eq:pilemmabase}\\), we get . \\[\\begin{align*} v^\\pi \\le T v^\\pi = T_{\\pi'} v^\\pi \\le T_{\\pi'}^{i+1} v^{\\pi}\\,, \\end{align*}\\] finishing the inductive step, and hence the proof. \\(\\qquad \\blacksquare\\) . The lemma shows that the value functions are monotonically increasing. Applying this lemma $k$ times starting with $\\pi = \\pi_0$ gives Eq. \\(\\eqref{eq:pilk}\\) and this implies the promised result: . Corollary (Geometric convergence): Let \\(\\{\\pi_k\\}_{k\\ge 0}\\) be the sequence of policies produced by policy iteration. Then, for any \\(k\\ge 0\\), . \\[\\begin{align} \\|v^{\\pi_k} - v^*\\|_\\infty \\leq \\gamma^k \\|v^{\\pi_0} - v^*\\|_\\infty\\,. \\label{eq:pig} \\end{align}\\] . Proof: By \\(\\eqref{eq:pilk}\\), . \\[T^k v^{\\pi_0} \\le v^{\\pi_k} \\le v^*\\,, \\qquad k=0,1,2,\\dots\\,.\\] Hence, . \\[v^* - v^{\\pi_k} \\le v^* - T^k v^{\\pi_0}\\,, \\qquad k=0,1,2,\\dots\\,.\\] Taking componentwise absolute values and then the maximum over the states, we get that . \\[\\|v^* - v^{\\pi_k}\\|_\\infty \\le \\|v^* - T^k v^{\\pi_0}\\|_\\infty = \\|T^k v^* - T^k v^{\\pi_0}\\|_\\infty \\le \\gamma^k \\|v^* - v^{\\pi_0}\\|_\\infty\\,,\\] which is the desired statement. In the equality above we used the Fundamental Theorem and in the last inequality we used that $T$ is a $\\gamma$-contraction. \\(\\qquad\\blacksquare\\) . We now set out to finish by showing the “strict progress lemma”. The lemma uses the corollary we just obtained, but it will also require some truly novel ideas. Lemma (Strict progress lemma): Fix an arbitrary suboptimal memoryless policy $\\pi_0$ and let \\(\\{\\pi_k\\}_{k\\ge 0}\\) be the sequence of policies produced by policy iteration. Then, there exists a state $s_0\\in \\mathcal{S}$ such that for any $k\\ge k^*:= \\lceil H_{\\gamma,1} \\rceil +1$, . \\[\\pi_k(s_0)\\ne \\pi_0(s_0)\\,.\\] . The lemma shows that after every \\(k^* = \\tilde O \\left( \\frac{1}{1-\\gamma}\\right)\\) iterations, policy iteration eliminates one action-choice at one state until there remains no suboptimal action to be eliminated. This can only be continued for at most $SA - S$ times: In every state, at least one action must be optimal. As an immediate corollary of the progress lemma, we get the main result of this lecture: . Theorem (Runtime Bound for Policy Iteration): Consider a finite, discounted MDP with rewards in $[0,1]$. Let \\(k^*\\) be as in the progress lemma, \\(\\{\\pi_k\\}_{k\\ge 0}\\) the sequence of policies obtained by policy iteration starting from an arbitrary initial policy $\\pi_0$. Then, after at most \\(k= k^* (\\mathrm{S}\\mathrm{A}-\\mathrm{S}) = \\tilde O\\left( \\frac{\\mathrm{S}\\mathrm{A}-\\mathrm{S} }{1-\\gamma } \\right)\\) iterations, the policy $\\pi_k$ produced by policy iteration is optimal: $v^{\\pi_k}=v^*$. In particular, policy iteration computes an optimal policy with at most \\(\\tilde O\\left( \\frac{ \\mathrm{S}^4 \\mathrm{A} +\\mathrm{S}^3{\\mathrm{A}^2} }{1-\\gamma} \\right)\\) arithmetic and logic operations. It remains to prove the progress lemma. We start with an identity which will be useful beyond the proof of this lemma. The identity is called the value difference identity and it gives us an alternate form of the difference of values functions of two memoryless policies. Let $\\pi,\\pi’$ be two memoryless policies. Recalling that $v^{\\pi’} = (I-\\gamma P_{\\pi’})^{-1} r_{\\pi’}$, by algebra, we find that . \\[\\begin{align*} v^{\\pi'} - v^{\\pi} &amp; = (I-\\gamma P_{\\pi'})^{-1} [ r_{\\pi'} - (I-\\gamma P_{\\pi'}) v^\\pi] \\\\ &amp; = (I-\\gamma P_{\\pi'})^{-1} [ T_{\\pi'} v^\\pi - v^\\pi]\\,. \\end{align*}\\] Introducing . \\[g(\\pi',\\pi) = T_{\\pi'} v^\\pi - v^\\pi\\,,\\] which we can think of the “advantage” of $\\pi’$ relative to $\\pi$, we get the following lemma: . Lemma (Value Difference Identity): For all memoryless policies \\(\\pi, \\pi'\\), . \\[v^{\\pi'} - v^\\pi = (I - \\gamma P_{\\pi'})^{-1} g(\\pi',\\pi)\\,.\\] . Of course, a symmetric relationship also holds. With this, we are now ready to prove the progress lemma. Note that if \\(\\pi^*\\) is an optimal memoryless policy then for any other memoryless policy $\\pi$, \\(g(\\pi,\\pi^*)\\le 0\\). In fact, the reverse statement also holds: if the above holds for any $\\pi$, $\\pi^*$ must be optimal. This makes it \\(-g(\\pi_k,\\pi^*)\\) an ideal target to track the progress that policy iteration makes. We expect this to start at a high value and decrease as $k$ increases. Note, in particular, that if . \\[\\begin{align} -g(\\pi_k,\\pi^*)(s_0)&lt;-g(\\pi_0,\\pi^*)(s_0) \\label{eq:strictprogress} \\end{align}\\] for some state $s_0\\in \\mathcal{S}$ then, by algebra, . \\[r_{\\pi_k(s_0)}(s_0) + \\gamma \\langle P_{\\pi_k(s_0)} , v^* \\rangle &gt; r_{\\pi_0(s_0)}(s_0) + \\gamma \\langle P_{\\pi_0(s_0)} , v^* \\rangle\\] which means that $\\pi_k(s_0)\\ne \\pi_0(s_0)$. Hence, the idea of the proof is to show that Eq. \\(\\eqref{eq:strictprogress}\\) holds for any $k\\ge k^*$. Proof (of the progress lemma): Fix $k\\ge 0$ and \\(\\pi_0\\) such that \\(\\pi_0\\) is not optimal. Let \\(\\pi^*\\) be an arbitrary memoryless optimal policy. Then, for policy \\(\\pi_k\\), by the value difference identity and since \\(\\pi^*\\) is optimal, . \\[- g(\\pi_k,\\pi^*) = (I - \\gamma P_{\\pi_k}) (v^* - v^{\\pi_k}) = (v^* - v^{\\pi_k}) - \\gamma P_{\\pi_k} (v^* - v^{\\pi_k}) \\leq v^* - v^{\\pi_k}\\,,\\] where the last inequality follows because $P_{\\pi_k}$ is stochastic and hence monotone and because \\(v^* - v^{\\pi_k}\\ge 0\\). Our goal is to relate the right-hand side to \\(-g(\\pi_0,\\pi^*)\\). Since Eq. \\(\\eqref{eq:pig}\\) allows us to relate the right-hand side to \\(v^*-v^{\\pi_0}\\), and the value difference identity then lets us bring in \\(-g(\\pi_0,\\pi^*)\\), preparing to use Eq. \\(\\eqref{eq:pig}\\), we first take the max-norm of both sides of the above inequality, noting that this keeps the inequality by the definition of the max-norm. Then, as planned, we use Eq. \\(\\eqref{eq:pig}\\) and the value difference identity to get . \\[\\begin{align} \\|g(\\pi_k,\\pi^*)\\|_\\infty &amp; \\leq \\|v^* - v^{\\pi_k}\\|_\\infty \\leq \\gamma^k \\|v^* - v^{\\pi_0}\\|_\\infty = \\gamma^k \\|(I - \\gamma P_{\\pi_0})^{-1} (-g(\\pi_0,\\pi^*))\\|_\\infty \\nonumber \\\\ &amp; \\leq \\frac{\\gamma^k}{1 - \\gamma} \\|g(\\pi_0,\\pi^*)\\|_\\infty\\,, \\label{eq:plmain} \\end{align}\\] where the last inequality follows by noting that \\((I - \\gamma P_{\\pi_0})^{-1} = \\sum_{i\\ge 0} \\gamma^i P_{\\pi_0}^i\\) and thus from the triangle inequality and because \\(P_{\\pi_0}\\) is a max-norm non-expansion, \\(\\| (I - \\gamma P_{\\pi_0})^{-1} x \\|_\\infty \\le \\frac{1}{1-\\gamma}\\| x \\|_\\infty\\) holds for any \\(x\\in \\mathbb{R}^{\\mathrm{S}}\\). Now, define $s_0\\in \\mathcal{S}$ to be the state that satisfies \\(-g(\\pi_0,\\pi^*)(s_0) = \\| g(\\pi_0,\\pi^*)(s_0)\\|_\\infty\\). Since $\\mathcal{S}$ is finite, this exists. Noting that \\(0\\le -g(\\pi_k,\\pi^*)(s_0)\\le \\| g(\\pi_k,\\pi^*)\\|_\\infty\\), we get from Eq. \\(\\eqref{eq:plmain}\\) that . \\[-g(\\pi_k,\\pi^*)(s_0) \\leq \\|g(\\pi_k,\\pi^*)\\|_\\infty \\leq \\frac{\\gamma^k}{1 - \\gamma} (-g(\\pi_0,\\pi^*)(s_0)).\\] Now when \\(k\\ge k^*\\), \\(\\frac{\\gamma^k}{1 - \\gamma} &lt; 1\\). Since \\(\\pi_0 \\neq \\pi^*\\), \\(0&lt;\\|g(\\pi_0,\\pi^*)\\|_\\infty = -g(\\pi_0,\\pi^*)(s_0)\\) and thus, . \\[\\begin{align*} -g(\\pi_k,\\pi^*)(s_0) \\leq \\frac{\\gamma^k}{1 - \\gamma} (-g(\\pi_0,\\pi^*)(s_0)) &lt; -g(\\pi_0,\\pi^*)(s_0)\\,, \\end{align*}\\] which is Eq. \\(\\eqref{eq:strictprogress}\\), and thus, by our earlier discussion, \\(\\pi_k(s_0)\\ne \\pi_0(s_0)\\). The proof is done because this holds for any \\(k\\ge k^*\\). \\(\\qquad\\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec4/#policy-iteration",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec4/#policy-iteration"
  },"255": {
    "doc": "4. Policy Iteration",
    "title": "Is Value Iteration Inferior?",
    "content": "Our earlier result on the runtime of value iteration involves a $\\log(1/\\delta)$ term which grows without bounds as $\\delta$, the required precision level, decreases towards zero. However, at this stage it is not clear whether this extra term is the result of a loose analysis or whether it is a property of value-iteration. Can value iteration be guaranteed to find an optimal policy with computation which is polynomial in $\\mathrm{S}$, $\\mathrm{A}$ and the planning horizon $1/(1-\\gamma)$, assuming all value functions takes values in $[0,1/(1-\\gamma)]$? . Calling any algorithm that achieves the above strongly polynomial, we see that with this terminology we can say that policy iteration is strongly polynomial. Note that in the above definition rather than assuming that the rewards lie in $[0,1]$, we use the assumption that the value functions for all policies take values in $[0,1/(1-\\gamma)]$. This is a weaker assumption, but checking our proof for the runtime on policy iteration we see that it only needed this assumption. However, as it turns out, value-iteration is not strongly polynomial: . Proposition: There exists a family of MDPs with deterministic transitions, three states, two actions and value functions for all policies taking values in $[0,1/(1-\\gamma)]$ such that the worst-case iteration complexity of value iteration over this set of MDPs to find an optimal policy is infinite. Here, iteration complexity means the smallest number of iterations $k$ after which $\\pi_k$, as computed by value iteration, is optimal, for any of the MDPs in the family. Of course, an infinite iteration complexity also implies an infinite runtime complexity. Proof: The MDP is depicted in the following figure: . The circles show the states with their names in the circles, the arrows with labels $a_0$ and $a_1$ show the transitions between the states as a result of using the actions. The label $r=\\cdot$ shows how much reward is incurred along a transition. On the figure, $R$ is not a return, but a free parameter, which is chosen in the interval $[0,\\gamma/(1-\\gamma)]$ and which will govern the iteration complexity of value iteration. We consider value iteration initialized at $v_0 = \\boldsymbol{0}$. It is easy to see that the unique optimal action at $s_1$ is $a_0$, incurring a value of $\\gamma/(1-\\gamma)$ at this state. It is also easy to see that $\\pi_0(s_1)=a_1\\ne a_0$. We will show that value iteration can “hug” action $a_1$ at state $s_0$ indefinitely as $R$ approaches $\\gamma/(1-\\gamma)$ from below. For this, just note that $v_k(s_0)=0$ and that $v_k(s_2) =\\frac{\\gamma}{1-\\gamma}(1-\\gamma^k)$ for any $k\\ge 0$. Then, a little calculation shows that $\\pi_k(s_1)=a_1$ as long as $R&gt;v_k(s_2)$. If we want value iteration to spend more than $k_0$ iterations, all we have to do is to choose $R = \\frac{v^*(s_2)+v_{k_0}(s_2)}{2}&lt;\\gamma/(1-\\gamma)$. \\(\\blacksquare\\) . It is instructive to note how policy iteration avoids the blow-up of the iteration-counts. This result shows that value-iteration, as far as we are concerned with calculating an optimal policy, exactly, is clearly inferior to policy iteration. However, we also had our earlier positive result for value iteration that showed that the cost of achieving $\\delta$-suboptimal policies is at most $\\log(1/\\delta)$ (and polynomial in the remaining quantities). What does this all mean? Should we really care about that value-iteration is not finite for exact computation? We have many reasons to not to care much about exact calculations. In the end, we will do sampling, learning, all of which make exact calculations impossible. Also, recall that our models are just models: The models themselves introduce errors. Why would we want to care about exact optimality? In summary: . Exact optimality is nice to have, but approximate computations with runtime growing mildly with the required precision should be almost equally acceptable. Yet, it remains intriguing to think of how policy iteration can just “snap” into the right solution and how by changing just a few lines of code, a drastic improvement in runtime may be possible. We will keep returning to the question of whether an algorithm has some provable advantage over some others. When this can be shown, it is a true win: We do not need to bother with the inferior algorithm anymore. While this is great, remember that all this depends on how the problems are defined. As we have seen before, and we will see many more times, changing the problem definition can drastically change the landscape of what works and what does not work. And who knows, some algorithm may be inferior in some context, and be superior in some other. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec4/#is-value-iteration-inferior",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec4/#is-value-iteration-inferior"
  },"256": {
    "doc": "4. Policy Iteration",
    "title": "Notes",
    "content": "The runtime bound on policy iteration . The first result that showed that after $\\text{poly}(\\mathrm{S},\\mathrm{A},\\frac{1}{1-\\gamma})$ arithmetic and logic operations one can compute an optimal policy is due to Yinyu Ye (2011). This was a real breakthrough of the time. The theorem we proved is by Bruno Scherrer (2016) and we followed closely his proof. This proof is much simpler than the first one by Yinyu Ye, though the main ideas can be traced back to the proof of Yinyu Ye. Runtime of value iteration . The example that shows that value iteration is not strongly polynomial is due to Eugene A. Feinberg, Jefferson Huang and Bruno Scherrer (2014). Ties and stopping . More often than one may imagine, two actions may tie for the maximum in the above problem. Which one to use in this case? As it turns out, it matters only if we want to build a stopping condition for the algorithm that stops the first time it detects that $\\pi_{k+1}=\\pi_k$. This stopping condition takes $O(\\mathrm{S})$ operations, so is quite cheap. If we use this stopping condition, we better make sure that when there are ties, the algorithm resolves them in a systematic fashion, meaning that it has a fixed preference relation over the actions that it respects in case of ties. Otherwise, in the case when there are two optimal actions at some state $s$, $\\pi_k$ is an optimal policy, $\\pi_{k+1}$ may choose the optimal action that $\\pi_k$ did not choose, and then $\\pi_{k+2}$ could choose the same action as $\\pi_k$ at the same state, etc. and the stopping condition would fail to detect that all these policies are optimal. Alternatively to resolving ties systematically one may simply change the stopping condition to checking whether $v^{\\pi_k} = v^{\\pi_{k+1}}$. The reader is invited to check that this would work. “In practice”, though, this may be problematic if $v^{\\pi_k}$ and $v^{\\pi_{k+1}}$ are computed with finite precision and somehow the approximation errors that arise in this calculation lead to different answers. Can this happen at all? It can! We may have $v^{\\pi_k} = v^{\\pi_{k+1}}$ (with infinite precision), while $r_{\\pi_k}\\ne r_{\\pi_{k+1}}$ and $I-\\gamma P_{\\pi_k} \\ne I-\\gamma P_{\\pi_{k+1}}$. And so with finite precision calculations, there is no guarantee that we get the same outcomes in the two cases! The only guarantee that we get with finite precision calculations is that with identical inputs, the outputs are identical. An easy way out, of course, is just to use the theorem above and stop after the number of iterations is sufficiently large. However, this may be, needlessly, wasteful. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec4/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec4/#notes"
  },"257": {
    "doc": "4. Policy Iteration",
    "title": "References",
    "content": ". | Feinberg, E. A., Huang, J., &amp; Scherrer, B. (2014). Modified policy iteration algorithms are not strongly polynomial for discounted dynamic programming. Operations Research Letters, 42(6-7), 429-431. [link] | Scherrer, B. (2016). Improved and generalized upper bounds on the complexity of policy iteration. Mathematics of Operations Research, 41(3), 758-774. [link] | Ye, Y. (2011). The simplex and policy-iteration methods are strongly polynomial for the Markov decision problem with a fixed discount rate. Mathematics of Operations Research, 36(4), 593-603. [link] | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec4/#references",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec4/#references"
  },"258": {
    "doc": "4. Policy Iteration",
    "title": "4. Policy Iteration",
    "content": "PDF Version . In this lecture we . | formally define policy iteration and | show that with $\\tilde O( \\textrm{poly}(\\mathrm{S},\\mathrm{A}, \\frac{1}{1-\\gamma}))$ elementary arithmetic operations, it produces an optimal policy | . This latter bound is to be contrasted with what we found out about the runtime of value-iteration in the previous lecture. In particular, value-iteration’s runtime bound that we discovered previously grew linearly with $\\log(1/\\delta))$ where $\\delta$ was the targeted suboptimality level. This may appear as a big difference in the limit of $\\delta\\to 0$. Is this difference real? Is value-iteration truly inferior to policy-iteration? We will discuss these at the end of the lecture. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec4/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec4/"
  },"259": {
    "doc": "5. Online Planning - Part I.",
    "title": "What is Online Planning?",
    "content": "In a previous lecture we have seen that in discounted MDP with $S$ states and $A$ actions, no algorithm can output a $\\delta\\le \\gamma/(1-\\gamma)$ optimal or better policy with a computation cost less than $\\Omega( S^2 A )$ provided that the MDP is given with a table representation. One of the $SA$ factors here comes from that to specify a policy one needs to compute (and output) what action to take in every state. The additional $S$ factor comes from because to figure out whether an action is any good, one needs to read almost all entries of the next-state distribution vector. An unpleasant tendency of the world is that if a problem is modelled as an MDP (that is, the Markov assumption is faithfully observed), the size of the state space tends to blow up. Bellman’s curse of dimensionality is one reason why this happens. To be able to deal with such large MDPs, we expect our algorithm’s runtime to be independent of the size of the state space. However, our lower bound tells us that this is a pipe dream. But why did we require the planner to output a full policy? And why did we assume that the only way to get information about the MDP is to read big tables of transition probabilities? In fact, if the planner is used inside an “agent” that is embedded in an environment, there is no need for the planner to output a full policy: In every moment, the planner just needs to calculate the action to be taken in the state corresponding to the current circumstances of the environment. In particular, there is no need to specify what action to take under any other circumstances than the current one! . As we usually do in these lectures, assume that the environment is an MDP and the agent gets access to the state in every step when it needs to make a decision. Further, assume that the agent is lucky to also have access to a simulator of the MDP that describes its environment. Just think of the simulator as a black box that can be, fed with a state-action pair and responds with the immediate reward and a random next state from the correct next-state distribution. One can then perhaps build a planner that uses this black box with a “few” queries and quickly returns an action, to be taken by the agent, moving the environment to a random next state, from where the process continues. Now, the planner does not need to output actions at all states and it does not need to spend time on reading long probability vectors. Hence, in theory, the obstacles that led to the lower bound are removed. The question still remains whether in this new situation planner’s can indeed get away with runtime independent of the size of the state space. To break the suspense, the answer is yes and it comes very easily for deterministic environments. For stochastic environments a little more work will be necessary. In the remainder of this lecture we give a formal problem definition for the online planning problem that was described informally above. Next, the result is explained for deterministic environments. This result will be matched with a lower bound. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec5/#what-is-online-planning",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec5/#what-is-online-planning"
  },"260": {
    "doc": "5. Online Planning - Part I.",
    "title": "Online Planning: Formal Definitions",
    "content": "We start with the definition of MDP simulators. We use a language similar to that used to describe optimization problems where one talks about optimization in the presence of various oracles (zeroth-order, first order, noisy, etc.). Because we assume that all MDPs are finite, we identify the state and action spaces with subsets of the natural numbers and for the action set we also require that the action set is $[\\mathrm{A}]$ where $\\mathrm{A}$ is the number of actions. This simplifies the description quite a bit. Definition (MDP simulator): A simulator implementing an MDP \\(M=(\\mathcal{S},\\mathcal{A},P,r)\\) is a “black-box oracle” that when queried with a state action pair \\((s,a)\\in \\mathcal{S}\\times\\mathcal{A}\\) returns the reward $r_a(s)$ and a random state \\(S' \\sim P_a(s)\\), where \\(r=(r_a(s))_{s,a}\\) and \\(P = (P_a(s))_{s,a}\\). Users of the black-box must pay attention avoid querying it for state-action pairs outside of $\\mathcal{S}\\times \\mathcal{A}$. Our next notion is that of an online planner: . Definition (Online Planner): An online planner takes as input the number of actions $\\mathrm{A}$, a state $s\\in \\mathbb{N}$, an MDP simulator “access point”. After querying this simulator finitely many times, the planner needs to return an action from $[\\mathrm{A}]$. (Online) planners may randomize their calculation. Even if they do not randomize, the action returned by a planner is in general random due to the randomness of the simulator that the planner uses. A planner is well-formed if no matter what MDP it interfaces with through a simulator, it returns an action after querying the simulator finitely many times. This also means that the planner can never feed the simulator with state-action pair outside of the set of such pairs. If an online planner is given access to a simulator of $M$, the planner and the MDP $M$ together induce a policy of the MDP. We will just refer to this policy as the planner-induced policy $\\pi$ when the MDP is clear from the context. Yet, this policy depends on the MDP implemented by the simulator. If an online planner is well-formed, this policy is well-defined no matter the MDP that is implemented by the simulator. Online planners are expected to produce good policies: . Definition ($\\delta$-sound Online Planner): We say that an online planner is $\\delta$-sound if it is well-formed and for any MDP $M$, the policy $\\pi$ induced by it and a simulator implementing $M$ is $\\delta$-optimal in $M$. In particular, . \\[v^\\pi \\ge v^* - \\delta \\boldsymbol{1}\\] must hold where $v^*$ is the optimal value function in $M$. The (per-state, worst-case) query-cost of an online planner is the maximum number of queries it submits to the simulator where the maximum is over both the MDPs and the initial states. The following vignette summarizes the problem of online planning: . | Model: | Any finite MDP $M$ | . | Oracle: | Black-box simulator of $M$ | . | Local input: | State $s$ | . | Local output: | Action $A$ | . | Outcome: | Policy $\\pi$ | . | Postcondition: | \\(v^\\pi_M \\ge v^*_M-\\delta \\boldsymbol{1}\\) | . As an optimization, we let online planners also take as input $\\delta$, the target suboptimality level. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec5/#online-planning-formal-definitions",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec5/#online-planning-formal-definitions"
  },"261": {
    "doc": "5. Online Planning - Part I.",
    "title": "Online Planning through Value Iteration and Action-value Functions",
    "content": "Recall value iteration: . | Let \\(v_0 = \\boldsymbol{0}\\) | For \\(k=1,2,\\dots\\) let \\(v_{k+1} = Tv_k\\) | . As we have seen, if the iteration is stopped so that $k\\ge H_{\\gamma,\\delta(1-\\gamma)/(2\\gamma)}$, the policy $\\pi_k$ defined via . \\[\\pi_k(s) = \\arg\\max_a r_a(s) + \\gamma \\langle P_a(s),v_k \\rangle\\] is guaranteed to be $\\delta$-optimal. Can this be used for online planning? As we shall see, in a way, yes. But before showing this, it will be worthwhile to introduce some additional notation that, in the short term, will save us some writing. More importantly, the new notation will also be seen to influence algorithm design. The observation is that to decide about what action to take, we need to calculate the one-step lookahead value of the various actions. Rather than doing this in a separate step as shown above, we could have as well chosen to keep track of these lookahead values throughout the whole procedure. Indeed, define \\(\\tilde T: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}\\) as . \\[\\tilde T q = r + \\gamma P M q, \\qquad (q \\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}})\\,,\\] where $r\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ and the operators $P: \\mathbb{R}^{\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ and $M: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}}$ are defined via . \\[\\begin{align*} r(s,a) = r_a(s)\\,, \\quad (P v)(s,a) = \\langle P_a(s), v \\rangle\\,, \\quad (M q)(s) = \\max_{a\\in \\mathcal{A}} q(s,a) \\end{align*}\\] with \\(s\\in \\mathcal{S}\\), \\(a\\in \\mathcal{A}\\), \\(v\\in \\mathbb{R}^{\\mathcal{S}}\\), \\(q\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}\\). Then the definition of $\\pi_k$ can be shortened to . \\[\\pi_k(s) = \\arg\\max_a (\\tilde T^{k+1} \\boldsymbol{0})(s,a)\\,.\\] It is instructive to write the above computation in a recursive, algorithmic form. Let . \\[q_k = \\tilde T^k \\boldsymbol{0}.\\] Using a Python-like pseudocode, our function to calculate the values $q_k(s,\\cdot)$ looks as follows: . 1. define q(k,s): 2. if k = 0 return [0 for a in A] # base case 3. return [ r(s,a) + gamma * sum( [P(s,a,s') * max(q(k-1,s')) for s' in S] ) for a in A ] 4. end . Line 3, which is where the recursive call happens uses Python’s list comprehensions: the brackets create lists and the function itself returns a list. This is a recursive function (since it calls itself in line 3. The runtime is easily seen to be $(\\mathrm{A}\\mathrm{S})^k$, which is not very hopeful until we notice that if the MDP was deterministic, that is, $P(s,a,\\cdot)$ has a single one entry, and we have a way of looking up which entry is this without going through all the states, say, $g: \\mathcal{S}\\times \\mathcal{A} \\to \\mathcal{S}$ is a function that gives the next states, we can rewrite the above as . 1. define q(k,s): 2. if k = 0 return [0 for a in A] # base case 3. return [ r(s,a) + gamma * max(q(k-1,g(s,a))) for a in A ] 4. end . As in line 3 there is no loop over the next states (no summing up over these), the runtime becomes . \\[O(A^k)\\,\\] which is the first time we see that a good action can be calculated with effort regardless of the size of the state space! And of course, if one is given a simulator of the underlying MDP, which is deterministic, calling $g$ is the same as calling the simulator (once). But will this idea extend to the stochastic case? The answer is yes, but the details will be given in the next lecture. Instead, in this lecture we take a brief look at whether there is any possibility to do better than the above recursive procedure. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec5/#online-planning-through-value-iteration-and-action-value-functions",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec5/#online-planning-through-value-iteration-and-action-value-functions"
  },"262": {
    "doc": "5. Online Planning - Part I.",
    "title": "Lower Bound",
    "content": ". Theorem (online planning lower bound): Take any online planner $p$ that is $\\delta$-sound with $\\delta&lt; 1$ for discounted MDPs with rewards in $[0,1]$. Then there exist some MDPs on which $p$ uses at least \\(\\Omega(\\mathrm{A}^{k})\\) queries at some state with . \\[\\begin{align} k=\\left\\lceil \\frac{\\ln( 1/(\\delta(1-\\gamma)) )}{\\ln(1/\\gamma)}\\right\\rceil, \\label{eq:kdeflb} \\end{align}\\] where \\(\\mathrm{A}\\) is the number of actions in the MDP. Denote by $k_\\gamma$ the value defined in \\eqref{eq:kdeflb}. Then, for $\\gamma\\to 1$, $k_\\gamma =\\Omega( H_{\\gamma,\\delta} )$. Proof: This is a typical needle-in-the-haystack argument. We saw in Question 5 on Homework 0 that no algorithm can find out which element of a binary array of length $m$ is one with less than $\\Omega(m)$ queries. Take a rooted regular $\\mathrm{A}$-ary tree of depth $k$. The tree has exactly $\\mathrm{A}^k$ leafs. Consider an MDP with states corresponding to the nodes of this tree. Call the root $s_0$. Let the dynamics be deterministic: Taking an action at a node (of the tree) makes the next state the child of that node, unless the node is a leaf node, which are absorbing states: The next state under any action at any leaf state $s$ is $s$ itself. Let all the rewards be zero except at exactly one of the leaf nodes, where the reward under any action is set to one. If a planner is $\\delta$-sound, we claim that it must find the optimal action at $s_0$. This holds because the value of this action is $\\sum_{i=k}^\\infty \\gamma^i=\\gamma^k/(1-\\gamma)$ and, by our choice of $k$, $\\gamma^k/(1-\\gamma) \\ge \\delta$, while the value of any other action at $s_0$ is zero. It follows that the planner needs to be able to identify the unique action at the unique leaf node whose reward is one, which, by Question 5 on Homework 0, needs at least $\\Omega(\\mathrm{A}^{k})$ queries. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec5/#lower-bound",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec5/#lower-bound"
  },"263": {
    "doc": "5. Online Planning - Part I.",
    "title": "Notes",
    "content": "Dealing with larger state spaces . For a fully formal specification the reader may worry about how a state is described to an online planner, especially, if we allowed uncountably many states. Because the online planner will only have access to the state that it receives as its input and the other states that are returned from the simulator, for the purpose of communication between the online planner and its environment and the simulator, all these states can just be assigned unique numbers to identify them. Gap between the lower and upper bound . There is an obvious gap between the lower and the upper bound that should be closed. Local planning vs. online planning . Last year’s lecture notes used the expression local planning in place of online planning. There are pros and cons for both expressions, but perhaps online planning better expresses that the planner will be used in an online fashion, that is, every time after a transition happens. On simulators and access modes . Simulators come in many shapes and forms. A general planner needs to be prepared to be used in an interconnection with any simulator. But this is too much: Every simulator provides an interface to the planners and planners need to be designed around these interfaces. Therefore, planners will be specialized to the specific interface used. Here, we distinguish three types of interfaces based on what access the interface allows to generating data. The access can be global, local or online. Global access means that the simulator provides a function that returns a description of the full state space. For finite MDPs this would just mean returning the number of states $S$. Then, the simulator can be called for any $(s,a)$ pair where $s\\in [S]$ and $a\\in [A]$ (the simulator should also have a function that returns the number of actions, $A$). Internally, the simulator then needs to translate the integer indices $s$ and $a$ into appropriate data for which the simulation can be done. Then, the simulator would generate the next state, and translate it back to an integer in $[S]$, which is the data returned from the call. The simulator should also return the associated reward. Often, the reward would also be random (in the lecture, we are concerned with deterministic rewards, but this is just done for the sake of simplicity: random rewards at this stage would not create further difficulties). Local access means that the simulator allows the planner to generate transitions starting only from states that were passed to the planner previously. To implement a local access simulator, one can just introduce an array that is used to remember all the states that have been returned to the planner. For the sake of interfacing with the planner, one can then use the indexing into this array. This way, the planner does not need to know the details of how states are internally represented and it also becomes possible to interface with simulators where the number of states is infinite, or when it is finite, but calculating this number would be impractical or intractable. Of course, the simulator needs the ability to “go back” to a previously visited state and generate new transition data from there. This can be usually implemented on the top of existing simulators without much trouble (the ability to do this is known as “checkpointing”). Online access simulators have an “internal state”, which the planners can manipulate in two ways: they can reset this internal state to the initial state (which is provided to the planner when the planner is called), or they can ask for a transition from the current internal state, by providing an action. As a result of this, the simulator’s internal state would move to a random next state, which is what would be returned to the planner (along with the associated reward). Clearly, any planner prepared to work with online access, can also be used with simulator that provide either local access or global access, and any planner prepared to work with local access can be used with simulators providing global access. In this way, online access is the most general of the access modes, local access is least general, and global access is the most restrictive. Note that even with online access there is the issue that state information about the state of the environment has to be communicated to the planner in a way that is consistent with how state information can be passed from the planner to the simulator. To keep planners general, the environment and the simulator need to work on an appropriate consistent way of serializing information about the state, which is a pure engineering issue and can usually be done without much trouble. “Planning with a generative models” is an alternative, early terminology that is still used in the literature today. Most commonly, this is means online planning with a global access simulator. However, as the expression itself is not as easy to adopt to different situations as described here, we will refrain from using it. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec5/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec5/#notes"
  },"264": {
    "doc": "5. Online Planning - Part I.",
    "title": "5. Online Planning - Part I.",
    "content": "PDF Version . In this lecture we . | introduce online planning; | show that for deterministic MDPs there is an online planner whose runtime per call is independent of the size of the state space; | show that this online planner has in fact a near-optimal runtime in a worst-case sense. | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec5/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec5/"
  },"265": {
    "doc": "6. online planning - Part II.",
    "title": "Sampling May Save the Day?",
    "content": "Assume now that the MDP is stochastic. Recall the pseudocode of the recursive form of value iteration from the last lecture that computes $(T^k \\boldsymbol{0})(s,\\cdot)$: . 1. define q(k,s): 2. if k = 0 return [0 for a in A] # base case 3. return [ r(s,a) + gamma * sum( [P(s,a,s') * max(q(k-1,s')) for s' in S] ) for a in A ] 4. end . Obviously, the size of the state space creeps in because in line 3 we need to calculate an expected value over the next state distribution at $(s,a)$. As noted beforehand, in deterministic systems when a simulator is available, the sum over the next-states can be replaced with a single simulator call. But the reader may remember from Probability 101 that sampling allows one to approximate expected values, where the error of approximation is independent of the cardinality of the set over which we average the values. Here, this set is $\\mathcal{S}$, the state space. This is extremely lucky! . To quantify the size of these errors, we recall Hoeffding’s inequality: . Lemma (Hoeffding’s Inequality): Given $m$ independent, identically distributed (i.i.d.) random variables that take values in the $[0,1]$ interval, for any \\(0 \\leq \\zeta &lt; 1\\), with probability at least \\(1 - \\zeta\\) it holds that . \\[\\left| \\frac{1}{m} \\sum_{i=1}^m X_i - \\mathbb{E}[X_1] \\right| \\leq \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} }\\,.\\] . Letting $S_1’,\\dots,S_m’ \\stackrel{\\textrm{i.i.d.}}{\\sim} P_a(s)$ for some state-action pair $(s,a)$ and $v:S \\to [0,v_{\\max}]$, by this result, for any $0\\le \\zeta &lt;1$, with probability $1-\\zeta$, . \\[\\begin{align} \\left|\\frac1m \\sum_{i=1}^m v(S_i') - \\langle P_a(s), v \\rangle\\right| \\le v_{\\max} \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} }. \\label{eq:hoeffbop} \\end{align}\\] This suggests the following approach: For each state action pair $(s,a)$ draw $S_1’,\\dots,S_m’ \\stackrel{\\textrm{i.i.d.}}{\\sim} P_a(s)$ and store it in a list $C(s,a)$. Then, whenever for some function $v$ we need the value of $\\langle P_a(s), v \\rangle$, just use the sample average . \\[\\frac1m \\sum_{s'\\in C(s,a)} v(s')\\,.\\] Plugging this approximation into our previous pseudocode gives the following new code: . 1. define q(k,s): 2. if k = 0 return [0 for a in A] # base case 3. return [ r(s,a) + gamma/m * sum( [max(q(k-1,s')) for s' in C(s,a)] ) for a in A ] 4. end . The total runtime of this function is now $O( (m\\mathrm{A})^{k+1} )$. What is important is that this will give us a compute time independent of the size of the state space as long as we can show that $m$ can be set independently of $\\mathrm{S}$ while meeting our target for the suboptimality of the induced policy. This pseudocode sweeps under the rug on who creates the lists $C(s,a)$ and when? A simple and effective approach is to use “lazy evaluation” (or memoization): Create $C(s,a)$ at the first time it is needed (and do not create it otherwise). An alternative to the approach we follow here is to avoid storing these lists and just create them on demand. Both procedures are valid, but we will stick to the procedure that creates the lists only once and will comment on the other approach at the end in the notes. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec6/#sampling-may-save-the-day",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec6/#sampling-may-save-the-day"
  },"266": {
    "doc": "6. online planning - Part II.",
    "title": "Good Action-Value Approximations Suffice",
    "content": "As a first step towards understanding the strength and weaknesses of this approach, let us define $\\hat T: \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}}$ by . \\[(\\hat T q)(s,a) = r_a(s) + \\frac{\\gamma}{m} \\sum_{s'\\in C(s,a)} \\max_{a'\\in \\mathcal{A}} q(s',a')\\,.\\] With the help of this definition, when called with state $s=s_0$, the planner computes . \\[\\begin{align*} A = \\arg\\max_{a\\in \\mathcal{A}} \\underbrace{ (\\hat T^H \\boldsymbol{0})(s_0,a) }_{Q_H(s_0,a)}\\,, \\end{align*}\\] The conciseness of this formulae, if anything, must please everyone! . Let us now turn to the question of whether the policy $\\hat \\pi$ induced by this planners is a good one. We start with a lemma that parallels our earlier result that bounded the suboptimality of a policy that is greedy w.r.t. a function over the states as a function of how well the function approximates the optimal value function. To state the lemma, we need the analog of optimal value functions but with action values. Suboptimality of $\\epsilon$-optimizing policies . Define . \\[q^*(s,a) = r_a(s) + \\gamma \\langle P_a(s), v^* \\rangle\\,.\\] We call this function $q^*$ the optimal action-value function (in our MDP). The function \\(q^*\\) is easily seen to satisfy \\(M q^* = v^*\\) and thus also \\(q^* = T q^*\\). The promised lemma is as follows: . Lemma (Policy error bound - I.): Let $\\pi$ be a memoryless policy and choose a function $q:\\mathcal{S}\\times\\mathcal{A} \\to \\mathbb{R}$ and $\\epsilon\\ge 0$. Then, the following hold: . | If $\\pi$ is $\\epsilon$-optimizing in the sense that \\(\\sum_a \\pi(a\\vert s) q^*(s,a) \\ge v^*(s)-\\epsilon\\) holds for every state $s\\in \\mathcal{S}$ then $\\pi$ is $\\epsilon/(1-\\gamma)$ suboptimal: \\(v^\\pi \\ge v^* - \\frac{\\epsilon}{1-\\gamma} \\boldsymbol{1}\\,.\\) . | If $\\pi$ is greedy with respect to $q$ then $\\pi$ is $2\\epsilon$-optimizing with \\(\\epsilon= \\|q-q^*\\|_\\infty\\) and thus . | . \\[v^\\pi \\ge v^* - \\frac{2\\|q-q^*\\|_\\infty}{1-\\gamma} \\boldsymbol{1}\\,.\\] . For the proof, which is partially left to the reader, we need to introduce a bit more notation. In particular, for a memoryless policy, define the operator $M_\\pi: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}}$: . \\[(M_\\pi q)(s) = \\sum_{a\\in \\mathcal{A}} \\pi(a|s) q(s,a)\\,, \\qquad (q\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}, \\, \\, s\\in \\mathcal{S}).\\] With the help of this operator the condition that $\\pi$ is greedy with respect to $q$ can be written as . \\[M_\\pi q = M q\\,.\\] Further, the second claim of the lemma can be stated in the more concise form $M_\\pi q^* \\ge v^* - 2\\epsilon\\boldsymbol{1}$. For future reference, we will also find it useful to define $P_\\pi: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$: . \\[P_\\pi = P M_\\pi\\,.\\] Note that here we abused notation as $P_\\pi$ has already been used to denote the operator that maps functions of the states to functions of the state. From the context, the meaning of $P_\\pi$ will always be clear. Proof: The first part of the proof is standard and is left to the reader. For the second part note that . \\[\\begin{align*} M_\\pi q^* &amp; \\ge M_\\pi(q-\\epsilon \\boldsymbol{1}) = M_\\pi q - \\epsilon\\boldsymbol{1}=M q - \\epsilon \\boldsymbol{1} \\ge M(q^* - \\epsilon \\boldsymbol{1}) - \\epsilon \\boldsymbol{1} = M q^* - 2\\epsilon \\boldsymbol{1} = v^* - 2\\epsilon\\boldsymbol{1}\\,. \\end{align*}\\] Then use the first part. \\(\\qquad \\blacksquare\\) . Suboptimality of almost $\\epsilon$-optimizing policies . There are two issues that need to be taken care of. One is that the planner is randomizing when computing the values $Q_H(s_0,\\cdot)$. What happens when the random next states obtained from the simulator are not “representative”? We cannot expect the outcome of this randomized computation to be precise! Indeed, the best we can expect is that the outcome is “accurate” with some probability, hopefully close to one. In fact, from Hoeffding’s inequality, we see that if we want to achieve small errors in the computation for some target probability, we need to increase the sample size. But Hoeffding’s inequality, in all cases, allows errors which are uncontrolled on some failure event. All in all, the best we can hope for is that with each call, $Q_H(s_0,\\cdot)$ is a good approximation to \\(q^*(s_0,\\cdot)\\) outside of some “failure event” $\\mathcal{F}$ whose probability we will control separately. Let us say the probability of $\\mathcal{F}$ is at most $\\zeta$: . \\[\\mathbb{P}_{s_0}(\\mathcal{F})\\le \\zeta\\,.\\] Here, $\\mathbb{P}_{s_0}$ denotes the probability measure induced by the interaction of the planner and the MDP simulator on an appropriate probability space. We will choose $\\mathcal{F}$ so that on $\\mathcal{F}^c$, the complementer of $\\mathcal{F}$ (a “good” event), it holds that . \\[\\begin{align} \\delta_H = \\| Q_H(s_0,\\cdot) - q^*(s_0,\\cdot)\\|_\\infty \\le \\epsilon\\,. \\label{eq:d0def} \\end{align}\\] Then, on $\\mathcal{F}^c$, . \\[q^*(s_0, A) \\ge Q_H(s_0,A)-\\epsilon = \\max_a Q_H(s_0,a)-\\epsilon \\ge \\max_a (q^*(s_0,a)-\\epsilon)-\\epsilon = v^*(s_0)-2\\epsilon\\,.\\] That is, on the good event $\\mathcal{F}^c$, the action $A$ returned by the planner is $2\\epsilon$ optimizing at state $s_0$. Let $\\hat \\pi(a \\vert s_0)$ denote the probability that action $A$ returned by the planner is $a$: \\(\\hat \\pi(a \\vert s_0)=\\mathbb{P}_{s_0}(A=a)\\). Then, . \\[\\begin{align*} \\sum_{a} &amp; \\hat \\pi(a \\vert s_0) \\mathbb{I}( q^*(s_0, a) \\ge v^*(s_0) - 2\\epsilon ) \\\\ &amp; = \\mathbb{P}_{s_0}( q^*(s_0, A) \\ge v^*(s_0) - 2\\epsilon )\\\\ &amp;= \\mathbb{P}_{s_0}( q^*(s_0, A) \\ge v^*(s_0) - 2\\epsilon, \\mathcal{F}^c ) + \\mathbb{P}_{s_0}( q^*(s_0, A) \\ge v^*(s_0) - 2\\epsilon, \\mathcal{F} )\\\\ &amp;\\ge \\mathbb{P}_{s_0}( q^*(s_0, A) \\ge v^*(s_0) - 2\\epsilon, \\mathcal{F}^c )\\\\ &amp;= \\mathbb{P}_{s_0}( \\mathcal{F}^c )\\\\ &amp;\\ge 1-\\zeta\\,. \\end{align*}\\] In words, with probability at least $1-\\zeta$, $\\hat \\pi$ chooses $2\\epsilon$-optimizing actions: The policy is almost $2\\epsilon$-optimizing. While this is not as good as always choosing $2\\epsilon$-optimizing actions, we expect that as $\\zeta\\to 0$ the difference in performance between $\\hat \\pi$ and a policy that always chooses $2\\epsilon$-optimizing actions disappears because performance is expected to depend on action probabilities in a continuous fashion. The next lemma makes this precise: . Lemma (Policy error bound II): Let $\\zeta\\in [0,1]$, $\\pi$ be a memoryless policy that selects $\\epsilon$-optimizing actions with probability at least $1-\\zeta$ in each state. Then, . \\[v^\\pi \\ge v^* - \\frac{\\epsilon+2\\zeta \\|q^*\\|_\\infty}{1-\\gamma} \\boldsymbol{1}\\,.\\] . Proof: By Part 1 of the previous lemma, it suffices to show that $\\pi$ is \\(\\epsilon+2\\zeta \\|q^*\\|_\\infty\\)-optimizing in every state. This follows from algebra and is left to the reader. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec6/#good-action-value-approximations-suffice",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec6/#good-action-value-approximations-suffice"
  },"267": {
    "doc": "6. online planning - Part II.",
    "title": "Error control",
    "content": "What remains is to show that with high probability, the error $\\delta_H$, defined in \\(\\eqref{eq:d0def}\\) is small. Intuitively, $\\hat T \\approx T$. To firm up this intuition, we may note that for any fixed $q\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ function over the state-action pairs such that \\(\\|q\\|_\\infty \\le \\frac{1}{1-\\gamma}\\) and for any fixed $(s,a)\\in \\mathcal{S}\\times \\mathcal{A}$, by Eq. \\(\\eqref{eq:hoeffbop}\\) and the choice of the sets $\\mathcal{C}(s,a)$, with probability $1-\\zeta$, . \\[\\begin{align} |\\hat T q (s,a)-T q(s,a)| &amp; = \\gamma \\left| \\frac1m \\sum_{s'\\in \\mathcal{C}(s,a)} v(s')\\,\\, - \\langle P_a(s), v \\rangle \\right| \\le \\gamma \\|q\\|_\\infty\\, \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} } \\nonumber \\\\ &amp;\\le \\frac{\\gamma}{1-\\gamma}\\, \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} } =: \\Delta(\\zeta,m), \\label{eq:basicerror} \\end{align}\\] where, for brevity, we introduced $v = Mq$ in the above formula. Union bounds . So we know that for any fixed state-action pair $(s,a)$, outside of a low probability event, $(\\hat T q)(s,a)$ is close to $( T q)(s,a)$. But can we conclude from this that, outside of some low probability event, $(\\hat T q)(s,a)$ is close to $( T q)(s,a)$ everywhere? . To answer this question, it will be easier to turn it around and just try to come up with some event that, on the one hand, has low probability, while, in the other hand, outside of this event, $(\\hat T q)(s,a)$ is close to $( T q)(s,a)$ regardless of $(s,a)$. Denoting by $\\mathcal{E}(s,a)$ the event when $(\\hat T q)(s,a)$ is not close to $( T q)(s,a)$, i.e., . \\[\\mathcal{E}(s,a) = \\{ |(\\hat T q)(s,a) - (Tq)(s,a)|&gt; \\Delta(\\zeta,m) \\}\\,,\\] it is clear that if $\\mathcal{E} = \\cup_{(s,a)} \\mathcal{E}(s,a)$ then outside of $\\mathcal{E}$, none of $\\mathcal{E}(s,a)$ holds and hence . \\[\\max_{(s,a)\\in \\mathcal{S}\\times \\mathcal{A}} |(\\hat T q)(s,a) - (T q)(s,a)|\\le \\Delta(\\zeta,m)\\,.\\] But how large can the probability of $\\mathcal{E}$ be? For this, recall the following elementary result, which follows directly from the properties of measures: . Lemma (Union Bound): For any probability measure $\\mathbb{P}$ and any countable sequence of events \\(A_1, A_2, \\ldots\\) of the underlying measurable space, . \\[\\mathbb{P}\\left(\\cup_i A_i \\right) \\leq \\sum_i \\mathbb{P}(A_i).\\] . By this result, using that $\\mathcal{S}\\times \\mathcal{A}$ is finite, . \\[\\mathbb{P}(\\mathcal{E}) \\le \\sum_{(s,a)\\in \\mathcal{S}\\times \\mathcal{A}} \\mathbb{P}( \\mathcal{E}(s,a)) \\le \\mathrm{S} \\mathrm{A} \\zeta\\,.\\] If we want this probability to be $0\\le \\zeta’\\le 1$, we can set $\\zeta = \\frac{\\zeta’}{\\mathrm{S}\\mathrm{A}}$ and conclude that with probability $1-\\zeta’$, for any state-action pair $(s,a)\\in \\mathcal{S}\\times \\mathcal{A}$, . \\[\\begin{align} |(\\hat T q)(s,a) - (T q)(s,a)| \\le \\Delta\\left(\\frac{\\zeta'}{\\mathrm{S}\\mathrm{A}},m\\right) = \\frac{\\gamma}{1-\\gamma} \\, \\sqrt{\\frac{\\log \\frac{2\\mathrm{S}\\mathrm{A}}{\\zeta'}}{2m} }\\,. \\label{eq:ropec} \\end{align}\\] The following diagram summarizes the idea of union bounds: . To control the error of some bad event happening, we can break the the bad event into a number of elementary parts. By controlling the probability of each such part, we can control the probability of the bad event, or, alternatively, control the probability of the complementary “good” event. The worst case for controlling the probability of the bad event is if the elementary parts do not overlap, but the argument of course works even in this case. Returning to our calculations, from the last formula we see that the errors grew a little compared to \\(\\eqref{eq:basicerror}\\), but the growth is modest: the errors scale with the logarithm of the number of state-action pairs. While this logarithmic error-growth is mild, it is unfortunate that the number of states appears here. To control the errors, by this formulae we would need to choose $m$ to be proportional to the logarithm of the size of the state space, which is better than a linear dependence, but still. One must wonder whether this dependence is truly necessary? If it was, there would be a big gap between the complexity of planning in deterministic and stochastic MDPs. We should not give in for this just yet! . Avoiding dependence on state space cardinality . The key to avoiding the dependence on the cardinality of the state is to avoid taking union bounds over the whole state-action set. That this may be possible follows from that, thinking back to the recursive implementation of the planner, we can notice that the planner does not necessarily rely on all the sets $\\mathcal{C}(s,a)$. To get a handle on this, it will be useful to introduce a notion of a distance induced by the set $\\mathcal{C}(s):=\\cup_{a\\in \\mathcal{A}}\\mathcal{C}(s,a)$ between the states. This distance between states $s$ and $s’$ (denoted by $\\text{dist}(s,s’)$) will be the smallest number of steps that we can take to get from $s$ to $s’$, if in each step we choose one “neighbouring” state to the last state, starting from state $s$. Formally, this is the length $n$ of the shortest sequence $s_0,s_1,\\dots,s_n$ such that $s_0=s$, $s_n = s’$ and for each $i\\in [n]$, $s_i \\in \\mathcal{C}(s_{i-1})$ (this is the distance between states in the directed graph over the states with edges induced by $\\mathcal{C}$). With this, for $h\\ge 0$, define . \\[\\begin{align*} \\mathcal{S}_h &amp;= \\{s \\in \\mathcal{S} | \\text{ dist}(s_0,s) \\leq h \\} \\end{align*}\\] as the set of states accessible from $s_0$ by at most $h$ steps. Note that this is a nested sequence of sets and $\\mathcal{S}_0 = {s_0}$, $\\mathcal{S}_1$ contains $s_0$ and its immediate “neighbors”, etc. We may now observe that in the calculation of $Q_H(s_0,\\cdot)$ when function $q$ is called with a certain value of $0\\le k \\le H$, for the state that appears in the call we have . \\[s\\in \\mathcal{S}_{H-k}\\,.\\] This can be proved by induction on $k$, starting with $k=H$. Click here for the proof. The base case follows because when $q$ calls itself it decrements $k$. Hence, when $q$ is called with $k=H$ and state $s$, $s=s_0$ must be true. Hence, $s\\in \\mathcal{S}_0$. Now, assume that the claim holds for $k=i+1$ with some $0\\le i&lt; H$. Take any state $s'$ that $q$ is called on while $k=i$. Since $i&lt;H$, this call must be a recursive call (from line 3). Going up on the call chain, at the time this recursive call is made, $k=i+1$ (since in the recursive calls the value of $k$ is decremented). This call happens when $s'\\in \\mathcal{C}(s,a)$ for some action $a\\in \\mathcal{A}$ and some state $s$, which, by the induction hypothesis, satisfies $s\\in \\mathcal{S}_{H-(i+1)}$. It follows that $s$ is at a distance of at most $H-i-1$ from $s_0$, while $s'$, a \"neighbour\" of $s$, is at most of a distance of $H-i-1+1=H-i$ from $s_0$. Hence, $s\\in \\mathcal{S}_{H-i}$, finishing the induction. $$\\blacksquare$$ Taking into account that when $q$ is called with $k=0$, the sets $\\mathcal{C}(s,a)$ are not used (line 2), we see that only states $s$ from $\\mathcal{S}_{H-1}$ are such that the calculation ever uses the set $\\mathcal{C}(s,a)$. Since $|\\mathcal{C}(s,a)|=m$, . \\[\\mathcal{S}_h \\le 1 + (mA) + \\dots + (mA)^h \\le (mA)^{h+1}\\] and in particular, $\\mathcal{S}_{H-1}\\le (mA)^H$, which is independent of the size of the state space. Of course, all along, we knew this very well: This is why the total runtime is also independent of the size of the state space. The plan is to take advantage of this to avoid a union bound over all possible state-action pairs. We start with a recursive expression for the errors. Recall that \\(\\delta_H = \\| (\\hat T^H \\boldsymbol{0})(s_0,\\cdot)-q^*(s_0,\\cdot)\\|_\\infty\\). By the triangle inequality, . \\[\\begin{align*} \\delta_H &amp; = \\| (\\hat T^H \\boldsymbol{0})(s_0,\\cdot)-q^*(s_0,\\cdot)\\|_\\infty \\le \\| (\\hat T \\hat T^{H-1} \\boldsymbol{0})(s_0,\\cdot)- \\hat T q^*(s_0,\\cdot)\\|_\\infty + \\| \\hat T q^*(s_0,\\cdot)- q^*(s_0,\\cdot)\\|_\\infty\\,. \\end{align*}\\] Now, observing that . \\[\\vert \\hat T q (s,a)-\\hat T q^* (s,a) \\vert \\le \\frac{\\gamma}{m} \\sum_{s'\\in \\mathcal{C}(s,a)} \\vert Mq - v^* \\vert (s') \\le \\gamma \\max_{s'\\in \\mathcal{C}(s)} \\vert Mq - v^* \\vert (s')\\,,\\] we see that . \\[\\begin{align*} \\delta_H &amp; \\le \\gamma \\max_{s'\\in \\mathcal{C}(s_0),a\\in \\mathcal{A}} | (\\hat T^{H-1} \\boldsymbol{0})(s',a)-q^*(s',a) | + \\| \\hat T q^*(s_0,\\cdot)- q^*(s_0,\\cdot)\\|_\\infty\\,. \\end{align*}\\] In particular, defining . \\[\\delta_{h} = \\underbrace{\\max_{s'\\in \\mathcal{S}_{H-h},a\\in \\mathcal{A}} | \\hat T^{h} \\boldsymbol{0}(s',a)-q^*(s',a)|}_{=:\\| \\hat T^h \\boldsymbol{0}-q^*\\|_{\\mathcal{S}_{H-h}}}\\,,\\] we see that . \\[\\delta_H \\le \\gamma \\delta_{H-1} + \\| \\hat T q^* - q^* \\|_{\\mathcal{S}_0}\\,,\\] where we use the notation \\(\\| q \\|_{\\mathcal{U}} = \\max_{s\\in \\mathcal{U},\\max_{a\\in \\mathcal{A}}} |q(s,a)|\\). More generally, we can prove by induction on $1\\le h \\le H$ (starting with $h=H$) that . \\[\\delta_h \\le \\gamma \\delta_{h-1} + \\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-h}} \\le \\gamma \\delta_{h-1} + \\underbrace{ \\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-1}}}_{=:\\varepsilon'/(1-\\gamma)} \\,,\\] while . \\[\\delta_0 = \\| q^*\\|_{\\mathcal{S}_{H}} \\le \\| q^* \\|_\\infty \\le \\frac{1}{1-\\gamma}\\,,\\] where the last inequality uses that $r_a(s)\\in [0,1]$, which we shall assume for simplicity. Unfolding this recursion for $(\\delta_h)_h$, letting . we get . \\[\\begin{align} \\delta_H &amp;\\leq \\frac{\\gamma^H + \\varepsilon'(1 + \\gamma + \\cdots + \\gamma^{H-1})}{1 - \\gamma} \\leq \\left(\\gamma^H + \\frac{\\varepsilon'}{1 - \\gamma} \\right) \\frac{1}{1 - \\gamma} \\label{eq:delta_H}. \\end{align}\\] We see that the first term in the sum on the right-hand side (in the parenthesis) is controlled by $H$. It remains to show that $\\varepsilon’$ can also be controlled (by choosing $m$ appropriately). In fact, notice that $\\varepsilon’ / (1 - \\gamma)$ is the maximum-norm error with which \\(\\hat T q^*\\) approximates \\(q^* = T q^*\\), but only for states in \\(\\mathcal{S}_{H-1}\\) we need to control this error. By our earlier argument, this set has at most \\((mA)^H\\) states, hence, it is believable that this error can be controlled even when $m$ is chosen independently of the number of states. Controlling \\(\\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-1}}\\) . Since \\(\\mathcal{S}_{H-1}\\) has only $(mA)^H$ states in it, one’s first instinct is to take a union bound over the error events for the states in this set. The trouble is that the set \\(\\mathcal{S}_{H-1}\\) itself is random. As such, it is not clear, what the failure events should be? And how many failure events are we going to have? The size of this set is also random! Notice that if \\((A_i)_{i\\in [n]}\\) are some events with \\(\\mathbb{P}(A_i)\\le \\delta\\) and \\(I_1,\\dots,I_k\\in [n]\\) are random indices, it does not hold that \\(\\mathbb{P}( \\cup_{j=1}^k A_{I_j}) \\le k \\delta\\): One cannot apply the union bound to randomly chosen events. In fact, in the worst case, \\(\\mathbb{P}( \\cup_{j=1}^k A_{I_j}) = n \\delta\\). To exploit that \\(\\mathcal{S}_{H-1}\\) is a small set, we need to use one more time the structure. The reason that the randomness of \\(\\mathcal{S}_{H-1}\\) is not going to matter too much is because of the special way this set is constructed. First of all, clearly, \\(s_0\\in \\mathcal{S}_{H-1}\\) always and at this state the error \\(\\|(\\hat T q^*)(s_0,\\cdot)- Tq^*(s_0,\\cdot)\\|_\\infty\\) is under control by Hoeffding’s inequality. Next, we may consider the neighbors of \\(s_0\\). If \\(S\\in \\mathcal{C}(s_0)\\), either \\(S=s_0\\), in which case we already know that the error at \\(S\\) is under control, or \\(S\\) is a “bona fide neighbor” and we can think of then generating the elements in \\(\\mathcal{C}(S,a)\\) just inside the call of $q$. Ultimately, the error at such a neighbor is under control because, by definition, all the sets \\(\\mathcal{C}(s,a)\\) (with $(s,a)$ sweeping through all possible state-action pairs) are independently chosen. This suggests that we should consider the chronological order in which in the recursive call of function $q$ the states in \\(\\mathcal{S}_{H-1}\\) appear. Let this order be $S_1,S_2,\\dots,S_n$, where $n = 1+(mA)+\\dots+(mA)^{H-1}$, $S_1=s_0$, $S_2$ is the second state that $q$ is called on (necessarily, \\(S_2\\in \\mathcal{C}(s_0)\\)), \\(S_3\\) is the third such state. Note that states may reappear in this sequence multiple times. Furthermore, by construction, \\(\\mathcal{S}_{H-1} = \\{ S_1,\\dots,S_n \\}\\). Also note that the length of this sequence is not random: This length is exactly the number of times $q$ is called, which is clearly not random. That \\(\\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-1}}=\\| \\hat T q^* - Tq^* \\|_{\\mathcal{S}_{H-1}}\\) is under control directly follows from the next lemma: . Lemma: Assume that the immediate rewards belong to the $[0,1]$ interval. For any $0\\le \\zeta \\le 1$ with probability $1-\\mathrm{A}n\\zeta$, for any $1\\le i \\le n$, . \\[\\begin{align*} \\| \\hat T q^* (S_i, \\cdot)- q^* (S_i,\\cdot) \\|_{\\infty} \\le \\Delta(\\zeta,m)\\,, \\end{align*}\\] where \\(\\Delta\\) is given by \\(\\eqref{eq:basicerror}\\). Proof: Recall that $\\mathcal{C}(s,a) = (S_1’(s,a),\\dots,S_m’(s,a))$ where (i) the \\((\\mathcal{C}(s,a))_{(s,a)}\\) are mutually independent and (ii) for any $(s,a)$, $(S_i’(s,a))_i$ is an i.i.d. sequence with common distribution $P_a(s)$. For \\(s\\in \\mathcal{S}\\), \\(a\\in \\mathcal{A}\\), \\(C\\in \\mathcal{S}^m\\), let . \\[\\begin{align*} g(s,a,C) =| \\frac{\\gamma}{m} \\sum_{s'\\in C} v^*(s') \\,\\, - \\langle P_a(s), v^* \\rangle | \\end{align*}\\] (as earlier, $s’\\in C$ means that $s’$ is an element of the set composed of the elements in the sequence $C$). Recall that by the definition of $\\hat T$ and the properties of $q^*$, . \\[\\begin{align} |\\hat T q^*(s,a)- q^*(s,a)| = | \\frac{\\gamma}{m} \\sum_{s'\\in \\mathcal{C}(s,a)} v^*(s') \\,\\, - \\langle P_a(s), v^* \\rangle | = g(s,a,\\mathcal{C}(s,a)) \\,. \\label{eq:dub} \\end{align}\\] Fix \\(1 \\le i \\le n\\). Let \\(\\tau = \\min\\{ 1\\le j \\le i\\,:\\, S_j = S_i \\}\\). That is, $\\tau$ is the time when $S_i$ first appears in the sequence \\(\\{S_i\\}_i\\). Fix $a\\in \\mathcal{A}$. We claim that given \\(S_{\\tau}\\), \\((S_j'(S_{\\tau},a))_{j=1}^m\\) is i.i.d. with common distribution \\(P_a(S_{\\tau})\\). That is, for any \\(s,s_1',\\dots,s_m'\\in \\mathcal{S}\\), . \\[\\begin{align} \\mathbb{P}( S_1'(S_{\\tau},a)=s_1',\\dots, S_m'(S_{\\tau},a)=s_m' \\, \\vert\\, S_{\\tau}=s) = \\prod_{j=1}^m P(s,a,s_j') \\label{eq:indep} \\end{align}\\] Note that given this, for any $\\Delta\\ge 0$, by \\(\\eqref{eq:dub}\\), . \\[\\begin{align*} \\mathbb{P}( &amp; |\\hat T q^*(S_i,a)- q^*(S_i,a)| &gt; \\Delta ) = \\mathbb{P}( g(S_i,a,\\mathcal{C}(S_i,a)) &gt; \\Delta ) \\\\ &amp; = \\mathbb{P}( g(S_{\\tau},a,\\mathcal{C}(S_\\tau,a)) &gt; \\Delta ) \\\\ &amp; = \\sum_{s} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, S_{\\tau}=s ) \\\\ &amp; = \\sum_{s} \\sum_{1\\le j \\le i} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, S_j=s, \\tau=j ) \\\\ &amp; = \\sum_{s} \\sum_{1\\le j \\le i} \\sum_{\\substack{s_{1:j-1} \\in \\mathcal{S}^{j-1}:\\\\ s\\not\\in s_{1:j-1}}} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, S_j=s, S_{1:j-1}=s_{1:j-1})\\\\ &amp; = \\sum_{s} \\sum_{1\\le j \\le i} \\sum_{\\substack{s_{1:j-1} \\in \\mathcal{S}^{j-1}:\\\\ s\\not\\in s_{1:j-1}}} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, \\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1 )\\,, \\end{align*}\\] for some binary valued functions $\\phi_1$, $\\dots$, $\\phi_i$ where for $1\\le j \\le i$, $\\phi_j$ is defined so that . \\[\\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1\\] holds if and only if $S_j=s, S_{1:j-1}=s_{1:j-1}$ holds, where $s\\in \\mathcal{S}$ and $s_{1:j-1}\\in \\mathcal{S}^{j-1}$ are arbitrary so that $s\\not\\in s_{1:j-1}$. That such functions exist follows because for any sequence $s_{1:j}$ to verify whether $S_{1:j}=s_{1:j}$ the knowledge of the sets $\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1})$ suffices: The appropriate function should first check $S_1=s_1$, then move on to checking $S_2=s_2$ only if $S_1=s_1$ holds, etc. Now, notice that by our assumptions, for $s\\not\\in s_{1:j-1}$, $\\mathcal{C}(s,a)$ and $\\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1$ are independent of each other. Hence, . \\[\\begin{align*} \\mathbb{P}( &amp; g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, \\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1 )\\\\ &amp; = \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta) \\cdot \\mathbb{P}(\\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1 )\\,. \\end{align*}\\] Plugging this back into the previous displayed equation, “unrolling” the expansion done using the law of total probability, we find that . \\[\\begin{align*} \\mathbb{P}( |\\hat T q^*(S_i,a)- q^*(S_i,a)| &gt; \\Delta ) &amp; = \\sum_{s} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta ) \\mathbb{P}( S_\\tau = s )\\,. \\end{align*}\\] Now, choose $\\Delta = \\Delta(\\zeta,m)$ from \\(\\eqref{eq:basicerror}\\) so that, thanks to $|q^*|_\\infty \\le 1/(1-\\gamma)$, for any fixed $(s,a)$, \\(\\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta(\\zeta,m) )\\le \\zeta\\) Plugging this in into the previous display we get . \\[\\begin{align*} \\mathbb{P}( |\\hat T q^*(S_i,a)- q^*(S_i,a)| &gt; \\Delta(\\zeta,m) ) \\le \\zeta \\sum_{s} \\mathbb{P}( S_\\tau = s ) = \\zeta\\,. \\end{align*}\\] The claim the follows by a union bound over all actions and all $1\\le i \\le n$. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec6/#error-control",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec6/#error-control"
  },"268": {
    "doc": "6. online planning - Part II.",
    "title": "Final error bound",
    "content": "Putting everything together, we get that for any $0\\le \\zeta \\le 1$, the policy $\\hat \\pi$ induced by the planner is $\\epsilon(m,H,\\zeta)$-optimal with . \\[\\epsilon(m,H,\\zeta):=\\frac{2}{(1-\\gamma)^2} \\left[\\gamma^H + \\frac{1}{1-\\gamma} \\sqrt{ \\frac{\\log\\left(\\frac{2n\\mathrm{A}}{\\zeta}\\right)}{2m} } + \\zeta \\right]\\,.\\] Thus, to obtain a planner that induces a $\\delta$-optimal policy, we can set $H$, $\\zeta$ and $m$ so that each term above contributes at most $\\delta/3$: . \\[\\begin{align*} \\frac{2\\gamma^H}{1-\\gamma} &amp; \\le (1-\\gamma)\\frac{\\delta}{3}\\,,\\\\ \\zeta &amp; \\le (1-\\gamma)^2\\frac{\\delta}{6}\\, \\qquad \\text{and}\\\\ \\frac{m}{\\log\\left(\\frac{2n\\mathrm{A}}{\\zeta}\\right)} &amp; \\ge \\frac{18}{\\delta^2(1-\\gamma)^6}\\,. \\end{align*}\\] For $H$ we get that we can set $H = \\lceil H_{\\gamma,(1-\\gamma)\\delta/6}\\rceil$. We can also set $\\zeta = (1-\\gamma)^2\\delta/6$. To solve for the smallest $m$ that satisfies the last inequality, recall that $n = (mA)^H$. To find the critical value of $m$ note the following elementary result which we cite without a proof: . Proposition: Let $a&gt;0$, $b\\in \\mathbb{R}$. Let \\(t^*=\\frac{2}{a}\\left[ \\log\\left(\\frac1a\\right)-b \\right]\\). Then, for any positive real $t$ such that $t\\ge t^*$, . \\[\\begin{align*} at+b &gt; \\log(t)\\,. \\end{align*}\\] . From this, defining . \\[c_\\delta = \\frac{18}{\\delta^2(1-\\gamma)^6}\\] and . \\[\\begin{align} m^*(\\delta,\\mathrm{A}) = 2c_\\delta \\left[ H \\log(c_\\delta H) + \\log\\left(\\frac{12}{(1-\\gamma)^2\\delta}\\right) + (H+1) \\log(\\mathrm{A}) \\right] \\label{eq:mstar} \\end{align}\\] if $m \\ge m^*$ then all the inequalities are satisfied. Putting things together, we thus get the following result: . Theorem: Assume that the immediate rewards belong to the $[0,1]$ interval. There is an online planner such that for any $\\delta\\ge 0$, in any discounted MDP with discount factor $\\gamma$, the planner induces a $\\delta$-optimal policy and uses at most \\(O( (m^* \\mathrm{A})^H )\\) elementary arithmetic and logic operations per its calls, where $m^*(\\delta,\\mathrm{A})$ is given by \\(\\eqref{eq:mstar}\\) and $H = \\lceil H_{\\gamma,(1-\\gamma)\\delta/3}\\rceil$. Overall, we see that the runtime did increase compared to the deterministic case (apart from logarithmic factors, in the above result $m = H^7/\\delta^2$ whereas in the deterministic case $m=1$!), but we managed to get a runtime that is independent of the cardinality of the state space. Again, what is troubling is the exponential dependence on the effective horizon, though as we have seen, in the worst-case, this is unavoidable. In the next lectures we will consider proving the planner with extra information so that this exponential dependence can be avoided. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec6/#final-error-bound",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec6/#final-error-bound"
  },"269": {
    "doc": "6. online planning - Part II.",
    "title": "Notes",
    "content": "Sparse lookahead trees . The idea of the algorithm that we analyzed comes from a paper by Kearns, Mansour and Ng from 2002. In their paper they consider the version of the algorithm which creates a fresh “new” random set $\\mathcal{C}(s,a)$ in every recursive call. This makes it harder to see their algorithm as approximating the Bellman operator, but in effect, the two approaches are by and large the same. In fact, if we introduce $H$ random operators, $\\hat T_1$, $\\dots$, $\\hat T_H$ which are the same as $\\hat T$ above but $\\hat T_h$ has its own “private” sets $( \\hat C_h(s,a) )_{(s,a)}$, then their algorithm can be written as computing . \\[A = \\arg\\max_{a} (\\hat T_1 \\dots \\hat T_h \\boldsymbol{0})(s_0,a)\\,.\\] It is not hard to modify the analysis given here to accommodate this change. With this, one can also interpret the calculations done by the algorithm as backing up values in a “sparse lookahead tree” built recursively from $s_0$. Much work has been devoted to improving these basic ideas and eventually these ideas led to various Monte-Carlo tree search algorithms, including yours truly’s UCT. In general, these algorithms attempt to improve on the runtime by building the trees when they need to be built. As it turns out, a useful strategy here is to expand nodes which in a way hold the greatest promise to improve the value at the “root”. This is known as the “optimisism in planning”. Note that A* (and its MDP relative, AO) are also based on optimism: A’s admissible heuristic functions in our language correspond to functions that upper bound the optimal value. The definite source on MCTS theory as of today is Remi Munos’s monograph. Measure concentration . Hoeffding’s inequality is a special case of what is known as measure concentration. This phrase refers to that the empirical measure induced by a sample is a good approximation to the whole measure. The simplest case is when one just compares the means of the measures (the empirical and the sample-generating one), giving rise to concentration inequalities around the mean. Hoeffding’s inequality is an example. What we like about Hoeffding’s inequality (besides that it is simple) is that the failure probability, $\\delta$ (later $\\zeta$) appears inside a logarithm. That means, that the price of being more stringent is mild. When the exact dependence is of type that appears in Hoeffding’s inequality (i.e., $\\sqrt{ \\log(1/\\delta)})$), we say that the deviation of the subgaussian type because Gaussian random variables also satisfy an inequality like this. Concentration of measure and concentration inequalities are a central topic in probability theory, with separate books devoted to them. A few favourites are given at the end of this notes . For learning purposes, Pollard’s mini-book is nice (but all these books have pros and cons), or Vershynin’s book. The comparison inequality . The comparison inequality between the logarithm and the linear function is given as Proposition 4 here. The proof is based on two observations: First, it is enough to consider the case when $b=0$. Then, if $a\\ge 1$, the result is trivial, while for $a&lt; 1$, the guess is based on doubling the value where the growth rate of $t\\mapsto at$ matches that of $t\\mapsto \\log(t)$. A model-centered view and random operators . A key idea of this lecture is that $\\hat T$ is a good (random) approximation to $T$, hence, it can be used in place of $T$. One can also tell this story by saying that the data underlying $\\hat T$ gives a random approximation to the MDP; the transition probabilities of this random approximating MDP would be defined using . \\[\\hat P(s,a,s') = \\frac1m \\sum_{s''\\in C(s,a)} \\mathbb{I}\\{ s''=s'\\}\\] It may seem quite miraculous that with only a few elements in $C(s,a)$ (i.e., small $m$) we get a good approximation to the next state distribution. But so is the magic of randomness! Using a random operator (or a sequence of them, if, as outlined above, one uses a fresh set of random next state every time an update is calculated) in a dynamic programming method has been coined empirical dynamic programming by Haskell et al.. A bigger point is that for a model to be a “good” approximation to the “true MDP”, it suffices that the Bellman optimality operator that it induces is a “close” approximation to the Bellman optimality operator of the true MDP. This in fact brings us to our next topic, which is what happens when the simulator is imperfect? . Imperfect simulation model? . We can rarely expect simulators to be perfect. Luckily, not all is lost in this case. As noted above, if the simulator induced an MDP whose Bellman optimality operator is in a way close to the Bellman optimality operator of the true MDP, we expect the outcome of planning to be still a good policy in the true MDP. In fact, the above proof has already all the key elements in place to show this. In particular, it is not hard to show that if $\\hat T$ is a $\\gamma$ max-norm contraction and $\\hat q^*$ is its fixed point then . \\[\\|\\hat q^* - q^*\\|_\\infty \\le \\frac{\\| \\hat T q^* - T q^* \\|_\\infty}{1-\\gamma}\\,,\\] which, combined with the our first lemma of this lecture on the policy error bound gives that the policy that is greedy with respect to $\\hat q^*$ is . \\[\\frac{2\\| \\hat T q^* - T q^* \\|_\\infty }{(1-\\gamma)^2}\\] optimal in the MDP underlying $T$. We will return to this in later lectures. In particular, in batch reinforcement learning, one of the basic methods is to learn a “model” of the environment and as such it is inevitable to study the error that results from modelling errors. See Lecture 17 and Lecture 18. Monte-Carlo methods . We saw in homework 0 that randomization may help a little, and today we saw that it can help in a more significant way. A major lesson again is that representations do matter: If the MDP is not given with a “generative simulator”, getting such a simulator may be really hard. This is good to remember when it comes to learning models: . One should insist on learning models that make the job of planners easier. Generative models are one such case, provably, as we have seen in today’s lecture put together with our previous lower bound that involved the number of states. Randomization, more generally, is a powerful tool in computing science, which brings us to a somewhat philosophical question: What is randomness? Does “true randomness” exist? Can we really build computers to harness this? . True randomness? . What is the meaning of “true” randomness? The margin is definitely not big enough to explain this. Hence, we just leave this there, hanging, for everyone to ponder about. But let’s also note that this is a thoroughly studied question in theoretical computing science, with many beautiful results and even books. Arora and Barak’s book on computational complexity (Chapters 7, 20 and 21) is a good start for exploring this. Can we recycle the sets $C(s,a)$ between the calls? . If simulation is expensive, it may be tempting to recycle the sets between calls of the planner. After all, even if we recycle these sets, \\(\\hat{\\pi}\\) will have the property that it selects $\\epsilon$-optimizing actions with high probability at every state. However, this may not be a good idea. The reader is challenged to think about what can go wrong? The proof actually uses that the planner construct a new random operator $\\hat T$ with every call. But where is this used? . The ubiquity of continuity arguments in the MDP literature . All the computations that we do with MDPs tend to be approximate. We evaluate policies approximately. We compute a Bellman back approximately. We have approximate models. We greedify approximately. If any of these operations could enlarge small errors, none of the approximate methods would work. The study of approximate computations (which is a necessity if one faces large MDPs) is a study of the sensitivity of the values of the resulting policies to the errors introduced in the computations. This, in numerical analysis, would be called error analysis. In other areas of mathematics, this is called sensitivity analysis. In fact, sensitivity analysis often involves computing derivatives to see how fast outputs change as the inputs change (which is that data that will be approximated). What should we be taking derivatives with respect to here? Well, it is always the data that is being changed. One can in fact use differentiation based sensitivity analysis everywhere. This has been tried a little in the “older” MDP literature and is also related to policy gradient theorems (that we will learn about laters). However, perhaps there are more nice things to be discovered about this approach. From local to online access . The algorithm that is analyzed in this lecture requires local access simulators. This is better than requiring global access, but worse than requiring online access. It remains an open question of whether with online access, one can also get a similar result than shown in the present lecture and if not, whether the sample complexity of planning remains finite under this setting. When the state space is small . For finite state-action MDPs where the rewards and transition probabilities are represented using tables, a previous lecture’s main result established that an optimal policy of the MDP can be calculated by using at most $O(H\\textrm{poly}(S,A))$ arithmetic and logic operations ($H=1/(1-\\gamma)$ here). In the current lecture we saw that even when $S$ is unbounded, given a simulator with local access, $\\tilde{O}((AH^7/\\delta^2)^H)$ such elementary operations and calls to a simulator are sufficient. In a finite MDP, depending on the values of $S,A$ and $H$, either policy iteration, or the online planner that builds the tree will be faster. But policy iteration (and value iteration) as described previously used a table representation. The question then arises of what is the sample complexity of planning with a simulator access to a finite MDP? If planning means outputting a policy, the complexity needs to scale with $S$. In the presence of global access simulators, a simple approach, is to sample an appropriate number of next states for each state-action pair to build an empirical (but “sparse”) transition model and use this in connection with any MDP solver. We will see later in Lecture 18 that in this case $O(H^3 SA/\\delta^2)$ samples (or $H^3/\\delta^2$ samples per state-action pair) are sufficient to obtain a $\\delta$-optimal policy. In the case of online planning with global access, the sample complexity cannot be worse, but it is unclear whether it can be improved. Similarly, it is unclear what the complexity is in the case of either local or online access. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec6/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec6/#notes"
  },"270": {
    "doc": "6. online planning - Part II.",
    "title": "References",
    "content": ". | Kearns, M., Mansour, Y., &amp; Ng, A. Y. (2002). A sparse sampling algorithm for near-optimal planning in large Markov decision processes. Machine learning, 49(2), 193-208. [link] | David Pollard (2015). A few good inequalities. Chapter 2 of a book under preparation with working title “MiniEmpirical”. [link] | Stephane Boucheron, Gabor Lugosi and Pascal Massart (2012). Concentration inequalities: A nonasymptotic theory of indepndence. Clarendon Press – Oxford. [link] | Roman Vershynin (2018). High-Dimensional Probability: An Introduction with Applications in Data Science. [link] | M. J. Wainwright (2019) High-dimensional statistics: A non-asymptotic viewpoint. Cambridge University Press. | Lafferty J., Liu H., &amp; Wasserman L. (2010). Concentration of Measure. [link] | Lattimore, T., &amp; Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press. | William B. Haskell, Rahul Jain, and Dileep Kalathil. Empirical dynamic programming. Mathematics of Operations Research, 2016. | Sanjeev Arora and Boaz Barak (2009). Computational Complexity: A Modern Approach. Cambridge University Press. | Remi Munos (2014). From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning. Foundations and Trends in Machine Learning: Vol. 7: No. 1, pp 1-129. | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec6/#references",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec6/#references"
  },"271": {
    "doc": "6. online planning - Part II.",
    "title": "6. online planning - Part II.",
    "content": "PDF Version . In the previous lecture online planning was introduced. The main idea is to amortize the cost of planning by asking a planner to produce an action to be taken at a particular state so that the policy induced by repeatedly calling the planner at the states just visited and then using the action returned by the planner is near-optimal. We have seen that with this, the cost of planning can be made independent of the size of the state space – at least for deterministic MDPs. For this, one can use just a recursive implementation of value iteration, which, for convenience, we wrote using action-value functions and the corresponding Bellman optimality operator, $T$, defined by . \\[\\begin{align*} T q(s,a) = r_a(s) + \\gamma \\langle P_a(s), M q \\rangle\\,. \\end{align*}\\] (in the previous lecture we used $\\tilde T$ to denote this operator, but to reduce clutter from now on, we will drop the tilde). We have also seen that no procedure can do significantly better in terms of its runtime (or query cost) than this simple recursive procedure. In this lecture we show that these ideas also extend to the stochastic case. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec6/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec6/"
  },"272": {
    "doc": "7. Function Approximation",
    "title": "Hints on value functions",
    "content": "The hints that we start with will concern the value functions. In particular, they state that either the optimal value, or the value function of all policies are effectively compressible. For motivation, consider the figure on the right. Imagine the state space is an interval of the real line and the optimal value function in an MDP looks like as shown on the figure: It is a nice, smooth function over the interval. As is well known, such relatively slowly changing functions can be well approximated by using the linear combination of a few fixed basis functions, like an appropriate polynomial, or Fourier basis, or using splines. Then, one hopes that even though the state space is large or even infinite as in this example, there could perhaps be a method that calculates the few coefficients needed get a good approximation to \\(v^*\\) with a runtime that depends polynomially on the horizon, the number of actions and the number of coefficients that one needs to calculate. Given the knowledge of $v^*$ and simulator access to the MDP, good actions can then be efficiently obtained by performing one-step lookahead computations. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec7/#hints-on-value-functions",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec7/#hints-on-value-functions"
  },"273": {
    "doc": "7. Function Approximation",
    "title": "Linear function approximation",
    "content": "If the basis functions mentioned are $\\phi_1,\\dots,\\phi_d: \\mathcal{S} \\to \\mathbb{R}$ then, formally, the hope is that with some coefficients $\\theta =(\\theta_1,\\dots,\\theta_d)^\\top\\in \\mathbb{R}^d$, we will have . \\[\\begin{align} v^*(s) = \\sum_{i=1}^d \\theta_i \\phi_i(s)\\, \\qquad \\text{for all } s\\in \\mathcal{S}\\,. \\label{eq:vstr} \\end{align}\\] In the reinforcement learning literature, the vector $(\\phi_1(s), \\dots, \\phi_d(s))^\\top$ is called the feature vector assigned to state $s$. For a more compact notation we also use $\\phi$ to be a map from $\\mathcal{S}$ to $\\mathbb{R}^d$ which assigns the feature vectors to the states: . \\[\\phi(s) = (\\phi_1(s),\\dots,\\phi_d(s))^\\top\\,.\\] Conversely, given $\\phi: \\mathcal{S}\\to \\mathbb{R}^d$, its component are denoted using $\\phi_1,\\dots,\\phi_d$. It will also be useful to introduce a matrix notation: Recall that the number of states is $\\mathrm{S}$ and without loss of generality we may assume that $\\mathcal{S} = [\\mathrm{S}]$. Then, we can treat each of $\\phi_1,\\dots,\\phi_d$ as $\\mathrm{S}$-dimensional vectors: The $i$th component of $\\phi_j$ is $\\phi_j(i)$. Then, we can stack $\\phi_1,\\dots,\\phi_d$ next to each other to form a matrix: . \\[\\Phi = \\begin{pmatrix} | &amp; | &amp; \\dots &amp; | \\\\ \\phi_1 &amp; \\phi_2 &amp; \\dots &amp; \\phi_d \\\\ | &amp; | &amp; \\dots &amp; | \\end{pmatrix} \\in \\mathrm{R}^{\\mathrm{S}\\times d}\\,.\\] That is, $\\Phi$ is a $\\mathrm{S}\\times d$ matrix. The set of real-valued functions over the state space that can be described with the linear combination of the basis functions is . \\[\\mathcal{F} = \\{ f: \\mathcal{S} \\to \\mathbb{R} \\,:\\, \\exists \\theta\\in \\mathbb{R}^d \\text{ s.t. } f(s) = \\langle \\phi(s),\\theta \\rangle \\}\\,.\\] Identifying the space of real-valued functions with the vector space $\\mathbb{R}^{\\mathrm{S}}$ in the natural way, $\\mathcal{F}$ is a $d$-dimensional subspace of $\\mathbb{R}^{\\mathrm{S}}$, which is the same as the “column space”, or the span, or the range space of $\\Phi$: . \\[\\mathcal{F} = \\{ \\Phi \\theta \\,:\\, \\theta\\in \\mathbb{R}^d \\} = \\text{span}(\\Phi)\\] If we need to indicate the dependence of $\\mathcal{F}$ on the choice of features, we will write either \\(\\mathcal{F}_{\\phi}\\) or \\(\\mathcal{F}_{\\Phi}\\). Now, we have three equivalent ways of specifying the “features”, either by specifying the basis functions $\\phi_1,\\dots,\\phi_d$, or the feature-map $\\phi$, or the feature matrix $\\Phi$, and we have a four equivalent way of specifying the functions that can be obtained via the linear combination of features. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec7/#linear-function-approximation",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec7/#linear-function-approximation"
  },"274": {
    "doc": "7. Function Approximation",
    "title": "Delivering the hint",
    "content": "Note that in the above problem description it is tacitly assumed that the feature-map, in some form or another, is available to the planner. In fact, the feature map can be made available in multiple ways. When we argue for lower bounds, especially for query complexity, we often assume that the whole feature-map is available for the algorithm. For upper bounds with online planning, the most natural assumption is that the planner gets from the simulator the feature vector of the states that it encounters. In particular, when it comes to online planning, the natural assumption is that the planner gets the feature vector of the initial state together with the state and with any subsequent calls to the simulator, the simulator returns the feature vector of the next states, together with the next states. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec7/#delivering-the-hint",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec7/#delivering-the-hint"
  },"275": {
    "doc": "7. Function Approximation",
    "title": "Typical hints",
    "content": "In what follows we will study planning under a number of different hints (or assumptions) that connect the MDP and a feature-map. The simplest of this just states that $\\eqref{eq:vstr}$ holds: . Assumption A1 ($v^*$-realizibility): The MDP $M$ and the featuremap $\\phi$ are such that \\(v^* \\in \\mathcal{F}_\\phi\\) . A second variation is when all value functions are realizable: . Assumption A2 (universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(v^\\pi \\in \\mathcal{F}_\\phi\\). Clearly, A2 implies A1, because by the fundamental theorem of MDPs, there exists a memoryless policy \\(\\pi\\) such that \\(v^\\pi = v^*\\). The figure on the right illustrates the set of all finite MDPs with some state space and within those the set of those MDPs that satisfy A1 with a specific feature map $\\phi$ (denoted by A1\\(\\mbox{}_\\phi\\) on the figure), as well as those MDPs that satisfy A2 with the same feature map (denoted by A2\\(\\mbox{}_\\phi\\)). Both of these sets represent a very small fraction of all MDPs. However, of one changes the feature map, the union of all these sets clearly covers the set of all MDPs: The hint is general. There are many variations of these assumptions. Often, we will find it useful to relax the assumption value functions are exactly realizable. Under the modified assumptions the value function does not need to lie in the span of the feature-map, but only in some vicinity of it. The natural error metric to be used is the maximum norm for reasons that will become clear later. To help with stating these assumptions in a compact form, introduce the notation . \\[v\\in_{\\varepsilon} \\mathcal{F}\\] to denote that . \\[\\inf_{f\\in \\mathcal{F}} \\| f - v \\|_\\infty \\le \\epsilon\\,.\\] That is, $v\\in_{\\varepsilon} \\mathcal{F}$ means that the best approximator to $v$ from $\\mathcal{F}$ approximates it within a uniform error of $\\varepsilon$. Fixing $\\varepsilon\\ge 0$ and replacing $\\in$ with $\\in_{\\varepsilon}$ in the above two assumptions gives the following: . Assumption A1$\\mbox{}_{\\varepsilon}$ (approximate $v^*$ realizability): The MDP $M$ and the featuremap $\\phi$ are such that \\(v^* \\in_{\\varepsilon} \\mathcal{F}_\\phi\\) . Assumption A2$\\mbox{}_{\\varepsilon}$ (approximate universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(v^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\phi\\). ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec7/#typical-hints",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec7/#typical-hints"
  },"276": {
    "doc": "7. Function Approximation",
    "title": "Action-value hints",
    "content": "We obtain new variants if we consider feature-maps that map state-action pairs to vectors. Concretely, (by abusing notation) let $\\phi: \\mathcal{S}\\times\\mathcal{A}\\to \\mathbb{R}^d$. Then, the analog of A1 is as follows: . Assumption B1 ($q^*$-realizibility): The MDP $M$ and the featuremap $\\phi$ are such that \\(q^* \\in \\mathcal{F}_\\phi\\) . Here, as expected, $\\mathcal{F}_\\phi$ is defined as the set of functions that lie in the span of the feature-map. The analog of A2 is as follows: . Assumption B2 (universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(q^\\pi \\in \\mathcal{F}_\\phi\\). We can also introduce positive approximation errors $\\varepsilon&gt;0$, which lead to B1\\(_{\\varepsilon}\\) and B2\\(_{\\varepsilon}\\): . Assumption B1$\\mbox{}_{\\varepsilon}$ (approximate $q^*$-realizibility): The MDP $M$ and the featuremap $\\phi$ are such that \\(q^* \\in_{\\varepsilon} \\mathcal{F}_\\phi\\) . Assumption B2$\\mbox{}_{\\varepsilon}$ (approximate universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(q^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\phi\\). One may wonder why not choose one of these assumptions? When one assumption implies another, then clearly there is a preference to choose the weaker assumption. But often, there is going to be a price and sometimes the assumptions are just not comparable. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec7/#action-value-hints",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec7/#action-value-hints"
  },"277": {
    "doc": "7. Function Approximation",
    "title": "Notes",
    "content": "Origin . The idea of using value function approximation in planning dates back to at least the 1960s if not earlier. I include some intriguing early references at the end. That these ideas already appeared at the down of computing where computers hardly even existed is quite intriguing. Infinite spaces . Function approximation is especially appealing when the state space, or the action space, or both are “continuous” (i.e., they are a subset of a Euclidean space). In this case, the compression is “infinite”. Experimental evidence suggests that function approximation can work quite well in the context of MDP planning in a surprisingly large number of different scenarios. When the spaces are infinite, all the “math” will still go through, except that occasionally one has to be a bit more careful. For example, one cannot clearly say that $\\Phi$ is a matrix, but $\\Phi$ can clearly be defined as a linear operator mapping $\\mathbb{R}^d$ to the vector space of all real-valued functions over the (say) state space (when the feature map is also over states). Where do the features come from? . It will be instructive to start with a special case. Low-rank MDPs are those where the transition kernel factorizes: For any $s,a,s’$ state-action-state triple, . \\[P(s'|s,a) = \\langle \\phi(s,a), \\nu(s') \\rangle\\] for some $\\phi: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}^d$ and $\\nu(s’)\\in \\mathbb{R}^d$. If in addition to the above, . \\[\\begin{align} r(s,a) = \\langle \\phi(s,a), \\nu' \\rangle \\label{eq:rfact} \\end{align}\\] also holds for some $\\nu’\\in \\mathbb{R}^d$, it is not hard to see that any action-value function lies in the space of the features $\\phi$. But what are the cases when the transition kernel factorizes? (If the transition kernel factorizes with some feature map $\\phi_0$, one can always arrange for $\\eqref{eq:rfact}$ to hold by adding an extra dimension to the feature map, filled with the values of the rewards.) A simple case is when state-action pairs can be clustered into non-overlapping groups such that for any two pairs $(s_1,a_1),(s_2,a_2)$ that belong to the same group, the transitions are identical: $P(\\cdot|s_1,a_1) = P(\\cdot|s_2,a_2)$. Assuming $d$ groups number from $1$ to $d$, $\\phi_i(s,a)$ can be chosen as the indicator that $(s,a)$ belongs to the $i$th group ($i\\in [d]$). Another interesting case which leads to a factored transition kernel is when the state-space is $\\mathbb{R}^p$ with some $p&gt;0$ and the dynamics takes the form . \\[S_{t+1} = f(S_t,A_t) + \\eta_{t+1}\\] with some function $f$, and $(\\eta_t)_t$ is a sequence of independent random variables with common density $g$. Then, the transition kernel takes the form \\(P(ds'|s,a) = g(s'-f(s,a)) ds'\\). The important point here is that the noise introduced is homoscedastic (does not change with $(s,a)$). Take, for example, the case when $g(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-x^2/2)$, i.e., $(\\eta_t)_t$ are standard normal random variables. It is well known then that . \\[g(x-y) = \\langle u(x;\\cdot,\\cdot), u(y;\\cdot,\\cdot) \\rangle\\,,\\] where . \\[u(x;\\omega,b) = \\sqrt{2} \\cos(\\omega^\\top x + b )\\] and for $n,m: \\mathcal{D} \\to \\mathbb{R}$, $\\mathcal{D}:=\\mathbb{R}^p \\times [0,2\\pi]$, . \\[\\langle n,m \\rangle = \\int_{\\mathbb{R}^p}\\, \\frac{1}{2\\pi} \\int_0^{2\\pi} n(\\omega,b) m(\\omega,b) \\, \\, db \\, \\prod_{i=1}^p g(\\omega_i)\\, d\\omega \\,.\\] From this, we get . \\[P(ds'|s,a) = g(s'-f(s,a)) ds' = \\langle u(s';\\cdot,\\cdot), u(f(s,a);\\cdot,\\cdot) \\rangle ds'\\,.\\] It follows that if we define \\(\\phi: \\mathcal{S}\\times\\mathcal{A} \\to \\mathbb{R}^{\\mathcal{D}}\\) via . \\[(\\phi(s,a))(\\omega,b) = u(f(s,a);\\omega,b)\\] then . \\[P(ds'|s,a) = \\langle \\phi(s,a), u(s';\\cdot,\\cdot) \\rangle ds'\\,,\\] which is the same as above, except here $\\phi$ is infinite dimensional. In a way, what happens here is that the noise introduces smoothness of the value functions. Smoothness of value functions can arise in some other ways. In the related topic of numerical computation of solutions of partial differential equations, Galerkin’s method also starts from assuming that the solution lies in the span of some features. In the relavant literature, various methods have been proposed to find appropriate features (or, basis functions, as they are called there). The book of Quarteroni et. al. gives several methods for automating the construction of these basis functions, and they also make a connection to optimal control. Nonlinear value function approximation . The most successful use of the idea of compressing value functions uses neural networks. Readers are most likely are already familiar with the ideas underlying neural networks. The hope here is that whatever we find in the case of linear function approximation will have implications in how to use nonlinear function approximation in MDP planning. In a way, the very first question is whether one can decouple the design of the planning algorithm from what function approximation technique it is used with. We will study this question by asking for planners that work with any feature map. If we find that we can identify planners that are performant no matter the feature map, the decoupling is successful and we can hope that the ideas will generalize to nonlinear function approximation. However, if we find that successful planners need to use intricate properties of the feature maps, then this is must be taken as a warning that complications may arise when the results are generalized to nonlinear function approximation. In any case, it appears to be a prudent strategy to first investigate the simpler, more straightforward linear case, before considering the nonlinear case. Computation with advice/Non-uniform Computation . Computation with advice is a general approach in computer science where a problem of computing a map is changed to computing a map which has an additional input, the advice. Clearly, the approach taken here can be seen as a special case of computation with advice. There is also the closely related notion of non-uniform computation studied in computability/complexity theory. In non-uniform computation, the Turing machine, in addition to its input, also receives some “advice” string. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec7/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec7/#notes"
  },"278": {
    "doc": "7. Function Approximation",
    "title": "References",
    "content": "The classical reference is a paper of Bellman et al. from 1963, where they proposed to use linear function approximation in a specific context for approximating the optimal value functions (Bellman et al. 1963). Other early papers are by Daniel (1976) and Schweitzer and Seidmann (1985). In the latter paper, the authors generalized the earlier constructions of Bellman and others and, with modern terminology, they introduced fitted value iteration, fitted policy iteration and approximate linear programming as possible approaches. The observation that homoscedastic noise makes it so that the transition kernel factorizes is due to Ren et al. (2022). The book of Quarteroni et al. (2016) describes various methods for automating the construction of basis functions for the solution of parametric family of partial differential equations. | Richard Bellman, Robert Kalaba and Bella Kotkin. 1963. Polynomial Approximation–A New Computational Technique in Dynamic Programming: Allocation Processes. Mathematics of Computation, 17 (82): 155-161 | Daniel, James W. 1976. “Splines and Efficiency in Dynamic Programming.” Journal of Mathematical Analysis and Applications 54 (2): 402–7. | Schweitzer, Paul J., and Abraham Seidmann. 1985. “Generalized Polynomial Approximations in Markovian Decision Processes.” Journal of Mathematical Analysis and Applications 110 (2): 568–82. | Brattka, Vasco, and Arno Pauly. 2010. Computation with Advice. arXiv [cs.LO]. | Quarteroni, Alfio, Andrea Manzoni, and Federico Negri. “Reduced Basis Methods for Partial Differential Equations”. Springer International Publishing. 2016. | Ren, T., T. Zhang, C. Szepesvári, and B. Dai. 2022. “A Free Lunch from the Noise: Provable and Practical Exploration for Representation Learning.” UAI. abstract | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec7/#references",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec7/#references"
  },"279": {
    "doc": "7. Function Approximation",
    "title": "7. Function Approximation",
    "content": "PDF Version . Our lower bound for online planners show that there are no online planners that lead to good policies in all MDPs while satisfying the following three requirements . | the planner induces policies that achieve some positive fraction of the optimal value in all MDPs; | the per-state runtime shows polynomial dependence on the planning horizon $H$ and | it shows a polynomial dependence on the number of actions and | it shows no dependence on the number of states in the MDP. | . Thus, one is left with no choice than to give up on one of the requirements. Since efficiency is clearly nonnegotiable (otherwise the runner just would not be practical), the only requirement that can be replaced is the first one. In what follows we will look at ways of relaxing this requirement. In all the relaxations we will look at, we will essentially restrict the set of MDPs that the planner is expected to work on. However, we will do this in such a way that no MDP will be ever ruled out. We achieve this by giving the planner some extra hint about the MDP and we demand good performance only when the hint is correct. Since the hint will take a general form, some hint is always correct for any MDP. Hence, no MDP is left behind and the planner can again demanded to be efficient and effective. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec7/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec7/"
  },"280": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Approximate Policy Evaluation: Done Well",
    "content": "Recall that in phase $k$ of policy iteration, given a policy $\\pi_k$, the next policy $\\pi_{k+1}$ is obtained as the policy that is greedy with respect to $q^{\\pi_k}$. If we found some coefficients $\\theta_k\\in \\mathbb{R}^d$ such that . \\[\\begin{align*} q^{\\pi_k} \\approx \\Phi \\theta_k\\,, \\end{align*}\\] then when it comes to “using” policy $\\pi_{k+1}$, we could just use $\\arg\\max_{a} \\langle \\theta_k,\\varphi(s,a)\\rangle$ when an action is needed at state $s$. Note that this action can be obtained at the cost of $O(d)$ elementary operations, a small overhead compared to a table lookup (with idealized $O(1)$ access times). Hence, the main question is how to obtain this parameter in an efficient manner. To be more precise, here we want to control the uniform error committed in approximating $q^{\\pi_k}$. To simplify the notation, let $\\pi = \\pi_k$. A simple idea is rolling out with the policy $\\pi$ from a fixed set $\\mathcal{C}\\subset \\mathcal{S}\\times \\mathcal{A}$ to “approximately” measure the value of $\\pi$ at the pairs in $\\mathcal{C}$. For concreteness, let $(s,a)\\in \\mathcal{C}$. Rolling out with policy this pair means using the simulator to simulate what would happen if we used policy $\\pi$ for a number of consecutive time steps when the initial state is $s$, the first action $a$, but for subsequent time steps the actions are chosen using policy $\\pi$ for whatever states are encountered. If the simulation goes on for $H$ steps, this way we get \\(m\\) trajectories starting in \\(z = (s, a)\\). For $1\\le j \\le m$ let the trajectory obtained be \\(\\tau_\\pi^{(j)}(s, a)\\). Thus, . \\(\\begin{align*} \\tau_\\pi^{(j)}(s, a) = \\left( S_0^{(j)}, A_0^{(j)}, S_1^{(j)}, A_1^{(j)}, \\ldots, S_{H-1}^{(j)}, A_{H-1}^{(j)} \\right)\\, \\end{align*}\\), . where \\(S_0^{(j)}=s\\), \\(A_0^{(j)}=a\\), and for $1\\le t \\le H-1$, \\(S_{t}^{(j)} \\sim P_{A_t^{(j)}} ( S_{t-1}^{(j)} )\\), and \\(A_t^{(j)} \\sim \\pi ( \\cdot | S_{t}^{(j)} )\\). The figure on the right illustrates these trajectories. Given these trajectories, the empirical mean of the discounted sum of rewards along these trajectories is used for approximating $q^\\pi(z)$: . \\[\\begin{align} \\hat R_m(z) = \\frac{1}{m} \\sum_{j=1}^m \\sum_{t=0}^{H-1} \\gamma^t r_{A_t^{(j)}}(S_t^{(j)}). \\label{eq:petargetsbiased} \\end{align}\\] Under the usual condition that the rewards are in the $[0,1]$ interval, the expected value of $\\hat{q}^\\pi(z)$ is in the $\\gamma^H/(1-\\gamma)$ vicinity of the $q^\\pi(z)$ and by averaging a large number of independent trajectories, we also achieve that the empirical means are tightly concentrated around their mean. Using a randomization device, it is possible to remove the error (“bias”) introduced by truncating the trajectories at a fixed time. For this, just let $(H^{(j)})_{j}$ be independent geometrically distributed random variables with parameter $1-\\gamma$, which are also independently chosen from the trajectories. By definition \\(H^{(j)}\\) is the number of $1-\\gamma$-parameter Bernoulli trials needed to get one success. With the help of these variables, define now $\\hat R_m(z)$ by . \\[\\begin{align} \\hat R_m(z) = \\frac{1}{m} \\sum_{j=1}^m \\sum_{t=0}^{H^{(j)}-1} r_{A_t^{(j)}}(S_t^{(j)})\\,. \\label{eq:petargetsunbiased} \\end{align}\\] Note that in the expression of \\(\\hat R_m(z)\\) the discount factor is eliminated. To calculate \\(\\hat R_m(z)\\) one can just perform a rollout with policy $\\pi$ as before, just in each time step $t=0,1,\\dots$, after obtaining $r_{A_t^{(j)}}(S_t^{(j)})$, draw a Bernoulli variable with parameter $(1-\\gamma)$ to decide whether the rollout should continue. To see why the above definition works, fix $j$ and note that by definition, for $h\\ge 1$, \\(\\mathbb{P}(H^{(j)}=h) = \\gamma^{h-1}(1-\\gamma)\\) and thus \\(\\mathbb{P}(H^{(j)}\\ge t+1) = \\gamma^t\\). Therefore, . \\[\\begin{align*} \\mathbb{E}[ \\sum_{t=0}^{H^{(j)}-1} r_{A_t^{(j)}}(S_t^{(j)}) ] &amp; = \\sum_{t=0}^\\infty \\mathbb{E}[ \\mathbb{I}\\{ t \\le H^{(j)}-1\\} r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = \\sum_{t=0}^\\infty \\mathbb{E}[ \\mathbb{I}\\{ t \\le H^{(j)}-1\\} ]\\, \\mathbb{E}[ r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = \\sum_{t=0}^\\infty \\mathbb{P}( t+1 \\le H^{(j)} )\\, \\mathbb{E}[ r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}[ r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = q^\\pi(z)\\,. \\end{align*}\\] All in all, this means, that we expect that if we solve for the least-squares problem . \\[\\begin{align} \\hat\\theta = \\arg\\min_{\\theta\\in \\mathbb{R}^d} \\sum_{z\\in \\mathcal{C}} \\left( \\langle \\theta,\\varphi(z) \\rangle - \\hat R_m(z)\\right)^2\\,, \\label{eq:lse} \\end{align}\\] we expect $\\Phi \\hat\\theta$ to be a good approximation to $q^\\pi$. Or at least, we can expect this hold at the points of $\\mathcal{C}$, where we are taking our measurements. The question is what happens outside of $\\mathcal{C}$: That is, what guarantees can we get for extrapolating to points of $\\mathcal{Z}:= \\mathcal{S}\\times \\mathcal{A}$. The first thing to observe that unless we are choosing $\\mathcal{C}$ carefully, there is no guarantee about the extrapolation error will be kept under control. In fact, if the choice of $\\mathcal{C}$ is so unfortunate that all the feature vectors for points in $\\mathcal{C}$ are identical, the least-squares problem will have many solutions. Our next lemma gives an explicit error bound on the extrapolation error. For the coming results we slightly generalize least-squares by introducing a weighting of the various errors in \\(\\eqref{eq:lse}\\). For this, let $\\varrho: \\mathcal{C} \\to (0,\\infty)$ be a weighting function assigning a positive weight to the various error terms and let . \\[\\begin{align} \\hat\\theta = \\arg\\min_{\\theta\\in \\mathbb{R}^d} \\sum_{z\\in \\mathcal{C}} \\varrho(z) \\left( \\langle \\theta,\\varphi(z) \\rangle - \\hat R_m(z)\\right)^2 \\label{eq:wlse} \\end{align}\\] be the minimizer of the resulting weighted squared-loss. A simple calculation gives that provided the (weighted) moment matrix . \\[\\begin{align} G_\\varrho = \\sum_{z\\in \\mathcal{C}} \\varrho(z) \\varphi(z) \\varphi(z)^\\top \\label{eq:mommx} \\end{align}\\] is nonsingular, the solution to the above weighted least-squares problem is unique and is equal to . \\[\\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\hat R_m(z') \\varphi(z')\\,,\\] From this expression we see that there is no loss of generality in assuming that the weights in the weighting function sum to one: \\(\\sum_{z\\in \\mathcal{C}} \\varrho(z) = 1\\). We will denote this by writing $\\varrho \\in \\Delta_1(\\mathcal{C})$ (here, $\\Delta_1$ refers to the fact that we can see $\\varrho$ as an element of a $|\\mathcal{C}|-1$ simplex). To state the lemma recall the notation that for a positive definite, $d\\times d$ matrix $Q$ and vector $x\\in \\mathbb{R}^d$, . \\[\\|x\\|_Q^2 = x^\\top Q x\\,.\\] ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec8/#approximate-policy-evaluation-done-well",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec8/#approximate-policy-evaluation-done-well"
  },"281": {
    "doc": "8. Approximate Policy Iteration",
    "title": "",
    "content": "Lemma (extrapolation error control in least-squares): Fix any \\(\\theta \\in \\mathbb{R}^d\\), \\(\\varepsilon: \\mathcal{Z} \\rightarrow \\mathbb{R}\\), $\\mathcal{C}\\subset \\mathcal{Z}$ and \\(\\varrho\\in \\Delta_1(\\mathcal{C})\\) such that the moment matrix $G_\\varrho$ is nonsingular. Define . \\[\\begin{align*} \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big(\\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z')\\,. \\end{align*}\\] Then, for any \\(z\\in \\mathcal{Z}\\) we have . \\[\\left| \\varphi(z)^\\top \\hat{\\theta} - \\varphi(z)^\\top \\theta \\right| \\leq \\| \\varphi(z) \\|_{G_{\\varrho}^{-1}}\\, \\max_{z' \\in C} \\left| \\varepsilon(z') \\right|\\,.\\] . Before the proof note that what his lemma tells us is that as long as we guarantee that the moment matrix is full rank, the extrapolation errors relative to predicting with some $\\theta\\in \\mathbb{R}^d$ can be controlled by controlling . | the value of \\(g(\\varrho):= \\max_{z\\in \\mathcal{Z}} \\| \\varphi(z) \\|_{G_{\\varrho}^{-1}}\\); and | the maximum deviation of the targets used in the weighted least-squares problem and the predictions with $\\theta$. | . Proof: First, we relate $\\hat\\theta$ to $\\theta$: . \\[\\begin{align*} \\hat{\\theta} &amp;= G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big(\\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z') \\\\ &amp;= G_\\varrho^{-1} \\left( \\sum_{z' \\in C} \\varrho(z') \\varphi(z') \\varphi(z')^\\top \\right) \\theta + G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\varepsilon(z') \\varphi(z') \\\\ &amp;= \\theta + G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\varepsilon(z') \\varphi(z'). \\end{align*}\\] Then for a fixed \\(z \\in \\mathcal{Z}\\), . \\[\\begin{align*} \\left| \\varphi(z)^\\top \\hat{\\theta} - \\varphi(z)^\\top \\theta \\right| &amp;= \\left| \\sum_{z' \\in C} \\varrho(z') \\varepsilon(z') \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') \\right| \\\\ &amp;\\leq \\sum_{z' \\in C} \\varrho(z') | \\varepsilon(z') | \\cdot | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') | \\\\ &amp;\\leq \\Big( \\max_{z' \\in C} |\\varepsilon(z')| \\Big) \\sum_{z' \\in C} \\varrho(z') | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') |\\,. \\end{align*}\\] To get a sense of how to control the sum notice that if $\\varphi(z)$ in the last sum was somehow replaced by $\\varphi(z’)$, using the definition of $G_\\varrho$ could greatly simplify the last expression. To get here, one may further notice that having the term in absolute value squared would help. Now, to get the squares, recall Jensen’s inequality, which states that for any convex function \\(f\\) and probability distribution \\(\\mu\\), \\(f \\left(\\int u \\mu(du) \\right) \\leq \\int f(u) \\mu(du)\\). Of course, this also works when $\\mu$ is a finitely supported, which is the case here. Thus, applying Jensen’s inequality with \\(f(x) = x^2\\), we thus get . \\[\\begin{align*} \\left(\\sum_{z' \\in C} \\varrho(z') | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') |\\right)^2 &amp; \\le \\sum_{z' \\in C} \\varrho(z') | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') |^2 \\\\ &amp;= \\sum_{z' \\in C} \\varrho(z') \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') \\varphi(z')^\\top G_\\varrho^{-1} \\varphi(z) \\\\ &amp;= \\varphi(z)^\\top G_\\varrho^{-1} \\left( \\sum_{z' \\in C} \\varrho(z') \\varphi(z') \\varphi(z')^\\top \\right) G_\\varrho^{-1} \\varphi(z) \\\\ &amp;= \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z) = \\|\\varphi(z)\\|_{G_\\varrho^{-1}}^2 \\end{align*}\\] Plugging this back into the previous inequality gives the desired result. \\(\\qquad \\blacksquare\\) . It remains to be seen of whether \\(g(\\varrho)=\\max_z \\|\\varphi(z)\\|_{G_\\varrho^{-1}}\\) can be kept under control. This is the subject of a classic result of Kiefer and Wolfowitz: . Theorem (Kiefer-Wolfowitz): Let $\\mathcal{Z}$ be finite. Let $\\varphi: \\mathcal{Z} \\to \\mathbb{R}^d$ be such that the underlying feature matrix $\\Phi$ is rank $d$. There exists a set \\(\\mathcal{C} \\subseteq \\mathcal{Z}\\) and a distribution \\(\\varrho: C \\rightarrow [0, 1]\\) over this set, i.e. \\(\\sum_{z' \\in \\mathcal{C}} \\varrho(z') = 1\\), such that . | \\(\\vert \\mathcal{C} \\vert \\leq d(d+1)/2\\); | \\(\\sup_{z \\in \\mathcal{Z}} \\|\\varphi(z)\\|_{G_\\varrho^{-1}} \\leq \\sqrt{d}\\); | In the previous line, the inequality is achieved with equality and the value of $\\sqrt{d}$ is best possible under all possible choices of $\\mathcal{C}$ and $\\rho$. | . We will not give a proof of the theorem, but we give references at the end where the reader can look up the proof. When $\\varphi$ is not full rank (i.e., $\\Phi$ is not rank $d$), one may reduce the dimensionality (and the cardinality of $C$ reduces accordingly). The problem of choosing $\\mathcal{C}$ and $\\rho$ such that $g(\\rho)$ is minimized is called the $G$-optimal design problem in statistics. This is a specific instance of optimal experimental design. Combining the Kiefer-Wolfowitz theorem with the previous lemma shows that least-squares amplifies the “measurement errors” by at most a factor of \\(\\sqrt{d}\\): . Corollary (extrapolation error control in least-squares via optimal design): Fix any $\\varphi:\\mathcal{Z} \\to \\mathbb{R}^d$ full rank. Then, there exists a set $\\mathcal{C} \\subset \\mathcal{Z}$ with at most $d(d+1)/2$ elements and a weighting function \\(\\varrho\\in \\Delta_1(\\mathcal{C})\\) such that for any \\(\\theta \\in \\mathbb{R}^d\\) and any \\(\\varepsilon: \\mathcal{C} \\rightarrow \\mathbb{R}\\), . \\[\\max_{z\\in \\mathcal{Z}}\\left| \\varphi(z)^\\top \\hat{\\theta} - \\varphi(z)^\\top \\theta \\right| \\leq \\sqrt{d}\\, \\max_{z' \\in C} \\left| \\varepsilon(z') \\right|\\,.\\] where $\\hat\\theta$ is given by . \\[\\begin{align*} \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big(\\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z')\\,. \\end{align*}\\] . Importantly, note that $\\mathcal{C}$ and $\\varrho$ are chosen independently of $\\theta$ and $\\epsilon$, that is, they are independent of the target. This suggests that in approximate policy evaluation, one should choose $(\\mathcal{C},\\rho)$ as in the Kiefer-Wolfowitz theorem and use the $\\rho$ weighted moment matrix. This leads to \\(\\begin{align} \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\hat R_m(z') \\varphi(z')\\,. \\label{eq:lspeg} \\end{align}\\) where $\\hat R_m(z)$ is defined by Eq. \\(\\eqref{eq:petargetsbiased}\\) and $G_\\varrho$ is defined by Eq. \\(\\eqref{eq:mommx}\\). We call this procedure least-square policy evaluation based on rollouts from $G$-optimal design points, or LSPE-$G$, for short. Note that we stick to the truncated rollouts, because this allows a simpler probabilistic analysis. That this properly controls the extrapolation error is as attested by the next result: . Lemma (LSPE-$G$ extrapolation error control): Fix any full-rank feature-map $\\varphi:\\mathcal{Z} \\to \\mathbb{R}^d$ and take the set $\\mathcal{C} \\subset \\mathcal{Z}$ and the weighting function \\(\\varrho\\in \\Delta_1(\\mathcal{C})\\) as in the Kiefer-Wolfowitz theorem. Fix an arbitrary policy $\\pi$ and let $\\theta$ and $\\varepsilon_\\pi$ such that $q^\\pi = \\Phi \\theta + \\varepsilon_\\pi$ and assume that immediate rewards belong to the interval $[0,1]$. Let $\\hat{\\theta}$ be as in Eq. \\eqref{eq:lspeg}. Then, for any $0\\le \\delta \\le 1$, with probability $1-\\delta$, . \\[\\begin{align} \\left\\| q^\\pi - \\Phi \\hat{\\theta} \\right\\|_\\infty &amp;\\leq \\|\\varepsilon_\\pi\\|_\\infty (1 + \\sqrt{d}) + \\sqrt{d} \\left(\\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}}\\right). \\label{eq:lspeee} \\end{align}\\] . Notice that that from the Kiefer-Wolfowitz theorem, \\(\\vert C \\vert = O(d^2)\\) and therefore nothing in the above expression depends on the size of the state space. Now, say we want to make the above error bound at most \\(\\|\\varepsilon_\\pi\\|_\\infty (1 + \\sqrt{d}) + 2\\varepsilon\\) with some value of $\\varepsilon&gt;0$. From the above we see that it suffices to choose $H$ and $m$ so that . \\[\\begin{align*} \\frac{\\gamma^H}{1 - \\gamma} \\leq \\varepsilon/\\sqrt{d} \\qquad \\text{and} \\qquad \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}} \\leq \\varepsilon/\\sqrt{d}. \\end{align*}\\] This, together with \\(\\vert\\mathcal{C}\\vert\\le d(d+1)/2\\) gives . \\[\\begin{align*} H \\geq H_{\\gamma, \\varepsilon/\\sqrt{d}} \\qquad \\text{and} \\qquad m \\geq \\frac{d}{(1 - \\gamma)^2 \\varepsilon^2} \\, \\log \\frac{d(d+1)}{\\delta}\\,. \\end{align*}\\] Proof: In a nutshell, we use the previous corollary, together with Hoeffding’s inequality and using that $|q^\\pi-T_\\pi^H \\boldsymbol{0}|_\\infty \\le \\gamma^H/(1-\\gamma)$, which follows since the rewards are bounded in $[0,1]$. Click here for the full proof. Fix $z\\in \\mathcal{C}$. Let us write $\\hat{R}_m(z) = q^\\pi(z) + \\hat{R}_m(z) - q^\\pi(z) = \\varphi(z)^\\top \\theta + \\varepsilon(z)$ where we define $\\varepsilon(z) = \\hat{R}_m(z) - q^\\pi(z) + \\varepsilon_\\pi(z)$. Then $$ \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big( \\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z'). $$ Now we will bound the difference between our action-value function estimate and the true action-value function: $$ \\begin{align} \\| q^\\pi - \\Phi \\hat\\theta \\|_\\infty &amp; \\le \\| \\Phi \\theta - \\Phi \\hat\\theta\\|_\\infty + \\| \\varepsilon_\\pi \\|_\\infty \\le \\sqrt{d}\\, \\max_{z\\in \\mathcal{C}} |\\varepsilon(z)|\\, + \\| \\varepsilon_\\pi \\|_\\infty \\label{eq:bound_q_values} \\end{align} $$ where the last line follows from the Corollary above. For bounding the first term above, first note that $\\mathbb{E} \\left[ \\hat{R}_m(z) \\right] = (T_\\pi^H \\mathbf{0})(z)$. Then, $$ \\begin{align*} \\varepsilon(z) &amp;= \\hat{R}_m(z) - q^\\pi(z) + \\varepsilon_\\pi(z) \\nonumber \\\\ &amp;= \\underbrace{\\hat{R}_m(z) - (T_\\pi^H \\mathbf{0})(z)}_{\\text{sampling error}} + \\underbrace{(T_\\pi^H \\mathbf{0})(z) - q^\\pi(z)}_{\\text{truncation error}} + \\underbrace{\\varepsilon_\\pi(z)}_{\\text{fn. approx. error}}. \\end{align*} $$ Since the rewards are assumed to belong to the unit interval, the truncation error is at most $\\frac{\\gamma^H}{1 - \\gamma}$. Concerning the sampling error (first term), Hoeffding's inequality gives that for any given $z\\in \\mathcal{C}$, $ \\left \\vert \\hat{R}_m(z) - (T_\\pi^H \\mathbf{0})(z) \\right \\vert \\leq \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 / \\delta)}{2m}}$ with at least $1 - \\delta$ probability. Applying a union bound, we get that with probability at least $1 - \\delta$, for all $z \\in \\mathcal{C}$, $ \\left \\vert \\hat{R}_m(z) - (T_\\pi^H \\mathbf{0})(z) \\right \\vert \\leq \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}}$. Putting things together, we get that with probability at least $1 - \\delta$, $$ \\begin{equation} \\max_{z \\in \\mathcal{C}} | \\varepsilon(z) | \\leq \\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}} + \\|\\varepsilon_\\pi\\|_\\infty\\,. \\label{eq:bound_varepsilon_z} \\end{equation} $$ Plugging this into Eq. \\eqref{eq:bound_q_values} and algebra gives the desired result. \\(\\blacksquare\\) . In summary, what we have shown so far is that if the features can approximate well the action-value function of a policy, then there is a simple procedure (Monte-Carlo rollouts and least-squares estimation based on an optimal experimental design) to produce an reliable estimate of the action-value function of the policy. The question remains whether if we use these estimates in policy iteration, the whole procedure will still give good policies after a sufficiently large number of iterations. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec8/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec8/"
  },"282": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Progress Lemma with Approximation Errors",
    "content": "Here we give a refinement of the geometric progress lemma of policy iteration that allows for “approximate” policy improvement steps. This previous lemma stated that the value function of the improved policy $\\pi’$ is at least as large as the Bellman operator applied to the value function of the policy $\\pi$ to be improved. Our new lemma is as follows: . Lemma (Geometric progress lemma with approximate policy improvement): Consider a memoryless policy \\(\\pi\\) and its corresponding value function \\(v^\\pi\\). Let \\(\\pi'\\) be any policy and define $\\varepsilon:\\mathcal{S} \\to \\mathbb{R}$ via . \\[T v^\\pi = T_{\\pi'} v^{\\pi} + \\varepsilon\\,.\\] Then, . \\[\\|v^* - v^{\\pi'}\\|_\\infty \\leq \\gamma \\|v^* - v^{\\pi}\\|_\\infty + \\frac{1}{1 - \\gamma} \\, \\|\\varepsilon\\|_\\infty.\\] . Proof: First note that for the optimal policy \\(\\pi^*\\), \\(T_{\\pi^*} v^* = v^*\\). We have . \\[\\begin{align} v^* - v^{\\pi'} &amp; = T_{\\pi^*}v^* - T_{\\pi^*} v^{\\pi} + \\overbrace{T_{\\pi^*} v^\\pi}^{\\le T v^\\pi} - T_{\\pi'} v^\\pi + T_{\\pi'} v^{\\pi} - T_{\\pi'} v^{\\pi'} \\nonumber \\\\ &amp;\\le \\gamma P_{\\pi^*} (v^*-v^\\pi) + \\varepsilon + \\gamma P_{\\pi'} (v^\\pi-v^{\\pi'})\\,. \\label{eq:vstar_vpiprime} \\end{align}\\] Using the value difference identity and that $v_\\pi =T_\\pi v^\\pi\\le T v^\\pi$, we calculate . \\[\\begin{align*} v^\\pi - v^{\\pi'} = (I-\\gamma P_{\\pi'})^{-1} [ v^\\pi - T_{\\pi'}v^\\pi] \\le (I-\\gamma P_{\\pi'})^{-1} [ T v^\\pi - (T v^\\pi -\\varepsilon) ] = (I-\\gamma P_{\\pi'})^{-1} \\varepsilon\\,, \\end{align*}\\] where the inequality follows because $(I-\\gamma P_{\\pi’})^{-1}= \\sum_{k\\ge 0} (\\gamma P_{\\pi’})^k$, the sum of positive linear operators, is a positive linear operator itself and hence is also monotone. Plugging the inequality obtained into \\eqref{eq:vstar_vpiprime} gives . \\[\\begin{align*} v^* - v^{\\pi'} \\le \\gamma P_{\\pi^*} (v^*-v^\\pi) + (I-\\gamma P_{\\pi'})^{-1} \\varepsilon. \\end{align*}\\] Taking the maximum norm of both sides and using the triangle inequality and that \\(\\| (I-\\gamma P_{\\pi'})^{-1} \\|_\\infty \\le 1/(1-\\gamma)\\) gives the desired result. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec8/#progress-lemma-with-approximation-errors",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec8/#progress-lemma-with-approximation-errors"
  },"283": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Approximate Policy Iteration",
    "content": "Notice that the progress lemma makes no assumptions about the origin of the errors. This motivates considering a generic version of approximate policy iteration where for $k\\ge 1$ in the $k$th update set, the new policy $\\pi_k$ is approximately greedy with respect to $v^{\\pi_k}$ in that sense that . \\[\\begin{align} T v^{\\pi_k} = T_{\\pi_{k+1}} v^{\\pi_k} + \\varepsilon_k\\,. \\label{eq:apidef} \\end{align}\\] The progress lemma implies that the resulting sequence of policies will have value functions that converge to a neighborhood of $v^*$ where the size of the neighborhood is governed by the magnitude of the error terms \\((\\varepsilon_k)_k\\). ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec8/#approximate-policy-iteration",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec8/#approximate-policy-iteration"
  },"284": {
    "doc": "8. Approximate Policy Iteration",
    "title": "",
    "content": "Theorem (Approximate Policy Iteration): Let \\((\\pi_k)_{k\\ge 0}\\), \\((\\varepsilon_k)_k\\) be such that \\eqref{eq:apidef} holds for all \\(k\\ge 0\\). Then, for any \\(k\\ge 1\\), . \\[\\begin{align} \\|v^* - v^{\\pi_k}\\|_\\infty \\leq \\frac{\\gamma^k}{1-\\gamma} + \\frac{1}{(1-\\gamma)^2} \\max_{0\\le s \\le k-1} \\|\\varepsilon_{s}\\|_\\infty\\,. \\label{eq:apieb} \\end{align}\\] . Proof: Left as an exercise. \\(\\qquad \\blacksquare\\) . Consider now a version of approximate policy iteration where the sequence of policies \\((\\pi_k)_{k\\ge 0}\\) is defined as follows: . \\[\\begin{align} q_k = q^{\\pi_k} + \\varepsilon_k', \\qquad M_{\\pi_{k+1}} q_k = M q_k\\,, \\quad k=0,1,\\dots\\,. \\label{eq:apiavf} \\end{align}\\] That is, for each \\(k=0,1,\\dots\\), \\(\\pi_k\\) is greedy with respect to \\(q_{k-1}\\). ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec8/#-1",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec8/#-1"
  },"285": {
    "doc": "8. Approximate Policy Iteration",
    "title": "",
    "content": "Corollary (Approximate Policy Iteration with Approximate Action-value Functions): The sequence defined in \\eqref{eq:apiavf} is such that . \\[\\| v^* - v^{\\pi_k} \\|_\\infty \\leq \\frac{\\gamma^k}{1-\\gamma} + \\frac{2}{(1-\\gamma)^2} \\max_{0\\le s \\le k-1} \\|\\varepsilon_{s}'\\|_\\infty\\,.\\] . Proof: To simplify the notation consider policies \\(\\pi,\\pi'\\) and functions \\(q,\\varepsilon'\\) over the state-action space such that \\(M_{\\pi'} q = M q\\) and \\(q=q^\\pi+\\varepsilon'\\). We have . \\[\\begin{align*} T v^\\pi &amp; \\ge T_{\\pi'} v^\\pi = M_{\\pi'} (r+\\gamma P v^\\pi) = M_{\\pi'} q^\\pi = M_{\\pi'} q - M_{\\pi} \\varepsilon' = M q - M_\\pi \\varepsilon'\\\\ &amp; \\ge M (q^\\pi - \\|\\varepsilon'\\|_\\infty \\boldsymbol{1}) - M_\\pi \\varepsilon' \\ge M q^\\pi - 2 \\|\\varepsilon'\\|_\\infty \\boldsymbol{1} = T v^\\pi - 2 \\|\\varepsilon'\\|_\\infty \\boldsymbol{1}\\,, \\end{align*}\\] where we used that \\(M_\\pi\\) is linear, monotone, and that $M$ is monotone, and both are nonexpansions in the maximum norm. Hence, if $\\varepsilon_k$ is defined by \\eqref{eq:apidef} then \\(\\|\\varepsilon_k\\|_\\infty \\le 2 \\|\\varepsilon_k'\\|_\\infty\\) and the result follows from the previous theorem. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec8/#-2",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec8/#-2"
  },"286": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Global planning with least-squares policy iteration",
    "content": "Putting things together gives the following planning method: . | Given the feature map $\\varphi$, find \\(\\mathcal{C}\\) and \\(\\rho\\) as in the Kiefer-Wolfowitz theorem | Let \\(\\theta_{-1}=0\\) | For \\(k=0,1,2,\\dots,K-1\\) do | \\(\\qquad\\) Roll out with policy \\(\\pi:=\\pi_k\\) for $H$ steps to get the targets \\(\\hat R_m(z)\\) where \\(z\\in \\mathcal{C}\\) \\(\\qquad\\) and \\(\\pi_k(s) = \\arg\\max_a \\langle \\theta_{k-1}, \\varphi(s,a) \\rangle\\) | \\(\\qquad\\) Solve the weighted least-squares problem given by Eq. \\(\\eqref{eq:wlse}\\) to get \\(\\theta_k\\). | Return \\(\\theta_{K-1}\\) | . We call this method least-squares policy iteration (LSPI) for obvious reasons. Note that this is a global planning method: The method makes no use of an input state and the parameter vector returned can be used to get the policy $\\pi_{K}$ (as in the method above). Theorem (LSPI performance): Fix an arbitrary full rank feature-map $\\varphi: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}^d$ and let $K,m,H\\ge 1$. Assume that B2\\(_{\\varepsilon}\\) holds. Then, for any $0\\le \\zeta \\le 1$, with probability at least $1-\\zeta$, the policy $\\pi_{K}$ which is greedy with respect to $\\Phi \\theta_{K-1}$ is $\\delta$-suboptimal with . \\[\\begin{align*} \\delta \\le \\underbrace{\\frac{2(1 + \\sqrt{d})}{(1-\\gamma)^2}\\, \\varepsilon}_{\\text{approx. error}} + \\underbrace{\\frac{\\gamma^{K-1}}{1-\\gamma}}_{\\text{iter. error}} + \\underbrace{\\frac{2\\sqrt{d}}{(1-\\gamma)^3} \\left(\\gamma^H + \\sqrt{\\frac{\\log( d(d+1)K / \\zeta)}{2m}}\\right)}_{\\text{pol.eval. error}} \\,. \\end{align*}\\] In particular, for any $\\varepsilon’&gt;0$, choosing $K,H,m$ so that . \\[\\begin{align*} K &amp; \\ge H_{\\gamma,\\gamma\\varepsilon'/2} \\\\ H &amp; \\ge H_{\\gamma,(1-\\gamma)^2\\varepsilon'/(8\\sqrt{d})} \\qquad \\text{and} \\\\ m &amp; \\ge \\frac{32 d}{(1-\\gamma)^6 (\\varepsilon')^2} \\log( (d+1)^2 K /\\zeta ) \\end{align*}\\] policy $\\pi_K$ is $\\delta$-optimal with . \\[\\begin{align*} \\delta \\le \\frac{2(1 + \\sqrt{d})}{(1-\\gamma)^2}\\, \\varepsilon + \\varepsilon'\\,, \\end{align*}\\] while the total computation cost is $\\text{poly}(\\frac{1}{1-\\gamma},d,\\mathrm{A},\\frac{1}{\\varepsilon’},\\log(1/\\zeta))$. Thus, with a polynomial cost, LSPI with the specific configuration at the cost of polynomial computation cost, but importantly, with a cost that is independent of the size of the state space, can result in a good policy as long as $\\varepsilon$, the worst-case error of approximating action-value functions of policies using the features provided, is sufficiently small. Proof: Note that B2\\(_\\varepsilon\\) and that $\\Phi$ is full rank implies that for any memoryless policy $\\pi$ there exists a parameter vector $\\theta\\in \\mathbb{R}^d$ such that \\(\\| \\Phi \\theta - q^\\pi \\|_\\infty \\le \\varepsilon\\) (cf. Part 2 of Question 3 of Assignment 2). Hence, we can use the “LSPE extrapolation error bound” (cf. \\(\\eqref{eq:lspeee}\\)). By this result, a union bound and of course by B2$_\\varepsilon$, we get that for any $0\\le \\zeta \\le 1$, with probability at least $1-\\zeta$, for any $0 \\le k \\le K-1$, . \\[\\begin{align*} \\| q^{\\pi_k} - \\Phi \\theta_k \\|_\\infty &amp;\\leq \\varepsilon (1 + \\sqrt{d}) + \\sqrt{d} \\left(\\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log( d(d+1)K / \\zeta)}{2m}}\\right)\\,, \\end{align*}\\] where we also used that \\(\\vert \\mathcal{C} \\vert \\le d(d+1)\\). Call the quantity on the right-hand side in the above inequality $\\kappa$. Take the event when the above inequalities hold and for now assume this event holds. By the previous theorem, $\\pi_K$ is $\\delta$-optimal with . \\[\\delta \\le \\frac{\\gamma^{K-1}}{1-\\gamma} + \\frac{2}{(1-\\gamma)^2} \\kappa \\,.\\] To obtain the second part of the result, we split $\\varepsilon’$ into two equal parts: $K$ is set to force the iteration error to be at most $\\varepsilon’/2$, while $H$ and $m$ are chosen to force the policy evaluation error to be at most $\\varepsilon’/2$. Here, to choose $H$ and $M$, $\\varepsilon’/2$ is again split into two equal parts. The details of this calculation are left to the reader. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec8/#global-planning-with-least-squares-policy-iteration",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec8/#global-planning-with-least-squares-policy-iteration"
  },"287": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Notes",
    "content": "Approximate Dynamic Programming (ADP) . Value iteration and policy iteration are specific instances of dynamic programming methods. In general, dynamic programming refers to methods that use value functions to calculate good policies. In approximate dynamic programming the methods are modified by introducing “errors” when calculating the values. The idea is that the origin of the errors does not matter (e.g., whether they come due to imperfect function approximation, linear, or nonlinear, or due to the sampling): The analysis is done in a general form. While here we met approximate policy iteration, one can also use the same ideas as shown here to study an approximate version of value iteration. A homework in problem set 2 asks you to study this method, which is usualy called approximate value iteration. In an earlier homework you were asked to study how linear programming can also be used to compute optimal value functions. Adding approximations we then get approximate linear programming. What function approximation technique to use? . We note in passing that fans of neural networks should like that the general, ADP-style results, like the theorem in the middle of this lecture, can be also applied to the case when neural networks are used as the function approximation technique. However, one main lesson of the lecture is that to control extrapolation errors, one should be quite careful in how the training data is chosen. For linear prediction and least-squares fitting, optimal design gives a complete answer, but the analog questions are completely open in the case of nonlinear function approximation, such as neural networks. There is also a sizable literature that connects nonparametric techniques (an analysis friendly relative of neural networks) to ADP methods. Concentrability coefficients and all that jazz . The idea of introducing approximate calculations has been introduced at the same time people got interested in Markov Decision Processes in the 1960s. Hence, the literature is quite enormous. However, the approach taken here which asks for error bounds where the algorithmic (not approximation-) error is uniformly controlled regardless of the MDP is quite recent and where the term that involves the approximation error is also uniformly bounded (for a fixed dimension and discount factor). Earlier literature often presented bounds where the magnification factor of the approximation and the algorithmic error involved terms which depended on the MDP. Often these came in the form of “concentrability coefficients” (and yours truly was quite busy with working on these results a while ago). The main conclusion of this earlier analysis is that more stochasticity in the transitions means less control, less concentrability, which is advantageous for the ADP algorithms. While this makes sense and this indicates that these earlier results are complementary to the results presented here, the issue is that these results are quite pessimistic for example when the MDP is deterministic (as in this case the concentrability coefficients can be as large as the size of the state space). While here we emphasized the importance of using a good design to control the extrapolation errors, in these earlier results, no optimal design was used. The upshot is that this saves the effort of coming up with a good design, but the obvious downside is that the extrapolation error may become uncontrolled. In the batch setting (which we will come back to later), of course, there is no way to control the sample collection, and this is in fact the setting where this earlier analysis was done. The strength of hints . A critical assumption in the analysis of API was that the approximation error is controlled uniformly for all policies. This feels limiting. Yet, there are some interesting sufficient conditions when this assumption is clearly satisfied. In general, these require that the transition dynamics and the reward are both “compressible”. For example, if the MDP is such that $r$, the immediate reward as a function of the state-action pairs satisfies \\(r = \\Phi \\theta_r\\) and the transition matrix, \\(P\\in [0,1]^{\\mathrm{S}\\mathrm{A} \\times \\mathrm{S}}\\) satisfies \\(P = \\Phi H\\) with some matrix \\(H\\in \\mathbb{R}^{d\\times \\mathrm{S}}\\), then for any policy policy \\(\\pi\\), \\(T_\\pi q = r+ \\gamma P M_\\pi q\\) has a range which is a subset of \\(\\text{span}(\\Phi)=\\mathcal{F}_{\\varphi}\\). Since \\(q^\\pi\\) is the fixed-point of \\(T_\\pi\\), i.e., \\(q^\\pi = T_\\pi q^\\pi\\), it follows that \\(q^\\pi\\) is also necessarily in the range space of \\(T_\\pi\\). As such, \\(q^\\pi \\in \\mathcal{F}_{\\varphi}\\) and \\(\\varepsilon_{\\text{apx}}=0\\). MDPs that satisfy the above two constraints are called linear in \\(\\Phi\\) (or sometimes, just “linear MDPs”). Exact linearity can be relaxed: If \\(r = \\Phi \\theta_r + \\varepsilon_r\\) and \\(P = \\Phi H +E\\), then for any policy \\(\\pi\\), \\(q^\\pi\\in_{\\varepsilon} \\mathcal{F}_{\\varphi}\\) with \\(\\varepsilon \\le \\|\\varepsilon_r\\|_\\infty+\\frac{\\gamma}{1-\\gamma}\\|E\\|_\\infty\\). Nevertheless, later we will investigate whether this assumption can be relaxed. The tightness of the bounds . It is not known whether the bound presented in the final result is tight. In fact, the dependence of $m$ on the $1/(1-\\gamma)$ is almost certainly not tight; in similar scenarios it has been shown in the past that replacing Hoeffding’s inequality with Bernstein’s inequality allows the reduction of this factor. It is more interesting whether the amplification factor of the approximation error, $\\sqrt{d}/(1-\\gamma)^2$, is best possible. In the next lecture we will show that the $\\sqrt{d}$ approximation error amplification factor cannot be removed while keeping the runtime under control. In a later lecture, we will show that the dependence on $1/(1-\\gamma)$ cannot be improved either – at least for this algorithm. However, we will see that if the main concern is the amplification of the approximation error, while keeping the runtime polynomial (perhaps with a higher order though) then under B2\\(_{\\varepsilon}\\) better algorithms exist. The cost of optimal experimental design . The careful reader would not miss that to run the proposed method one needs to find the set $\\mathcal{C}$ and the weighting function $\\rho$. The first observation here is that it is not crucial to find the best possible $(\\mathcal{C},\\rho)$ pair. The Kiefer-Wolfowitz theorem showed that with this best possible choice, $g(\\rho) = \\sqrt{d}$. However, if one finds a pair such that $g(\\rho)=2\\sqrt{d}$, the price of this is that wherever $\\sqrt{d}$ appears in the final performance bound, a submultiplicative factor of $2$ will also need to be introduced. This should be acceptable. In relation to this note that by relaxing this optimality requirement, the cardinality of $\\mathcal{C}$ can be reduced. For example, by introducing the factor of $2$ as suggested above allows one to reduce the cardinlity to $O(d \\log \\log d)$; which may actually be a good tradeoff as this can save much on the runtime. However, the question still remains of who computes these (approximately) optimal designs and at what cost. While this calculation only needs to be done once and is independent of the MDP (just depends on the feature map), the value of these methods remains unclear because of this compute cost. General methods to compute approximately optimal designs needed here are known, but their runtime for our case will be proportional to the number of state-action pairs. In the very rare cases when simulating transitions is very costly but the number of state-action pairs is not too high, this may be a viable option. However, these cases are rare. For special choices of the feature-map, optimal designs may be known. However, this reduces the general applicability of the method presented here. Thus, a major question is whether the optimal experimental design can be avoided. What is known is that for linear prediction with least-squares, clearly, they cannot be avoided. One suspects that this is true more generally. Can optimal designs be avoided while keeping the results essentially unchanged? Of particular interest would be if the feature-map would also be only “locally explored” as the planner interacts with the simulator. Altogether, one suspects that two factors contributed here for the appearance of optimal experimental design: One factor is that the planner is global: It comes up with a parameter vector that leads to a policy that can be used regardless of the state. The other (perhaps) factor is that the approach was based on simple “patching up” a dynamic programming algorithm with a function approximator. While this is a common approach, controlling the extrapolation errors in this approach is critical and is likely only possible with something like an optimal experimental design. As we shall see soon, there are indeed approaches that avoid the optimal experimental design step and which are based on online planning and they also deviate from the ADP approach. Policy evaluation alternatives . The policy evaluation method presented here feels unsophisticated. It uses simple Monte-Carlo rollouts, with truncation, averaging and least-squares regression. The reinforcement learning literature offers many alternatives, such as the “temporal difference” learning type methods that are based on solving the fixed point equation $q^\\pi = T_\\pi q^\\pi$. One can indeed try to use this equation to avoid the crude Monte-Carlo approach presented here, in the hope of reducing the variance (which is currently rather crudely upper bounded using the $1/(1-\\gamma)$ term in the Hoeffding bound). Rewriting the fixed point as $(I-\\gamma P_\\pi) q^\\pi = r$, and then plugging in $q^\\phi = \\Phi \\theta + \\varepsilon$, we see that the trouble is that to control the extrapolation errors, the optimal design must likely depend on the policy to be evaluated (because of the appearance of $(I-\\gamma P_\\pi)\\Phi$). Alternative error control: Bellman residuals . Let \\((\\pi_k)_{k\\ge 0}\\) and \\((q_k,\\varepsilon_k)_{k\\ge 0}\\) be so that . \\[\\varepsilon_k = q_k - T_{\\pi_k} q_k\\] Here, \\(\\varepsilon_k\\) is called the “Bellman residual” of \\(q_k\\). The policy evaluation alternatives above aim at controlling these residuals. The reader is invited to derive the analogue of the “approximate policy iteration” error bound in \\eqref{eq:apieb} for this scenario. The role of $\\rho$ in the Kiefer-Wolfowitz result . One may wonder about how critical is the presence of $\\rho$ in the results presented. For this, we can say that it is not critical. Unweighted least-squares does not perform much worse. Least-squares error bound . The error bound presented for least-squares does not use the full power of randomness. When part of the errors $\\varepsilon(z)$ with $z\\in \\mathcal{C}$ are random, some helpful averaging effects can appear, which we ignored for now, but which could be used in a more refined analysis. Optimal experimental design – a field on its own . Optimal exoerimental design is a subfield of statistics. The design considered here is just one possibility. In fact, this design which is called G-optimal design (G stands, uninspiringly, for the word “general”). The Kiefer-Wolfowitz theorem actually also states that this is equivalent to the D-optimal designs. Lack of convergence . The results presented show convergence to a ball around the optimal target. Some people think this is a major concern. While having a convergent method may look more appealing, as long as one controls the size of the ball, I will not be too concerned. Approximate value iteration (AVI) . Similarly to what is done here, one can introduce an approximate version of value-iteration. This is the subject of Question 3 of homework 2. While the conditions are different, the qualitative behavior of AVI is similar to that of approximate policy iteration. In particular, as for approximate policy iteration, there are two steps to this proof: One is to show that the residuals $\\varepsilon_k = q_k - T q_{k-1}$ can be controlled and the second is that if they are controlled then the policy that is greedy with respect to (say) $q_K$ is $\\delta$-optimal with $\\delta$ controlled by \\(\\varepsilon_{1:K}:=\\max_{1\\le k \\le K} \\| \\varepsilon_k \\|_\\infty\\). For this second part, we have the following bound: . \\[\\begin{align} \\delta \\le 2 H^2 (\\gamma^K + \\varepsilon_{1:K})\\,. \\label{eq:lsvibound} \\end{align}\\] where $H=1/(1-\\gamma)$. The procedure that uses least-squares fitting to get the iterates $(q_k)_k$ is known under various names, such as least-squares value iteration (LSVI), fitted Q-iteration (FQI), least-squares Q iteration (LSQI). This proliferation of abbreviations and names is unfortunate, but there is not much that can be done at this stage. To add insult to injury, when neural networks are used to represent the iterates and an incremental stochastic gradient descent algorithm is used for “fitting” the weights of these networks by resampling old data from a “replay buffer”, the resulting procedure is coined “Deep Q-Networks” (training), or DQN for short. Bounds on the parameter vector . The Kiefer-Wolfowitz theorem implies the following: . Proposition: Let $\\phi:\\mathcal{Z}\\to\\mathbb{R}^d$ and $\\theta\\in \\mathbb{R}^d$ be such that $\\sup_{z\\in \\mathcal{Z}}|\\langle \\phi(z),\\theta \\rangle|\\le 1$ and \\(\\sup_{z\\in \\mathcal{Z}} \\|\\phi(z)\\|_2 &lt;+\\infty\\). Then, there exist a matrix $S\\in \\mathbb{R}^{d\\times d}$ such that for $\\tilde \\phi$ . \\[\\begin{align*} \\tilde\\phi(z) &amp; = S\\phi(z)\\,, \\qquad z\\in \\mathcal{Z} \\end{align*}\\] there exists \\(\\tilde \\theta\\in \\mathbb{R}^d\\) such that the following hold: . | \\(\\langle \\phi(z),\\theta \\rangle = \\langle \\tilde \\phi(z),\\tilde \\theta \\rangle\\), \\(z\\in \\mathcal{Z}\\); | \\(\\sup_{z\\in \\mathcal{Z}} \\| \\tilde \\phi(z) \\|_2 \\le 1\\); | \\(\\|\\tilde \\theta \\|_2 \\le \\sqrt{d}\\). | . Proof: Let $\\rho:\\mathcal{Z} \\to [0,1]$ be the $G$-optimal design whose existence is guaranteed by the Kiefer-Wolfowitz theorem. Let \\(M = \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\phi(z)\\phi(z)^\\top\\) be the underlying moment matrix. Then, by the definition of $\\rho$, \\(\\sup_{z\\in \\mathcal{Z}}\\|\\phi(z)\\|_{M^{-1}}^2 \\le d\\). Define \\(S= (dM)^{-1/2}\\) and \\(\\tilde \\theta = S^{-1} \\theta\\). The first property is clearly satisfied. As to the second property, . \\[\\|\\tilde \\phi(z)\\|_2^2 = \\| (dM)^{-1/2}\\phi(z)\\|_2^2 = \\phi(z)^\\top (dM)^{-1} \\phi(z) \\le 1\\,.\\] Finally, for the third property, . \\[\\| \\tilde \\theta \\|_2^2 = d \\theta^\\top \\left( \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\phi(z) \\phi(z)^\\top \\right) \\theta = d \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\underbrace{(\\theta^\\top \\phi(z))^2}_{\\le 1} \\le d\\,,\\] finishing the proof. \\(\\qquad \\blacksquare\\) . Thus, if one has access to the full feature-map then knowing that a function realized is bounded, one may as well assume that the feature map is bounded and the parameter vector is bounded just by $\\sqrt{d}$. Regularized least-squares . The linear least-squares predictor given by a feature-map $\\phi$ and data $(z_1,y_1),\\dots,(z_n,y_n)$ predicts a response at $z$ via $\\langle \\phi(z),\\hat\\theta \\rangle$ where . \\[\\begin{align} \\hat\\theta = G^{-1}\\sum_{i=1}^n \\phi_i y_i\\,, \\label{eq:ridgesol} \\end{align}\\] with . \\[G = \\sum_{i=1}^n \\phi_i \\phi_i^\\top\\,.\\] Here, by abusing notation for the sake of minimizing clutter, we use $\\phi_i=\\phi(z_i)$, $i=1,\\dots,n$. The problem is that $G$ may not be invertible (i.e., $\\hat \\theta$ may not be defined as written above). “By continuity”, it is nearly equally problematic when $G$ is ill-conditioned (i.e., its minimum eigenvalue is “much smaller” than its maximum eigenvalue). In fact, this leads to poor “generalization”. One remedy, often used, is to modify $G$ by shifting it with a small constant multiple of the identity matrix: . \\[G = \\lambda I + \\sum_{i=1}^n \\phi_i \\phi_i^\\top\\,.\\] Here, $\\lambda&gt;0$ is a tuning parameter, whose value is often chosen based on cross-validation or with a similar process. The modification guarantees that $G$ is invertible and it overall improves the quality of predictions, especially when $\\lambda$ is tuned base on data. Above, the choice of the identity matrix, while is common in the literature, is completely arbitrary. In particular, invertibility will be guaranteed if $I$ is replaced with any other positive definite matrix $P$. In fact, the matrix one should use here should be one that makes $|\\theta|_P^2$ small (while, say, keeping the minimum eigenvalue of $P$ at constant). That this is the choice that makes sense can be argued for by noting that with . \\[G = \\lambda P + \\sum_{i=1}^n \\phi_i \\phi_i^\\top\\,.\\] the $\\hat\\theta$ vector defined in \\eqref{eq:ridgesol} is the minimizer of . \\[L_n(\\theta) = \\sum_{i=1}^n ( \\langle \\phi_i,\\theta \\rangle - y_i)^2 \\,\\,+ \\lambda \\| \\theta\\|_P^2\\,,\\] and thus, the extra penalty has the least impact for the choice of $P$ that makes the norm of $\\theta$ the smallest. If we only know that $\\sup_{z} |\\langle \\phi(z),\\theta \\rangle|\\le 1$, by our previous note, a good choice is $P=d M$, where \\(M = \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\phi(z)\\phi(z)^\\top\\) where \\(\\rho\\) is a $G$-optimal design. Indeed, with this choice, \\(\\|\\theta\\|_P^2 = d \\|\\theta \\|_M^2 \\le d\\). Note also that if we apply the feature-standardization transformation of the previous note, we have . \\[(dM)^{-1/2} (\\sum_i \\phi_i \\phi_i^\\top + \\lambda d M ) (dM)^{-1/2} = \\sum_i \\tilde \\phi_i \\tilde \\phi_i^\\top + \\lambda I\\,,\\] showing that the choice of using the identity matrix is justified when the features are standardized as in the proposition of the previous note. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec8/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec8/#notes"
  },"288": {
    "doc": "8. Approximate Policy Iteration",
    "title": "References",
    "content": "We will only scratch the surface now; expect more references to be added later. The bulk of this lecture is based on . | Tor Lattimore, Csaba Szepesvári, and Gellért Weisz. 2020. “Learning with Good Feature Representations in Bandits and in RL with a Generative Model.” ICML and arXiv:1911.07676, | . who introduced the idea of using \\(G\\)-optimal designs for controlling the extrapolation errors. A very early reference on error bounds in “approximate dynamic programming” is the following: . | Whitt, Ward. 1979. “Approximations of Dynamic Programs, II.” Mathematics of Operations Research 4 (2): 179–85. | . The analysis of the generic form of approximate policy iteration is a refinement of Proposition 6.2 from the book of Bertsekas and Tsitsiklis: . | Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont, Massachusetts, 1996. | . However, there are some differences between the “API” theorem presented here and Proposition 6.2. In particular, the theorem presented here appears to capture all sources of errors in a general way, while Proposition 6.2 is concerned with value function approximation errors and errors introduced in the “greedification step”. The form adopted here appears, for example, in Theorem 1 of a technical report of Scherrer, who also gives earlier references: . | Scherrer, Bruno. 2013. “On the Performance Bounds of Some Policy Search Dynamic Programming Algorithms.” arxiv. | . The earliest of these references is perhaps . | Munos, R. 2003. “Error Bounds for Approximate Policy Iteration.” ICML. | . Least-squares policy iteration appears in . | Lagoudakis, M. G. and Parr, R. Least-squares policy iteration. The Journal of Machine Learning Re-search, 4:1107–1149, 2003. | . The particular form presented in this work though uses value function approximation based on minimizing the Bellman residuals (using the so-called LSTD method). Two books that advocate the ADP approach: . | Powell, Warren B. 2011. Approximate Dynamic Programming. Solving the Curses of Dimensionality. Hoboken, NJ, USA: John Wiley &amp; Sons, Inc. | Lewis, Frank L., and Derong Liu. 2013. Reinforcement Learning and Approximate Dynamic Programming for Feedback Control. Hoboken, NJ, USA: John Wiley &amp; Sons, Inc. | . And a chapter: . | Bertsekas, Dimitri P. 2009. “Chapter 6: Approximate Dynamic Programming,” January, 1–118. | . A paper that is concerned with API and least-squares methods, but uses concentrability is: . Antos, Andras, Csaba Szepesvári, and Rémi Munos. 2007. “Learning near-Optimal Policies with Bellman-Residual Minimization Based Fitted Policy Iteration and a Single Sample Path.” Machine Learning 71 (1): 89–129. Optimal experimental design has a large literature. A nice book concerned with computation is this: . | M. J. Todd. Minimum-volume ellipsoids: Theory and algorithms. SIAM, 2016. | . The Kiefer-Wolfowitz theorem is from: . | J. Kiefer and J. Wolfowitz. The equivalence of two extremum problems. Canadian Journal of Mathematics, 12(5):363–365, 1960. | . More on computation here: . | E. Hazan, Z. Karnin, and R. Meka. Volumetric spanners: an efficient exploration basis for learning. Journal of Machine Learning Research, 17(119):1–34, 2016 | M. Grötschel, L. Lovász, and A. Schrijver. Geometric algorithms and combinatorial optimization, volume 2. Springer Science &amp; Business Media, 2012. | . The latter book is a very good general starting point for convex optimization. That the features are standardized as shown in the notes is assumed (and discussed), e.g., in . | Wang, Ruosong, Dean P. Foster, and Sham M. Kakade. 2020. “What Are the Statistical Limits of Offline RL with Linear Function Approximation?” arXiv [cs.LG]. arXiv | . which we will meet later. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec8/#references",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec8/#references"
  },"289": {
    "doc": "8. Approximate Policy Iteration",
    "title": "8. Approximate Policy Iteration",
    "content": "PDF Version . Note: On March 13, 2021, these notes were updated as follows: . | Tighter bounds are derived; the old analysis was based on bounding \\(\\| q^*-q^{\\pi_k} \\|_\\infty\\); the new analysis directly bounds \\(\\| v^* - v^{\\pi_k} \\|_\\infty\\), which leads to a better dependence on the approximation error; | Unbiased return estimates are introduced that use rollouts of random length. | . One simple idea to use function approximation in MDP planning is to take a planning method that uses internal value functions and add a constraint that restrict the value functions to have a compressed representation. As usual, two questions arise: . | Does this lead to an efficient planner? That is, can the computation be carried out in time polynomial in the relevant quantities, but not the size of the state space? In the case of linear functions the question is whether we can calculate the coefficients efficiently. | Does this lead to an effective planner? In particular, how good a policy can we arrive at with a limited compute effort? | . In this lecture, as a start into exploring the use of value function approximation in planning, we look at modifying policy iteration in the above described way. The resulting algorithm belongs to the family of approximate policy iteration algorithms, which consists of all algorithms derived from policy iteration by adding approximation to it. We will work with linear function approximation. In particular, we will assume that the planner is given as a hint a feature-map $\\varphi: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$. In this setting, since policy iteration hinges upon evaluating the policies obtained, the hint given to the planner is considered to be “good” if the (action-)value functions of all policies are well-represented with the features. This means, that we will work under assumption B2$_\\varepsilon$ from the previous lecture, which we copy here for convenience. In what follows we fix $\\varepsilon&gt;0$. Assumption B2$\\mbox{}_{\\varepsilon}$ (approximate universal value function realizibility) The MDP $M$ and the featuremap $\\varphi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(q^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\varphi\\). Recall that here the notation $q^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\varphi$ means that $q^\\pi$ can be approximated up to a uniform error of $\\varepsilon$ using linear combinations of the basis functions underlying the feature-map $\\varphi$: . For any policy $\\pi$, . \\[\\begin{align*} \\inf_{\\theta\\in \\mathbb{R}^d} \\max_{(s,a)} | q^\\pi(s,a) - \\langle \\theta, \\varphi(s,a) \\rangle | \\left(= \\inf_{\\theta\\in \\mathbb{R}^d} \\| q^\\pi - \\Phi\\theta \\|_\\infty\\right) \\le \\varepsilon\\,. \\end{align*}\\] One may question whether it is reasonable to expect that the value functions of all policies can be compressed. We will come back to this question later. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec8/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec8/"
  },"290": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Query lower bound for MDPs with large action sets",
    "content": "For the statement of our results, the following definitions will be useful: . Definition (soundness): An online planner is $(\\delta,\\varepsilon)$-sound if for any finite discounted MDP $M=(\\mathcal{S},\\mathcal{A},P,r,\\gamma)$ and feature-map $\\varphi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ such that $\\varepsilon^*(M,\\Phi)\\le \\varepsilon$, when interacting with $(M,\\varphi)$, the planner induces a $\\delta$-suboptimal policy of $M$. Definition (memoryless planner): Call a planner memoryless if it does not retain any information between its calls. The announced result is as follows: . Theorem (Query lower bound: large action sets): For any $\\varepsilon&gt;0$, $0&lt;\\delta\\le 1/2$, positive integer $d$ and for any $(\\delta,\\varepsilon)$-sound online planner $\\mathcal{P}$ there exists a “featurized-MDP” $(M,\\varphi)$ with rewards in $[0,1]$ with $\\varepsilon^*(M,\\Phi)\\le \\varepsilon$ such that when interacting with a simulator of $(M,\\varphi)$, the expected number of queries used by $\\mathcal{P}$ is at least . \\[\\begin{align*} \\Omega\\left( \\exp\\left( \\frac{1}{32} \\left(\\frac{\\sqrt{d}\\varepsilon}{\\delta}\\right)^2 \\right) \\right)\\,. \\end{align*}\\] . Note that if \\(\\delta=\\varepsilon\\) or smaller, the number of queries is exponential in \\(d\\). For the proof we need a result that shows that one can pack the \\(d\\)-dimensional unit sphere with exponential in \\(d\\) many vectors that are nearly orthogonal. The precise result, which is stated without proof, is as follows: . Lemma (Johnson-Lindenstrauss (JL) Lemma) For every \\(\\tau &gt; 0\\) and integers \\(d,k\\) such that . \\[\\left\\lceil\\frac{8 \\ln k}{\\tau^2}\\right\\rceil \\leq d \\leq k\\] then there exists \\(v_1,...,v_k\\) vectors of the \\(d\\)-dimensional unit sphere such that for all \\(1\\le i&lt;j\\le k\\), . \\[\\lvert \\langle v_i,v_j \\rangle | \\leq \\tau\\,.\\] . Note that for a fixed dimension \\(d\\), the valid range for \\(k\\) is . \\(\\begin{align} d\\le k \\le \\exp\\left(\\frac{d\\tau^2}{8}\\right)\\,. \\label{eq:krange} \\end{align}\\) In particular, \\(k\\) can be “exponentially large” in \\(d\\) when \\(\\tau\\) is a constant. We can directly relate this lemma to our feature matrices. In particular, the lemma is equivalent to the following result: . Proposition (JL feature matrix): For any $\\tau,d,k$ as in the JL lemma there exists a matrix $\\Phi \\in \\mathbb{R}^{k\\times d}$ such that for any $i\\in[k]$, . \\[\\begin{align} \\max_{i\\in [k]} \\inf_{\\theta\\in \\mathbb{R}^d} \\|\\Phi \\theta - e_i \\|_\\infty \\le \\tau\\,, \\label{eq:featjl} \\end{align}\\] where \\(e_i\\) is the \\(i\\)th basis vector of standard Euclidean basis of \\(\\mathbb{R}^k\\), and in particular if \\(\\varphi_i^\\top\\) is the \\(i\\)th row of \\(\\Phi\\), \\(\\|\\Phi \\varphi_i - e_i\\|_\\infty \\le \\tau\\) holds. Proof: Choose $v_1,\\dots,v_k$ from the JL lemma as the rows of $\\Phi$. Fix $i\\in [k]$. Then, \\(\\begin{align*} \\Phi v_i - e_i = (v_1^\\top v_i,\\dots,v_i^\\top v_i,\\dots, v_k^\\top v_i)^\\top - e_i = (v_1^\\top v_i,\\dots,0,\\dots, v_k^\\top v_i)^\\top\\,. \\end{align*}\\) Since by construction $|v_j^\\top v_i|\\leq \\tau$ for $j\\ne i$, the statement follows. \\(\\qquad \\blacksquare\\) . Finally, we need a variation of the result of Question 6 of Assignment 0. This question asked for proving that any algorithm that identifies the single nonzero entry in a binary array of length \\(k\\) requires to look at at least \\((k+1)/2-1/k\\) entries of the array on expectation. A similar lower bound applies if we require the algorithm to be correct with, say, probability \\(1/2\\): . Lemma (High-probability needle lemma): Let $p&gt;0$. Any algorithm that correctly identifies the single nonzero entry in any binary array of length \\(k\\) with probability at least $p$ has the property that the expected number of queries that the algorithm uses is at least \\(\\Omega(p k)\\). In fact, if $q_k$ is the worst-case expected number of queries used by an algorithm that is correct with probability $p$ then one can show that for $k\\ge 2$, $q_k \\ge p( \\frac{k+1}{2}-\\frac{1}{k})$. Proof: Left as an exercise. \\(\\qquad \\blacksquare\\) . With this we are ready to give the proof of the theorem: . Proof (of the theorem): We only give a sketch. Fix the planner $\\mathcal{P}$ with the said properties. Let $k$ be a positive integer to be chosen later. We construct a feature map $\\varphi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ and $k$ MDPs $M_1,\\dots,M_k$ that share $\\mathcal{S}=\\{s,s_{\\text{end}}\\}$ and $\\mathcal{A}=[k]$ as state and action-spaces, respectively. Here $s$ will be chosen as the initial state where the planners will be tested from and $s_{\\text{end}}$ will be an absorbing state with zero reward. The MDPs share the same deterministic transition dynamics: All actions in $s$ end up in $s_{\\text{end}}$ with probability one and all actions taken in $s_{\\text{end}}$ end up in $s_{\\text{end}}$ with probability one. The rewards for actions taken in $s_{\\text{end}}$ are all zero. Finally, we choose the reward of MDP $M_i$ in state $s$ to be . \\[\\begin{align*} r_a^{(i)}(s)=\\mathbb{I}(a=i) r^*\\,, \\end{align*}\\] where the value of $r^*\\in (0,1]$ is left to be chosen later. Then, denoting by $A$ the action returned by the planner when called with state $s$, one can see that the value of the policy induced at $s$ in MDP $M_i$ is \\(r^*\\mathbb{P}_i(A=i)\\), where $\\mathbb{P}_i$ is the distribution induced by the interconnection of the planner and MDP $M_i$. Thus, for \\(r^*=2\\delta\\), the planner needs to return $A$ so that $\\mathbb{P}_i(A=i)\\ge 1/2$. Hence, it needs at least $\\Omega(k)$ calls by the high-probability needle lemma. Finally, the JL feature matrix construction allows us to construct a feature-map for this MDP as the action-value functions take the form \\(q^\\pi(s,a)=\\mathbb{I}(a=i)r^*\\), $q^\\pi(s_{\\text{end}},a)=0$ in this MDP. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec9/#query-lower-bound-for-mdps-with-large-action-sets",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec9/#query-lower-bound-for-mdps-with-large-action-sets"
  },"291": {
    "doc": "9. Limits of query-efficient planning",
    "title": "A lower bound when the number of actions is constant",
    "content": "The previous result leaves open whether query-efficient planners exist with a fixed number of actions. Our next result shows that the problem does not get much easier in this setting either. The result is stated for fixed-horizon MDPs. Given an MDP \\(M=(\\mathcal{S},\\mathcal{A},P,r)\\), a policy \\(\\pi\\), a positive integer \\(h&gt;0\\) and state \\(s\\in \\mathcal{S}\\) of the MDP, let . \\[\\begin{align*} v_h^\\pi(s) = \\mathbb{E}_s^{\\pi}[ \\sum_{t=0}^{h-1} r_{A_t}(S_t)] \\end{align*}\\] be the total reward collected by \\(\\pi\\) when it is used for \\(h\\) steps. The action-value functions \\(q_h^\\pi: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}\\) are defined similarly. The optimal \\(h\\)-step value function is . \\[\\begin{align*} v_h^*(s) = \\sup_{\\pi} v_h^\\pi(s)\\,, \\qquad s\\in \\mathcal{S}\\,. \\end{align*}\\] The Bellman optimality operator \\(T: \\mathbb{R}^{\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}}\\) is defined via . \\[\\begin{align*} T v(s) = \\max_{a\\in \\mathcal{A}} r_a(s) + \\langle P_a(s), v \\rangle\\,. \\end{align*}\\] The policy evaluation operator \\(T_\\pi: \\mathbb{R}^{\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}}\\) of a memoryless policy \\(\\pi\\) is . \\[\\begin{align*} T_\\pi v(s) = \\sum_{a\\in \\mathcal{A}} \\pi(a|s) \\left( r_a(s) + \\langle P_a(s), v \\rangle \\right)\\,. \\end{align*}\\] A policy \\(\\pi\\) is \\(h\\)-step optimal if \\(v_h^\\pi = v_h^*\\). Also, \\(\\pi\\) is greedy with respect to \\(v:\\mathcal{S}\\to \\mathbb{R}\\) if \\(T_\\pi v = T v\\). The analogue of the fundamental theorem looks as follows: . Theorem (fixed-horizon fundamental theorem): We have \\(v_0^*\\equiv \\boldsymbol{0}\\) and for any \\(h\\ge 0\\), \\(v_{h+1}^* = T v_h^*\\). Furthermore, for any \\(\\pi_0^*,\\dots,\\pi_h^*, \\dots\\) such that for \\(i\\ge 0\\), \\(\\pi_i^*\\) is greedy with respect to \\(v_i^*\\), for any \\(h&gt;0\\) it holds that \\(\\pi=(\\pi_{h-1}^*,\\dots,\\pi_0^*,\\dots)\\) (i.e., the policy which in step \\(1\\) uses \\(\\pi_{h-1}^*\\), in step \\(2\\) uses \\(\\pi_{h-2}^*\\), \\(\\dots\\), in step \\(h\\) uses \\(\\pi_0^*\\), after which it continues arbitrarily) is \\(h\\)-step optimal: . \\[\\begin{align*} v_h^{\\pi} = v_h^*\\,. \\end{align*}\\] . Proof: Left as an exercise. Hint: Use induction. \\(\\qquad \\blacksquare\\) . In the theorem our earlier notion of policies is slightly abused: \\(\\pi\\) is only specified for $h$ steps. In any case, according to this result for a fixed horizon \\(H&gt;0\\), the natural analogue for memoryless policies are these $H$-step nonstationary memoryless policies. Let us denote the set of these by \\(\\Pi_H\\). In the next result, we will only care about optimality with respect to a fixed initial state \\(s_0\\in \\mathcal{S}\\). Then, without loss of generality, we also assume that the set of states \\(\\mathcal{S}_h\\) reachable from \\(s_0\\) in \\(h\\ge 0\\) steps are disjoint: \\(\\mathcal{S}_h\\cap \\mathcal{S}_{h'}=\\emptyset\\) for \\(h\\ne h'\\) (why?). It follows that we can also find a memoryless policy \\(\\pi\\) that is optimal at \\(s_0\\): \\(v^{\\pi}_H(s_0)=v_H^*(s_0)\\). In fact, one can even find a memoryless policy that also satisfies . \\[\\begin{align} v^{\\pi}_{H-i}(s)=v_{H-i}^*(s), \\qquad s\\in \\mathcal{S}_i \\end{align}\\] simultaneously for all \\(0\\le i \\le H-1\\). Furthermore, the same holds for the action-value functions: . \\[\\begin{align} q^{\\pi}_{H-i}(s,a)=q_{H-i}^*(s,a), \\qquad s\\in \\mathcal{S}_i, a\\in \\mathcal{A}, 0\\le i \\le H-1\\,. \\end{align}\\] Thus, the natural analogue that all action-value functions are well-approximated with some feature-map is that there are feature-maps \\((\\varphi_h)_{0\\le h \\le H-1}\\) such that for \\(0\\le h \\le H-1\\), \\(\\varphi_h: \\mathcal{S}_h \\times \\mathcal{A} \\to \\mathbb{R}^d\\) and for any memoryless policy \\(\\pi\\), the \\(H-h\\)-step action value function of \\(\\pi\\), when restricted to \\(\\mathcal{S}_h\\), is well-approximated by the linear combination of the basis functions induced by \\(\\varphi_h\\). Since we will not need \\(q^{\\pi}_{H-h}\\) outside of \\(\\mathcal{S}_h\\), in what follows, we assume that these are restricted to \\(\\mathcal{S}_h\\). Writing \\(\\Phi_h\\) for the feature matrix induced by \\(\\varphi_h\\) (the rows of \\(\\Phi_h\\) are the feature vectors under \\(\\varphi_h\\) for some ordering of the state-action pairs from \\(\\mathcal{S}_{h}\\times \\mathcal{A}\\)), we redefine \\(\\varepsilon^*(M,\\Phi)\\) as follows: . \\(\\begin{align} \\varepsilon^*(M,\\Phi) : = \\sup_{\\pi \\text{ memoryless}} \\max_{0\\le h \\le H-1}\\inf_{\\theta\\in \\mathbb{R}^d} \\| \\Phi_h \\theta - q^{\\pi}_{H-h} \\|_\\infty\\,. \\end{align}\\) . Since we changed the objective, we also need to change the definition of $(\\delta,\\varepsilon)$-sound online planners: These planners now need to induce policies that are $\\delta$-suboptimal or better when evaluated with the $H$-horizon undiscounted total reward criterion from the designated start-state \\(s_0\\) provided that the MDP satisfies $\\varepsilon^*(M,\\Phi)\\le \\varepsilon$. In what follows, we call these planners $(\\delta,\\varepsilon)$-sound for the $H$-step criterion. With this, we are ready to state the main result of this section: . Theorem (Query lower bound: small action sets, fixed-horizon objective): For $\\varepsilon&gt;0$, $0&lt;\\delta\\le 1/2$ and positive integer $d$, let . \\[\\begin{align*} u(d,\\varepsilon,\\delta) = \\left\\lfloor\\exp\\left(\\frac{d (\\frac{\\varepsilon}{2\\delta})^2}{8}\\right)\\right\\rfloor\\,. \\end{align*}\\] Then, for any $\\varepsilon&gt;0$, $0&lt;\\delta\\le 1/2$, positive integers $\\mathrm{A},H,d$ such that \\(d\\le\\mathrm{A}^H\\) and for any online planner $\\mathcal{P}$ that is $(\\delta,\\varepsilon)$-sound for MDPs with at most $\\mathrm{A}$ actions and the $H$-step criterion, there exists a “featurized-MDP” $(M,\\varphi)$ with \\(\\mathrm{A}\\) actions and rewards in $[0,1]$ such that when interacting with a simulator of $(M,\\varphi)$, the expected number of queries used by $\\mathcal{P}$ is at least . \\[\\begin{align*} \\tilde\\Omega\\left( \\frac{u(d,\\varepsilon,\\delta)}{d(\\varepsilon/\\delta)^2}\\right)\\, \\end{align*}\\] provided that \\(\\mathrm{A}^H&gt;u(d,\\varepsilon,\\delta)\\) (“large horizons”), while it is . \\[\\begin{align*} \\tilde\\Omega\\left( \\frac{\\mathrm{A}^H}{ H }\\right)\\, \\end{align*}\\] otherwise (“small horizon”). In words, if the horizon is large enough, the previous exponential-in-\\(d\\) lower bound continues to hold, while for horizons that are smaller, a lower bound that is exponential in the horizon holds. Note that above \\(\\tilde\\Omega(\\cdot)\\) hides logarithmic terms. Note that the condition \\(d\\le\\mathrm{A}^H\\) is reasonable: We do not expect the feature-space dimension to be comparable to \\(\\mathrm{A}^H\\). Proof: Fix a planner $\\mathcal{P}$ with the required properties. We consider $k=\\mathrm{A}^H$ MDPs $M_1,\\dots,M_k$ that share the state space \\(\\mathcal{S} = \\cup_{0\\le h \\le H} \\mathcal{A}^h\\) and action space \\(\\mathcal{A}\\). Here, by convention, \\(\\mathcal{A}^0\\) is a singleton with the single element \\(\\perp\\), which will play the role of the start state \\(s_0\\). The transition dynamics are also shared by these MDPs: When in state \\(s\\in \\mathcal{S}\\) and action \\(a\\in \\mathcal{A}\\) is taken, the next state is \\(s'=(a)\\) when \\(s=\\perp\\), while if \\(s=(a_1,\\dots,a_h)\\) with some \\(1\\le h \\le H-1\\) then \\(s'=(a_1,\\dots,a_h,a)\\) and when \\(h=H\\) then the next state is \\(s\\) (ever state in \\(\\mathcal{A}^H\\) is absorbing). The MDPs differ in their reward functions. To describe the rewards let \\(f\\) be a bijection from \\([k]\\) to \\(\\mathcal{A}^H\\). Now, fix \\(1\\le i \\le k\\) and define \\((a_0^*,\\dots,a_{H-1}^*)\\) by \\(f(i)=(a_0^*,\\dots,a_{H-1}^*)\\). Let \\(s_0^*=s_0\\), \\(s_1^*=(a_0^*)\\), \\(s_2^*=(a_0^*,a_1^*)\\), \\(\\dots\\), \\(s_H^*=(a_0^*,\\dots,a_{H-1}^*)\\). Then, in MDP \\(M_i\\), \\(r_{a_{H-1}^*}(s_{H-1}^*)=2\\delta\\) while \\(r_a(s)=0\\) for any other state-action pair. Note that the optimal reward in \\(H\\) steps from \\(\\perp\\) is \\(2\\delta\\) and the only policy that achieves this reward is the one that goes through the states in \\(s_0^*,s_1^*,\\dots,s_{H-1}^*\\). We can visualize MDP \\(M_i\\) as a tree, as seen on the figure on the right. The green nodes on the figure correspond to the states \\(s_0^*,s_1^*,\\dots,s_{H-1}^*,s_H^*\\). Note also that \\(\\mathcal{S}_h = \\mathcal{A}^h\\) for \\(0\\le h \\le H\\). We will now describe the action-value functions of the memoryless policies in \\(M_i\\) as this will be useful later. Fix \\(0\\le h \\le H-1\\). Then, \\(q^{\\pi}_{H-h}\\), by our convention, is defined over \\(\\mathcal{S}_h\\). Then, for any \\(s\\in \\mathcal{S}_h (=\\mathcal{A}^h)\\) and \\(a\\in \\mathcal{A}\\), . \\[\\begin{align} q^\\pi_{H-h}(s,a) = \\begin{cases} 2\\delta\\,, &amp; \\text{if } h=H-1, s=s_{H-1}^*, a=a_{H-1}^*\\,;\\\\ v^\\pi_{H-h-1}(s_{h+1}^*)\\,, &amp; \\text{if } h&lt;H-1, s=s_h^*, a=a_h^*\\,;\\\\ 0\\,, &amp; \\text{otherwise}\\,. \\end{cases} \\label{eq:qpiinfh} \\end{align}\\] Note that here \\(0\\le v^\\pi_{H-h-1}(g(s,a))\\le 2\\delta\\). We see that for each stage \\(0\\le h \\le H-1\\), there is only one state-action pair such that the value of \\(q^\\pi_{H-h}\\) is nonzero, and in this case the value is in the \\([0,2\\delta]\\) interval. Now, since the planner induces a policy with suboptimality \\(\\delta\\), for the action \\(A\\) it returns it holds that \\(\\mathbb{P}(A\\ne a_0^*)\\le 1/2\\) (any other action than \\(a_0^*\\) incurs zero total expected reward in our construction). Then with \\(b\\ge \\log(2H)/\\log(2)=\\log_2(2H)\\) fresh calls, by taking the action \\(A_0\\) that is returned most often in these calls, we get \\(\\mathbb{P}(A_0\\ne a_0^*)\\le 1/(2H)\\). Repeating this process in state \\(S_1=g(s_0,A_0)\\) we get action \\(A_1\\) so that . \\[\\begin{align*} \\mathbb{P}(A_0\\ne a_0^* \\text{ or } A_1\\ne a_1^*) &amp; = \\mathbb{P}(A_0\\ne a_0^*)+ \\mathbb{P}(A_0= a_0^*,A_1\\ne a_1^*)\\\\ &amp; \\le \\mathbb{P}(A_0\\ne a_0^*)+ \\mathbb{P}(A_1\\ne a_1^*|A_0= a_0^*) \\le \\frac{1}{2H}+\\frac{1}{2H}\\,. \\end{align*}\\] Now, repeating again the process in state \\(S_2 = g(S_1,A_1)\\) gives \\(A_2\\), etc. Eventually, we get a sequence of actions \\(A_0,\\dots,A_{H-1}\\) such that \\(\\mathbb{P}(A_0\\ne a_0^* \\text{ or } \\dots \\text{ or } A_{H-1}\\ne a_{H-1}^*)\\le 1/2\\). By our previous argument (reduction to the “needle” problem), this whole process needs \\(\\Omega(k)\\) queries. If the expected number of queries issued by \\(\\mathcal{P}\\) is \\(q\\), the expected number of queries issued here is \\(H \\log_2(2H) q\\). Hence, . \\[q = \\Omega\\left( \\frac{k}{\\log_2(2H)H} \\right)\\,.\\] Let us now consider a choice for \\(\\Phi = (\\Phi_h)_{0\\le h \\le H-1}\\) such that \\(\\varepsilon^*(M,\\Phi)\\le \\varepsilon\\). For \\(\\Phi_h\\) choose first a “JL feature matrix” \\(\\tilde \\Phi_h\\in \\mathbb{R}^{|\\mathcal{S}_h| \\times d}\\) such that Eq. \\eqref{eq:featjl} holds. Then let \\(\\Phi_h = \\sqrt{2\\delta} \\tilde \\Phi_h\\). Choose \\(\\theta_h = v^\\pi_{H-h-1}( s_{h+1}^* ) \\varphi_h( s_h^*,a_{h}^* )/(2\\delta)\\) if \\(h&lt;H-1\\) and choose \\(\\theta_h = \\varphi_h( s_h^*,a_{h}^* )\\), otherwise. Then, by Eq. \\eqref{eq:qpiinfh}, for \\((s,a)\\ne (s_h^*,a_{h}^*)\\), \\(|\\varphi_h(s,a)^\\top \\theta_h-q_{H-h}(s,a)| \\le |v^\\pi( s_{h+1}^* )| \\, |\\tilde \\varphi_h(s,a)^\\top \\tilde \\varphi_h(s_h^*,a_{h}^*)|\\le 2\\delta \\tau\\) and for \\((s,a)=(s_h^*,a_{h}^*)\\), \\(\\varphi_h(s,a)^\\top \\theta_h=q_{H-h}(s,a)\\). Hence, \\(\\varepsilon^*(M,\\Phi)\\le \\varepsilon\\) holds if we set \\(\\tau=\\varepsilon/(2\\delta)\\). From Eq. \\eqref{eq:krange}, \\(\\tilde \\Phi_h\\) exists if \\(d\\le k\\) and . \\[\\begin{align*} k \\le u:=\\left\\lfloor\\exp\\left(\\frac{d (\\frac{\\varepsilon}{2\\delta})^2}{8}\\right)\\right\\rfloor\\,. \\end{align*}\\] Recall that \\(k = \\mathrm{A}^H\\). Thus, the required claim holds for the case when \\(\\mathrm{A}^H\\le u\\) (“small horizon case”). In the opposite case (“large horizon”), let \\(\\tilde H\\) be the largest positive number such that \\(\\mathrm{A}^{\\tilde H}\\le u\\) holds. Repeating the above argument with horizon \\(\\tilde H\\) gives the lower bound \\(q = \\Omega\\left( \\frac{\\mathrm{A}^{\\tilde H}}{\\log_2(2\\tilde H)\\tilde H} \\right) = \\Omega\\left( \\frac{u}{\\log_2(2\\tilde H)\\tilde H} \\right)\\,,\\) which finishes the proof. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec9/#a-lower-bound-when-the-number-of-actions-is-constant",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec9/#a-lower-bound-when-the-number-of-actions-is-constant"
  },"292": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Proof of the JL lemma",
    "content": "For completeness, we include a proof of the JL lemma. The proof uses the so-called probabilistic method The idea of this is that sometimes it is easier to establish the existence of some “good configuration” (like the nearly orthogonal vectors on the unit sphere in the JL lemma) by establishing that such a configuration has positive probability under some probability distribution over possible configurations. In our case, this works as follows: Let $V_1,\\dots,V_k$ be random vectors, each uniformly distributed on the $d$-dimensional unit sphere and so that the distinct vectors in this sequence are pairwise independent of each other. Take $i\\ne j$. If we show that $|\\langle V_i, V_j \\rangle| \\le \\tau$ holds with probability at least $1-1/k^2$, by a union bound over the $k(k-1)/2$ pairs $1\\le i &lt;j \\le k$, it follows that $\\max_{i\\ne j} |\\langle V_i,V_j | \\le \\tau$ holds with probability at least $1/2$, from which, the lemma follows. Thus, it remains to show that the angle between the random vectors $V_i$ and $V_j$ is “small” with the claimed probability. Since the uniform distribution is rotation invariant and $V_i$ and $V_j$ are independent of each other, $\\langle V_i, V_j \\rangle$ has the same distribution as $\\langle e_1, V_1 \\rangle = V_{11}\\in [-1,1]$. To see this take a rotation $R$ that rotates $V_i$ to $e_1$; then $\\langle V_i, V_j \\rangle = \\langle R V_i, R^{-1} V_j \\rangle = \\langle e_1, R^{-1} V_j \\rangle$. Now, since $R$ and $V_j$ are independent of each other, $R^{-1} V_j$ is still uniformly distributed on the sphere, hence, $\\langle e_1, R^{-1} V_j \\rangle$ and $\\langle e_1, V_1 \\rangle$ share the same distribution. A tedious calculation shows that for any $x\\ge 6$, . \\[\\begin{align} \\mathbb{P}( V_{11}^2 &gt; x/d) \\le \\exp(-x/4)\\,. \\label{eq:dgtail} \\end{align}\\] (The idea of proving this is to notice that if $X$ is $d$-dimensional standard normal variable then \\(V=X/\\|X\\|_2\\) is uniformly distributed on the sphere. Then, one proceeds using Chernoff’s method.) The result now follows from \\eqref{eq:dgtail} by choosing $x$ so that $\\tau^2 = x/d$ holds. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec9/#proof-of-the-jl-lemma",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec9/#proof-of-the-jl-lemma"
  },"293": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Notes",
    "content": ". | The lower bound for the discounted case is missing the planning horizon. In the fixed-horizon setting, the lower bound is again missing the horizon when the horizon is large. It remains to be seen whether the extra “horizon terms” in Eq. \\eqref{eq:limit} are necessary. | In any case, the main conclusion is that even when we require “strong features”, high-accuracy planning is intractable. | The reader familiar with the TCS literature may recognize a close resemblance to questions studied there which are concerned with the existence of “fully polynomial time approximation schemes” (FPTAS). | There are many open questions. For one, is there a counterpart of the second theorem for the discounted setting? . | . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec9/#notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec9/#notes"
  },"294": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Bibliographical notes",
    "content": "The idea of using the Johnson-Lindenstrauss lemma in this context is due to Du, Kakade, Wang and Yang (DKWY, for short). The first theorem is a variant of a result from this paper. The second theorem is a variation of Theorem 4.1 from the paper of Du et al. mentioned above who prove the analoge result for global planners. The proof of the lemma also follows the proof given in this paper. The proof of inequality \\eqref{eq:dgtail} is given in a paper of Dasgupta and Gupta, which also gives the “full version” of the Johnson-Lindenstrauss lemma which states that logarithmically many dimensions are sufficient to keep pairwise distances between a finite set of points. | Dasgupta, Sanjoy; Gupta, Anupam (2003), “An elementary proof of a theorem of Johnson and Lindenstrauss” link, Random Structures &amp; Algorithms, 22 (1): 60–65 | . The presentation of the first result which is for “bandits” (fixed horizon problems with $H=1$) follows closely that of a paper by Lattimore, Weisz and yours truly. This, and a paper by van Roy and Dong were both prompted by the DKWY paper, whose initial version focused on the case when $\\delta \\ll \\sqrt{d} \\varepsilon$, which made the outlook for designing robust RL methods quite bleak. While it is true that in this high-precision regime nothing much can be done (unless further restricting the features), both papers emphasized that the hardness result disappears when the algorithm can deliver $\\delta$ optimal policies with $\\delta \\gtrsim \\sqrt{d} \\varepsilon$. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec9/#bibliographical-notes",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec9/#bibliographical-notes"
  },"295": {
    "doc": "9. Limits of query-efficient planning",
    "title": "9. Limits of query-efficient planning",
    "content": "PDF Version . In the last lecture we have seen that given a discounted MDP $M = (\\mathcal{S},\\mathcal{A},P,r,\\gamma)$, a feature-map $\\varphi: \\mathcal{S}\\times\\mathcal{A}\\to \\mathbb{R}^d$ and a precomputed, suitably small core set, for any $\\varepsilon’&gt;0$ target and any confidence parameter $0\\le \\zeta \\le 1$, interacting with a simulator of $M$, with at most $\\text{poly}(\\frac{1}{1-\\gamma},d,\\mathrm{A},\\frac{1}{(\\varepsilon’)^2},\\log(1/\\zeta))$, compute time, LSPI returns some weight vector $\\theta\\in \\mathbb{R}^d$ such that with probability $1-\\zeta$, the policy that is greedy with respect to $q = \\Phi \\theta$ is $\\delta$-suboptimal with . \\[\\begin{align} \\delta \\le \\frac{2(1 + \\sqrt{d})}{(1-\\gamma)^2}\\, \\varepsilon + \\varepsilon'\\,, \\label{eq:suboptapi} \\end{align}\\] where $\\varepsilon$ is the error with which the features can approximate the action-value functions of the policies of the MDP: . \\[\\begin{align} \\varepsilon = \\varepsilon^*(M,\\Phi) : = \\sup_{\\pi \\text{ memoryless}} \\inf_{\\theta\\in \\mathbb{R}^d} \\| \\Phi \\theta - q^\\pi \\|_\\infty\\,. \\label{eq:polerr} \\end{align}\\] Here, following our earlier convention, $\\Phi$ refers to the \\(| \\mathcal{S}\\times\\mathcal{A} | \\times d\\) matrix that is obtained by stacking the feature vectors $\\varphi^\\top(s,a)$ of all possible state-action pairs on the top of each other in some fixed order. Setting $\\varepsilon’$ to match the first term in Eq. \\eqref{eq:suboptapi}, we can keep the effort polynomial in the relevant quantities (including $1/\\varepsilon$), but even in the limit of infinite computation, the best bound we can obtain is . \\[\\begin{align} \\delta \\le \\frac{2(1 + \\sqrt{d})}{(1-\\gamma)^2}\\, \\varepsilon\\,. \\label{eq:limit} \\end{align}\\] While it makes sense that with a reasonable compute effort $\\delta$ cannot be better than $\\varepsilon$ or a constant multiple of $\\varepsilon$, it is unclear whether the extra $\\sqrt{d}/(1-\\gamma)^2$ factor is an artifact of the proof. We may suspect that some power of $1/(1-\\gamma)$ may be necessary, because even if we knew the parameter vector that gives the best approximation to \\(q^*\\), the error incurred by acting greedily with respect to $q^*$ could be as large as . \\[\\frac{\\varepsilon}{1-\\gamma}\\,.\\] However, at this point, it is completely unclear whether the extra \\(\\sqrt{d}\\) factor is necessary. The main question asked in this lecture: Are the “extra” factors truly necessary in the above bound? Or are there some other polynomial runtime algorithms that are able to produce policies with smaller suboptimality? . In this lecture we will give a partial answer to this question: We will justify the presence of $\\sqrt{d}$. We start with a lower bound that shows that when there is no limit on the number of actions, efficient algorithms are limited to $\\delta = \\Omega( \\varepsilon\\sqrt{d})$. ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps/lec9/",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps/lec9/"
  },"296": {
    "doc": "Planning in MDPs",
    "title": "Planning in MDPs",
    "content": "PDF Version . ",
    "url": "/2024/w2022-lecture-notes/planning-in-mdps",
    
    "relUrl": "/w2022-lecture-notes/planning-in-mdps"
  },"297": {
    "doc": "Batch RL",
    "title": "Batch RL",
    "content": " ",
    "url": "/2024/w2021-lecture-notes/batch-rl",
    
    "relUrl": "/w2021-lecture-notes/batch-rl"
  },"298": {
    "doc": "17. Introduction",
    "title": "How good is the plug-in method?",
    "content": "The plug-in method estimates a model and uses the estimated model in place of the real one to solve the problem at hand. Let $M = (\\mathcal{S},\\mathcal{A},P,r)$ be a finite MDP, $\\hat M = (\\mathcal{S},\\mathcal{A},\\hat P,\\hat r)$ be an estimate. The estimate can be produced in a number of ways, but from the perspective of the result that comes, how the estimate is produced does not matter. We consider the discounted case with a discount factor $0\\le \\gamma&lt;1$. We will use $\\hat v^\\pi$ to denote the value function of a policy $\\pi$ in $\\hat M$ (as opposed to $v^\\pi$, which is the value function of policy in $M$), and similarly, we will use \\(\\hat v^*\\) to denote the optimal value function in $\\hat M$. We analogously use $\\hat q^\\pi$ and \\(\\hat q^*\\). Every other quantity that is usually associated with an MDP but which now is associated with $\\hat M$ receives a “hat”. For example, we use $\\hat T_\\pi$ for the policy evaluation operator of memoryless policy $\\pi$ in $\\hat M$ (either for the state values, or the action-values), while we use $\\hat T$ to denote the Bellman optimality operator underlying $\\hat M$ (again, both for the state and action-values). We start with a generic result about contraction mappings: . Proposition (residual bound): Let $F: V \\to V$ be a $\\gamma$-contraction over a normed vector space $V$ and let $x\\in V$ be a fixed-point of $F$. Then for any $y\\in V$, . \\[\\begin{align} \\| x - y \\| \\le \\frac{\\| Fy - y \\|}{1-\\gamma}\\,. \\label{eq:resbound} \\end{align}\\] . Proof: By the triangle inequality, . \\[\\| x- y \\| \\le \\| Fx - Fy \\| + \\| F y - y \\| \\le \\gamma \\| x-y \\| + \\| Fy - y \\|\\,.\\] Reordering and solving for $| x-y |$ gives the result. \\(\\qquad \\blacksquare\\) . An immediate implication is that good model estimates are guaranteed to give rise to (relatively) good value estimates. Proposition (value estimation error): Let $H_\\gamma = 1/(1-\\gamma)$ and assume that the rewards in $M$ are in the $[0,1]$ interval. For any policy $\\pi$, the following holds: . \\[\\begin{align} \\| v^\\pi - \\hat v^\\pi \\|_\\infty &amp; \\le H_\\gamma \\left( \\| r_\\pi-\\hat r_\\pi\\|_\\infty + \\gamma \\| ( P_\\pi - \\hat P_\\pi) v^\\pi \\|_\\infty \\right) \\\\ &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma H_\\gamma \\| P - \\hat P\\|_\\infty \\right)\\,. \\end{align}\\] Also, . \\[\\begin{align} \\| v^* - \\hat v^* \\|_\\infty &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma \\| (P - \\hat P ) v^*\\|_\\infty \\right) \\\\ &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma H_\\gamma \\| P - \\hat P\\|_\\infty \\right)\\,. \\end{align}\\] Similarly, . \\[\\begin{align} \\| q^\\pi - \\hat q^\\pi \\|_\\infty &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma \\| (P - \\hat P ) v^\\pi\\|_\\infty \\right) \\\\ &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma H_\\gamma \\| P - \\hat P\\|_\\infty \\right)\\,. \\end{align}\\] and . \\[\\begin{align} \\| q^* - \\hat q^* \\|_\\infty &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma \\| (P - \\hat P)v^*\\|_\\infty \\right) \\label{eq:qsdiff1} \\\\ &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma H_\\gamma \\| P - \\hat P\\|_\\infty \\right)\\,. \\label{eq:qsdiff} \\end{align}\\] . Note that in general the value estimates are more sensitive to errors in the transition probabilities then in the rewards. In particular, the transition errors can be magnified by a factor as large as $H_\\gamma$, while the reward errors are magnified by at most $H_\\gamma$. Also note that sometimes one can obtain tighter estimates with stopping earlier in the derivations of these bounds. We will see some examples of how this can help later. Proof: To reduce clutter, we write \\(\\| \\cdot \\|\\) for \\(\\|\\cdot\\|_\\infty\\). Let $F = \\hat T_\\pi$, where $\\hat T_\\pi$ is defined via $\\hat T_\\pi v = \\hat r_\\pi + \\gamma \\hat P_\\pi v$. By the residual bound \\eqref{eq:resbound}, . \\[\\| \\hat v^\\pi - v^\\pi \\| \\le H_\\gamma \\| \\hat T_\\pi v^\\pi - v^\\pi \\| = H_\\gamma \\| \\hat T_\\pi v^\\pi - T_\\pi v^\\pi \\| \\le H_\\gamma \\left( \\| r_\\pi-\\hat r_\\pi\\| + \\gamma \\| (P_\\pi - \\hat P_\\pi) v^\\pi \\|\\right).\\] The second inequality follows from separating $v^\\pi$ from the second term and bounding it using \\(\\| v^\\pi \\| \\le H_\\gamma\\) and also using that $r_\\pi = M_\\pi r$, $\\hat r_\\pi = M_\\pi \\hat r$, $P_\\pi = M_\\pi P$ and $\\hat P_\\pi = M_\\pi \\hat P$ and finally using that $M_\\pi$ is a nonexpansion. The remaining inequalities can be obtained in an entirely analogous manner and hence their proof is omitted. \\(\\qquad \\blacksquare\\) . The result just shown suffices to quantify the size of the value errors. For quantifying the policy optimization error that results from finding an optimal (or near optimal) policy for $\\hat M$, recall the Policy Error Bound from Lecture 6: . Lemma (Policy error bound - I.): Let $\\pi$ be a memoryless policy and choose a function $q:\\mathcal{S}\\times\\mathcal{A} \\to \\mathbb{R}$ and $\\epsilon\\ge 0$. Then, the following hold: . | If $\\pi$ is $\\epsilon$-optimizing in the sense that \\(\\sum_a \\pi(a\\vert s) q^*(s,a) \\ge v^*(s)-\\epsilon\\) holds for every state $s\\in \\mathcal{S}$ then $\\pi$ is $\\epsilon/(1-\\gamma)$ suboptimal: \\(v^\\pi \\ge v^* - \\frac{\\epsilon}{1-\\gamma} \\boldsymbol{1}\\,.\\) . | If $\\pi$ is greedy with respect to $q$ then $\\pi$ is $2\\epsilon$-optimizing with \\(\\epsilon= \\|q-q^*\\|_\\infty\\) and thus . | . \\[v^\\pi \\ge v^* - \\frac{2\\|q-q^*\\|_\\infty}{1-\\gamma} \\boldsymbol{1}\\,.\\] . This leads to the following result: . Theorem (bound on policy optimization error): Assume that the rewards both in $M$ and $\\hat M$ belong to the $[0,1]$ interval. Take any $\\varepsilon&gt;0$ and $\\varepsilon$-optimal policy $\\pi$ in $\\hat M$: $\\hat v^\\pi \\ge \\hat v^* - \\varepsilon \\boldsymbol{1}$. Then, $\\pi$ is $\\delta$-optimal in $M$ with $\\delta$ satisfying . \\[\\begin{align*} \\delta \\le (1+2\\gamma) H_\\gamma \\varepsilon + 2 H_\\gamma^2\\left\\{ \\|r-\\hat r\\|_\\infty + \\gamma \\| (P-\\hat P)v^*\\|_\\infty \\right\\}\\,. \\end{align*}\\] . Note that, up to a small constant factor, the optimization error is magnified by a factor of $H_\\gamma$, the reward errors are magnified by a factor of $H_\\gamma^2$, while the transition errors can get magnified by a factor of up to $H_\\gamma^3$, depending on the magnitude of $v^*$. Proof: Let $\\pi$ be a policy as in the theorem statement. Our goal now is to use the first part of the “Policy error bound”, i.e., that $\\pi$ is $\\varepsilon’$-optimizing with some $\\varepsilon’&gt;0$. On the one hand, we have . \\[M_\\pi \\hat q^\\pi = \\hat v^\\pi \\ge \\hat v^* - \\varepsilon \\boldsymbol{1} = M \\hat q^* - \\varepsilon\\boldsymbol{1} \\ge M \\hat q^\\pi - \\varepsilon \\boldsymbol{1}\\,.\\] Let $z$ be defined by $M_\\pi \\hat q^\\pi = M \\hat q^\\pi + z$. From the previous inequality, we know that \\(\\| z \\|_\\infty \\le \\varepsilon\\). We also have . \\[\\begin{align*} M_\\pi q^* &amp; = M_\\pi \\hat q^\\pi + M_\\pi (q^* - \\hat q^\\pi) \\\\ &amp; = M \\hat q^\\pi + M_\\pi(q^*-\\hat q^\\pi) + z \\\\ &amp; = M q^* + M\\hat q^\\pi-M q^* + M_\\pi(q^*-\\hat q^\\pi) + z \\\\ &amp; \\ge M q^* - (2\\| \\hat q^\\pi - q^* \\|+\\varepsilon) \\boldsymbol{1}\\\\ &amp; = v^* - (2\\| \\hat q^\\pi - q^* \\|+\\varepsilon) \\boldsymbol{1}\\,. \\end{align*}\\] Hence, by Part 1. of the “Policy Error Bound I.” lemma from above, . \\[v^\\pi \\ge v^* - H_\\gamma (2\\| \\hat q^\\pi - q^* \\|+\\varepsilon) \\boldsymbol{1}\\,.\\] By the triangle inequality and the assumption on $\\pi$, . \\[\\begin{align*} \\| \\hat q^\\pi - q^* \\|_\\infty &amp; \\le \\| \\hat q^\\pi - \\hat q^* \\|_\\infty + \\| \\hat q^* - q^* \\|_\\infty \\le \\gamma \\varepsilon + \\| \\hat q^* - q^* \\|_\\infty\\,. \\end{align*}\\] By Eq. \\eqref{eq:qsdiff1}, . \\[\\begin{align*} \\| q^* - \\hat q^* \\|_\\infty &amp; \\le H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma \\| (P - \\hat P) v^*\\|_\\infty \\right)\\,. \\end{align*}\\] The result is obtained by chaining the inequalities: . \\[\\begin{align*} \\| v^*-v^\\pi\\|_\\infty &amp; \\le H_\\gamma (2\\| \\hat q^\\pi - q^* \\|+\\varepsilon)\\\\ &amp; \\le H_\\gamma \\left\\{2\\gamma \\varepsilon + 2 H_\\gamma \\left( \\| r-\\hat r\\|_\\infty + \\gamma \\| (P - \\hat P) v^*\\|_\\infty \\right)+\\varepsilon\\right\\}\\,. \\qquad \\qquad \\qquad \\qquad \\blacksquare \\end{align*}\\] ",
    "url": "/2024/w2021-lecture-notes/batch-rl/lec17/#how-good-is-the-plug-in-method",
    
    "relUrl": "/w2021-lecture-notes/batch-rl/lec17/#how-good-is-the-plug-in-method"
  },"299": {
    "doc": "17. Introduction",
    "title": "Model estimation error: Tabular case",
    "content": "As usual, it is worthwhile to clean up the foundations by considering the tabular case. In this case, the model can be estimared by using sample means. To allow for a unified presentation, let the data available be given in the form of triplets of the form $E_i=(S_i,A_i,R_i,S_{i+1})$ where $i=1,\\dots,n$ and $S_{i+1}\\sim P_{A_i}(S_i)$ given $E_1,\\dots,E_{i-1},S_i,A_i$ and $\\mathbb{E}[R_i|S_i,A_i,E_1,\\dots,E_{i-1}] = r_{A_i}(S_i)$. Introducing the visit counts . \\[\\begin{align*} N(s,a,s') = \\sum_{i=1}^n \\mathbb{I}(S_i=s,A_i=a,S_{i+1}=s') \\end{align*}\\] and \\(N(s,a) = \\sum_{s'} N(s,a,s')\\), provided that the visit count for $(s,a)$ is positive, for the transition probability estimates we have . \\[\\begin{align*} \\hat P_{a}(s,s') = \\frac{N(s,a,s')}{N(s,a)} \\end{align*}\\] and for the reward estimate we have . \\[\\begin{align*} \\hat r_{a}(s) = \\frac{1}{N(s,a)} \\sum_{i=1}^n \\mathbb{I}( S_i=s,A_i=a) R_i\\,. \\end{align*}\\] For ensuring that these are always defined, let $\\hat P_a(s)$ be the uniform distribution over the states and let $\\hat r_a(s)=0$ when $N(s,a)=0$. From the perspective of the results to be presented, the particular values chosen here do not matter. Consider now the simple case when the above triplets are so that for each state-action pair $(s,a)$, $N(s,a)=n(s,a)$ for some deterministic counts $(n(s,a))_{s,a}$. Say, one has access to a generative model (simulator) and for each state-action pair the model is used to generate a fixed number of independent transitions. In this case, one can use Hoeffding’s inequality. In particular, defining . \\[\\beta(n,\\zeta) = \\sqrt{ \\frac{\\log\\left(\\frac{\\mathrm{SA}}{\\zeta}\\right)}{2 n}}\\] provided that $R_i\\in [0,1]$, Hoeffding’s inequality gives that with probability $1-2\\zeta$, for any $s,a$, . \\[\\begin{align*} | \\hat r_a(s) - r_a(s) | &amp; \\le \\beta(n(s,a),\\zeta)\\,,\\\\ | \\langle \\hat P_a(s) - P_a(s), v^* \\rangle | &amp; \\le H_\\gamma \\beta(n(s,a),\\zeta)\\,, \\end{align*}\\] from which it follows that with probability $1-2\\zeta$, . \\[\\begin{align*} \\| \\hat r - r \\|_{\\infty} &amp; \\le \\beta(n_{\\min},\\zeta)\\,,\\\\ \\| ( \\hat P - P) v^* \\|_\\infty &amp; \\le H_\\gamma \\beta(n_{\\min},\\zeta) \\,, \\end{align*}\\] where $n_{\\min} = \\min_{s,a} n(s,a)$. Plugging the obtained deviation bound into our policy suboptimality bound, we get that with probability $1-\\zeta$, . \\[\\begin{align*} \\delta \\le (1+2\\gamma) H_\\gamma \\varepsilon + 2 H_\\gamma^2 (1+\\gamma H_\\gamma) \\beta(n_{\\min},\\zeta) \\,. \\end{align*}\\] One can alternatively write this in terms of the total number of observations, $n$. The best case is when $n(s,a)=n_{\\min}$ for all $(s,a)$ pairs, in which case $n = \\mathrm{SA} n_{\\min}$ and the above bound gives . \\[\\begin{align*} \\delta \\le (1+2\\gamma) H_\\gamma \\varepsilon + 2 H_\\gamma^2 (1+\\gamma H_\\gamma) \\sqrt{ \\mathrm{SA} \\frac{\\log\\left(\\frac{\\mathrm{SA}}{\\zeta}\\right)}{2 n}} \\,. \\end{align*}\\] It follows that for any target suboptimality $\\delta_{\\text{trg}}$, as long as $n$, the number of observations satisfies . \\[n \\ge \\frac{8 H_\\gamma^6 SA \\log\\left(\\frac{\\mathrm{SA}}{\\zeta}\\right)}{\\delta_{\\text{trg}}^2}\\,,\\] we are guaranteed that the optimal policy of the estimated model is at most $\\delta_{\\text{trg}}$ suboptimal. As we shall see soon, the optimal dependence on the horizon $H_\\gamma$ is cubic, unlike the dependence shown here. ",
    "url": "/2024/w2021-lecture-notes/batch-rl/lec17/#model-estimation-error-tabular-case",
    
    "relUrl": "/w2021-lecture-notes/batch-rl/lec17/#model-estimation-error-tabular-case"
  },"300": {
    "doc": "17. Introduction",
    "title": "Notes",
    "content": "Between batch and online learning . In applications it may happen that one can change the data collection strategy a limited number of times. This creates a scenario that is in between batch and online learning. This setting can be thought to be between batch and online learning. From the perspective of online learning, this is learning in the presence of constraints on the data collection strategy. One such widely studied constraint is the number of switches of the data collection strategy. As it happens, only very few switches are necessary to get the full power of online learning and this is not really specific to reinforcement learning but follows because the empirical distribution converges are a slow rate to the true distribution. For parametric problems, the rate is $O(1/\\sqrt{n})$ where $n$ is the number of observations. Thus, to change “accuracy” of the estimates of any quantity in a significant fashion, the sample size should increase by much, which means, few changes to the data collection are sufficient. In other words, there is no reason to change the data collection strategy before one obtains sufficient new evidence that can help with deciding in what way the data collection strategy should be changed. This usually means that with only logarithmically many changes in the total sample size, one gets the full power of online methods. Batch RL with no access to state information . For simplicity, we stated the batch learning problem in a way that assumes that the states in the transitions are observed. This may be seen as problematic. One “escape” is to treat the whole history as the state: Indeed, in a causal, controlled stochastic process, the history can always be used as a Markov state. Because of this, the assumption that the state is observed is not restrictive, though the state space becomes exponential in the length of the trajectories. This reduces to the problem to learning in large state-space MDPs. Of course, even lower bounds for planning tell us that in lack of extra structure, all algorithms need a sample size proportional to the size of the state-action space, hence, one needs to add extra structure to deal with this case, such as function approximation. It also holds that if one uses, say, linear function approximation, then only the features of the states (or state-action pairs) need to be recorded in the data. Causal reasoning and batch RL . Whether a causal effect can be learned from a batch of data (to be more precise, from data drawn from a specific distribution) is the topic of causal reasoning. In batch RL, the “effect” is the value of a policy, which, in the language of causal reasoning, would be called a multistage treatment. As the example in the text shows, in batch RL, just because of our assumptions on how the data is collected, the identifiability problem is just “assumed away”. When the assumption on how the data is generated/collected is not met, the tools of causal reasoning can potentially be still used. It is important to emphasize though that there is no causality without assuming causality. The statements that causal reasoning can make are conditional on the data sampling assumptions met. Even “causal discovery” is contingent on these assumptions. However, with care, oftentimes it is possible to argue for that some suitable assumptions are met (e.g., arguing based on what information is available at what time in a process), in which case, the nontrivial tools of causal reasoning may be very useful. Nevertheless, especially in engineered systems, our standard data collection assumptions are reasonable and can be arranged for, though in large engineered systems, mistakes, such as not logging critical quantities may happen. One example of this is an action to be taken is overriden by some part of a system, which will, say, later be turned off. Clearly, if no one logs the actual actions taken, the effects of actions become unidentifiable. As we shall see later, batch RL and the causality literature share some of their vocabulary, such as “instrumental variables”, “propensity scores”, etc. Plug-in or certainty equivalence . Plug-in generally means that a model is estimated and then is used as if it was the “true” model. In control, when a controller (policy) is derived with this approach, this is known as the “certainty equivalence” controller. The “certainty equivalence principle” states that the “random” errors can be neglected. The principle originates from the observation that in various scenarios, the optimal controller (optimal policy) has a special form that confirms this principle. In particular, this was first observed in the control of linear quadratic Gaussian control, where the optimal controller can be obtained by solving for the optimal control under perfect state information then substituting optimal state prediction for the the perfect state information. This strict optimality result is quite brittle. As we shall see soon, from the perspective of minimax optimality, certainty equivalent policies are not a bad choice. ",
    "url": "/2024/w2021-lecture-notes/batch-rl/lec17/#notes",
    
    "relUrl": "/w2021-lecture-notes/batch-rl/lec17/#notes"
  },"301": {
    "doc": "17. Introduction",
    "title": "Bibliographic remarks",
    "content": "In the early RL literature, online learning was dominant. When people tried to apply RL to various “industrial”/”applied” settings, they were forced to think about how to learn from data collected before learning starts. One of the first papers to push this agenda is the following one: . | Tree-Based Batch Mode Reinforcement Learning Damien Ernst, Pierre Geurts, Louis Wehenkel; 6(18):503−556, 2005. | . Earlier mentions of “batch-mode RL” include . | Efficient Value Function Approximation Using Regression Trees (1999) by Xin Wang , Thomas G. Dietterich, Proceedings of the IJCAI Workshop on Statistical Machine Learning for Large-Scale Optimization. pdf | . Even in online learning, efficient learning may force one to save all the data to be used for learning. The so-called LSTD algorithm, and later the LSPI algorithm, were explicitly proposed to address this challenge: . | J. A. Boyan. Technical update: least-squares temporal difference learning. Machine Learning, 49 (2-3):233–246, 2002. | M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:1107–1149, 2003a. | . Off-policy learning refers to the case when an algorithm needs to produce value function (or action-value function) estimates for some policy and the data available is not generated by the policy to be evaluated. In all the above examples, we are thus in the setting of off-policy learning. The policy evaluation problem, accordingly, is often called the off-policy policy evaluation (OPPE) problem, while the problem of finding a good policy is called the off-policy policy optimization (OPPO) problem. For a review of the literature of around 2012, consult the following paper: . | S. Lange, T. Gabel, M. Riedmiller (2012) Batch Reinforcement Learning. In: M. Wiering, M. van Otterlo (eds) Reinforcement Learning. Adaptation, Learning, and Optimization, vol 12. Springer, Berlin, Heidelberg pdf | . ",
    "url": "/2024/w2021-lecture-notes/batch-rl/lec17/#bibliographic-remarks",
    
    "relUrl": "/w2021-lecture-notes/batch-rl/lec17/#bibliographic-remarks"
  },"302": {
    "doc": "17. Introduction",
    "title": "17. Introduction",
    "content": "Batch learning is concerned with problems when a learning algorithm must work with data collected in some manner that is not under the control of the learning algorithm: on a batch of data. In batch RL the data is given in the form of a sequence of trajectories of varying length, where each trajectory is of the form $\\tau=(S_0,A_0,R_0,S_1,A_1,R_1,\\dots,S_t,A_t,R_t,S_{t+1})$, where $A_i$ is chosen in a causal fashion (based on “past” data), $(R_t,S_{t+1})\\sim Q_{A_t}(S_t)$, where $Q = (Q_a(s))_{s,a}$ is a collection of probability distributions over pairs of reals and states, as usual (when we want to allow stochastic rewards). Batch RL problems fall into two basic categories: . | Value prediction: Predict the value $\\mu v^\\pi$ of using a policy $\\pi$ from the initial distribution $\\mu$, where both $\\mu$ and $v^\\pi$ are given in an explicit form. | Policy optimization: Find a good (ideally, near optimal) policy given the batch of data from an MDP. | . These two problems are intimately related. On the one hand, a good value predictor can potentially be used to find good policies. On the other hand, a good policy optimizer can also be used to decide about whether the value of some policy is above or below some fixed threshold by appropriately manipulating the data fed to the policy optimizers. One can then put a binary search procedure around this decision routine to find out the value of some policy. Value prediction problems have some common variations. In policy evaluation, rather than evaluating a policy for some fixed initial distribution, the goal is to estimate the entire value function of the policy. Of course, this is at least as hard as the simpler, initial value estimation problem. However, much of the hardness of the problem is already captured by the initial value estimation problem. In initial value prediction, oftentimes the goal is to predict an interval that contains the true unknown value with a prescribed probability, rather than just producing a “point estimate”. In the case of policy evaluation, the analogue is to predict a set that contains the true unknown value function with a prescribed probability. Here, a simpler goal is to estimate confidence intervals for each potential input (state), which when “pasted together” can be visualized as forming a confidence band. There is also the question of how to collect data. In statistics, the problem of designing a “good way” of collecting the data is called the experimental design problem. The best is of course, if data can be collected in an active manner: This is when the data collection strategy changes in response to what data has been collected so far. The problem of designing good active data collection strategies belongs to the bigger group of designing online learning algorithms. These are defined exactly based on that the data is collected in a way that depends on what data has been previously collected. The last segment of the part will be solely devoted to these online learning strategies. In many applications, active data collection is not an option. There can be many reasons for this: active data collection may be deemed to be risky, expensive, or just technically challenging. When data is collected in a passive fashion, it may simply miss key information that would allow for good solutions. Still, in this case, there may be better and worse ways collecting data. Optimizing experimental designs is the problem of choosing good passive data collection strategies that lead to good learning outcomes. This topic came up in the context planning algorithms as they also need to create value function estimates and for this the data collection is better to be planned so that learning can succeed. Oftentimes though, there is no control over how data is collected. Even worse, the method that was used to collect data may be unknown. When this is the case, not much can be done, as the following example shows: . Consider a bandit problem with two actions, denoted by ${0,1}$ and a Bernoulli reward. Assume that the reward distribution is Bernoulli with parameter $0.1$ when $a=1$ and Bernoulli with parameter $0.9$ when $a=0$. Let $Z$ be a random variable, which is normally unavailable, but which, together with the action $a$ taken completely determines the reward. For example, $Z$ could have a Bernoulli distribution with parameter $p=0.1$, and if action $a$ is chosen, the reward $R(a)$ obtained is . \\[\\begin{align*} R(a) = aZ + (1-a)(1-Z)\\,. \\end{align*}\\] This is indeed consistent with that $R(a)$ has Bernoulli $0.1$ distribution when $a=1$ and has Bernoulli $0.9$ distribution when $a=0$. Assume know, that during data collection, the actions are chosen based on $Z$: $A=\\pi(Z)$ with some $\\pi$. For concreteness, assume that during data collection $A=Z$. Then, the action is random, yet, if the data is composed of pairs that have the distribution shared by $(A,R(A))$, or $(Z,1)$, clearly no method will be able to properly estimate the mean of $R(0)$ or $R(1)$, let alone choosing the action that leads to a higher reward. It is not hard to construct examples when the conditional mean of the observed data makes an optimal action look worse than a suboptimal action. This is an example where the correct model cannot be estimated because of the way data is collected: The presence of the spurious correlation between a variable that controls outcomes but is not available when decisions need to be made can easily make the data collected useless, regardless of quantities. This is an instance when the model becomes unidentifiability from the data collected. When data collection is as arbitrary as in the above example, only a very careful study of the domain can tell us whether there is a chance to avoid this unidentifiability problem. Note that this is an activity that involves thinking about the structure of the problem at hand. The best is of course if data collection can be influenced to avoid building up spurious correlations. When data is collected in a causal way (following a policy), spurious correlations are avoided and the remaining problem is to guarantee sufficient “coverage” to achieve statistical efficiency. ",
    "url": "/2024/w2021-lecture-notes/batch-rl/lec17/",
    
    "relUrl": "/w2021-lecture-notes/batch-rl/lec17/"
  },"303": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "Improved analysis of the plug-in method: First attempt",
    "content": "The improvement in the analysis of the plug-in method comes from two sources: . | Using a version of the value-difference identity and avoiding the use of the policy error bound | Using Bernstein’s inequality in place of Hoeffding’s inequality | . In this section, we focus on the first aspect. The second aspect will be considered in the next section. We continue to use the same notation as in the previous lecture. In particular, $M$ denotes the “true” MDP, $\\hat M$ denotes the estimated MDP and we put $\\hat\\cdot$ on quantities related to this second MDP. We further let $\\pi^*$ be one of the memoryless optimal policies of $M$. For simplicity, we will assume that the reward function in $\\hat M$ is the same as in $M$: As we have seen, the higher order term in our error bound came from errors in the transition probability; the simplifying assumption allows us to focus on reducing this term while minimizing clutter. The arguments are easy to extend to the case when $\\hat r\\ne r$. Let $\\hat \\pi$ be a policy whose suboptimality in $M$ we want to bound. The idea is to bound the suboptimality of $\\hat \\pi$ by its suboptimality in $\\hat M$ and also by how much value functions for fixed policies differ when we switch from $P$ to $\\hat P$. In particular, we have . \\[\\begin{align} v^* - v^{\\hat \\pi} &amp; = v^* - \\hat v^* \\, + \\, \\hat v^* - v^{\\hat \\pi} \\nonumber \\\\ &amp; \\le v^{\\pi^*} - \\hat v^{\\pi^*} \\, + \\, \\underbrace{\\hat v^*-\\hat v^{\\hat \\pi}}_{\\text{opt. error}} \\, + \\, \\hat v^{\\hat \\pi} - v^{\\hat \\pi}\\,, \\label{eq:subbd} \\end{align}\\] where \\(\\hat \\pi^{*}\\) denotes an optimal policy in \\(\\hat M\\) and the inequality holds because \\(\\hat v^{*}=\\hat v^{\\hat \\pi^{*}}\\ge \\hat v^{\\pi^{*}}\\). The term marked as “opt. error” is the optimization error that arises when $\\hat \\pi$ is not (quite) optimal in $\\hat M$. This term is controlled by the choice of $\\hat \\pi$. For simplicity, assume for now that $\\hat \\pi$ is an optimal policy in $\\hat M$, so that we can drop this term. We further assume that $\\hat \\pi$ is a deterministic optimal policy of $\\hat M$. It remains to bound the first and last terms. Both of these terms have the form $v^\\pi - \\hat v^\\pi$, i.e., the difference between the value functions of the same policy $\\pi$ in the two MDPs (here, $\\pi$ is either $\\pi^*$ or $\\hat\\pi$). This difference, similar to the value difference identity, can be expressed as a function of the difference $P-\\hat P$, as shown in the next result: . Lemma (value difference from transition differences): Let $M$ and $\\hat M$ be two MDPs sharing the same state-action space, rewards, but differing in their transition probabilities. Let $\\pi$ be a memoryless policy over the shared state-action space of the two MDPs. Then, the following identities holds: . \\[\\begin{align} v^\\pi - \\hat v^\\pi &amp; = \\gamma \\underbrace{(I-\\gamma P_\\pi)^{-1} M_\\pi (P-\\hat P) \\hat v^{\\pi}}_{\\delta(\\hat v^\\pi) }\\,, \\label{eq:vdpd} \\\\ \\hat v^\\pi - v^\\pi &amp; = \\gamma \\underbrace{(I-\\gamma \\hat P_\\pi)^{-1} M_\\pi (\\hat P- P) v^{\\pi}}_{\\hat{\\delta}(v^\\pi) }\\,. \\label{eq:vdpd2} \\end{align}\\] . Proof: We only need to prove \\eqref{eq:vdpd} since \\eqref{eq:vdpd2} follows from this identity by symmetry. Concerning the proof of \\eqref{eq:vdpd}, we start with the closed form expression for value functions. From this we get . \\[\\begin{align*} v^\\pi - \\hat v^\\pi = (I-\\gamma P_\\pi)^{-1} r_\\pi - (I-\\gamma \\hat P_\\pi)^{-1} r_\\pi\\,. \\end{align*}\\] Inspired by the elementary identity that states that $\\frac{1}{1-x} - \\frac{1}{1-y} = \\frac{x-y}{(1-x)(1-y)}$, we calculate . \\[\\begin{align*} v^\\pi - \\hat v^\\pi &amp; = (I-\\gamma P_\\pi)^{-1} \\left[(I-\\gamma \\hat P_\\pi)-(I-\\gamma P_\\pi) \\right](I-\\gamma \\hat P_\\pi)^{-1} r_\\pi \\\\ &amp; = \\gamma (I-\\gamma P_\\pi)^{-1} \\left[P_\\pi -\\hat P_\\pi\\right](I-\\gamma \\hat P_\\pi)^{-1} r_\\pi \\\\ &amp; = \\gamma (I-\\gamma P_\\pi)^{-1} M_\\pi \\left[P -\\hat P\\right] \\hat v^\\pi \\,, \\end{align*}\\] finishing the proof. \\(\\qquad \\blacksquare\\) . Note that in \\eqref{eq:vdpd2}, the empirical transition kernel $\\hat P$ appears through its inverse by left-multiplying $M_\\pi (\\hat P-P)$, while in \\eqref{eq:vdpd}, through $\\hat v^\\pi$, it appears by right-multiplying the same deviation term. In the remainder of this section we use \\eqref{eq:vdpd2}, but in the next section we will use \\eqref{eq:vdpd}. Combining \\eqref{eq:vdpd2} with our previous inequality, we immediately get that . \\[\\begin{align} v^* - v^{\\hat \\pi} &amp; \\le \\frac{\\gamma}{1-\\gamma} \\left[ \\|(P-\\hat P) v^{\\pi^*}\\|_\\infty + \\|(P-\\hat P) v^{\\hat \\pi}\\|_\\infty \\right]\\,. \\label{eq:vdeb} \\end{align}\\] Assume that $\\hat P$ is obtained by sampling $m$ next states at each state-action pair. By Hoeffding’s inequality and a union bound over the state-action pairs, for any fixed $v\\in [0,H]^{SA}$ and $0\\le \\zeta &lt;1$, with probability $1-\\zeta$, we have . \\[\\begin{align} \\|(P-\\hat P) v \\|_\\infty = H\\sqrt{\\frac{\\log(SA/\\zeta)}{2m}}\\, \\label{eq:vbound} \\end{align}\\] and in particular with $v=v^{\\pi^*}$, we have . \\[\\|(P-\\hat P) v^{\\pi^*} \\|_\\infty = \\tilde O\\left( H/ \\sqrt{m} \\right) \\,.\\] Controlling the second term in \\eqref{eq:vdeb} requires more care as $\\hat \\pi$ is random and depends on the same data that is used to generate $\\hat P$. To deal with this term, we use another union bound. Let \\(\\tilde V = \\{ v^\\pi \\,:\\, \\pi: \\mathcal{S} \\to \\mathcal{A}\\}\\) be the set of all possible value functions that we can obtain by considering deterministic policies. Since by construction $\\hat \\pi$ is also a deterministic policy, \\(\\hat v^{\\hat \\pi}\\in \\tilde V\\). Hence, . \\[\\|(P-\\hat P)\\hat v^{\\hat \\pi} \\|_\\infty \\le \\sup_{v\\in \\tilde V}\\|(P-\\hat P)v \\|_\\infty\\,.\\] and thus by a union bound over the $|\\tilde V|\\le A^S$ functions $v$ in $\\tilde V$, we get that with probability $1-\\zeta$, . \\[\\begin{align*} \\|(P-\\hat P)\\hat v^{\\hat \\pi} \\|_\\infty &amp; \\le H\\sqrt{\\frac{\\log(SA|\\tilde V|/\\zeta)}{2m}} = H\\sqrt{\\frac{\\log(SA/\\zeta)+S \\log(A)}{2m}} = \\tilde O\\left(H\\sqrt{S/m}\\right)\\,. \\end{align*}\\] Putting things together, we see that . \\[v^* - v^{\\hat \\pi} = \\tilde O\\left(H^2\\sqrt{S/m}\\right)\\,,\\] which reduces the dependence on $H$ of the sample size bound from $H^6$ to $H^4$. As we shall see soon, this is not the best possible dependence on $H$. This method also falls short of giving the best possible dependence on the number of states. In particular, inverting the above bound, we see that with this method we can only guarantee a $\\delta$-optimal policy if the total number of samples, $n=SA m$ is at least . \\[\\tilde O( S^2 A H^4/\\delta^2)\\,\\] while below we will see that the optimal bound is $\\tilde O(SA H^3/\\delta^2)$. ",
    "url": "/2024/w2021-lecture-notes/batch-rl/lec18/#improved-analysis-of-the-plug-in-method-first-attempt",
    
    "relUrl": "/w2021-lecture-notes/batch-rl/lec18/#improved-analysis-of-the-plug-in-method-first-attempt"
  },"304": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "Improved analysis of the plug-in method: Second attempt",
    "content": "There are two further ideas that help one achieve the sample complexity which will be seen to be optimal. One is to use what is known as Bernstein’s inequality in place of Hoeffding’s inequality, together with a clever observation on the “total variance” and the second is to improve the covering argument. The first idea helps with improving the horizon dependence, the second helps with improving the dependence on the number of states. In this lecture, we will only cover the first idea and sketch the second. Bernstein’s inequality is a classic result in probability theory: . Theorem (Bernstein’s inequality): Let $b&gt;0$ and let $X_1,\\dots,X_m\\in [0,b]$ be an i.i.d. sequence and define $\\bar X_m$ as the sample mean of this sequence: $\\bar X_m = \\frac{1}{m}(X_1+\\dots+X_m)$. Then, for any $\\zeta\\in (0,1)$, with probability at least $1-\\zeta$, . \\[|\\bar X_m - \\mathbb{E}[X_1]| \\le \\sigma \\sqrt{ \\frac{2\\log(2/\\zeta)}{m}} + \\frac{2}{3} \\frac{b \\log(2/\\zeta)}{m}\\,,\\] where $\\sigma^2 = \\text{Var}(X_1)$. To set expectations, it will be useful to compare this bound to Hoeffding’s inequality. In particular, in the setting of the lemma Hoeffding’s inequality also applies and gives . \\[|\\bar X_m - \\mathbb{E}[X_1]| \\le b \\sqrt{ \\frac{\\log(2/\\zeta)}{2m}}\\,.\\] Since in our case $b=H$ (the value functions take values in the $[0,H]$ interval), using $m=H^3/\\delta^2$ (which would give rise to the optimal sample size), Hoeffding’s inequality gives a bound of size $H H^{-3/2}\\delta = H^{-1/2}\\delta$ (cf. \\eqref{eq:vbound}). This is a problem: Ideally, we would like to see $H^{-1}\\delta$ here, because inequality \\cref{eq:vdeb} introduces an additional $H$ factor. We immediately see that for Bernstein’s inequality to make a difference, just focusing on the first term in Bernstein’s inequality, we need $\\sigma=O(H^{1/2})$. In fact, since $b/m=H^{-2}\\delta^2 = o(H^{-1}\\delta)$, we see that this is also sufficient to take off the $H$ factor from the sample complexity bound. It thus remains to be seen whether the variance could indeed be this small. To find this out, fix a state-action pair $(s,a)$ and let $S_1’,\\dots,S_m’ \\sim P_a(s)$ be an i.i.d. sequence of next states at $(s,a)$. Then, $((\\hat P-P) v^\\pi)(s,a)=(\\hat P_a(s)-P_a(s))v^\\pi$ has the same distribution as . \\[\\Delta(s,a)=\\frac{1}{m} \\sum_{i=1}^m v^\\pi(S_i') - P_a(s) v^{\\pi}\\,.\\] Defining $X_i = v^\\pi(S_i’)$ and \\(\\sigma^2_\\pi(s,a)=\\mathrm{Var}(X_1)\\), we see that it is $\\sigma_\\pi(s,a)$ that appears when Bernstein’s inequality is used to bound $((\\hat P-P) v^\\pi)(s,a)$. It remains to be seen how large values $\\sigma_\\pi(s,a)$ can take on. Sadly, one can quickly discover that the range of $\\sigma_\\pi(s,a)$ is sometimes also as large as $H$. Is Bernstein’s inequality a dead-end then? . Of course, it is not, otherwise we would not have introduced it. In particular, a better bound is possible by directly bounding the maximum-norm of . \\[\\delta(v^\\pi) = (I-\\gamma P_\\pi)^{-1} M_\\pi (P-\\hat P) v^{\\pi} \\,,\\] which is close to the actual term that we need to bound. Indeed, by \\cref{\\eqref{eq:vdpd}} from the value difference lemma, $v^\\pi - \\hat v^\\pi = \\gamma \\delta(\\hat v^\\pi)$ and thus . \\[v^\\pi - \\hat v^\\pi = \\gamma \\delta(v^\\pi) + \\gamma( \\delta(\\hat v^\\pi)-\\delta(v^\\pi))\\,.\\] The second term on the right-hand side is of order $1/m$ (since $(P-\\hat P)(\\hat v^\\pi-v^\\pi)$ appears there and both $P-\\hat P$ and $\\hat v^\\pi - v^\\pi$ have been seen to be of order $1/\\sqrt{m}$). As we expect $\\delta(v^\\pi)$ to be of order $1/\\sqrt{m}$, we will focus on this term. For simplicity, take now the case when $\\pi$ is a fixed, nonrandom policy (we need to bounded $\\delta(v^\\pi)$ for $\\pi=\\pi^*$ and also for $\\pi=\\hat \\pi$, the second of which is random). In this case, by a union bound and Bernstein’s inequality, with probability $1-\\zeta$, . \\[\\begin{align*} |(P-\\hat P) v^{\\pi}| &amp; \\le \\sqrt{ \\frac{2\\log(2SA/\\zeta)}{m} } \\sigma_\\pi + \\frac{2H}{3} \\frac{\\log(2/\\zeta)}{m} \\boldsymbol{1}\\,. \\end{align*}\\] Multiplying both sides by $(I-\\gamma P_\\pi)^{-1} M_\\pi$, using a triangle inequality and the special properties of $(I-\\gamma P_\\pi)^{-1} M_\\pi$, we get . \\[\\begin{align} |\\delta(v^\\pi)| &amp; \\le (I-\\gamma P_\\pi)^{-1} M_\\pi |(P-\\hat P) v^{\\pi}| \\nonumber \\\\ &amp; \\le \\sqrt{ \\frac{2\\log(2SA/\\zeta)}{m} } (I-\\gamma P_\\pi)^{-1} M_\\pi \\sigma_\\pi + \\frac{2H^2}{3} \\frac{\\log(2SA/\\zeta)}{m} \\boldsymbol{1}\\,. \\label{eq:dvpib} \\end{align}\\] The following beautiful result, whose proof is omitted, gives an $O(H^{3/2})$ bound on the first term appearing on the right-hand side of the above display: . Lemma (total discounted variance bound): For any discounted MDP $M$ and policy $\\pi$ in $M$, \\(\\| (I-\\gamma P_\\pi)^{-1} M_\\pi \\sigma_{\\pi} \\|_\\infty \\le \\sqrt{ \\frac{2}{(1-\\gamma)^3}}\\,.\\) . Since the bound that we get from here is $H^{3/2}$ and not $H^2$, “we are saved”. Indeed, plugging this into \\eqref{eq:dvpib} gives . \\[\\| \\delta(v^\\pi) \\|_\\infty \\le 2\\sqrt{ \\frac{H^3\\log(2SA/\\zeta)}{m} } + \\frac{2H^2}{3} \\frac{\\log(2SA/\\zeta)}{m} \\,,\\] which holds with probability $1-\\zeta$. Choosing $m=H^3/\\delta^2$, we see that both terms are $O(\\delta)$. It remains to show that a similar result holds for $\\pi = \\hat \\pi$. If we use the union bound that we used before, we introduce an extra $S$ factor. Avoiding this extra $S$ factor requires new ideas, but with these we get the following result: . Theorem (upper bound for $Z$-designs): Let $\\hat \\pi$ be an optimal policy in the MDP whose transition kernel is $\\hat P$, a kernel estimated based on a sample of $m$ next states from each state-action pair. Letting $0\\le \\zeta &lt;1$ and $0\\le \\delta \\le \\sqrt{H}$, if . \\[m \\ge \\frac{ c \\gamma H^3 \\log(SAH/\\delta) }{\\delta^2}\\] then with probability $1-\\zeta$, $\\hat \\pi$ is $\\delta$-optimal, where $c$ is a universal constant. In short, for any $0\\le \\delta \\le \\sqrt{H}$ there exist an algorithm that produces a $\\delta$-optimal policy from a total number of . \\[\\tilde O\\left( \\frac{ \\gamma SA H^3 }{ \\delta^2 } \\right)\\] samples under a uniform $Z$-design. It remains to be seen whether the same sample complexity holds for larger values of $\\delta$, e.g., for $\\delta = H/2$. ",
    "url": "/2024/w2021-lecture-notes/batch-rl/lec18/#improved-analysis-of-the-plug-in-method-second-attempt",
    
    "relUrl": "/w2021-lecture-notes/batch-rl/lec18/#improved-analysis-of-the-plug-in-method-second-attempt"
  },"305": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "Lower bound for $Z$-designs",
    "content": "A natural question is whether we can improve on the $H^3 SA/\\delta^2$ upper bound, or whether this can be matched by a lower bound. For this, we have the following result: . Theorem (lower bound for $Z$-designs): Any algorithm that uses $Z$-designs and is guaranteed to produce a $\\delta$-optimal policy needs at least $\\Omega( H^3 SA/\\delta^2)$ samples. Proof (sketch): As we have seen in the proof of the upper bound, the key to achieve the cubic dependence was that the sample mean $\\bar X_m$ of $m$ i.i.d. bounded random variables is within a distance of $\\sigma \\sqrt{1/m}$ to the true mean. In a way, the converse of this is also true: It is “quite likely” that the distance between the sample and true mean is this large. This is not too hard to see for specific distributions, such as when the $X_i$ are normally distributed, or when $X_i$ are Bernoulli distributed (in a way, this is the essence of the central-limit theorem, though the central-limit theorem is restricted for $m\\to\\infty$). So how can we use this to establish the lower bound? In an MDP the randomness comes either from the rewards or the transitions. But in the upper bound above, the rewards were given, so the only source of randomness is transitions. Also, the cubic dependence must hold even if the number of states is a constant. What all this implies is that somehow learning the transition structure with a few states is what makes the sample complexity large as $\\gamma\\to 1$ (or $H\\to \\infty$). Clearly, this can only happen if the (small) MDP has self-loops. The smallest example of an MDP with a self-loop is if one has an action and state such that taking that action from that state leads to same action with some positive probability, while with the complementary probability the next state is some other state. This leads to the structure shown on the figure on the right. As can be seen, there are two states. The transition at the first state, call it state $1$, is stochastic and leads to itself with probability $p$, while it leads to state $2$ with probability $1-p$. The reward associated with both transitions is $1$. The second state, call it state $2$, has a self-loop. The reward associated with this transition is zero. There are no actions (alternatively, there is only one action at both states). However, if we can show that in the lack of knowledge of $p$, estimating the value of state $1$ up to a precision of $\\delta$ takes $\\Omega(H^3/\\delta^2)$ samples, the sample complexity result will follow. In particular, if we repeat the transition structure $A$ times (sharing the same two states), one can make the value of $p$ for one of this actions ever slightly so different from the others so that its value differs by (say) $2\\delta$ from the others. Then, by construction, a learner who uses fewer than $\\Omega(A H^3/\\delta^2)$ total samples at state $1$ will not be able to reliably tell the difference between the value of the special action and the other actions, hence, will not be to choose the right action and will thus be unable to produce a $\\delta$-optimal policy. To also add the state dependence, the structure can then be repeated $S$ times. So it remains to be seen whether the said sample complexity result holds for estimating the value of state $1$. Rather than giving a formal proof, we give a quick heuristic argument, hoping that readers will find this more intuitive. The starting point for this heuristic argument is the general observation that sample complexity questions concerning estimation problems are essentially questions about the sensitivity of the quantity to be estimated to the unknown parameters. Here, sensitivity means how much the quantity changes if we change the underlying parameter. This sensitivity for small deviations and a single parameter is exactly the derivative of the quantity of interest with respect to the parameter. In our special case, the value of state $1$, call it $v_p(1)$ (also showing the dependence on $p$) is the quantity to be estimated. Since the value of state $2$ is zero, $v_p(1)$ must satisfy $v_p(1)=p(1+\\gamma v_p(1))+(1-p)1$. Solving this we get . \\[v_p(1) = \\frac{1}{1-p\\gamma}\\,.\\] The derivative of this with respect to $p$ is . \\[\\frac{d}{d p} v_p(1) =\\frac{\\gamma}{(1-\\gamma p)^2}\\,.\\] To get a $\\delta$-accurate estimate of $v_{p_0}(1)$, we need . \\[\\begin{align*} \\delta &amp; \\ge |v_{p_0}(1)-v_{\\bar X_m}(1)| \\approx \\frac{d}{dp} v_p(1)|_{p=p_0} |p_0-\\bar X_m| = \\frac{\\gamma}{(1-\\gamma p_0)^2} |p_0-\\bar X_m|\\\\ &amp; \\approx \\frac{\\gamma}{(1-\\gamma p_0)^2} \\sqrt{ \\frac{p_0(1-p_0)}{m}}\\,. \\end{align*}\\] Inverting for $m$, we get that . \\[m \\gtrsim \\frac{\\gamma^2 p_0(1-p_0)}{(1-\\gamma p_0)^4 \\delta^2}\\,.\\] It remains to choose $p_0$ as a function of $\\gamma$ to show that the above can be lower bounded by $1/(1-\\gamma)^3$. If we choose $p_0=\\gamma$, we have $1-\\gamma p_0 = 1-\\gamma^2 = (1-\\gamma)(1+\\gamma)\\le 2(1-\\gamma)$ and hence . \\[\\frac{\\gamma^2 p_0(1-p_0)}{(1-\\gamma p_0)^4 \\delta^2} \\ge \\frac{\\gamma^2 \\gamma(1-\\gamma)}{2^4(1-\\gamma)^4 \\delta^2} = \\frac{\\gamma^3 }{2^4(1-\\gamma)^3 \\delta^2}\\,.\\] Putting things together finishes the proof sketch. \\(\\qquad \\blacksquare\\) . A homework problem is included which explains how to fill in the gaps in the last section of the proof, while pointers to the literature are given that one can use to figure out how to fill the remaining gaps. ",
    "url": "/2024/w2021-lecture-notes/batch-rl/lec18/#lower-bound-for-z-designs",
    
    "relUrl": "/w2021-lecture-notes/batch-rl/lec18/#lower-bound-for-z-designs"
  },"306": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "Policy-based designs",
    "content": "When the data is generated by following some policy, we talk about policy based designs. Here, the design decision is what policy to use to generate the data. The sample complexity of learning with policy based designs is the number of observations necessary and sufficient for some algorithm to figure out a policy of a fixed target suboptimality, from a fixed initial state, based on data generated by following a policy where the MDP where the policy is followed can be any of the MDPs within the class. Three questions arise then. (i) The first question (the design question) is what policy to follow during data collection. If the policy can use the full history, the problem is not much different than online learning, which we will consider later. From this perspective the interesting (and perhaps more realistic) case is when the data-collection policy is memoryless and is fixed before the data collection begins. Hence, in what follows, we will restrict our attention to this case. (ii) The second question is what algorithm to use to compute the policy given the data generated. (iii) The final, third question is how large is the sample complexity of learning with policy induced data for some fixed MDP class. Learning and estimating a good policy from policy induced data is much closer to reality than the same problem from $Z$-designs. Practical problems, such as problems in health care, robotics, etc., are so that we can obtain data generated by following some fixed policy, while it is usually not possible to demand obtaining sample transitions from arbitrary state-action pairs. For simplicity, let us still consider the case of finite state-action MDPs, but to further simplify matters, let us now consider (homogenous kernel) finite horizon problems with a horizon $H$. As it turns out, the plug-in algorithm of the previous section is still a good algorithm in the sense that it achieves the optimal (minimax) sample complexity. However, the minimax sample complexity is much higher than it is for $Z$-designs: . Theorem (sample complexity lower bound with policy induced data): For any $S,A,H$, $0\\le \\delta$, any (memoryless) data collection policy $\\pi$ over the state-action spaces $[S]$ and $[A]$, for any $n\\le c A^{\\min(S-1,H)}/\\delta^2$ and any algorithm $\\mathcal{L}$ that maps data that has $n$ transitions to a policy there exist an MDP $M$ with state space $[S]$ and action space $[A]$ such that with constant probability, the policy $\\hat \\pi$ produced by $\\mathcal{L}$ is not $\\delta$-optimal with respect to the $H$-horizon total reward criterion when the algorithm is fed with data by following $\\pi$ in $M$. Proof (sketch): Without loss of generality assume that $S=H+1$; if there are more states, just ignore them, while if there are fewer states then just decrease $H$. Consider an MDP where states ${1,\\dots,H}$ are organized in a chain under the effect of some actions, and state $H+1$ is an absorbing state with zero associated reward. For $1\\le i \\le H$, let action $a_i$ be the one that gets to be chosen with the smallest probability in state $i$ under the data generating policy $\\pi$: $a_i = \\arg\\min_{a\\in [A]} \\pi(a|i)$. We choose action $a_i$ as the action that moves the state from $i$ to $i+1$, deterministically. Any other action leads to state $H+1$, deterministically. All rewards are zero, except when transitioning from state $H$ to state $H+1$ under action $a_H$, where the reward is stochastic with a normal distribution with mean $\\mu$ either $-2\\delta$ or $+2\\delta$ and a variance of one. The structure of the MDP is shown on the figure in the left-hand side. Now, because of the choice of $a_i$, $\\pi(a_i|i)\\le 1/A$. Hence, the probability that starting from state $1$, following policy $\\pi$ for $H$ steps will generate the sequence of states $1,2,\\dots,H,H+1$, including the critical transition from state $H$ to state $H+1$, is at most $(1/A)^H$. This transition is critical in the sense is that only data from this transition decides whether in state $1$ it is worth taking action $a_1$ or not. In particular, if $\\mu=-2\\delta$, taking $a_1$ is a poor choice, while if $\\mu=2\\delta$, taking $a_1$ is the optimal choice. The expected number of times this critical transition is seen is at most $m=n(1/A)^H$. With $m$ observations, the value of $\\mu$ will be estimated up to an accuracy of $O(\\sqrt{1/m})$. When this is smaller than $2\\delta$, with constant probability, the sign of $\\mu$ cannot be decided and thus with constant probability, any algorithm will fail to identify whether $a_1$ should be taken in state $1$ or not (with a probability, of say, at least $1/2$). Plugging in the expected value of $m$, we get that the condition on $n$ is that $\\sqrt{c A^H/n}\\le 2\\delta$ where $c&gt;0$ is some universal constant. Equivalently, the condition is that $n\\ge c A^H/(4\\delta^2)$, which is the statement to be proven. \\(\\qquad \\blacksquare\\) . The lower bound construction suggests that the best policy to be used in the lack of extra information about the MDPs is the uniform policy. Note that a similar statement holds for the discounted setting. The contrast between this lower bound and the polynomial upper bound of the previous section are in strike contrast: Data obtained from following policies can be very poor. One may wonder whether the situation can be improved assuming that the data is obtained from a good policy (say, $2\\delta$ optimal policy), but the proof of the previous result in fact shows that this is not the case. While the exponential lower bound on the sample complexity of learning from policy induced data is already bad enough, one may worry that the situation could be even worse. Could it happen that even the best algorithm needs double exponential number of samples? Or even infinite? A moment of thought shows that the latter is the case is switch to the average reward setting: This is because in the average reward setting the value of an action can depend on the value of a state whose hitting probability within an arbitrary fixed number of transitions is positive, just arbitrarily low. Can something similar happen perhaps in the finite-horizon setting, or the discounted setting? As it turns out, the answer is no. The previous lower bound gives the correct order of the sample complexity of finding a near-optimal policy using policy induced data: . Theorem (sample complexity upper bound with policy induced data): With $m=\\Omega(S^3 H^4 A^{\\min(H,S-1)+2}/\\delta^2)$ episodes of length $H$ collected with the uniform policy from a fixed initial distribution $\\mu$, with a constant probability, the plug-in algorithm produces a policy that is $\\delta$-optimal when started from $\\mu$. Proof (sketch): For simplicity assume that the reward function is known. Let $\\pi_{\\log}$ be the logging policy, which is uniform. Again, assume that the plug-in algorithm produces a deterministic policy. The proof is based on the decomposition of the suboptimality gap of the policy $\\hat \\pi$ produced that was used before. In particular, by \\eqref{eq:subbd}, . \\[\\begin{align} v^*(\\mu) - v^{\\hat \\pi}(\\mu) &amp; \\le v^{\\pi^*}(\\mu) - \\hat v^{\\pi^*}(\\mu) \\, + \\, \\hat v^{\\hat \\pi}(\\mu) - v^{\\hat \\pi}(\\mu)\\,, \\label{eq:subbd2} \\end{align}\\] where as before, $\\hat v^\\pi$ denotes the value function of policy $\\pi$ in the empirically estimated MDP. Further, we also used $v(\\mu)$ as a shorthand for $\\sum_s \\mu(s) v(s) (= \\langle \\mu, v \\rangle)$, where $v:[S]\\to \\mathbb{R}$, . One then needs a counterpart of the value difference lemma. In this case, the following version is convenient: For any policy $\\pi$, . \\[q^\\pi_H-\\hat q^\\pi_H = \\sum_{h=0}^{H-1} (P_\\pi)^h (P-\\hat P) \\hat v_{H-h-1}^\\pi\\,,\\] and . \\[\\hat q^\\pi_H- q^\\pi_H = \\sum_{h=0}^{H-1} (\\hat P_\\pi)^h (\\hat P-P) v_{H-h-1}^\\pi\\,,\\] where $P_\\pi$ and $\\hat P_\\pi$ are $SA \\times SA$ matrices. These can be proved by using the Bellman equation for the action-value functions and a simple recursion and noting that $q_0 = r = \\hat q_0$. Next, we can observe that $v^\\pi(\\mu) = \\langle \\mu^\\pi, q^\\pi \\rangle$ where $\\mu^\\pi$ is a distribution over $[S]\\times [A]$ which assigns probability $\\mu(s)\\pi(a|s)$ to $(s,a)\\in [S]\\times [A]$. This, combined with the value difference identity makes $\\nu_h^\\pi:=\\mu^\\pi (P_\\pi)^h$ appear in the bounds. This is the probability distribution over the state-action space after using $\\pi$ for $h$ steps when the initial distribution is $\\mu^\\pi$. Now, as this is multiplied by $P-\\hat P$, and for a given state-action pair $(s,a)$, \\(\\| P(s,a)-\\hat P(s,a) \\|_1 \\lesssim 1/\\sqrt{N(s,a)} \\le 1/\\sqrt{N_h(s,a)} \\approx 1/\\sqrt{m \\nu_h^{\\pi_{\\log}}}\\), using that $\\nu_h^\\pi(s,a)\\le \\sqrt{\\nu_h^\\pi(s,a)}$ which holds because $0\\le \\nu_h^\\pi\\le 1$, we see that it suffices if the ratios $\\rho_h^\\pi(s,a):=\\nu_h^\\pi(s,a)/\\nu_h^{\\pi_{\\log}}(s,a)$ (or their square root) are controlled. Above, $N(s,a)$ is the number of times $(s,a)$ is seen in the data, and $N_h(s,a)$ is the number of times $(s,a)$ is seen in the data in the $h$th transition. Here we should also mention that we only control these terms for state-action pairs $(s,a)$ that satisfy $\\nu_h^{\\pi}(s,a)\\gtrsim 1/m$ as the total contribution of the other state-action pairs is $O(1/m)$, i.e., small. For these state action pairs, $\\nu_h^{\\pi_{\\log}}(s,a)$ is also positive and with high probability, the counts are also positive. Next, one can show that . \\[\\rho_h^\\pi(s,a)\\le A^{\\min(h+1,S)}\\,.\\] This is done in two steps. First, show that $\\nu_h^{\\pi}(s,a) \\le A^{h+1} \\nu_h^{\\pi_{\\log}}(s,a)$. This follows from the law of total probability: Write $\\nu_h^{\\pi}(s,a)$ as the sum of probabilities of all trajectories that end with $(s,a)$ after $h$ transitions. Next, for a given trajectory, replace each occurrence of $\\pi$ with $\\pi_{\\log}$ at the expense of introducing a factor of $A^{h+1}$ (this comes from $\\pi(a’|s’)\\le 1 \\le A \\pi_{\\log}(a’|s’)$). The next step is to show that $\\nu_h^{\\pi}(s,a) \\le A^{S} \\nu_h^{\\pi_{\\log}}(s,a)$ also holds. This inequality follows by observing that the uniform policy and the uniform mixture of all deterministic (memoryless) policies induce the same distribution over the trajectories. Then by letting $\\textrm{DET}$ denote the set of all deterministic policies, using that $\\pi\\in \\textrm{DET}$, we have $\\nu_h^{\\pi}(s,a)\\le \\sum_{\\pi’ \\in \\textrm{DET}} \\nu_h^{\\pi’}(s,a) =A^S \\nu_h^{\\pi_{\\log}}(s,a)$, where we used that $\\vert \\textrm{DET} \\vert =A^S$. Putting things together, applying a union bound when it comes to argue for $\\hat \\pi$ and collecting terms gives the result. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/batch-rl/lec18/#policy-based-designs",
    
    "relUrl": "/w2021-lecture-notes/batch-rl/lec18/#policy-based-designs"
  },"307": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "Bibliographic remarks",
    "content": "Finding a good policy from a sample drawn from a $Z$-design and finding a good policy from a sample given a generative model, or random access simulator of the MDP (which we extensively studied in previous lectures on planning) are almost the same. The random access model however allows the learner to determine which state-action pair the next transition data should be generated at in reaction to the sample collected in a sequential fashion. Thus, computing a good policy with a random access simulator gives more power to the “learner” (or planner). The lower bound presented for $Z$-design can in fact be shown to hold for the generative setting, as well (the proof in the paper cited below goes through in this case with no changes). This shows that in the tabular case, adaptive random access to the simulator provides no benefits to the planner over non-adaptive random access. The result of the $O(H^3 SA/\\delta^2)$ sample complexity bound to find a $\\delta$-optimal policy with uniform $Z$-design using the plug-in method is from the following paper: . | Agarwal, Alekh, Sham Kakade, and Lin F. Yang. 2020. “Model-Based Reinforcement Learning with a Generative Model Is Minimax Optimal.” COLT, 67–83. arXiv link | . This paper also contains a number of pointers to the literature. Interestingly, earlier approaches often used more complicated approaches which directly worked with value functions rather than the more natural plug-in approach. The problem of whether the plug-in method is minimax optimal in $Z$ design for finite-horizon problem is open. The result which was included in this lecture limits the range of $\\delta$ to $\\sqrt{H}$. Equivalently, the result is not applicable for a small number of observations $m$ per state-action pair. This limitation has been removed in a follow-up to this work: . | Li, Gen, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. 2020. “Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model.” NeurIPS | . This paper still uses the plug-in method, but adds random noise to the observed rewards to help with tie-breaking. The variance bound, which is the key to achieving the cubic dependence on the horizon is from the following paper: . | Mohammad Gheshlaghi Azar, Rémi Munos, and Hilbert J Kappen. Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):325–349, 2013. | . This paper also has the essential ideas for the matching lower bound. The 2020 paper is notable for some novel proof techniques, which were developed to bound the error terms whose control is not included in this lecture. The results for learning with policy-induced data are from . | Xiao, Chenjun, Ilbin Lee, Bo Dai, Dale Schuurmans, and Csaba Szepesvari. 2021. “On the Sample Complexity of Batch Reinforcement Learning with Policy-Induced Data.” arXiv | . which also has the details that were omitted in these notes. This paper also gives a modern proof for the $Z$-design sample complexity lower bound. One may ask whether the results for $Z$-design that show cubic dependence on the horizon $H$ extend to the case of large MDPs when value function approximation is used. In a special case, this has been positively resolved in the following paper: . | Yang, Lin F., and Mengdi Wang. 2019. “Sample-Optimal Parametric Q-Learning Using Linearly Additive Features.” ICML arXiv version | . which uses an approach similar to Politex in a more restricted setting, but achieves an optimal dependence on $H$. ",
    "url": "/2024/w2021-lecture-notes/batch-rl/lec18/#bibliographic-remarks",
    
    "relUrl": "/w2021-lecture-notes/batch-rl/lec18/#bibliographic-remarks"
  },"308": {
    "doc": "18. Sample complexity in finite MDPs",
    "title": "18. Sample complexity in finite MDPs",
    "content": "Let $Z = \\mathcal{S}\\times \\mathcal{A}$ be the set of state-action pairs. A $Z$-design assigns a count to every member of $Z$, that is, to every state-action pair. In the last lecture we saw that . \\[n = \\tilde O\\left( \\frac{H^6 \\mathrm{SA}}{\\delta_{\\text{trg}}} \\right)\\] samples are sufficient to obtain a $\\delta_{\\text{trg}}$-suboptimal policy with high probability provided that data is generated from a $Z$-design that assigns the same count to each state-action pair and to get a policy one uses the straightforward plug-in approach that estimates the rewards and transitions using empirical estimates and uses the policy that is optimal with respect to the estimated model. Above, the dependence on the number of state-action pairs is optimal, but the dependence on the horizon $H = \\frac{1}{1-\\gamma}$ is suboptimal. In the first half of this lecture, I sketch how the analysis presented in the previous lecture can be improved to get the optimal cubic dependence, together with a sketch that shows that the cubic dependence is indeed optimal. In the second half of the lecture, we consider policy-based data collection, or experimental designs, where the goal is to find a near optimal policy from an initial state, where the data consists of trajectories obtained by rolling out the data-collection policy from the said initial state. Here, we will show a lower bound that shows that the sample complexity in this case is at least as large $\\Omega(\\mathrm{A}^{\\min(\\mathrm{S},H)})$, which shows that there exist an exponential separation between both $Z$-designs and policy-based designs, and also between passive and active learning. To see the latter, note that in the presence of a simulator, with only a reset to an initial state, one can use approximate policy iteration with rollouts, or Politex with rollouts, to get a policy that is near-optimal when started from the initial state that one can reset to but with polynomially many samples in $\\mathrm{S},\\mathrm{A}$ and $H$ (cf. Lecture 8 and Lecture 14). ",
    "url": "/2024/w2021-lecture-notes/batch-rl/lec18/",
    
    "relUrl": "/w2021-lecture-notes/batch-rl/lec18/"
  },"309": {
    "doc": "Blank",
    "title": "Blank",
    "content": " ",
    "url": "/2024/w2021-lecture-notes/online-rl/blank/",
    
    "relUrl": "/w2021-lecture-notes/online-rl/blank/"
  },"310": {
    "doc": "Online RL",
    "title": "Online RL",
    "content": " ",
    "url": "/2024/w2021-lecture-notes/online-rl",
    
    "relUrl": "/w2021-lecture-notes/online-rl"
  },"311": {
    "doc": "1. Introductions",
    "title": "Introduction",
    "content": "Hello everyone and welcome to CMPUT 653: Theoretical Foundations of Reinforcement Learning at the University of Alberta. We are very excited to be teaching this course and hope that you are excited to journey with us through reinforcement learning theory. The course will cover three sub-topics of RL theory: (1) Planning, (2) Batch RL, and (3) Online RL: . | Planning refers to the problem of computing plans, or policies, or just action by interacting with some model. | Batch RL refers to the problem of coming up with plans, policies, value predictions but when the input is just some data obtained by interacting with an environment. | Online RL refers to the problem of coming up with actions that maximize total reward while interacting with an environment. | . In all of these subproblems, we will use Markov Decision Processes, to describe how either the simulation models, or the environments work. Thus, we start by introducing the formal definition of a Markov Decision Process (MDP). ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec1/#introduction",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec1/#introduction"
  },"312": {
    "doc": "1. Introductions",
    "title": "Markov Decision Process",
    "content": "A Markov Decision Process is a mathematical model for modelling sequential decision making in an environment that undergoes stochastic transitions. An MDP consists of the following elements: states, actions, rules of stochastic transitions between states, rewards, and an objective, which we take for now to be the discounted total expected reward, or return. States are considered to be primitive thus we do not explicitly define what they are. The set of states will be denoted by \\(\\mathcal{S}\\). Actions are also primitive and their set is denoted by \\(\\mathcal{A}\\). For simplicity, we assume that both sets are finite. We will let the number of states be denoted by \\(\\mathrm{S}\\), and similarly, we let the number of actions be denoted by \\(\\mathrm{A}\\). Stochastic transitions between states \\(s\\) and \\(s'\\) are the result of choosing some action $a$ in a given state. For a fixed state \\(s\\) and action \\(a\\), the probabilities of landing in the various states \\(s'\\) is collected into a probability vector, which is denoted by \\(P_a(s)\\). To minimize clutter, by slightly abusing notation, we will write \\(P_a(s,s')\\) as the \\(s'\\in \\mathcal{S}\\) component of this probability vector. This is the probability that the process will transition into state \\(s'\\), when in state \\(s\\) it takes action \\(a\\). Rewards are scalars and the reward incurred as a result of taking action \\(a\\) in state \\(s\\) is denoted by \\(r_a(s)\\). Since the number of states and actions are finite, there is no loss in generality by assuming that all the rewards belong to the \\([0,1]\\) interval. Taking action \\(A_t\\) at time step \\(t\\) gives rise to an infinitely long trajectory of state-action pairs \\(S_0,A_0,S_1,A_1,...\\): here, \\(S_{t+1}\\) is the state that results from taking action \\(A_t\\) in time step \\(t\\ge 0\\) and the assumption is that as long as \\(A_t\\) is chosen based on the “past” only, the distribution of \\(S_{t+1}\\) given \\(S_0,A_0,\\dots,S_t,A_t\\) is solely determined by \\(P_{A_t}(S_t)\\), and, in particular, \\(\\mathbb{P}\\)-almost surely, . \\[\\begin{align} \\label{eq:markov} \\mathbb{P}(S_{t+1}=s|S_0,A_0,\\dots,S_t,A_t) = P_{A_t}(S_t,s)\\,. \\end{align}\\] The objective is to find a way of choosing the actions that results in the largest possible return along the trajectories that arise. The return along a trajectory is defined as . \\[R = r_{A_0}(S_0) + \\gamma r_{A_1}(S_1) + \\gamma^2 r_{A_2}(S_2) + \\dots + \\gamma^t r_{A_t}(S_t) + \\dots\\] where \\(\\gamma \\in [0,1)\\) is the discount factor. Formally, a (discounted) MDP will thus be described by the \\(5\\)-tuple \\(M = (\\mathcal{S},\\mathcal{A},P,r,\\gamma)\\), where \\(P=(P_a(s))_{s,a}\\) and \\(r=(r_a(s))_{s,a}\\) collect the transitions and the rewards, respectively. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec1/#markov-decision-process",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec1/#markov-decision-process"
  },"313": {
    "doc": "1. Introductions",
    "title": "On Discounting",
    "content": "Note that \\(\\gamma\\) makes it so that the future reward does not matter as much as the present reward. Also, if we truncate the above sum after \\(H\\ge 0\\) terms, by our assumption on the rewards, the difference between the return and the truncated return is between zero and . \\[\\gamma^H \\Big[r_{A_H}(S_H)+\\gamma r_{A_{H+1}}(S_{H+1})+\\dots \\Big]\\le \\gamma^H \\sum_{s\\ge 0} \\gamma^s = \\frac{\\gamma^H}{1-\\gamma}\\] by using the summation rule for geometric series. Solving for the largest \\(H\\) under which the above upper bound on the difference is below \\(\\varepsilon\\), we get that this bound on the difference holds as long as \\(H\\) satisfies . \\[H \\ge \\underbrace{\\frac{\\ln \\left( \\frac{1}{\\varepsilon(1-\\gamma)} \\right)}{\\ln(1/\\gamma)}}_{H_{\\gamma,\\varepsilon}^*} \\,.\\] For the sake of simplicity, oftentimes this requirement is strengthened to $H\\ge H_{\\gamma,\\varepsilon}$, where the latter quantity is defined as . \\[H_{\\gamma,\\varepsilon}:=\\frac{\\ln \\left( \\frac{1}{\\varepsilon(1-\\gamma)} \\right)}{1-\\gamma}\\,.\\] The reader is encouraged to verify that $H_{\\gamma,\\varepsilon}\\ge H_{\\gamma,\\varepsilon}^*$ indeed holds. The loosening makes no essential difference . Furthermore, $H_{\\gamma,\\varepsilon}/H_{\\gamma,\\varepsilon}^*\\to 1$ as $\\gamma \\to 1$. As the interesting case is when $\\gamma \\to 1$, there is no . Oftentimes, for simplicity, and thus it suffices if $H$ is larger than equal to the latter term, which is easier to compare at times to other terms. Note also that the gap between the last two terms becomes negligible as $\\gamma \\to 1$. This means that ignoring of at most \\(\\varepsilon\\) of the return, the return is maximized already when considering only the first \\(H\\) time steps. Notice that the critical value of \\(H\\) depends on not only \\(\\varepsilon\\) but also \\(\\gamma\\). For a fixed \\(\\varepsilon\\), this critical value (we often use the looser critical value, which constraints \\(H\\) more) is called the effective horizon. The discounted setting may occasionally feel a bit cringey. Where is the discount factor coming from? One approach is to think about how many time steps in the future we think the optimization should look into for some level of desired accuracy and then work backwards to set \\(\\gamma\\) so that the resulting effective horizon matches our expectation. However, it is more honest to admit that the discounted objective may not faithfully capture the nature of a decision problem. Indeed, there are other objectives that one can consider, such as the finite horizon, undiscounted (or discounted) setting, the infinite horizon setting with no discounting (“total reward”), or the infinite horizon with the average reward. All these have their own pros and cons and we will consider some of these objectives and their relationships in future lectures. For now, we will stick to the discounted objective for pedagogical reasons: the math underlying the discounted objective is simple and elegant. Also, many results transfer to the other settings mentioned, perhaps with some extra conditions, or a little change. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec1/#on-discounting",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec1/#on-discounting"
  },"314": {
    "doc": "1. Introductions",
    "title": "Policies",
    "content": "A policy is a rule that describes how the actions should be taken in light of the past. Here, the past at time step \\(t\\ge 0\\) is defined as . \\[H_t = (S_0,A_0,\\dots,S_{t-1},A_{t-1},S_t)\\] which is the sequence of state-action pairs leading up to the state of the process at the current time step \\(t\\). We allow policies to randomize. As such, formally, a policy becomes an infinite sequence \\(\\pi = (\\pi_t)_{t\\ge 0}\\) of maps of histories to distributions over actions. For a (finite) set \\(X\\) let \\(\\mathcal{M}_1(X)\\) denote the set of probability distributions over \\(X\\). These probability distributions are uniquely determined by what probability they assign to the individual elements of \\(X\\). Hence, they will be identified with the probability vectors with \\(|X|\\) components, each component giving the probability of some $x\\in X$. If $p\\in \\mathcal{M}_1(X)$, we use both $p_x$ and $p(x)$ to denote this probability (whichever is more convenient). With this, we can write that the \\(t\\)th “rule” in \\(\\pi\\), which will be used in the \\(t\\)th time step to come up with the action for that time step, as . \\[\\pi_t: \\mathcal{H}_t \\to \\mathcal{M}_1(\\mathcal{A})\\,,\\] where . \\[\\mathcal{H}_t = (\\mathcal{S} \\times \\mathcal{A})^{t-1} \\times \\mathcal{S}\\,.\\] Note that \\(\\mathcal{H}_0 = \\mathcal{S}\\). Intuitively, following a policy \\(\\pi\\) means that in time step \\(t\\ge 0\\), the distribution of the action \\(A_t\\) to be chosen for that timestep is \\(\\pi_t(H_t)\\): the probability that \\(A_t=a\\) is \\(\\pi_t(H_t)(a)\\). Since writing \\(\\pi_t(H_t)(a)\\) is quite cumbersome, we abuse notation and will write $\\pi_t(a|H_t)$ instead. Thus, when following a policy \\(\\pi\\), in time step \\(t\\ge 0\\) we get that, \\(\\mathbb{P}\\)-almost surely, . \\[\\begin{align} \\label{eq:pol} \\mathbb{P}(A_t=a|H_t) = \\pi_t(a|H_t)\\,. \\end{align}\\] ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec1/#policies",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec1/#policies"
  },"315": {
    "doc": "1. Introductions",
    "title": "Initial State Distributions, Distributions over Trajectories",
    "content": "When a policy is interconnected with an MDP, the interconnection, together with an initial distribution \\(\\mu\\in \\mathcal{M}_1(\\mathcal{S})\\) over the states, uniquely determines a distribution over the infinite-long trajectories . \\[T = (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}\\] such that for every time step \\(t\\ge 0\\), both \\(\\eqref{eq:markov}\\) and \\(\\eqref{eq:pol}\\) hold, in addition to that . \\[\\begin{align} \\label{eq:init} \\mathbb{P}(S_0=s) = \\mu(s)\\,, \\qquad s\\in \\mathcal{S}\\,. \\end{align}\\] In fact, this distribution could be over some potentially bigger probability space, in which case uniqueness does not hold. When we want to be specific and take the distribution that is defined over the infinite-long state-action trajectories, we will say that this is the distribution over the canonical probability space induced by the interconnection of the policy and the MDP. To emphasize the dependence of the probability distribution \\(\\mathbb{P}\\) on \\(\\mu\\) and \\(\\pi\\), we will often use \\(\\mathbb{P}_\\mu^\\pi\\), but we will also take the liberty to drop any of these indices when its identity can be uniquely deduced from the context. When needed, the expectation operator corresponding to \\(\\mathbb{P}\\) (or \\(\\mathbb{P}_\\mu^\\pi\\)) will be denoted by \\(\\mathbb{E}\\) (respectively, \\(\\mathbb{E}_\\mu^\\pi\\)). What is the probability assigned to a trajectory \\(\\tau = (s_0,a_0,s_1,a_1,\\dots)\\in T\\)? Let \\(h_t = (s_0,a_0,\\dots,s_{t-1},a_{t-1},s_t)\\). Recall that \\(H_t = (S_0,A_0,\\dots,S_{t-1},A_{t-1},S_{t})\\). By a repeated application of the chain rule of probabilities, we get . \\[\\begin{align*} \\mathbb{P}(&amp;H_t=h_t)\\\\ &amp;= \\mathbb{P}(S_0=s_0,A_0=a_0,S_1=s_1,\\dots,S_t=s_t)\\\\ &amp;= \\mathbb{P}(S_t=s_t|H_{t-1}=h_{t-1},A_{t-1}=a_{t-1}) \\mathbb{P}(H_{t-1}=h_{t-1},A_{t-1}=a_{t-1}) \\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\mathbb{P}(H_{t-1}=h_{t-1},A_{t-1}=a_{t-1}) \\tag{by \\eqref{eq:markov}}\\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\mathbb{P}(A_{t-1}=a_{t-1}|H_{t-1}=h_{t-1}) \\mathbb{P}(H_{t-1}=h_{t-1})\\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\pi_{t-1}(a_{t-1}|h_{t-1}) \\mathbb{P}(H_{t-1}=h_{t-1}) \\tag{by \\eqref{eq:pol}}\\\\ &amp; \\;\\; \\vdots \\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\pi_{t-1}(a_{t-1}|h_{t-1}) \\times \\dots\\times P_{a_0}(s_0,s_1) \\pi_{0}(s_0|s_0) \\mathbb{P}(S_0=s_0) \\\\ &amp;= P_{a_{t-1}}(s_{t-1},s_t) \\pi_{t-1}(a_{t-1}|h_{t-1}) \\times \\dots\\times P_{a_0}(s_0,s_1) \\pi_{0}(s_0|s_0) \\mu(s_0)\\,. \\tag{by \\eqref{eq:init}} \\end{align*}\\] Collecting the terms, . \\[\\mathbb{P}(H_t=h_t) = \\mu_0(s_0) \\left\\{ \\Pi_{i=0}^{t-1} \\pi_i(a_i|h_i)\\right\\} \\, \\left\\{ \\Pi_{i=0}^{t-1} P_{a_i}(s_i,s_{i+1})\\right\\}\\,.\\] Similarly, . \\[\\mathbb{P}(H_t=h_t,A_t=a_t) = \\mu_0(s_0) \\left\\{ \\Pi_{i=0}^{t} \\pi_i(a_i|h_i)\\right\\} \\, \\left\\{ \\Pi_{i=0}^{t-1} P_{a_i}(s_i,s_{i+1})\\right\\}\\,.\\] ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec1/#initial-state-distributions-distributions-over-trajectories",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec1/#initial-state-distributions-distributions-over-trajectories"
  },"316": {
    "doc": "1. Introductions",
    "title": "Value Functions, the Optimal Value Function and the Objective",
    "content": "The total expected discounted reward, or the expected return of policy \\(\\pi\\) in MDP \\(M\\) when the initial state is sampled from $\\mu$ is . \\[v^\\pi(\\mu) = \\mathbb{E}_\\mu^\\pi \\left[ R \\right] \\,.\\] When \\(\\mu=\\delta_s\\) where \\(\\delta_s\\) is the “Dirac” probability distribution that puts a point mass at \\(s\\), we use \\(v^\\pi(s)\\) to denote the resulting value. Since this assigns a value to every state, \\(v^\\pi\\) can be viewed as a function assigning a value to every state in \\(\\mathcal{S}\\). This function will be called the value function of policy \\(\\pi\\). When the dependence on the MDP is important, we may add “in MDP \\(M\\)” and denote the dependence by introducing an index: \\(v^\\pi_M\\). The best possible value in state \\(s\\in \\mathcal{S}\\) that can be obtained by optimizing over all possible policies is . \\[v^*(s) = \\sup_{\\pi} v^\\pi(s)\\,.\\] Then, \\(v^*: \\mathcal{S}\\to \\mathbb{R}\\), viewed as a function, is called the optimal value function. A policy is optimal in state \\(s\\) if \\(v^\\pi(s)=v^*(s)\\). A policy is uniformly optimal if it is optimal in every state. In what follows, we will drop uniformly as we will usually be interested in finding uniformly optimal policies. Given an MDP, we are interested in efficiently computing an optimal policy. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec1/#value-functions-the-optimal-value-function-and-the-objective",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec1/#value-functions-the-optimal-value-function-and-the-objective"
  },"317": {
    "doc": "1. Introductions",
    "title": "Planning=Computation",
    "content": "Computing an optimal policy can be seen as a planning problem: the optimal policy answers the question of how to take actions so that the expected return is maximized. This is also an algorithmic problem. The input, in the simplest case, is a big table (or a number of tables) that describes the transition probabilities and rewards. The interest is to develop algorithms that read in this table and then as output should return a description of an optimal policy. At this stage, it may seem unlikely that an efficient algorithm could do this: in the above unrestricted form, policies have an infinite description. As we shall find out soon though, we will be lucky with the finite MDPs we consider as in such MDPs one can always find optimal policies that have a short description. Then, the algorithmic question becomes interesting! . As for any algorithmic problem, the main question is how many elementary computational steps are necessary to solve an MDP? As can be suspected, the number of steps will need to scale with the number of states and actions. Indeed, even the size of the input scales with these. If computation indeed needs to scale with the number of state-action pairs, is there still any reason to consider this problem given that the number of states and actions in MDPs that one typically encounters in practical problems is astronomically large, if not infinite? Yes, there are: . | Not all MDPs are in fact large and it may be useful to know what it takes to “solve” a small MDP. Good solvers for “small” MDPs may serve as benchmarks for solvers developed for the “large MDP” case. | Even if a problem is large (or infinite), one may be able to approximate it well with a small MDP. Then, a solver for a small MDP may be useful. | Some ideas and tools developed for this problem also generalize (perhaps) with some twists to the “large” MDP setting. | . At this stage, the reader may be wondering about what is meant by “small” and “large”? As a rough guideline, by “small” we mean problems where the tables describing the MDP (and/or policy) comfortably fit in the memory of whatever computer one has access to. Large is everything else. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec1/#planningcomputation",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec1/#planningcomputation"
  },"318": {
    "doc": "1. Introductions",
    "title": "Miscellaneous Remarks",
    "content": "Probabilities of infinite long trajectories? . Based on the above calculations, one expects that the probability of a trajectory $\\tau = (s_0,a_0,s_1,a_1,\\dots)$ that never ends is . \\[\\begin{align*} \\mathbb{P}(S_0=s_0,A_0=a_0,S_1=s_1,A_1=a_1,\\dots) &amp;= \\mu(s_0) \\times \\pi_0(a_0|h_0) \\times P_{a_0}(s_0,s_1) \\\\ &amp; \\qquad\\quad\\;\\; \\times \\pi_1(a_1|h_1) \\times P_{a_1}(s_1,s_2) \\\\ &amp; \\qquad\\quad\\;\\; \\times \\cdots \\\\ &amp; \\qquad\\quad\\;\\; \\times \\pi_{t}(a_t|h_t) \\times P_{a_t}(s_t,s_{t+1}) \\\\ &amp; \\qquad\\quad\\;\\; \\times \\cdots \\end{align*}\\] where \\(h_t = (s_0,a_0,\\dots,s_{t-1},a_{t-1},s_t)\\) as before. However, this does not work: if in the trajectory, each action is taken with probability $1/2$ by the policy on the given history, the infinite product on the right-hand side is zero! This should make one pause at least for a moment: how is then \\(\\mathbb{P}\\) even defined? Does this distribution even exist? If yes, and it assigns zero probability to trajectories like above, could not it be that it assigns zero to all the trajectories of infinite length? In the world of infinite, one must tread carefully! The way out of this conundrum is that we must use measure theoretic probabilities, or we need to give up on objects like the return, \\(R= \\sum_{t\\ge 0}\\gamma^t r_{A_t}(S_t)\\), which is defined on trajectories of infinite length. The alternative to measure theoretical probability is to define everything through by taking limits (and always taking expectations over finite-length prefixes of the infinite long trajectories). As this would be quite cumbersome, we will take the measure-theoretic route, which will be explained in the next lecture. Why Markov? . Equation \\(\\eqref{eq:markov}\\) tells us that the only thing that matters from the history of the process as far as the prediction of the next state is concerned is the last action and the last state. This is known as the Markov property. More generally, Markov chains, which are specific stochastic processes, have a similar property. Bellman’s curse of dimensionality . Richard Bellman, who has made many foundational contributions to the early theory, coined the term the “curse of dimensionality”. By this, Bellman meant the following: oftentimes when MDPs are used to model a practical decision making problem, the state space oftentimes takes the product form \\(\\mathcal{S} = \\mathcal{S}_1 \\times \\dots \\times \\mathcal{S}_d\\) with some $d&gt;0$. If each set \\(\\mathcal{S}_i\\) here has at only two(!) elements, the state space will have at least \\(2^d\\) elements. This is an exponential growth as a function of \\(d\\), which is taken as the fundamental scaling quantity. Thus, any algorithm that needs to even just enumerate the states in the state space is “cursed” to perform a very lengthy calculation. While we start with considering the case when both the state and the action space are small (as described above), the main focus will be on the case when this is not true anymore. In this way, the problem will be to figure out ways of breaking the curse. But just to make things clear, in the worst-case, there is no cure to this curse, as we shall see it soon in a rigorous fashion. Any cure will come by changing the problem, either by changing the objective, or by changing the inputs available, or both. Actions shared across states? . We described MDPs as if the same set of actions was available in all the states. This may create the (false) impression that action $a_1$ in state $s_1$ has something to do with action $a_1$ in state $s_2$ (i.e., their rewards, or next state distributions are shared or are similar). Given the MDP definition though, clearly, no such assumptions are made. In a way, a better way of describing an MDP is using a set \\(Z\\) and an equivalence relation over \\(Z\\), or, equivalently, the partition induced by it over \\(Z\\). We should think of \\(Z\\) as the set of possible state-action pairs: The equivalence relation over \\(Z\\) then gives which of these share a common state. Alternatively, if \\(z_1\\) and \\(z_2\\) are in the same partition, they share a state, which we can identify with the partition. Then, for every \\(z\\in Z\\), the MDP would specify a distribution over the parts of the partition (the “next states”) and one should specify a reward. While this description is appealing from a mathematical perspective, it is nonstandard and would make it harder to relate everything to the literature. Furthermore, the description chosen, apart from the inconvenience that one need to forcefully remember that actions do not keep their identity across states, is quite intuitive and compact. A common variation in the literature, which avoids the “sharing issue” is to assume that every state is equipped with a set \\(\\mathcal{A}(s)\\) of actions admissible to the state and these sets are disjoint across the states. This description allows the number of actions to be varied across the states. While this has a minor advantage, our notation is simpler and tends not to lose much in comparison to these more sophisticated alternatives. Are states observed? . In many practical problems it is not a priori clear whether the problem has a good approximate description as an MDP. One critical aspect that is missing from the MDP description is that the states of the MDP may not be available for measurement and thus the control (the choice of the action) cannot use state information. For now, we push this problem aside, but we shall return to it time-to-time. The reason is that it is best to start with the simpler questions and, at least intuitively, the problem of finding a policy that can use state information feels easier than finding one that cannot even access the state information. First, at least, we should find out what can be done in this case (and how efficiently), hoping that the more complex cases will either be reducible to this case, or will share some common patterns. On the notation . Why use \\(r_a(s)\\) rather than, say, \\(r(s,a)\\)? Or \\(P_a(s)\\), or \\(P_a(s,s')\\) rather than \\(P(s'|s,a)\\)? All these notations have pros and cons. None of them is ideal for all purposes. One explanation for using this notation is that later we will replace $a$ with $\\pi$, where $\\pi$ will be a special policy (a memoryless, or stationary Markov policy). When doing so, the notation of $r_\\pi$ (suppressing $s$) and \\(P_\\pi\\) (a stochastic matrix!) will be tremendously useful. A bigger question is why use $s$ for states and $a$ for actions. Is not the answer in the words? Well, people working in control would disagree. They would prefer to use $x$ for state and $u$ for actions, and I am told by Maxim Raginsky, that these come from Russian abbreviations, so they make at least as much as sense as the notation used here. That is, if one speaks Russian (and if not, why not learn it?). Dimitri Bertsekas likes using $i,j$ etc. for states, which seems fine if one has discrete (countable) state spaces. Stochastic rewards . Some authors (e.g., this author in some of their papers or even in his book) considers rewards which are stochastic. This may matter when the problem is to learn a good policy, or to find a good plan while interacting with a stochastic simulator. However, when it comes to defining the object of computation, we can safely ignore (well-behaved) stochastic rewards. Here, the well-behaved stochastic rewards are those whose conditional expectation given an arbitrary history up to a state $s$ and an action $a$ taken in that state depends only on $(s,a)$. Which is what we start here from. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec1/#miscellaneous-remarks",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec1/#miscellaneous-remarks"
  },"319": {
    "doc": "1. Introductions",
    "title": "References",
    "content": "“The” book about MDPs is: . Puterman, Martin L. 2005. Markov Decision Processes (Discrete Stochastic Dynamic Programming). Wiley-Interscience. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec1/#references",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec1/#references"
  },"320": {
    "doc": "1. Introductions",
    "title": "1. Introductions",
    "content": " ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec1/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec1/"
  },"321": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Planning under \\(q^*\\) realizability",
    "content": "We consider fixed horizon local planning in large finite MDPs $(\\mathcal{S},\\mathcal{A},P,r)$. As usual, the horizon is denoted by $H&gt;0$ and we consider planning with a fixed initial state $s_0$, as in the previous lecture. Let us denote by \\(\\mathcal{S}_i\\) the states that are reachable from $s_0$ in $0\\le i \\le H$ steps. As before, we assume that \\(\\mathcal{S}_i\\cap \\mathcal{S}_j=\\emptyset\\) when $i\\ne j$. Recall that in this case the action-value functions depend on the number of steps left, or the current stage. For a fixed $0\\le h\\le H-1$, let \\(q^*_h:\\mathcal{S}_{h} \\times \\mathcal{A}\\to \\mathbb{R}\\) be the optimal action-value function with $h$ stages in the process, $H-h$ stages left. Since we do not need the values of $q^*_h$ outside of $\\mathcal{S}_h\\times \\mathcal{A}$, we abuse notation by redefining it restricted to this set. Important note: The indexing of $q^*_h$ used here is not consistent with the indexing used in the previous lecture, where it was more convenient to index value functions based on the number of stages left. The planner will be given a feature map $\\phi_h$ for every stage $0\\le h\\le H-1$ such that \\(\\phi_h:\\mathcal{S}_h \\times \\mathcal{A} \\to \\mathbb{R}^d\\). The realizability assumption means that . \\[\\begin{align} \\inf_{\\theta\\in \\mathbb{R}^d} \\max_{0\\le h \\le H-1}\\|\\Phi_h \\theta - q^*_{h} \\|_\\infty = 0\\,. \\label{eq:qsrealizability} \\end{align}\\] Note that we demand that the same parameter vector is shared between all stages. As it turns out, this makes our result stronger. Regardless, at the price of increasing the dimension from $d$ to $dH$, one can always assume that the parameter vector is shared. Since we will give a negative result concerning the query-efficiency of planners, we allow the planners access to the full feature-map: The negative result still applies even if the planner is allowed to perform any sort of computation with the feature-map during or before the planning process. For $\\delta&gt;0$, we call a local planner $\\delta$-sound for the $H$-step criterion if for any MDP $M$ and feature map $\\phi = (\\phi_h)_h$ pair such that the optimal action-value function of $M$ is realizable with the features $\\phi$ in the sense that \\eqref{eq:qsrealizability} holds, the planner induces a policy that is $\\delta$-suboptimal or better when evaluated with the $H$-horizon undiscounted total reward criterion from the designated start-state \\(s_0\\) in MDP $M$. Note that this is very much the same as the previous $(\\delta,\\varepsilon=0)$ soundness criterion, except that the definition of the approximation error is relaxed, while we demand $\\varepsilon=0$. The result below uses MDPs where the immediate reward (obtained from the simulator) can be random. The random reward is used to make the job of the planners harder and it allows us to consider MDPs with deterministic dynamics. (The result could also be proven for MDPs with deterministic rewards and random transitions.) . The usual definition of MDPs with random transitions and rewards is in a way even simpler: Such a (finite) MDP is given by the tuple \\(M=(\\mathcal{S},\\mathcal{A},Q)\\) where \\(Q = (Q_a(s))_{s,a}\\) is a collection of distributions over state-reward pairs. In particular, for all state-action pairs $(s,a)$, \\(Q_a(s)\\in \\mathcal{M}_1(\\mathcal{S}\\times\\mathbb{R})\\). Letting \\((S',R)\\sim Q_a(s)\\) (i.e., $(S’,R)$ is drawn from \\(Q_a(s)\\) at random), we can recover $P_a(s)$ as the distribution of $S’$ and $r_a(s)$ as the expected value of $R$. That the reward can be random forces a change to the notion of the canonical probability spaces, since histories now also show include rewards, $R_0,R_1,\\dots$ incurred in each time step $t=0,1,\\dots$. With appropriate modifications, we can nevertheless still introduce \\(\\mathbb{P}_\\mu^\\pi\\) and the corresponding expectation operator, \\(\\mathbb{E}_\\mu^\\pi\\), as well. The natural definition of the value of a policy $\\pi$ at state $s$, say, in the discounted setting is then \\(v^\\pi(s) = \\mathbb{E}_s^\\pi[ \\sum_{t=0}^\\infty \\gamma^t R_t]\\). However, it is easy to see that for any $t\\ge 0$, \\(\\mathbb{E}_\\mu^\\pi[R_t]=\\mathbb{E}_\\mu^\\pi[r_{A_t}(S_t)]\\), and, as such, nothing changes in the theoretical results derived so far. For $a,b$ reals, let $a\\wedge b = \\min(a,b)$. The main result of this lecture as follows: . Theorem (worst-case query-cost is exponential under $q^*$-realizability): For any $d,H$ large enough and any local planner $\\mathcal{P}$ that is $9/128$-sound for the $H$-horizon planning problem, there exists a triplet $(M,s_0,\\phi)$ where $M$ is a finite MDP with random rewards taking values in $[0,1]$ and deterministic transitions, $s_0$ is a state of this MDP and $\\phi$ is a $d$-dimensional feature-map such that \\eqref{eq:qsrealizability} holds for the optimal action-value function \\(q^* = (q^*_h)_{0\\le h \\le H-1}\\) and the expected number of queries $q$ that $\\mathcal{P}$ uses when interconnected with $(M,s_0,\\phi)$ satisfies . \\[q = e^{\\Omega(d\\wedge H )}\\] . Note that with random rewards with no control on their tail behavior (e.g., unbounded variance) it would not be hard to make the job of any planner arbitrarily hard. As such, it is quite important that the MDPs that are constructed for the result, the rewards, while random, lie in a fixed interval. Note that the specific choice of this interval does not matter: If there is a hard example with some interval, that example can be translated into another by shifting and scaling, and at the price of introducing an extra dimension in the feature map to account for the shifts. A similar comment applies to $\\delta = 9/128$ (which, nevertheless, needs to be scaled to the range of the rewards). ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec10/#planning-under-q-realizability",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec10/#planning-under-q-realizability"
  },"322": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "The main ideas of the proof",
    "content": "Rather than giving the full proof, we will just explain the main ideas behind it. At a high-level, the proof merges the ideas behind the lower bound for the small action-set case and the lower bound of the large action-set case. That is, we will consider an action set that is exponentially large in $d$. In particular, we will consider action sets that have $k=e^{\\Theta(d)}$ elements. Note that because realizability holds, having a large action set but with a trivial dynamics (as in the lower bound in the last lecture) does not lead to the lower bound of the desired form. In particular, if the dynamics is trivial (i.e., $\\mathcal{S}_i={s_i}$, see the figure on the right) then the optimal action to be taken at $s_0$ does not depend on what actions are taken at later stages and can be efficiently found by just maximizing for the reward received in that stage, which can be done efficiently due to our realizability assumption, even in the presence of random rewards. Whether an example exist with only a few actions but with a more complicated dynamics remains open. With the construction provided here (which is based on a tree dynamics and zero intermediate reward in the tree), this clearly fails, as we will make it clear below. In any case, since the “chain dynamics” does not work, the next simplest approach is to have a tree, but with exponentially many actions in every node. Since this creates many many states ($e^{\\Theta(dh)}$ states at stage $h$) the next question then is how to ensure realizability. There are two issues: We need to be able to keep the dimension fixed at $d$ at every stage and somehow we will need to have a way of controlling which action should be optimal at each state at each stage. Indeed, realizability means that we need to ensure that for all $0\\le h \\le H-1$ and $(s,a)\\in \\mathcal{S}_h \\times \\mathcal{A}$, . \\[\\begin{align} q_{h}^*(s,a) = r_a(s)+v_{h+1}^*(sa) \\label{eq:cons} \\end{align}\\] Here, $sa$ stands for the state that is reached by taking action $a$ in state $s$ (in the tree, every node, or state is uniquely indexed by the action sequence that reaches it). Now, in the definition of $v_{h}^*$, for all $h$, we also have \\(v_{h}^*(s) = \\max_{a\\in \\mathcal{A}} q_{h+1}^*(s,a)\\), which calls for the need to know the identity of the maximizing action. What is more, since the solution to the Bellman optimality equations is unique, if we guarantee that \\eqref{eq:cons} holds at all state-action pairs for \\(q_h(s,a) = \\langle \\phi_h(s,a), \\theta^* \\rangle\\) with some features and parameter vectors, it also follows that \\(q_h = q^*_h\\) for all \\(h\\ge 0\\), that is, \\(q^*\\) is realizable with the features. A simple approach to resolve all of these issues is to let a fixed action $a^*\\in \\mathcal{A}$ be the optimal action at all the states, together with using the JL features from the previous lecture (the identity of this action is of course hidden from the planner). In particular, the JL feature-matrix lemma from the previous lecture furnishes us with $k$ $d$-dimensional unit vectors $(u_a)_{a\\in \\mathcal{A}}$ such that for $a\\ne a’$, . \\[\\begin{align*} \\vert \\langle u_a, u_a' \\rangle \\vert \\le \\frac{1}{4}\\,. \\end{align*}\\] Fix these vectors. That $a^*$ should be optimal at all states $s$ is equivalent to that . \\[\\begin{align} q_h^*(s,a)\\le q_h^*(s,a^*) (=v_h^*(s)), \\qquad 0\\le h \\le H-1, s\\in \\mathcal{S}_h, a\\in \\mathcal{A}\\,. \\label{eq:aopt} \\end{align}\\] In our earlier proof we used \\(\\phi_h(s,a) = u_a\\) and \\(\\theta^* = u_{a^*}\\). Will this still work? Unfortunately, it does not. The first observation is that from this it follows that for any $h$, $s$, $a$, . \\[\\begin{align*} q_{h}^*(s,a) = \\langle u_{a^*}, u_a \\rangle\\,. \\end{align*}\\] As such, for almost all the actions $a$, we expect \\(|q_h^*(s,a)|\\) to be close to \\(1/4\\). Now, under this choice we also have that \\(v_h^*(s)=1\\) for all states and all stages $0\\le h \\le H-1$. This creates essentially the same problem as what we saw above with the trivial chain dynamics. In particular, from \\eqref{eq:cons} we get that \\(q_h^*(s,a) = r_a(s)+1\\). As such, we expect $r_a(s)$ to be close to either $-3/4$ or $-5/4$ (since \\(|q_h^*(s,a)|\\) is close to $1/4$). Putting aside the issue that we wanted the immediate reward be in $[0,1]$, we see that if the reward noise is not large, \\(\\theta^*\\) and thus the identity of $a^*$ can be obtained with just a few queries: The signal to noise ratio is just too good! . This problem replicates itself at the very last stage: Here, \\(v_H^*(s')=0\\) for any state $s’$, hence . \\[\\begin{align} q^*_{H-1}(s,a)=r_a(s) \\label{eq:laststage} \\end{align}\\] for any $(s,a)$ pair. Unless we choose \\(q^*_{H-1}(s,a)\\) to be small, say, \\(e^{-\\Theta(H)}\\), a planner will succeed with fewer queries than in our desired bound. This motivates us to introduce a scaling of the features (recall that the parameter vector is shared between the stages) with some scaling factors. For maximum generality, we allow for the scaling factor of the feature vector of \\((s,a)\\in \\mathcal{S}_h\\times \\mathcal{A}\\) to depend on \\((s,a)\\) itself (since states between stages are not shared, scaling can depend on the stage with this choice). Let \\((3/2)^{-h+1}\\sigma_{sa}\\) be the scaling factor we intend to use with \\((s,a)\\) where we intend to keep $\\sigma_{sa}$ in a constant range (so the scaling with the stage index works as intended) while we aim to use \\(\\phi_h(s,a) =(3/2)^{-h+1} \\sigma_{sa} u_a\\). Now, we can explain the need for many actions. By the Bellman optimality equation \\eqref{eq:cons} we have that for any suboptimal action, $a$, . \\[r_{a^*}(s)-r_a(s) =q_h^*(s,a^*)-q_h^*(s,a) \\approx (3/2)^{-h} \\langle u_{a^*}-u_a,u_{a^*} \\rangle \\ge (3/2)^{-h} (3/4),\\] where \\(\\approx\\) uses that \\(\\sigma_{sa}\\approx\\sigma_{sa^*}\\approx \\text{const}\\). From this we see that close to the initial state \\(s_0\\) the reward gaps are of constant order. In particular, if there were only a few actions per state, a planner could identify the optimal action by finding the action whose reward is significantly larger than that of the others. By choosing to have many actions, the planner faces a “needle-in-a-haystack” situation, which makes their job hopeless even with perfect signal (no noise). The next idea is to force “clever” planners to only experiment with actions in the last stage. Since here, the signal-to-noise ratio will be very poor, if we manage to achieve this, even clever planners will need to use a large number of queries. A simple way of forcing this is to choose all the rewards while transitioning in the tree and taking suboptimal actions to be identically zero except for stage $h=H-1$, where, in accordance to our earlier plan, the rewards are chosen at random to ensure consistency but the signal to noise ratio will be poor. Since the dynamics in the tree is known, and it is known that all rewards are zero with the possible exception of when using the optimal action (one of exponentially many actions and is thus hard to find), planners are either left with either solving the needle in a haystack problem of identifying the optimal action by randomly stumbling upon it, or they need to experiment with actions in the last stage. That the rewards are chosen to be identically zero is not critical: From the point of view of this argument, what is critical is that they are all the same. It remains to be seen that consistency can be achieved and also that the optimal action at $s_0$ has a large value compared to the values of suboptimal actions at the same state. Here, we still face some challenges with consistency. Since we want the immediate rewards to belong to the $[0,1]$ interval, all the action values have to be nonnegative. As such, it will be easier if we introduce an additional bias component $c_h$ in the feature vectors, which we allow to scale with the stage. To summarize, we let . \\[\\begin{align*} \\phi_h(s,a) = ( c_h, (3/2)^{-h+1} \\sigma_{sa} u_a^\\top )^\\top\\,. \\end{align*}\\] while we propose to use . \\[\\begin{align*} \\theta^* = \\frac{1}{3} (1, u_{a^*}^\\top)^\\top \\,. \\end{align*}\\] It remains to show that \\eqref{eq:aopt} and \\eqref{eq:cons} can be satisfied with \\(q_h(s,a):=\\langle \\phi_h(s,a), \\theta^* \\rangle\\), while also keeping the suboptimal gap of \\(a^*\\) at \\(s_0\\) large, and while the last stage rewards (\\eqref{eq:laststage}) are in $[0,1]$ and are of size \\(e^{-\\Theta(H)}\\) as planned. Assume for a moment that \\(a^*\\) is optimal in all states, i.e., that \\eqref{eq:aopt} holds. Then, \\(a^*\\) is also optimal in state $sa$, hence, under \\(q^*_h=q_h\\), \\eqref{eq:cons} for any \\(a\\ne a^*\\) is equivalent to . \\[\\begin{align*} q_h(s,a) = q_{h+1}(sa,a^*) \\end{align*}\\] where we also used that by assumption $r_a(s)=0$ because \\(a\\ne a^*\\). Plugging in the definitions, . \\[\\begin{align} \\sigma_{sa,a^*} = \\left(\\frac{3}{2}\\right)^h \\left(c_h-c_{h+1}\\right) + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a^*} \\rangle\\,. \\label{eq:sigmarec} \\end{align}\\] Define \\((c_h)_{0\\le h\\le H-1}\\) so that . \\[\\begin{align*} \\left(\\frac{3}{2}\\right)^h \\left(c_h-c_{h+1}\\right) =\\frac{5}{8}\\,. \\end{align*}\\] with \\(C_{H-1} = \\frac{1}{2}\\left(\\frac32\\right)^{-H}\\) (i.e., $c_h$ is a decreasing geometric sequence) This has two implications: \\eqref{eq:sigmarec} simplifies to . \\[\\begin{align} \\sigma_{sa,a^*} = \\frac{5}{8} + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a^*} \\rangle\\,, \\label{eq:sigmarec2} \\end{align}\\] and also for the last stage rewards, from \\eqref{eq:laststage} we get . \\[\\begin{align*} r_a(s) = \\frac{1}{3} \\left(\\frac32\\right)^{-H} \\left( \\frac{1}{2} + \\sigma_{sa} \\frac32 \\langle u_a,u_{a^*}\\rangle\\right)\\,. \\end{align*}\\] Clearly, if $\\sigma_{sa}\\in [-4/3,4/3]$, since for \\(a\\ne a^*\\), \\(\\vert \\langle u_a,u_{a^*}\\rangle \\vert \\le 1/4\\), \\(r_a(s)\\in [0,(3/2)^{-H}/3]\\) while also \\(r_{a^*}(s)\\in [0,1]\\). With this, to satisfy \\eqref{eq:cons}, on the one hand we choose to define $\\sigma_{sa}$ with the following “downward recursion” in the tree: For any $s$ in the tree and actions $a,a’$, . \\[\\begin{align} \\sigma_{sa,a'} = \\frac{5}{8} + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a'} \\rangle\\,. \\label{eq:sigmarec3} \\end{align}\\] Note that this is consistent with \\eqref{eq:sigmarec2}. The next challenge is to show that $\\sigma_{sa}$ stays within a constant range. In fact, with the above definition, this will not hold. In particular, when $a=a’$, the right-hand side can be as large as \\(5/8+3/2 \\sigma_{sa} \\ge 3/2 \\sigma_{sa}\\), which means that the scaling coefficients will exponentially increase with a base of $(3/2)$. Note, however, that if $a\\ne a’$, then provided that \\(\\sigma_{sa}\\in [1/4,1]\\) (which can be ensured at the root by choosing \\(\\sigma_{s_0,a}=1\\) for all actions \\(a\\)), . \\[\\frac{1}{4} = \\frac{5}{8} - \\frac{3}{8} \\le \\frac{5}{8} + \\frac{3}{2} \\sigma_{sa} \\langle u_a,u_{a'} \\rangle \\le \\frac{5}{8} + \\frac{3}{8} \\le 1\\,,\\] and thus \\(\\sigma_{sa,a'}\\in [1/4,1]\\) will also hold. Hence, we modify the construction so that the definition \\eqref{eq:sigmarec3} is never needed for $a=a’$. This is achieved by changing the dynamics: We introduce a special set of states, ${e_1,\\dots,e_H}$, the exit lane. Once, the process gets into this lane, there is now return and in fact all the remaining rewards up the end are zero. Specifically, all the actions in $e_h$ lead to state $e_{h+1}$ and we set the feature vector of all states in the exit-lane zero: . \\[\\phi_h(e_h,a) = \\boldsymbol{0}\\,.\\] This way, regardless the choice of the parameter vector, we ensure that the Bellman optimality equations hold at these state and the optimal values are correctly set to zero. The exit lane is introduced to remove the need to use \\eqref{eq:sigmarec3} with repeat actions. In particular, for any \\(s\\in \\mathcal{S}_h\\) with some $h\\ge 1$, say, \\(s=(a_1,\\dots,a_h)\\) (i.e., $s$ is obtained by following these actions) then if for \\(a\\in \\{a_1,\\dots,a_h\\}\\), the next state is $e_{h+1}$. Since the optimal value of $e_{h+1}$ is zero and we don’t intend to introduce an immediate reward, we set . \\[\\phi_h(s,a)=\\boldsymbol{0}\\,,\\] making the value of repeat actions zero. The next complication is that this ruins our plan to keep \\(a^*\\) optimal at all states: Indeed, \\(a^*\\) could be applied multiply times in a path from \\(s_0\\) to a leaf of the tree, and by the second application, the new rule forces the value of \\(a^*\\) to be zero. Hence, we need to modify this rule when the action is \\(a^*\\). Clearly, whether a suboptimal action, or \\(a^*\\) is repeated is problematic for the recursive definition of $\\sigma_{sa}$. Hence, it is better if \\(a^*\\) is also forced to use the exit lane. Thus, if \\(a^*\\) is used in \\(s\\in \\mathcal{S}_h\\) with \\(h\\ge 0\\), the next state is \\(e_{h+1}\\). However, we do not zero out \\(\\sigma_{sa^*}\\), but keep the recursive definition and we rather introduce an immediate reward to match \\(q_h(s,a^*) = \\langle \\phi_h(s,a^*), \\theta^* \\rangle\\). It is not hard to check that this reward is also in the \\([0,1]\\) range. Note that here if \\(s = (a_1,\\dots,a_h)\\) then by definition \\(a^*\\not\\in \\{a_1,\\dots,a_h\\}\\). This completes the description of the structure of the MDPs. That the action gap at \\(s_0\\) is large follows from the choice of the JL feature vectors. It remains to be seen that \\(a^*\\) is indeed the optimal action at any state. This boils down to checking that for \\(a'\\ne a^*\\), \\(q_{h+1}(sa,a^*)-q_{h+1}(sa,a')\\ge 0\\). When \\(a'\\) is a repeat action, this is trivial. When \\(a'\\) is not a repeat action, we have . \\[q_{h+1}(sa,a^*)-q_{h+1}(sa,a') = \\frac{1}{3}\\left(\\frac{3}{2}\\right)^{-h} \\left[ \\sigma_{sa,a^*}-\\sigma_{sa,a'}\\langle u_{a'},u_{a^*}\\rangle \\right] \\ge \\frac{1}{3}\\left(\\frac{3}{2}\\right)^{-h} \\left[ \\frac{1}{4}-\\frac{1}{4} \\right] = 0\\] where we used that \\(\\sigma_{sa,a^*}\\ge 1/4\\) and \\(1/4\\le \\sigma_{sa,a'}\\le 1\\) and thus \\(\\sigma_{sa,a'}\\langle u_{a'},u_{a^*}\\rangle\\ge -\\frac{1}{4}\\) by the choice of \\((u_a)_a\\) and since \\(a\\ne a'\\). Let \\(M_{a^*}\\) denote the MDP constructed this way when the optimal action is \\(a^*\\) (the feature maps, of course, are common between these MDPs). For a formal proof, one also needs to argue that planners that do not use many queries cannot distinguish between these MDPs. Intuitively, this is because such planners will receive, with high probability, identical observations under different MDPs in this class. As such, these planners can at best randomly choose an action (“needle in a haystack”) and since in MDP \\(M_{a}\\) only action \\(a\\) incurs high values, they cannot induce a policy with a near-optimal value. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec10/#the-main-ideas-of-the-proof",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec10/#the-main-ideas-of-the-proof"
  },"323": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Computation with many actions",
    "content": "In the construction given the number of actions was allowed to scale exponentially with the dimension. The above proof would show a separation between the query and computation complexity of planning, if one could demonstrate that there is a choice of the JL feature vectors when the optimization problems . \\[\\begin{align*} \\arg\\max_{a\\in \\mathcal{A}} \\langle \\phi(s,a), \\theta \\rangle \\end{align*}\\] admits a computationally efficient solver regardless of the choice of $\\theta\\in \\mathbb{R}^d$ and $s\\in \\mathcal{S}$ (for simplicity, we suppress dependence on $h$). Whether such a solver exist will depend on the choice of the feature-map and this is a fascinating question on its own. One approach to arrive at such a solver is to rewrite this problem as the problem of finding . \\[\\begin{align} \\arg\\max_{v\\in V_s} \\langle v, \\theta \\rangle \\label{eq:linopt} \\end{align}\\] where $V_s \\subset \\mathbb{R}^d$ is the convex hull of the feature vectors \\(\\{ \\phi(s,a) \\}_{a\\in \\mathcal{A}}\\). Provided that this problem admits an efficient solution and given any extreme point of $v\\in V_s$, we can efficiently recover an action $a\\in \\mathcal{A}$ such that $\\phi(s,a)=v$ (this amounts to “inverting” the feature map), the first problem can also be solved efficiently. Note that \\eqref{eq:linopt} is a linear optimization problem over a convex set $V_s$ and the question whether this problem admits an efficient solver lies at the heart of computer science. The general lesson is that the answer can be expected to be yes when $V_s$ has some “convenient” description other than the one that is used to define it. The second problem of inverting the feature map is known as the “decomposition problem” and the same conclusions hold for this problem. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec10/#computation-with-many-actions",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec10/#computation-with-many-actions"
  },"324": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Notes",
    "content": ". | It is possible to modify the construction to make it work in the discounted setting. The paper cited below shows how. | Back to the finite horizon setting, for an upper bound, one can employ the least-squares value iteration algorithm with $G$-optimal design (LSVI-G), which we have met in Homework 2. What results is that to get a $\\delta$-sound (global) planner with this approach, . | . \\[\\begin{align*} O\\left( \\frac{H^5(2d)^{H+1}}{\\delta^2}\\right) \\end{align*}\\] queries are sufficient (and the compute cost is also of similar order). We see that as far as the exponents in the lower and upper bounds are concerned, in the upper bound the exponent is \\(\\Theta(H \\log_2(d))\\) while in the lower bound it is \\(O(H\\wedge d)\\). Thus, there remains a logarithmic gap between them when $H\\ll d$, while the gap is unbounded when \\(H \\gg d\\), i.e., for long horizon problems. In particular, in the constant dimension and long-horizon featurized planning problem, the LSVI-G algorithm seems to be suboptimal because it calculates the optimal action-value function stage-wise. One conjectures that the upper bound for LSVI-G is tight, while the lower bound in this lecture is also essentially correct. This would means that there is an alternate algorithm that could perform much better than LSVI-G in large-horizon planning with constant feature-dimension. Clearly, for the specific construction used in this lecture, a planner that tries all actions, say at \\(s_0\\), will find the optimal action and the cost of this planner is independent of the horizon. Hence, at least in this case, the lower bound can be matched with an alternate algorithm. One may think that this problem is purely of theoretical interest. To counter this note that long-horizon planning is a really important practical question: Many applications require thousands of steps, if not millions, while perhaps the feature space dimension does not need to be very large. Whether there exist an algorithm that works better than LSVI-G thus remains to be a fascinating open problem with good potential for having a real impact on applications. | For infinite horizon undiscounted problems and \\(v^*\\) realizability, there is a simple example that shows that with \\(\\Theta(d)\\) actions and $d$-dimensional features, any query efficient planner that guarantees a constant suboptimality gap needs \\(\\Omega(2^d/d)\\) queries per state. This is based on a shortest path problem on a regular grid. Here, the obstruction is simply algebraic: There is no noise in either the transitions or the rewards. | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec10/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec10/#notes"
  },"325": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "Bibliographical notes",
    "content": "This lecture is entirely based on the paper . | Weisz, Gellert, Philip Amortila, and Csaba Szepesvári. 2020. “Exponential Lower Bounds for Planning in MDPs With Linearly-Realizable Optimal Action-Value Functions.”, | . which is available on arXiv and which will also soon appear at ALT. The second lower for the undiscounted setting mentioned in the notes is from . | Weisz, Gellert, Philip Amortila, Barnabás Janzer, Yasin Abbasi-Yadkori, Nan Jiang, and Csaba Szepesvári. 2021. “On Query-Efficient Planning in MDPs under Linear Realizability of the Optimal State-Value Function.” | . available on arXiv. A beautiful book that is a very good source on reading about the linear optimization problem mentioned above is . | Grotschel, Martin, László Lovász, and Alexander Schrijver. 1993. Geometric Algorithms and Combinatorial Optimization. Vol. 2. Algorithms and Combinatorics. Berlin, Heidelberg: Springer Berlin Heidelberg. | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec10/#bibliographical-notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec10/#bibliographical-notes"
  },"326": {
    "doc": "10. Planning under $q^*$ realizability",
    "title": "10. Planning under $q^*$ realizability",
    "content": "The lesson from the last lecture is that efficient planners are limited to induce policies whose suboptimaly gap is polynomially larger than the misspecification error of the feature-map supplied to the planner. We have also seen) that if we accept this polynomial in the feature-space-dimension error amplification, a relatively straightforward adaptation of policy iteration gives rise to a computationally efficient (global) planner – at least, when the planner is furbished with the solution to an underlying optimal experimental design problem. In any case, the planner is query efficient. All this was shown in the context when the misspecification error is relative to the set of action value functions underlying all possible policies. In this lecture we look into whether this error metric could be changed so that the misspecification error is measured by how well the optimal action-value function, $q^*$, is approximated by the features, while still retaining the positive result. As the negative result already implies that there are no efficient planners unless the suboptimality gap of the induced policy is polynomially larger than the approximation error, we look into the case when the optimal action-value function is perfectly representable with the features supplied to the planner. This assumption is also known as “\\(q^*\\)-realizability”, or, “\\(q^*\\) linear realizability”, if we want to be more specific about the nature of the function approximation technique used. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec10/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec10/"
  },"327": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "TensorPlan: An optimistic planner",
    "content": "The planner that is referred to in the previous theorem is called TensorPlan. The reason for this name will become clear after we describe the algorithm. TensorPlan belongs to the class of optimistic algorithms. Since knowing \\(\\theta^*\\), the parameter vector that realizes \\(v^*\\), would be sufficient for acting near-optimally, the algorithm aims to find a good approximation to this vector. A suitable estimate is constructed in a two-step process: . | The algorithm maintains a non-empty “hypothesis” set \\(\\Theta\\subset \\mathbb{R}^d\\), which contains those parameter vectors that are consistent with the data that the algorithm has seen. The details of the construction of this set are at the heart of the algorithm and will come soon. | Given \\(\\Theta\\), an estimate $\\theta^+$ is produced by solving a maximization problem: | . \\[\\begin{align} \\theta^+=\\arg\\max_{\\theta\\in \\Theta} \\phi_0(s_0)^\\top \\theta\\,. \\label{eq:optplanning} \\end{align}\\] Here, \\(s_0\\) is the initial state of the episode, i.e., this is the state the planner is called when $h=0$. Recalling that \\(\\phi_0(s_0)^\\top \\theta^* = v_0^*(s_0)\\), we see that provided that \\(\\theta^*\\in \\Theta\\), . \\[v_0(s_0;\\theta^+)\\ge v_0^*(s_0)\\,,\\] where, for convenience, we introduce \\(v_h(s;\\theta) = \\phi_h(s)^\\top \\theta\\). When $\\theta^+$ is close enough to \\(\\theta^*\\), one hopes that the policy induced by \\(\\theta^+\\) will be near-optimal. Hence, the approach is to “roll out” with the induced policy (using the simulator) and verify whether during the rollout the data received is consistent with the Bellman equation, and as a result of this, also whether the episode return observed is close to \\(v_0(s_0;\\theta^+)\\). When a contradiction to any of these is detected, the data can be used to shrink the set \\(\\Theta\\) of consistent parameter vectors. The approach described leaves open the question of what we mean by a policy “induced” by \\(\\theta^+\\). The naive approach is to base this on the Bellman optimality equation, which states that . \\[\\begin{align} v_h^*(s) = \\max_a r_a(s)+ \\langle P_a(s), v_{h+1}^* \\rangle \\label{eq:fhboe} \\end{align}\\] holds for $h=0,1,\\dots,H-1$ with \\(v_H^* = \\boldsymbol{0}\\). If \\(\\theta^+= \\theta^*\\), \\(v_h(\\cdot;\\theta^+)\\) will also satisfy this equation and thus one might define the policy induced by \\(\\theta^+\\) that achieves the maximum above when \\(v_{h+1}^*\\) is replaced by \\(v_{h+1}(\\cdot;\\theta^+)\\). Consistency of \\(\\theta^+\\) would also mean checking whether \\eqref{eq:fhboe} holds (approximately) when \\(v^*_{\\cdot}(\\cdot)\\) is replaced in this equation by \\(v_{\\cdot}(\\cdot;\\theta^+)\\), which, one may imagine can be checked by generating data from the simulator. While this may approach work, it is not easy to see whether it does. (It is open problem whether this works!) TensorPlan defines induced policies and consistency slightly differently. The changed definition allows not only for proving that TensorPlan is query-efficient, but it even makes the guarantees for TensorPlan stronger than what was announced above in the theorem. What makes the analysis of the algorithm that is based on the Bellmean optimality equation difficult is the presence of the maximum in this equation. Hence, TensorPlan removes this maximum. Accordingly, the policy induced by $\\theta^+$ is defined as any policy $\\pi_{\\theta^+}$ which in state $s$ and stage $h$ chooses any action $a\\in \\mathcal{A}$ which ensures that . \\[\\begin{align} v_h(s;\\theta^+) = r_a(s)+ \\langle P_a(s), v_{h+1}(\\cdot;\\theta^+) \\rangle\\,. \\label{eq:tpcons} \\end{align}\\] If there is no such action, \\(\\pi_{\\theta}\\) is free to choose any action. We say that local consistency holds at $(s,h,\\theta^+)$ when there exists an action $a\\in \\mathcal{A}$ such that \\eqref{eq:tpcons} holds. If there are multiple actions that satisfy \\eqref{eq:tpcons}, any of them will do: Choosing the maximizing action is not enforced. However, when \\(v^*\\) is realizable and \\(\\theta^+=\\theta^*\\), any action that satisfies \\eqref{eq:tpcons} will be a maximizing action and the policy induced will be optimal. The advantage of the relaxed notion of induced policy is that with this choice, TensorPlan will also be able to compete with any deterministic policy whose value-function is realizable. This expands the scope of TensorPlan: Perhaps the optimal value function is not realizable with the features handed to TensorPlan, but if there is any deterministic policy whose value-function is realizable with them, then TensorPlan will be guaranteed to produce almost as much as reward as that policy. In fact, it will produce nearly as much reward as the policy that achieves the best value. TensorPlan . To summarize, after generating a hypothesis \\(\\theta^+\\), TensorPlan will run a number of rollouts using the simulator so that for each state $s$ encountered TensorPlan first finds an action $a$ satisfying \\eqref{eq:tpcons}. If this succeeds, the rollout continues by TensorPlan getting a next state from the simulator at $(s,a,h)$ and $h$ is incremented. This continues up to $h=H$, which ends a rollout. TensorPlan will run \\(m\\) rollouts of this type and if all of them succeeds, TensorPlan stops and will use the parameter vector \\(\\theta^+\\) in the rest of the episode and the same policy \\(\\pi_{\\theta^+}\\) as used during the rollouts. If during the rollouts an inconsistency is detected, TensorPlan will decrease the hypothesis set \\(\\Theta\\) and continue with a next experiment. It remains to be seen why TensorPlan (1) stops with a bounded number of queries and (2) why it is sound. Boundedness . We start with boundedness. This is where the change of how policies are induced by parameters is used in a critical manner. Introduce the discriminants: . \\[\\begin{align*} \\Delta(s,a,h,\\theta) = r_a(s) = \\langle P_a(s)\\phi_{h+1},\\theta \\rangle - \\phi_h(s)^\\top \\theta\\,. \\end{align*}\\] Note that \\(\\Delta(s,a,h,\\theta)\\) is just the difference between the right-hand and the left-hand side of \\eqref{eq:tpcons}, where we plugged in the definition $v_h$ and $v_{h+1}$ and we define . \\[P_a(s)\\phi_{h+1} = \\sum_{s'\\in \\mathcal{S}} P_a(s,s') \\phi_{h+1}(s')\\,;\\] thus \\(P_a(s)\\phi_{h+1}\\) is the “expected next feature vector” given $(s,a)$. Then, by definition, local consistency holds for $(s,h,\\theta)$ if and only if there exists some action $a\\in \\mathcal{A}$ such that $\\Delta(s,a,h,\\theta)=0$. Exploiting that the product of numbers is zero if and only if some of them is zero, we see that local consistency is equivalent to . \\[\\begin{align} \\prod_{a\\in \\mathcal{A}} \\Delta(s,a,h,\\theta) = 0\\,. \\label{eq:diprod} \\end{align}\\] The reason this purely algebraic reformulation of local consistency is helpful is because the product of the discriminants can be see as a linear function of the $A$-fold tensor product of \\((1,\\theta^\\top)^\\top\\). To see why this holds, it will be useful to introduce some extra notation: For a real $r$ and a finite-dimensional vector $u$, we will denote by \\(\\overline{ r u}\\) the vector \\((r,u^\\top)^\\top\\) (i.e., adding $r$ to the first position and shifting down all other entries in $u$). With this notation, we can write the discriminants as an inner product: . \\[\\begin{align*} \\Delta(s,a,h,\\theta) = \\langle \\overline{r_a(s)\\, (P_a(s)\\phi_{h+1}-\\phi_h(s))}, \\overline{1 \\, \\theta} \\rangle \\end{align*}\\] Now, recall that the tensor product \\(\\otimes\\) of vectors satisfies the following property: . \\[\\begin{align*} \\prod_a \\langle x_a, y_a \\rangle = \\langle \\otimes_a x_a, \\otimes_a y_a \\rangle\\,, \\end{align*}\\] where the inner product between two tensors is defined in the usual way, by overlaying them and then taking the sum of the products of the entries that are on the top of each other. Based on this identity, we see that \\eqref{eq:diprod}, and thus local consistency, is equivalent to . \\[\\begin{align*} \\langle \\underbrace{\\otimes_a \\overline{r_a(s)\\, (P_a(s)\\phi_{h+1}-\\phi_h(s))}}_{D(s,h)}, \\underbrace{\\otimes_a \\overline{1 \\, \\theta}}_{F(\\theta)} \\rangle = 0\\,. \\end{align*}\\] Note that while $F(\\theta)\\in \\mathbb{R}^{(d+1)^A}$ is a nonlinear function of \\(\\theta\\), the above equation is linear in \\(F(\\theta)\\). Imagine for a moment that the data \\(D(s,h)\\) above can be obtained with no errors and assume that \\(v^*\\) is realizable. Let $k = (d+1)^A$. We can think of both \\(D(s,h)\\) and \\(F(\\theta)\\) taking values in \\(\\mathbb{R}^k\\) (this corresponds to “flattening” these tensors). TensorPlan can be seen as an algorithm that generates a sequence $(\\theta_1,x_1), (\\theta_2,x_2), \\dots$ such that \\(\\theta_i\\in \\mathbb{R}^d\\) is the \\(i\\)th hypothesis that TensorPlan chooses, \\(x_i\\in \\mathbb{R}^k\\) is the \\(i\\)th data of the form \\(D(s,h)\\) with some \\((s,h)\\) where TensorPlan detects an inconsistency. When inconsistency is detected, the hypothesis set is shrunk: . \\[\\Theta_{i+1} = \\Theta_i \\cap \\{ \\theta\\,:\\, F(\\theta)^\\top x_i=0 \\},\\] and \\(\\theta_{i+1}\\) is chosen in \\(\\Theta_{i+1}\\) by \\eqref{eq:optplanning}. Together with \\(\\Theta_1 = B_2^d(B)\\) (the \\(\\ell^2\\) ball of radius \\(B\\) in \\(\\mathbb{R}^d\\)), we have that for $i&gt;1$, . \\[\\Theta_i = \\{ \\theta\\in B_2^d(B)\\,:\\, F(\\theta)^\\top x_1 = 0, \\dots, F(\\theta)^\\top x_{i-1}=0 \\}.\\] Let \\(f_i = F(\\theta_i)\\). By its construction, for any \\(i\\ge 1\\), \\(\\theta_i\\in \\Theta_i\\) and hence \\(f_i\\) is orthogonal to \\(x_1,\\dots,x_{i-1}\\). Also by its construction, \\(x_i\\) is not orthogonal to \\(f_i\\). Because of this, \\(x_i\\) cannot lie in the span of \\(x_1,\\dots,x_{i-1}\\) (if it did, it would be orthogonal to \\(f_i\\)). Hence, the vectors \\(x_1,x_2,\\dots\\) are linearly independent. As there are at most \\(k\\) linearly independent vectors in \\(\\mathbb{R}^k\\), Tensorplan will generate at most \\(k\\) of these data vectors (in fact, for TensorPlan, this is \\(k-1\\), can you explain why?). This means that after at most \\(k\\) “contradictions” to local consistency, TensorPlan will cease to detect more inconsistencies and thus it will stop. Soundness . It remains to be seen that TensorPlan is sound. Let \\(\\theta^+\\) be the parameter vector that TensorPlan generated when it stops. This means that during the \\(m\\) rollouts, TensorPlan did not detect any inconsistencies. Take a trajectory \\(S_0^{(i)},A_0^{(i)},\\dots,S_{H-1}^{(i)},A_{H-1}^{(i)},S_H^{(i)}\\) generated during the \\(i\\)th rollout of \\(m\\) rollouts. Since there is no inconsistency along it, for any \\(0\\le t \\le H-1\\) we have . \\[\\begin{align} r_{A_t^{(i)}}(S_t^{(i)}) = v_t(S_t^{(i)};\\theta^+)-\\langle P_{A_t^{(i)}}(S_t^{(i)}), v_{t+1}(\\cdot;\\theta^+) \\rangle\\,. \\label{eq:constr} \\end{align}\\] Hence, with probability \\(1-\\zeta\\), . \\[\\begin{align*} v_0^{\\pi_{\\theta^+}}(s_0) &amp; \\ge \\frac1m \\sum_{i=1}^m\\sum_{t=0}^{t-1} r_{A_t^{(i)}}(S_t^{(i)}) - H \\sqrt{ \\frac{\\log(1/\\zeta)}{2m}} \\\\ &amp; = \\frac1m \\sum_{i=1}^m\\sum_{t=0}^{t-1} v_t(S_t^{(i)};\\theta^+)-\\langle P_{A_t^{(i)}}(S_t^{(i)}), v_{t+1}(\\cdot;\\theta^+)\\rangle - H \\sqrt{ \\frac{\\log(2/\\zeta)}{2m}} \\\\ &amp; \\ge \\frac1m \\sum_{i=1}^m\\sum_{t=0}^{t-1} v_t(S_t^{(i)};\\theta^+)- v_{t+1}(S_{t+1}^{(i)};\\theta^+) - (H+2B) \\sqrt{ \\frac{\\log(2/\\zeta)}{2m}} \\\\ &amp; = v_0(s_0;\\theta^+) - (H+2B) \\sqrt{ \\frac{\\log(2/\\zeta)}{2m}}\\,, \\end{align*}\\] where the first inequality is by Hoeffding’s inequality and uses that rewards are bounded in \\([0,1]\\), the equality after it uses \\eqref{eq:constr}, the second inequality is again by Hoeffding’s inequality and uses that . \\[\\begin{align*} \\langle P_{A_t^{(i)}} (S_t^{(i)}), v_{t+1}(\\cdot;\\theta^+)\\rangle = \\mathbb{E} [ v_{t+1}(S_{t+1}^{(i)};\\theta^+) | S_t^{(i)},A_t^{(i)}] \\end{align*}\\] and that \\(v_t\\) is bounded between \\([-B,B]\\) (note that we could truncate \\(v_t\\) to \\([0,H]\\) to replace \\(H+2B\\) above by \\(2H\\)), while the last equality uses that \\(v_H(\\cdot;\\theta^+)=\\boldsymbol{0}\\) by definition and that \\(S_0^{(i)}=s_0\\) by definition. Setting \\(m\\) high enough (\\(m=\\tilde O((H+B)^2/\\delta^2)\\)) we can guarantee . \\[v_0^{\\pi_{\\theta^+}}(s_0) \\ge v_0(s_0;\\theta^+)-\\delta.\\] We now argue that this implies soundness. Letting \\(\\Theta^\\circ \\subset B_2^d(B)\\) be the set of \\(B\\)-bounded parameter vectors \\(\\theta\\) such that for some deterministic policy \\(\\pi\\), \\(v^\\pi = \\Phi \\theta\\). By the definition of \\(D\\) and \\(F\\), for any \\(i\\ge 1\\), \\(\\Theta^\\circ \\subset \\Theta_{i}\\) (no correct hypothesis is ever eliminated). It also follows that at any stage of the process, . \\[v_0(s_0;\\theta^+)\\ge \\max_{\\theta\\in \\Theta^\\circ} v^{\\pi_{\\theta}}_0(s_0).\\] Hence, when TensorPlan stops with parameter \\(\\theta^+\\), with high probability, . \\[v^{\\pi_{\\theta^+}}_0(s_0)\\ge v_0(s_0;\\theta^+)-\\delta \\ge \\max_{\\theta\\in \\Theta^\\circ} v^{\\pi_{\\theta}}_0(s_0)-\\delta\\,.\\] In particular, if \\(v^*\\) is \\(B\\)-realizable, \\(v^{\\pi_{\\theta^+}}_0(s_0) \\ge v^*_0(s_0)-\\delta\\). Thus, after stopping, for the rest of the episode, TensorPlan can safely use the policy induced by \\(\\theta^+\\). Summary . So far we have seen that if somehow TensorPlan would be able to get \\(\\Delta(s,a,h,\\theta)\\) with no errors, (1) it would stop after refining its hypothesis set at most \\(k\\) times and (2) when it stops, with high probability it would return with a parameter vector that induces a policy with high value. Regarding the number of queries used, if obtaining \\(\\Delta(s,a,h,\\theta)\\) is counted as a single query, TensorPlan would need at most \\(maH k =maH (d+1)^A\\) queries (\\(m\\) rollouts, for each of the \\(H\\) states in the rollout, \\(A\\) queries are needed). It remains to be seen how to adjust this argument to the case when \\(\\Delta(s,a,h,\\theta)\\) need to be estimated based on interactions with a stochastic simulator. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec11/#tensorplan-an-optimistic-planner",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec11/#tensorplan-an-optimistic-planner"
  },"328": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "Notes",
    "content": ". | It is not known whether TensorPlan can be computationally efficiently implemented. I suspect it cannot. This is because \\(\\Theta_i\\) is specified with a number of highly nonlinear constraints (in the parameter vector). | The essence of the construction here is lifting the problem into a higher-dimensional linear space. This is a standard technique in machine learning but in a very different context when data is mapped to a higher dimensional space to strengthen the power of linear predictors. The once popular RKHS methods take this to the extreme. Note that here, in contrast to this classic lifting procedure, the parameter vector is mapped through a nonlinear function to a higher dimensional space and the purpose is to simply have a clear grasp on why learning stops. | We call \\(\\Delta\\) here the discriminant function because what is important about it is that it discriminates between “good” and “bad” cases and it does it by using the special value of zero. Readers familiar with the RL literature will note, however, that \\(\\Delta\\) is nothing but, what is known as the “temporal difference error” (under some fixed action). | It is curious that the algorithm builds up a data-bank of critical data that it uses to restrain the set of parameter vectors and that it is quite selective in adding new data here. That is, TensorPlan may generate a lot more data then goes on the list $x_1,x_2,\\dots$. If we wanted to be philosophical and would not mind antropomorphising algorithms, we could say that TensorPlan remembers what it is “surprised by”. This is very much unlike other algorithms, like LSVI-$G$, which may generate a lot of redundant data. The other difference is that TensorPlan uses the data to generate a hypothesis set. The choice of the parameter vector from this set is dictated by the optimization (reward maximization) problem solved by TensorPlan. | There are quite a few examples of optimistic algorithms in planning; there is a considerable literature of using optimisim in tree search. However, classics, such as the \\(A^*\\) algorithm can also be seen as an optimistic algorithm (at least when used with an “admissible heuristic”, which is just a way of saying that \\(A^*\\) uses an optimistic estimate of the values). The \\(LAO^*\\) algorithm is another example. However, the real “homeland” of optimistic algorithms in online learning, a topic that will be covered later in the course. | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec11/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec11/#notes"
  },"329": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "Bibliographical notes",
    "content": "This lecture is entirely based on the paper . | Weisz, Gellert, Philip Amortila, Barnabás Janzer, Yasin Abbasi-Yadkori, Nan Jiang, and Csaba Szepesvári. 2021. “On Query-Efficient Planning in MDPs under Linear Realizability of the Optimal State-Value Function.” | . available on arXiv. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec11/#bibliographical-notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec11/#bibliographical-notes"
  },"330": {
    "doc": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "title": "11. Planning under $v^*$ realizability (TensorPlan I.)",
    "content": "In the last lecture we saw that under \\(q^*\\) linear realizability, query-efficient fixed-horizon local planning with a constant suboptimality gap is intractable provided that there is no limit on the number of actions. In particular, the MDPs that were used to show intractability use $e^{\\Theta(d)}$ actions, where $d$ is the dimension of the feature-map that realizes the optimal action-value function. At the end of the lecture, we also noted that intractabality also holds for undiscounted infinite horizon problems under \\(v^*\\) linear realizability in the regime when the number of actions scales linearly with $d$. In this lecture we further dissect \\(v^*\\) realizability, but return to the fixed horizon setting and we will consider the case when the number of actions is fixed. As it turns out, in this case, query-efficient local planning is possible. Before giving the details of this result, we need to firm up some and refine other definitions. First, \\(v^*\\) realizability under a feature map \\(\\phi=(\\phi_h)_{0\\le h \\le H-1}\\) in the $H$-horizon setting means that . \\[\\begin{align} \\inf_{\\theta\\in \\mathbb{R}^d} \\max_{0\\le h \\le H-1}\\|\\Phi_h \\theta - v^*_{h} \\|_\\infty = 0\\,, \\label{eq:vsrealizability} \\end{align}\\] where \\(v^*_h\\) is the optimal-value function when \\(H-h\\) steps are left (in particular, \\(v^*_H=\\boldsymbol{0}\\)). Again, this uses the indexing introduced in the previous lecture. In what follows, without the loss of generality we assume that the feature map is such that all the feature-vectors lie within the a ($2$-norm) ball of radius one. When realizability holds with a parameter vector bounded in $2$-norm by $B$, we say that \\(v^*\\) is $B$-realizable under the feature map $\\phi$. We also slightly modify the interaction protocol between the planner and the simulator, as shown on the figure below. The main new features are introducing stages, and restricting the planners to access states and features only through local calls to the simulator. Illustration of the interaction protocol between the planner and the simulator. Because in fixed-horizon problems the stage index influences what actions should be taken, the planner is called with an initial state \\(s_0\\) and a stage index \\(h\\). For defining the policy induced the planner, it is assumed that the planner is first called with \\(h=0\\) at some state, then it is called with \\(h=1\\) with a state obtained following a transition by taking the action returned by the planner, etc. While interacting with the simulator, the planner is restricted to use only states that it has encountered before. Also, the planner can feed a stage index to the simulator, to get the features of the next state corresponding to the incremented input stage index. There is no other access to the features. Note also that just like in the previous lecture, we allow the MDPs to generate random rewards. In this setting a \\(\\delta\\)-sound planner is one which, under the above protocol, induces a policy of the MDP whose simulator it interacts with which is at most \\(\\delta\\)-suboptimal. Theorem (query-efficient planning under \\(v^*\\)-realizability): For any integers $A,H&gt;0$ and reals $B,\\delta&gt;0$, there exists a local planner $\\mathcal{P}$ with the following properties: . | The planner $\\mathcal{P}$ is \\(\\delta\\)-sound for the $H$-horizon planning problem and the class of MDP-feature-map pairs $(M,\\phi)$ such that $v^*$ is $B$-realizable under $\\phi$ and $M$ has at most $A$ actions and its rewards are bounded in $[0,1]$; | The number of queries used by the planner in each of its call is at most | . \\[\\begin{align*} \\text{poly}\\left( \\left(\\frac{dH}{\\delta}\\right)^A, B \\right) \\end{align*}\\] . Note that for $A&gt;0$ fixed the query-cost is polynomial in $d,H,1/\\delta$ and $B$. It remains to be seen whether this bound can be improved. However, this is somewhat of a theoretical question as under \\(v^*\\)-realizability, even if the coefficients $\\theta\\in \\mathbb{R}^d$ that realize \\(v^*\\) are known, in the lack of extra information, one needs to perform \\(\\Theta(A)\\) simulation calls to be able to get good approximations to the action-value function \\(q^*\\), which seems necessary for inducing a good policy. Hence, the query cost must scale at least linearly with \\(A\\), hence, no algorithm is expected to be even query-efficient when the number of actions is large. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec11/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec11/"
  },"331": {
    "doc": "12. TensorPlan and eluder sequences",
    "title": "12. TensorPlan and eluder sequences",
    "content": " ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec12/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec12/"
  },"332": {
    "doc": "13. From API to Politex",
    "title": "Politex",
    "content": "Politex comes from Policy Iteration with Expert Advice. Assume that one is given a featurized MDP \\((M,\\phi)\\) with state-action feature-map \\(\\phi\\) and access to a simulator, and a $G$-optimal design \\(\\mathcal{C}\\subset \\mathcal{S}\\times\\mathcal{A}\\) for \\(\\phi\\). Politex generates a sequence of policies \\(\\pi_0,\\pi_1,\\dots\\) such that for \\(k\\ge 1\\), . \\[\\pi_k(a|s) \\propto \\exp\\left( \\eta \\bar q_{k-1}(s,a)\\right)\\,,\\] where . \\[\\bar q_{k} = \\hat q_0 + \\dots + \\hat q_j,\\] with . \\[\\hat q_j = \\Pi \\Phi \\hat \\theta_j,\\] where for \\(j\\ge 0\\), \\(\\hat\\theta_j\\) is the parameter vector obtained by running the least-squares policy evaluation algorithm based on G-optimal design (LSPE-G) to evaluate policy \\(\\pi_j\\) (see this lecture). In particular, recall that this algorithm rolls out policy \\(\\pi_j\\) from the points of a G-optimal design to produce \\(m\\) independent trajectories of length \\(H\\) each, calculates the average return for each of these design points and then solves the (weighted) least-squares regression problem where the features are used to regress on the obtained values. Above, \\(\\Pi : \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{S}}\\) truncates its argument to the \\([0,1/(1-\\gamma)]\\) interval: . \\[(\\Pi q)(s,a) = \\max(\\min( q(s,a), 1/(1-\\gamma)), 0), \\qquad (s,a) \\in \\mathcal{S}\\times \\mathcal{A}\\,.\\] Note that to calculate \\(\\pi_k(a\\vert s)\\), one does needs to calculate \\(E_k(s,a)=\\exp\\left( \\eta \\phi(s,a)^\\top \\bar \\theta_{k-1}\\right)\\) and then compute \\(\\pi_k(a\\vert s) = E_k(s,a)/\\sum_{a'} E_k(s,a')\\). Unlike in policy iteration, the policy returned by Politex after $k$ iterations is either the “mixture policy” . \\[\\bar \\pi_k = \\frac{1}{k} (\\pi_0+\\dots+\\pi_{k-1})\\,,\\] or the policy which gives the best value with respect to the start state, or start distribution. For simplicity, let us just consider the case when $\\bar \\pi_k$ is used as the output. The meaning of a mixture policy is simply that one of the $k$ policies is selected uniformly at random and then the selected policy is followed for the rest of time. Homework 3 gives precise definitions and asks you to prove that the value function of $\\bar \\pi_k$ is just the mean of the value functions of the constituent policies: . \\[\\begin{align} v^{\\bar \\pi_k} = \\frac1n \\left(v^{\\pi_0}+\\dots+v^{\\pi_{k-1}}\\right)\\,. \\label{eq:avgpol} \\end{align}\\] We now argue that the dependence on the approximation error of the suboptimality gap of $\\bar \\pi_k$ only scales with $1/(1-\\gamma)$, unlike the case of approximate policy iteration. For this, recall that by the value difference identity . \\[v^{\\pi^*} - v^{\\pi_j} = (I-\\gamma P_{\\pi^*})^{-1} \\left[T_{\\pi^*} v^{\\pi_j} - v^{\\pi_j} \\right]\\,.\\] Summing up, dividing by $k$, and using \\eqref{eq:avgpol} gives . \\[v^{\\pi^*} - v^{\\bar \\pi_k} = \\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\sum_{j=0}^{k-1} T_{\\pi^*} v^{\\pi_j} - v^{\\pi_j}\\,.\\] Now, \\(T_{\\pi^*} v^{\\pi_j} = M_{\\pi^*} (r+\\gamma P v^{\\pi_j}) = M_{\\pi^*} q^{\\pi_j}\\). Also, \\(v^{\\pi_j} = M_{\\pi_j} q^{\\pi_j}\\). Let \\(\\hat q_j = \\Pi \\Phi \\hat \\theta_j\\). Elementary algebra then gives . \\[\\begin{align*} v^{\\pi^*} - v^{\\bar \\pi_k} &amp; = \\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\sum_{j=0}^{k-1} M_{\\pi^*} q^{\\pi_j} - M_{\\pi_j} q^{\\pi_j}\\\\ &amp; = \\frac1k(I-\\gamma P_{\\pi^*})^{-1} \\underbrace{ \\sum_{j=0}^{k-1} M_{\\pi^*} \\hat q_j - M_{\\pi_j} \\hat q_j}_{T_1} + \\underbrace{\\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\sum_{j=0}^{k-1} ( M_{\\pi^*} - M_{\\pi_j} )( q^{\\pi_j}-\\hat q_j)}_{T_2} \\,. \\end{align*}\\] We see that the approximation errors $\\varepsilon_j = q^{\\pi_j}-\\hat q_j$ appear only in term $T_2$. In particular, taking pointwise absolute values, using the triangle inequality, we get that . \\[\\|T_2\\|_\\infty \\le \\frac{2}{1-\\gamma} \\max_{0\\le j \\le k-1}\\| \\varepsilon_j\\|_\\infty\\,,\\] which shows the promised dependence. It remains to show that \\(\\|T_1\\|_\\infty\\) above is also under control. However, this is left to the next lecture. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec13/#politex",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec13/#politex"
  },"333": {
    "doc": "13. From API to Politex",
    "title": "Notes",
    "content": "State aggregation and extrapolation friendliness . The $\\sqrt{d}$ in our results comes from controlling the extrapolation errors of linear prediction. In the case of state-aggregretion, however, this extra \\(\\sqrt{d}\\) error amplification is completely avoided: Clearly, if we measure a function with a precision \\(\\varepsilon\\) and there is at least one measurement per part, then by using the value measured at each part (at an arbitrary state there) over the whole part, the worst-case error is bounded by \\(\\varepsilon\\). Weighted least-squares in this context just takes the weighted average of the responses over each part and uses this as the prediction, so it also avoids amplifying approximation errors. In this case, our analysis of extrapolation errors is clearly conservative. The extrapolation error was controlled in two steps: In our first lemma, for \\(\\rho\\) weighted least-squares we reduced this problem to that of controlling \\(g(\\rho)=\\max_{z\\in \\mathcal{Z}} \\| \\phi(z) \\|_{G_{\\rho}^{-1}}\\) where \\(G_{\\rho}\\) is the moment matrix for \\(\\rho\\). In fact, the proof of this lemma is the culprit: By carefully inspecting the proof, we can see that the application of Jensen’s inequality introduces an unnecessary term: For the case of state aggregation (orthonormed feature matrix), . \\[\\sum_{z' \\in C} \\varrho(z') \\varepsilon(z') \\varphi(z') = 1\\,\\] as long as the design \\(\\rho\\) is such that it chooses any group exactly once. Thus, the case of state-aggregation shows that some feature-maps are more extrapolation friendly than others. Also, note that the Kiefer-Wolfowitz theorem, of course, still gives that \\(\\sqrt{d}\\) is the smallest value that we can get for \\(g\\) when optimizing for \\(\\rho\\). It is a fascinating question of how extrapolation errors behave for various feature-maps. Least-squares value iteration (LSVI) . In homework 2, Question 3 was concerned with least-squares value iteration. The algorithm concerned (call it LSVI-G) uses a random approximation of the Bellman operator, based on a G-optimal design (and action-value functions). The problem was to show a result similar to what holds for LSPI-G holds for LSVI-G, as well. That is, for any MDP feature-map pair $(M,\\phi)$ and any $\\varepsilon’&gt;0$ excess suboptimality target, with a total runtime of . \\[\\text{poly}\\left( d, \\frac{1}{1-\\gamma}, A, \\frac{1}{\\varepsilon'} \\right)\\,,\\] least-squares policy iteration with $G$-optimal design (LSPI-G) can produce a policy $\\pi$ such that the suboptimality gap $\\delta$ of $\\pi$ satisfies . \\[\\begin{align} \\delta \\le \\frac{4(1+\\sqrt{d})}{(1-\\gamma)^{\\color{red} 2}} \\varepsilon_\\text{BOO} + \\varepsilon'\\,. \\label{eq:lsviup} \\end{align}\\] Thus, the dependence on the horizon of the approximation error is similar to the one that was obtained for LSPI. Note that the definition of \\(\\varepsilon_\\text{BOO}\\) is different from what we have used in analyzing LSPI: . \\[\\varepsilon_{\\text{BOO}} := \\sup_{\\theta}\\inf_{\\theta'} \\| \\Phi \\theta' - T \\Pi \\Phi \\theta \\|_\\infty\\,.\\] Above, \\(T\\) is the Bellman optimality oerator for action-value functions and $\\Pi$ is defined so that for \\(f:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}\\), \\(\\Pi f\\) is also a $\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}$ function which is obtained from $f$ by truncating for each input $(s,a)$ the value $f(s,a)$ to $[0,1/(1-\\gamma)]$: $(\\Pi(f))(s,a) = \\max(\\min( f(s,a), 1/(1-\\gamma) ), 0)$. In $\\varepsilon_{\\text{BOO}}$, “BOO” stands for “Bellman-optimality operator” in reference to the appearance of $T$ in the definition. In general, the error measures \\(\\varepsilon\\) used in LSPI and \\(\\varepsilon_{\\text{BOO}}\\) are incomparable. The latter quantity measures a “one-step error”, while \\(\\varepsilon\\) is concerned with approximating functions defined over an infinite-horizon. Linear MDPs . Call an MDP linear if both the reward function and the next state distributions for each state lie in the span of the features: \\(r = \\Phi \\theta_r\\) with some $\\theta_r\\in \\mathbb{R}^d$ and $P$, as an $\\mathrm{S}\\mathrm{A}\\times \\mathrm{S}$ matrix takes the form \\(P = \\Phi W\\) with some \\(W\\in \\mathbb{R}^{d\\times \\mathrm{S}}\\). Clearly, this is a notion that captures how well the “dynamics” (including the reward) of the MDP can be “compressed”. When an MDP is linear, \\(\\varepsilon_{\\text{BOO}}=0\\). We also have in this case that $\\varepsilon=0$. More generally, defining \\(\\zeta_r = \\inf_{\\theta}\\| \\Phi \\theta_r - r \\|_\\infty\\) and \\(\\zeta_P=\\inf_W \\|\\Phi W - P \\|_\\infty\\), it is not hard to see that \\(\\varepsilon_{\\text{BOO}}\\le \\zeta_r + \\zeta_P/(1-\\gamma)\\) and \\(\\varepsilon\\le \\frac{1}{1-\\gamma} (\\zeta_r + \\zeta_P/(1-\\gamma))\\), which shows that both policy iteration (and its soft versions) and value iteration are “valid” approaches, though, by ignoring the fact that we are comparing upper bounds, this also shows that value iteration may have an edge over policy iteration when the MDP itself is compressible. This should not be too surprising given that value-iteration is “more direct” in aiming to calculate \\(q^*\\). Yet, they may exist cases when the action-value functions are compressible, while the dynamics is not. Stationary points of a policy search objective . Let \\(J(\\pi) = \\mu v^\\pi\\). A stationary point of \\(J\\) with respect to some set of memoryless policies \\(\\Pi\\) is any \\(\\pi\\in \\Pi\\) such that . \\[\\langle \\nabla J(\\pi), \\pi'- \\pi \\rangle \\le 0\\,.\\] It is known that if $\\phi$ are state-aggregation features then any stationary point \\(\\pi\\) of \\(J\\) satisfies . \\[\\nu v^\\pi \\ge \\nu v^* - \\frac{4\\varepsilon_{\\text{apx}}}{1-\\gamma}\\,,\\] where $\\varepsilon_{\\text{apx}}$ is defines as the worst-case error of approximation action-value functions of $\\phi$-measurable policies with the features (the same constant as used in the analysis of approximate policy iteration). Soft-policy iteration . Politex can be seen as a “soft” version of policy iteration. The softness is controlled by $\\eta$: When $\\eta\\to \\infty$, Politex reduces to LSPI-G. As we have seen, in this case the approximation error can get quadratically amplified with the horizon $1/(1-\\gamma)$. Thus, it is important to stay soft. As we shall see in the next lecture, the price of this a relatively slower convergence to a target suboptimality excess value. Nevertheless, the promise is that the algorithm will still stay polynomial in all the relevant quantities. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec13/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec13/#notes"
  },"334": {
    "doc": "13. From API to Politex",
    "title": "References",
    "content": "Politex was introduced in the paper . | POLITEX: Regret Bounds for Policy Iteration using Expert Prediction. Abbasi-Yadkori, Y.; Bartlett, P.; Bhatia, K.; Lazic, N.; Szepesvári, C.; and Weisz, G. In ICML, pages 3692–3702, May 2019. pdf | . However, as this paper also notes, the basic idea goes back to the MDP-E algorithm by Even-Dar et al: . | Even-Dar, E., Kakade, S. M., and Mansour, Y. Online Markov decision processes. Mathematics of Operations Research, 34(3):726–736, 2009. | . This algorithm considered a tabular MDP with nonstationary rewards – a completely different setting. Nevertheless, this paper introduces the basic argument presented above. The Politex paper notices that the argument can be extended to the case of function approximation. In particular, it also notes the nature of the function approximator is irrelevant as long as the approximation and estimation errors can be tightly controlled. The Politex paper presented an analysis for online RL and average reward MDPs. Both add significant complications. The argument shown here is therefore a simpler version. Connecting Politex to LSPE-G in the discounted setting is trivial, but has not been presented before in the literature. The first paper to use the error decomposition shown here together with function approximation is . | Abbasi-Yadkori, Y., Lazic, N., and Szepesvári, C. Modelfree linear quadratic control via reduction to expert prediction. In AISTATS, 2019. | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec13/#references",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec13/#references"
  },"335": {
    "doc": "13. From API to Politex",
    "title": "13. From API to Politex",
    "content": "In the lecture on approximate policy iteration, we proved that for any MDP feature-map pair $(M,\\phi)$ and any $\\varepsilon’&gt;0$ excess suboptimality target, with a total runtime of . \\[\\text{poly}\\left( d, \\frac{1}{1-\\gamma}, A, \\frac{1}{\\varepsilon'} \\right)\\,,\\] least-squares policy iteration with $G$-optimal design (LSPI-G) can produce a policy $\\pi$ such that the suboptimality gap $\\delta$ of $\\pi$ satisfies . \\[\\begin{align} \\delta \\le \\frac{2(1+\\sqrt{d})}{(1-\\gamma)^{\\color{red} 2}} \\varepsilon + \\varepsilon'\\,, \\label{eq:lspiup} \\end{align}\\] where \\(\\varepsilon\\) is the worst-case error with which the $d$-dimensional features can approximate the action-value functions of memoryless policies of the MDP $M$. In fact, the result continues to hold if we restrict the memoryless policies to those that are $\\phi$-measurable in the sense that the probability assigned by such a policy to taking some action $a$ in some state $s$ depends only on \\(\\phi(s,\\cdot)\\). Denote the set of such policies by \\(\\Pi_\\phi\\). Then, for an MDP $M$ and associated feature-map $\\phi$, let . \\[\\tilde\\varepsilon(M,\\phi) = \\sup_{\\pi \\in \\Pi_\\phi}\\inf_{\\theta} \\|\\Phi \\theta - q^\\pi\\|_\\infty\\,.\\] Checking the proof, noticing that LSPI produces \\(\\phi\\)-measurable policies only, it follows that provided the first policy it uses is also \\(\\phi\\)-measurable, $\\varepsilon$ in \\eqref{eq:lspiup} can be replaced by \\(\\tilde \\varepsilon(M,\\phi)\\). Earlier, we also proved that the amplification of $\\varepsilon$ by the $\\sqrt{d}$-factor is unavoidable by any efficient planner. However, this leaves open the question of whether the amplification by a polynomial power of $1/(1-\\gamma)$ is necessary, and whether in particular, the quadratic dependence is necessary? Our first result, which is given without proof, shows that in the case of LSPI this amplification is real and the quadratic dependence cannot be improved. Theorem (LSPI error amplification lower bound): The quadratic dependence in \\eqref{eq:lspiup} is tight: There exists a constant $c&gt;0$ such that for every $0\\le \\gamma&lt;1$ and every \\(\\varepsilon&gt;0\\) there exists a featurized MDP \\((M,\\phi)\\), a policy \\(\\pi\\) of the MDP, a distribution \\(\\mu\\) over the states such that LSPI when it is allowed infinitely many rollouts of infinite length produces a sequence of policies \\(\\pi_0=\\pi,\\pi_1,\\dots\\) such that . \\[\\inf_{k\\ge 1} \\mu (v^*-v^{\\pi_k}) \\ge \\frac{c\\tilde\\varepsilon(M,\\phi)}{(1-\\gamma)^2}\\,.\\] . The result of the theorem holds even when LSPI is used with state-aggregation. Intuitively, state-aggregation means that states are groups into a number of groups and states belonging to the same group are treated identically when it comes to representing value functions. This, value-functions based on state-aggregation are constant over any group. When we are concerned with state-value functions, aggregating the states based on a partitioning of the states \\(\\mathcal{S}\\) into the groups \\(\\{\\mathcal{S}_i\\}_{1\\le i \\le d}\\) (i.e., \\(\\mathcal{S}_i\\subset \\mathcal{S}\\) and all the subsets are disjoint from each other), a feature-map that allows to represent these piecewise constant functions is . \\[\\phi_i(s) = \\mathbb{I}(s\\in \\mathcal{S}_i)\\,, \\qquad i\\in [d]\\,,\\] where $\\mathbb{I}$ is the indicator function that takes the value of one when its argument (a logical expression) is true, and is zero otherwise. In other words, \\(\\phi: \\mathcal{S} \\to \\{ e_1,\\dots,e_d\\}\\). Any feature map of this form defines a partitioning of the state-space and thus corresponds to the state-aggregation. Note that the piecewise constant functions can also be represented if we rotate all the features by the same rotation. The only important aspect here is that the features of different states are either identical, or orthogonal to each other, making the rows of the feature matrix an orthonormal system. For approximating action-value functions, state-aggregation uses the same partitioning of states regardless of the identity of the actions: In effect, for each action, one uses the feature map from above, but with a private parameter vector. This effectively amounts to stacking $\\phi(s)$ \\(\\mathrm{A}\\)-times, to get one copy of it for each action $a\\in \\mathcal{A}$. Note that for state-aggregation, there is no $\\sqrt{d}$ amplification of the approximation errors: State-aggregation is extrapolation friendly, as will be explained at the end of the lecture. Returning to the result, an inspection of the actual proof reveals that in this case LSPI leads to a sequence of policies that alternate between the initial policy and $\\pi_1$. “Convergence” is fast, yet, the guarantee is far from satisfactory. In particular, in the same example, an alternate algorithm, which we will cover next can reduce the quadratic dependence on the horizon to a linear dependence. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec13/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec13/"
  },"336": {
    "doc": "14. Politex",
    "title": "Online linear optimization",
    "content": "As it happens, the problem of controlling terms of this type is the central problem studied in a subfield of learning theory, online learning. In particular, in online linear optimization, the following problem is studied: . An adversary and a learner are playing a zero-sum minimax game in $k$ discrete rounds, taking actions in an alternating manner. In round $j$ ($0\\le j \\le k-1$), first, the learner needs to choose a vector \\(x_j\\in \\mathcal{X}\\subset \\mathbb{R}^d\\). Then, the adversary chooses a vector, \\(y_j \\in \\mathcal{Y}\\subset \\mathbb{R}^d\\). Before its choice, the adversary learns about all previous choices of the learner, and the learner also learns about all previous choices of the adversary. They also remember their own choices. For simplicity, let us constraint the adversary and the learner to be deterministic. The payoff to the adversary at the end of the $k$ rounds is . \\[\\begin{align} R_k = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} \\langle x, y_j \\rangle - \\langle x_j,y_j \\rangle\\,. \\label{eq:regretdefolo} \\end{align}\\] In particular, the adversary’s goal is maximize this, while the learner’s goal is to minimize this (the game is zero-sum). Both the adversary and the learner are given $k$ and the sets $\\mathcal{X},\\mathcal{Y}$. Letting $L$ to denote the learner’s strategy (a sequence of maps of histories to $\\mathcal{X}$) and $A$ to denote the adversary’s strategy (a sequence of maps of histories to $\\mathcal{Y}$), the above quantity depends on $L$ and $A$: $R_k = R_K(A,L)$. Taking the perspective of the learner, the quantity defined in \\eqref{eq:regretdefolo} is called the learner’s regret. Denote the minimax value of the game by \\(R_k^*\\): \\(R_k^* = \\inf_L \\sup_A R_k(A,L)\\). Thus, this only depends on \\(k\\), \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). The dependence is suppressed when it is clear from the context. The central question then is how $R_k^*$ depends on $k$ and also on $\\mathcal{X}$ and $\\mathcal{Y}$. In online linear optimization both sets $\\mathcal{X}$ and $\\mathcal{Y}$ are convex. Connecting these games to our problem, we can see that \\(T_1(s)\\) in \\eqref{eq:t1s} matches the regret definition in \\eqref{eq:regretdefolo} if we let \\(d=\\mathrm{A}\\), \\(\\mathcal{X} = \\mathcal{M}_1(\\mathrm{A}) = \\{ p\\in [0,1]^{\\mathrm{A}} \\,:\\, \\sum_a p_a = 1 \\}\\) be the \\(\\mathrm{A}-1\\) simplex of \\(\\mathbb{R}^{\\mathrm{A}}\\) and \\(\\mathcal{Y} = [0,1/(1-\\gamma)]^{\\mathrm{A}}\\). Furthermore, \\(\\pi_j(s,\\cdot)\\) needs to be chosen first, which is followed by the choice of \\(\\hat q_j(s,\\cdot)\\). While \\(\\hat q_j(s,\\cdot)\\) will not be chosen in an adversarial fashion, a bound $B$ on the regret against arbitrary choices will also serve as a bound for the specific choice we will need to make for \\(\\hat q_j(s,\\cdot)\\). ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec14/#online-linear-optimization",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec14/#online-linear-optimization"
  },"337": {
    "doc": "14. Politex",
    "title": "Mirror descent",
    "content": "Mirror descent (MD) is an algorithm that originates in optimization theory. In the context of online linear optimization, MD is a strategy for the learner which is known to guarantee near minimax regret for the learner under a wide range of circumstances. To align with the large body of literature on online linear optimization, it will be beneficial to switch signs. Thus, in what follows we assume that the learner will aim at minimizing \\(\\langle x,y \\rangle\\) by its choice \\(x\\in \\mathcal{X}\\) and the adversary will aim at maximizing the same expression over its choice \\(y\\in \\mathcal{Y}\\). This means that we also redefine the regret to . \\[\\begin{align} R_k &amp; = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} \\langle x_j, y_j \\rangle - \\langle x,y_j \\rangle \\nonumber \\\\ &amp; = \\sum_{j=0}^{k-1} \\langle x_j, y_j \\rangle - \\min_{x\\in \\mathcal{X}} \\sum_{j=0}^{k-1}\\langle x,y_j \\rangle\\,. \\label{eq:regretdefololosses} \\end{align}\\] Everything else remains the same: The game is zero-sum, minimax, the regret is the payoff for the adversary and the negative regret is the payoff of the learner. This version is called a loss-game. The reason to prefer the loss game is because most of optimization theory is written for minimizing convex functions rather than for maximizing concave functions. However, clearly, this is an arbitrary choice. The second form of the regret shows that the player’s goal is to compete with the best single decision from \\(\\mathcal{X}\\) but chosen given the hindsight of knowing all the choices of the adversary. That is, the learner’s goal is to keep its cumulative loss \\(\\sum_{j=0}^{k-1} \\langle x_j, y_j \\rangle\\) close to, or even below the best cumulative loss in hindsight, \\(\\min_{x\\in \\mathcal{X}} \\sum_{j=0}^{k-1}\\langle x,y_j \\rangle\\). (With this, $T_1(s)$ matches \\(R_k\\) when we change \\(\\mathcal{Y} = [-1/(1-\\gamma),0]^{\\mathrm{A}}\\).) . MD is recursively defined and in its simplest form it has two design parameters. The first is an extended real-valued convex function \\(F: \\mathbb{R}^d \\to \\bar {\\mathbb{R}}\\), called the “regularizer”, while the second is a stepsize, or learning rate parameter $\\eta&gt;0$. (The extended reals is just \\(\\mathbb{R}\\) together with \\(+\\infty,-\\infty\\) and an appropriate extension of basic arithmetic. By allowing convex functions to take the value \\(+\\infty\\) allows to merge “constraints” with objectives in a seamless fashion. The value $-\\infty$ is added because sometimes we have to work with negated extended real-valued convex functions.) . The specification of MD is as follows: In round $0$, $x_0\\in \\mathcal{X}$ is picked to minimize \\(F\\): . \\[x_0 = \\arg\\min_{x\\in \\mathcal{X}} F(x)\\,.\\] In what follows, we assume that all the minimizers that we need in the definition of MD do exist. In the specific case that we need, \\(\\mathcal{X}\\) is the \\(d-1\\) simplex, which is a closed convex set, and since convex functions are also continuous, the minimizers that we will need are guaranteed to exist. Then, in round \\(j&gt;0\\), MD chooses \\(x_j\\) as follows: . \\[\\begin{equation} \\begin{split} x_j &amp; = \\arg\\min_{x\\in \\mathcal{X}}\\,\\,\\eta \\langle x, y_{j-1} \\rangle + D_F(x,x_{j-1}) \\\\ \\end{split} \\label{eq:mddef} \\end{equation}\\] Here, . \\[D_F(x,x') = F(x)-(F(x')+\\langle \\nabla F(x'), x-x'\\rangle)\\] is the remainder term in the first-order Taylor-series expansion of the value of $F$ at $x$ when the expansion is carried out at \\(x'\\) and, for simplicity, we assume that \\(F\\) is differentiable on the interior of its domain \\(\\text{dom}(F) = \\{ x\\in \\mathbb{R}\\,:\\, F(x)&lt;+\\infty \\}\\). Since for any convex function and any linear approximation of it stays below the graph of the convex function, we immediately get that \\(D_F\\) is nonnegative valued. For an illustration see the figure on the right, which shows a convex function, the first-order Taylor approximation of the function at some point. One should think of \\(F\\) is a “nonlinear distance inducing function”; above \\(D_F(x,x')\\) can be thought of penalty imposed on deviating from \\(x'\\). However, \\(D_F\\) is more often than not is not a distance, i.e., often it is not even symmetric. Because of this, we can’t really call $D_F$ a distance. Hence, it is called a divergence. In particular, \\(D_F(x,x')\\) is called the Bregman divergence of \\(x\\) from \\(x'\\). In the definition of the MD update rule, we tacitly assumed that \\(D_F(x,x_{j-1})\\) is well-defined. This requires that \\(F\\) should be differentiable at \\(x_{j-1}\\), which one needs to check when applying MD. In our specific case, this will hold, again. The idea of the MD update rule is to (1) allow the learner to react to the last loss \\(y_{j-1}\\) vector chosen by the adversary, while also (2) limiting how much \\(x_j\\) can depart from \\(x_{j-1}\\), thus, effectively stabilizing the algorithm, the tradeoff governed by the choice of $\\eta&gt;0$. (Separating \\(\\eta\\) from \\(F\\) only makes sense because there are some standard choices for \\(F\\), but \\(\\eta\\) is really just a scale parameter for \\(F\\)). In particular, the larger the value of \\(\\eta\\) is, the less “data-sensitive” MD will be (here, \\(y_0,\\dots,y_{k-1}\\) constitute the data), and vice versa, the smaller \\(\\eta\\) is, the more data-sensitive MD will be. Where is the mirror? . Under some technical conditions on \\(F\\), the update rule \\eqref{eq:mddef} has a two step-implementation: . \\[\\begin{align} \\tilde x_j &amp; = (\\nabla F)^{-1} ( \\nabla F (x_{j-1}) - \\eta y_{j-1} )\\,,\\label{eq:mds1}\\\\ x_j &amp;= \\arg\\min_{x\\in \\mathcal{X}} D_F(x,\\tilde x_j)\\,.\\label{eq:mds2} \\end{align}\\] The first equation above explains the name: To obtain \\(\\tilde x_j\\), one first transforms \\(x_{j-1}\\) using \\(\\nabla F: \\text{dom}(\\nabla F ) \\to \\mathbb{R}^d\\) to the “mirror” (dual) space where “gradients”/”slopes live”, where one then adds to the result \\(-\\eta y_{j-1}\\), which can be seen as a “gradient step” (interpreting \\(y_{j-1}\\) as the gradient of some loss). Finally, the result is then mapped back to the original (primal) space using the inverse of \\(\\nabla F\\). The second step of the update takes the resulting point \\(\\tilde x_j\\) and “projects” it to \\(\\mathcal{X}\\) in a way that respects the “geometry induced by \\(F\\)” on the space \\(\\mathbb{R}^d\\). The use of complex terminology, like “primal” and “dual” spaces, which happen to be the same old Euclidean space, \\(\\mathbb{R}^d\\), probably sounds like an overkill. Indeed, in the simple case we consider when these spaces are identical it is. The distinction would become important when working with infinite dimensional spaces, which we leave to others for now. Besides helping with understanding the terminology, the two-step update shown can also be useful for computation. In fact, this will be the case in the special case that we need. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec14/#mirror-descent",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec14/#mirror-descent"
  },"338": {
    "doc": "14. Politex",
    "title": "Mirror descent on the simplex",
    "content": "We have seen that in the special case we need, . \\[\\begin{align*} \\mathcal{X} &amp;= \\mathcal{P}_{d-1}:=\\{ p\\in [0,1]^{d}\\,:\\, \\sum_a p_a = 1 \\}\\,, \\\\ \\mathcal{Y} &amp;= [-1/(1-\\gamma),0]^d\\,, \\text{and} \\\\ d &amp;= \\mathrm{A}\\,. \\end{align*}\\] To use MD we need to specify the regularizer \\(F\\) and the learning rate. For the former, we choose . \\[F(x) = \\sum_i x_i \\log(x_i) - x_i\\,,\\] which is known as the unnormalized negentropy function. Note that \\(F\\) takes on finite values when \\(x\\in [0,\\infty]^d\\) (since \\(\\lim_{x\\to 0+} x \\log(x)=0\\), we set \\(x_i \\log(x_i)=0\\) whenever $x_i=0$). Outside of this quadrant, we define the value of \\(F\\) to be \\(+\\infty\\). The plot of \\(x\\log(x)-x\\) for \\(x\\ge 0\\) is shown on the right. It is not hard to verify that \\(F\\) is convex: First, \\(\\text{dom}(F) = [0,\\infty]^d\\) is convex. Taking the first derivative, we find that for any \\(x\\in (0,\\infty)^d\\), . \\[\\nabla F(x) = \\log(x)\\,,\\] where \\(\\log\\) is applied componentwise. Taking the derivative again, we find that for \\(x\\in (0,\\infty)^d\\), . \\[\\nabla^2 F(x) = \\text{diag}(1/x)\\,,\\] i.e., the matrix whose $(i,i)$th diagonal entry is \\(1/x_i\\). Clearly, this is a positive definite matrix, which suffices to verify that \\(F\\) is a convex function. The Bregman divergence induced by \\(F\\) is . \\[\\begin{align*} D_F(x,x') &amp; = \\langle \\boldsymbol{1}, x \\log(x) - x - x' \\log(x')+x'\\rangle - \\langle \\log(x'), x-x'\\rangle \\\\ &amp; = \\langle \\boldsymbol{1}, x \\log(x/x') - x +x'\\rangle \\,, \\end{align*}\\] where again we use an “intuitive” notation when operations are first applied componentwise (i.e., $x \\log(x)$ denotes a vector whose $i$th component is $x_i \\log(x_i)$). Note that the domain of $D_F$ is \\([0,\\infty)^d \\times (0,\\infty)^d\\). If both $x$ and $x’$ lie in the $d-1$-simplex, $D_F$ becomes the well-known relative entropy, or Kullback-Leibler (KL) divergence. It is not hard to verify that $x_j$ can be obtained as shown in \\eqref{eq:mds1}-\\eqref{eq:mds2} and in particular this two-step update takes the form . \\[\\begin{align*} \\tilde x_{j,i} &amp;= x_{j-1,i} \\exp(-\\eta y_{j-1,i})\\,, \\qquad x_{j,i} = \\frac{\\tilde x_{j,i}}{ \\sum_{i'} \\tilde x_{j,i'}}\\,, \\quad i\\in [d]\\,. \\end{align*}\\] Unrolling the recursion, we can also that this is the same as . \\[\\begin{equation} \\tilde x_{j,i} = \\exp(-\\eta (y_{0,i}+\\dots + y_{j-1,i}))\\,, \\qquad x_{j,i} = \\frac{\\tilde x_{j,i}}{ \\sum_{i'} \\tilde x_{j,i'}}\\,, \\quad i\\in [d]\\,. \\label{eq:mdunrolled} \\end{equation}\\] Based on this, it is obvious that MD can be efficiently implemented with this choice of $F$. As far as the regret is concerned, the following theorem holds: . Theorem (MD with negentropy on the simplex): Let \\(\\mathcal{X}= \\mathcal{P}_{d-1}\\) amd \\(\\mathcal{Y} = [0,1]^d\\). Then, no matter the adversary, a learner using MD with . \\[\\eta = \\sqrt{ \\frac{2\\log(d)}{k}}\\] is guaranteed that its regret \\(R_k\\) in \\(k\\) rounds is at most . \\[R_k \\le \\sqrt{2k \\log(d)}\\,.\\] . When the adversary plays in \\(\\mathcal{Y} = [a,b]^d\\) with \\(a&lt;b\\), we can use MD on the transformed sequence \\(\\tilde y_j = (y_j-b \\boldsymbol{1})/(b-a) \\in [0,1]^d\\). Then, for any $x\\in \\mathcal{X}$, . \\[\\begin{align*} R_k(x) &amp; := \\sum_{j=0}^{k-1} \\langle x_j-x, y_j \\rangle \\\\ &amp; = \\sum_{j=0}^{k-1} \\langle x_j-x, (b-a)\\tilde y_j+b \\boldsymbol{1} \\rangle \\\\ &amp; = (b-a)\\sum_{j=0}^{k-1} \\langle x_j-x, \\tilde y_j \\rangle \\\\ &amp; \\le (b-a) \\sqrt{2k \\log(d)}\\,, \\end{align*}\\] where the third equality used that $\\langle x_j,\\boldsymbol{1}\\rangle = \\langle x, \\boldsymbol{1} \\rangle = 1$. Taking the maximum over $x\\in \\mathcal{X}$ gives that . \\[\\begin{align} R_k \\le (b-a) \\sqrt{2k \\log(d)}\\,. \\label{eq:mdrbscaled} \\end{align}\\] By the update rule in \\eqref{eq:mdunrolled}, . \\[\\begin{align*} \\tilde x_{j,i} = \\exp(-\\eta (\\tilde y_{0,i}+\\dots + \\tilde y_{j-1,i})) = \\exp(-\\eta/(b-a) (y_{0,i}+\\dots + y_{j-1,i}-j b) )\\,, \\qquad i\\in [d]\\,. \\end{align*}\\] Note that the “shift” by $-jb$ cancels out in the normalization step. Hence, MD in this case takes the form . \\[\\begin{equation} \\begin{split} \\tilde x_{j,i} &amp;= \\exp(-\\eta/(b-a) (y_{0,i}+\\dots + y_{j-1,i}))\\,, \\qquad x_{j,i} = \\frac{\\tilde x_{j,i}}{ \\sum_{i'} \\tilde x_{j,i'}}\\,, \\quad i\\in [d]\\,, \\label{eq:mdunrolledscaled} \\end{split} \\end{equation}\\] which is the same as before, except that the learning rate is scaled by $1/(b-a)$. In particular, in this case one can set . \\[\\begin{align} \\eta = \\frac{1}{b-a} \\sqrt{\\frac{2\\log(d)}{k}}\\,. \\label{eq:etascaled} \\end{align}\\] and use update rule \\eqref{eq:mdunrolled}. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec14/#mirror-descent-on-the-simplex",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec14/#mirror-descent-on-the-simplex"
  },"339": {
    "doc": "14. Politex",
    "title": "MD applied to MDP planning",
    "content": "As agreed, \\(T_1(s)\\) from \\eqref{eq:t1s} takes the form of a $k$-round regret against \\(\\pi^*(s,\\cdot)\\) in online linear optimization on the simplex with losses in \\([-1/(1-\\gamma),0]^{\\mathrm{A}}\\). This suggest to use MD in a state-by-state manner to control \\(T_1(s)\\). Using \\eqref{eq:mdunrolled} and \\eqref{eq:etascaled} gives . \\[E_j(s,a) = \\exp(\\eta (\\hat q_0(s,a) +\\dots + \\hat q_{j-1}(s,a)))\\,, \\qquad \\pi_j(a|s) = \\frac{E_j(s,a)}{ \\sum_{a'} E_j(s,a')}\\,, \\quad a\\in \\mathcal{A}\\] to be used with . \\[\\eta = (1-\\gamma) \\sqrt{\\frac{2\\log(\\mathrm{A})}{k}}\\,.\\] Note that this is the update used by Politex. Then, \\eqref{eq:mdrbscaled} gives that simultaneously for all $s\\in \\mathcal{S}$, . \\[\\begin{align} |T_1(s)| \\le \\frac{1}{1-\\gamma} \\sqrt{2k \\log(\\mathrm{A})}\\,. \\label{eq:t1sbound} \\end{align}\\] Putting things together, we get the following result: . Theorem (Politex suboptimality gap bound): Pick a featurized MDP $(M,\\phi)$ with a full rank feature-map $\\varphi: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}^d$ and let $K,m,H\\ge 1$. Assume that B2\\(_{\\varepsilon}\\) holds for $(M,\\phi)$ and the rewards in $M$ are in the $[0,1]$ interval. For $0\\le \\zeta&lt;1$, define . \\[\\kappa(\\zeta) = \\varepsilon (1 + \\sqrt{d}) + \\sqrt{d} \\left(\\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log( d(d+1)K / \\zeta)}{2m}}\\right)\\,,\\] Then, in $K$ iterations, Politex produces a mixed policy $\\bar \\pi_K$ such that with probability $1-\\zeta$, the suboptimality gap $\\delta$ of $\\bar \\pi_K$ satisfies . \\[\\begin{align*} \\delta \\le \\frac{1}{(1-\\gamma)^2}\\sqrt{\\frac{2\\log(\\mathrm{A})}{K}}+\\frac{2 \\kappa(\\zeta) }{1-\\gamma} \\,. \\end{align*}\\] In particular, for any $\\varepsilon’&gt;0$, choosing $K,H,m$ so that . \\[\\begin{align*} K &amp; \\ge \\frac{32 \\log(A)}{ (1-\\gamma)^4 (\\varepsilon')^2}\\,, \\\\ H &amp; \\ge H_{\\gamma,(1-\\gamma)\\varepsilon'/(8\\sqrt{d})} \\qquad \\text{and} \\\\ m &amp; \\ge \\frac{32 d}{(1-\\gamma)^4 (\\varepsilon')^2} \\log( (d+1)^2 K /\\zeta )\\,, \\end{align*}\\] policy $\\pi_K$ is $\\delta$-optimal with . \\[\\begin{align*} \\delta \\le \\frac{2(1 + \\sqrt{d})}{1-\\gamma}\\, \\varepsilon + \\varepsilon'\\,, \\end{align*}\\] while the total computation cost is $\\text{poly}(\\frac{1}{1-\\gamma},d,\\mathrm{A},\\frac{1}{(\\varepsilon’)^2},\\log(1/\\zeta))$. Note that as compared to the result of LSPI with G-optimal design, the amplification of the approximation error $\\varepsilon$ is reduced by a factor of $1/(1-\\gamma)$, as it was promised. The price is that now the number of iterations $K$, is a polynomial of $\\frac{1}{(1-\\gamma)\\varepsilon’}$, whereas before it was logarithmic. This suggest that perhaps a higher learning rate can help initially to speed up convergence to get the best of both words. Proof: As in the proof of the suboptimality gap for LSPI, we get that for any $0\\le \\zeta \\le 1$, with probability at least $1-\\zeta$, for any $0 \\le k \\le K-1$, . \\[\\begin{align*} \\| q^{\\pi_k} - \\hat q_k \\|_\\infty =\\| q^{\\pi_k} - \\Pi \\Phi \\hat \\theta_k \\|_\\infty \\le \\| q^{\\pi_k} - \\Phi \\hat \\theta_k \\|_\\infty &amp;\\leq \\kappa(\\zeta)\\,, \\end{align*}\\] where the first inequality uses that \\(q_{\\pi_k}\\) takes values in $[0,1]$. On the event when the above inequalities hold, by \\eqref{eq:polsubgapgen} and \\eqref{eq:t1sbound}, . \\[\\begin{align*} \\delta \\le \\frac{1}{(1-\\gamma)^2}\\sqrt{\\frac{2\\log(\\mathrm{A})}{K}}+\\frac{2 \\kappa(\\zeta) }{1-\\gamma} \\,. \\end{align*}\\] The details of this calculation are left to the reader. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec14/#md-applied-to-mdp-planning",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec14/#md-applied-to-mdp-planning"
  },"340": {
    "doc": "14. Politex",
    "title": "Notes",
    "content": "Online convex optimization, online learning . Online linear optimization is a special case of online convex/concave optimization, where the learner chooses elements of some nonempty convex set \\(\\mathcal{X}\\subset \\mathbb{R}^d\\) and the adversary needs to choose an element of a nonempty set \\(\\mathcal{Y}\\) of concave functions over \\(\\mathcal{X}\\): \\(\\mathcal{Y} \\subset \\{ f: \\mathcal{X} \\to \\mathbb{R}\\,:\\, f \\text{ is concave} \\}\\). Then, the definition of regret is changed to . \\[\\begin{align} R_k = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} y_j(x) - y_j(x_j) \\,, \\label{eq:regretdefoco1} \\end{align}\\] where as before \\(x_j\\in \\mathcal{X}\\) is the choice of the learner for round \\(j\\) and \\(y_j\\in \\mathcal{Y}\\) is the choice of the adversary for the same round. Identifying any vector \\(u\\) of \\(\\mathbb{R}^d\\) with the linear map \\(x \\mapsto \\langle x, u \\rangle\\), we see that online linear optimization is a special case of this problem. Of course, by negating all functions in \\(\\mathcal{Y}\\) (i.e., letting \\(\\tilde {\\mathcal{Y}} = \\{ - y \\,:\\, y\\in \\mathcal{Y} \\}\\)) and redefining the regret to . \\[\\begin{align} R_k = \\max_{x\\in \\mathcal{X}}\\sum_{j=0}^{k-1} \\tilde y_j(x_j)- \\tilde y_j(x) \\, \\label{eq:regretdefoco} \\end{align}\\] we get a definition that is used in the literature, which prefers the convex case to the concave. Here, the interpretation is that \\(\\tilde y_j\\in \\tilde {\\mathcal{Y}}\\) is a “loss function” chosen by the adversary in round \\(j\\). The standard function notation (\\(y_j\\) is applied to \\(x\\)) injects unwarranted asymmetry in the notation. After all, from the perspective of the learner, they need to choose a value in \\(\\mathcal{X}\\) that works for the various functions in \\(\\mathcal{Y}\\). Thus, we can consider any element of \\(\\mathcal{X}\\) as a function that maps elements of \\(\\mathcal{Y}\\) to reals through \\(y \\mapsto y(x)\\). Whether \\(\\mathcal{Y}\\) has functions in them or \\(\\mathcal{X}\\) has functions in them does not matter that much; it is the interconnection between \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\) that matters more. For this reason, one can study online learning when \\(y(x)\\) above is replaced by \\(b(x,y)\\), where \\(b: \\mathcal{X}\\times \\mathcal{Y} \\to \\mathbb{R}\\) is a specific map that assigns payoffs to every pair of points in \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). When the map is fixed, one can spare an extra symbol by just using \\([x,y]\\) in place of \\(b(x,y)\\), which makes things almost a full circle given that we started with the linear case when \\([x,y] = \\langle x,y \\rangle\\). Truncation or no truncation? . We introduced truncation to simplify the analysis. The proof can be made to go through even without it, with a mild increase of the suboptimality gap (or runtime). The advantage of removing the projection is that without projection, \\(\\hat q_0 + \\dots + \\hat q_{j-1} = \\Phi (\\hat \\theta_0 + \\dots + \\hat \\theta_{j-1})\\), which leads to a practically significant reduction of the runtime. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec14/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec14/#notes"
  },"341": {
    "doc": "14. Politex",
    "title": "14. Politex",
    "content": "The following lemma can be extracted from the calculations found at the end of the last lecture: . Lemma (Mixture policy suboptimality): Fix an MDP $M$. For any sequence \\(\\pi_0,\\dots,\\pi_{k-1}\\) of policies, any sequence $\\hat q_0,\\dots,\\hat q_{k-1}: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}$ of functions, and any policy \\(\\pi^*\\), the mixture policy $\\bar \\pi_k = 1/k(\\pi_0+\\dots+\\pi_{k-1})$ satisfies . \\[\\begin{align} v^{\\pi^*} - v^{\\bar \\pi_k} &amp; \\le \\frac1k (I-\\gamma P_{\\pi^*})^{-1} \\underbrace{ \\sum_{j=0}^{k-1} M_{\\pi^*} \\hat q_j - M_{\\pi_j} \\hat q_j}_{T_1} + \\frac{2 \\max_{0\\le j \\le k-1}\\| q^{\\pi_j}-\\hat q_j\\|_\\infty }{1-\\gamma} \\,. \\label{eq:polsubgapgen} \\end{align}\\] . In particular, the only restriction is on policy $\\pi^*$ so far and that is that it has to be a memoryless policy. To control the suboptimality of the mixture policy, one just needs to control the action-value approximation errors \\(\\| q^{\\pi_j}-\\hat q_j\\|_\\infty\\) and the term $T_1$ and for this we are free to choose the policies \\(\\pi_0,\\dots,\\pi_{k-1}\\) in any way we want them to be chosen. To help with this choice, let us now inspect \\(T_1(s)\\) for a fixed state $s$: . \\[\\begin{align} T_1(s) = \\sum_{j=0}^{k-1} \\langle \\pi^*(s,\\cdot),\\hat q_j(s,\\cdot)\\rangle - \\langle \\pi_j(s,\\cdot),\\hat q_j(s,\\cdot)\\rangle \\,, \\label{eq:t1s} \\end{align}\\] where, abusing notation, we use \\(\\pi(s,a)\\) for \\(\\pi(a|s)\\). Now, recall that \\(\\hat q_j\\) will be computed based on \\(\\pi_j\\) while \\(\\pi^*\\) is unknown. One must thus wonder whether it is possible to control this term? . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec14/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec14/"
  },"342": {
    "doc": "15. From policy search to policy gradients",
    "title": "Policy search",
    "content": "A reasonable goal then is to ask for a planner that competes with the best policy within the parameterized family, or the $\\varepsilon$-best policy policy for some positive $\\varepsilon$. Since there may not be a parameter $\\theta$ such that $v^{\\pi_\\theta}\\ge v^{\\pi_{\\theta’}}-\\varepsilon\\boldsymbol{1}$ for any $\\theta’\\in \\mathbb{R}^d$, we simplify the problem by requiring that the policy computed is nearly best when started from some initial distribution $\\mu \\in \\mathcal{M}_1(\\mathcal{S})$. Defining $J: \\text{ML} \\to \\mathbb{R}$ as . \\[J(\\pi) = \\mu v^{\\pi} (=\\sum_{s\\in \\mathcal{S}}\\mu(s)v^{\\pi}(s)),\\] the policy search problem is to find a parameter $\\theta\\in \\mathbb{R}^d$ such that . \\[\\begin{align*} J(\\pi_{\\theta}) = \\max_{\\theta'} J(\\pi_{\\theta'})\\,. \\end{align*}\\] The approximation version of the problem asks for finding $\\theta’\\in \\mathbb{R}^d$ such that . \\[\\begin{align*} J(\\pi_{\\theta}) = \\max_{\\theta'} J(\\pi_{\\theta'}) - \\varepsilon\\,. \\end{align*}\\] The formal problem definition then is as follows: a planning algorithm is given the MDP $M$ and a policy parameterization $(\\pi_\\theta)_{\\theta}$ and we are asking for an algorithm that returns the solution to the policy search problem in time polynomial in the number of actions $\\mathrm{A}$ and the number of parameters $d$ that describes the policy. An even simpler problem is when the MDP has finitely many states, and the algorithm needs to run in polynomial time in $\\mathrm{S}$, $\\mathrm{A}$ and $d$. In this case, it is clearly advantageous for the algorithm if it is given the exact description of the MDP (as described in Lecture 3) Sadly, even this mild version of policy search is intractable. Theorem (Policy search hardness): Unless $\\text{P}=\\text{NP}$, there is no polynomial time algorithm for the finite policy search problem even when the policy space is restricted to the constant policies and the MDPs are restricted to be deterministic with binary rewards. The constant policies are those that assign the same probability distribution to each state. This is a special case of state aggregation when all the states are aggregated into a single class. As the policy does not depend on the state, the problem is also known as the blind policy search problem. Note that the result holds regardless of the representation used to express the set of constant policies. Proof: Let $\\mathcal{S} = \\mathcal{A}=[n]$. The dynamics is deterministic: The next state is $a$ if action $a\\in \\mathcal{A}$ is taken in state $n$. A policy is simple a probability distribution \\(\\pi \\in \\mathcal{M}_1([n])\\) over the action space, which we shall view as a column vector taking values in $[0,1]^n$. The transition matrix of $\\pi$ is $P_{\\pi}(s,s’) = \\pi(s’)$, or, in matrix form, $P_\\pi = \\boldsymbol{1} \\pi^\\top$. Clearly, $P_\\pi^2 = \\boldsymbol{1} \\pi^\\top \\boldsymbol{1} \\pi^\\top = P_\\pi$ (i.e., $P_\\pi$ is idempotent). Thus, $P_\\pi^t = \\boldsymbol{1}\\pi^\\top$ for any $t&gt;0$ and hence . \\[\\begin{align*} J(\\pi) &amp; = \\mu (r_\\pi + \\sum_{t\\ge 1} \\gamma^t P_\\pi^t r_\\pi) = \\mu \\left(I + \\frac{\\gamma}{1-\\gamma} \\boldsymbol{1} \\pi^\\top \\right)r_\\pi\\,. \\end{align*}\\] Defining $R_{s,a} = r_a(s)$ so that $R\\in [0,1]^{n\\times n}$, we have $r_\\pi = R\\pi$. Plugging this in into the previous displayed equation and using that $\\mu \\boldsymbol{1}=1$, we get . \\[\\begin{align*} J(\\pi) &amp; = \\mu R \\pi + \\frac{\\gamma}{1-\\gamma} \\pi^\\top R \\pi\\,. \\end{align*}\\] Thus we see that the policy search problem is equivalent to maximizing the quadratic expression in the previous display over the probability simplex. Since there is no restriction on $R$, one may at this point conjecture that this will be hard to do. That this is indeed the case can be shown by a reduction to the maximum independent set problem, which asks for checking whether the independence number of a graph is above a threshold and which is known to be NP-hard even for $3$-regular graphs (i.e., graphs where every vertex has exactly three neighbours). Here, the independence number of a graph is defined as follows: We are given a simple graph $G=(V,E)$ (i.e., there are no self-loops, no double edges, and the graph is undirected). An independent set in $G$ is a neighbour-free subset of vertices. The independence number of $G$ is defined as . \\[\\begin{align*} \\alpha(G) = \\max \\{ |V'| \\,:\\, V'\\subset \\text{ independent in } G \\}\\,. \\end{align*}\\] Quadratic optimization has close ties to the maximum independent set problem: . Lemma (Motzkin-Strauss ‘65): Let \\(G\\in \\{0,1\\}^n\\) be the vertex-vertex adjacency matrix of simple graph (i.e., $G_{ij}=1$ if and only if $(i,j)$ is an edge of the graph). Then, for \\(I\\in \\{0,1\\}^{n\\times n}\\) the $n\\times n$ identity matrix, . \\[\\begin{align*} \\frac{1}{\\alpha(G)} = \\min_{y\\in \\mathcal{M}_1([n])} y^\\top (G+I) y\\,. \\end{align*}\\] . We now show that if there is an algorithm that solves policy search in polynomial time then it can also be used to solve the maximum independent set problem for simple, $3$-regular graphs. For this pick a $3$-regular graph $G$ with $n$ vertices. Define the MDP as above with $n$ states and actions and the rewards chosen to that $R = E-(I+G)$ where $G$ is the vertex-vertex adjacency matrix of the graph and $E$ is the all-ones matrix: $E = \\boldsymbol{1} \\boldsymbol{1}^\\top$. We add $E$ so that the rewards are in the $[0,1]$ interval and in fact are binary as required. Choose $\\mu$ as the uniform distribution over the states. Note that $\\boldsymbol{1}^\\top (I+G) = 4 \\boldsymbol{1}^\\top$ because the graph is $3$-regular. Then, for $\\pi \\in \\mathcal{M}_1(\\mathcal{A})$, . \\[\\begin{align*} J(\\pi) &amp; = \\frac{1}{1-\\gamma}- \\mu(E+I+G) \\pi - \\frac{\\gamma}{1-\\gamma} \\pi^\\top (E+I+G) \\pi \\\\ &amp; = \\frac{1}{1-\\gamma}- \\frac{1}{n} \\boldsymbol{1}^\\top (I+G) \\pi - \\frac{\\gamma}{1-\\gamma} \\pi^\\top (I+G) \\pi \\\\ &amp; = \\frac{1}{1-\\gamma}- \\frac{4}{n} - \\frac{\\gamma}{1-\\gamma} \\pi^\\top (I+G) \\pi\\,. \\end{align*}\\] Hence, \\(\\begin{align*} \\max_{\\pi \\in \\mathcal{M}_1([n]} J(\\pi) &amp; = \\frac{1}{1-\\gamma}- \\frac{4}{n} - \\frac{\\gamma}{1-\\gamma} \\frac{1}{\\alpha(G)} \\ge \\frac{1}{1-\\gamma}- \\frac{4}{n} - \\frac{\\gamma}{1-\\gamma} \\frac{1}{m} \\end{align*}\\) holds if and only if $\\alpha(G)\\ge m$. Thus, the decision problem of deciding that $J(\\pi)\\ge a$ is at least as hard as the maximum independent set problem. As noted, this is an NP-hard problem, hence the result follows. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec15/#policy-search",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec15/#policy-search"
  },"343": {
    "doc": "15. From policy search to policy gradients",
    "title": "Potential remedy: Local search",
    "content": "Based on the theorem just proved it is not very likely that we can find computationally efficient planners to compete with the best policy in a restricted policy class, even if the class looks quite benign. This motivates aiming at some more modest goal, one possibility of which is to aim for computing stationary points of the map $J:\\pi \\mapsto \\mu v^{\\pi}$. Let $\\Pi = { \\pi_\\theta \\,:\\, \\theta\\in \\mathbb{R}^d } \\in [0,1]^{\\mathcal{S}\\times\\mathcal{A}}$ be the set of policies that can represented; we view these now as “large vectors”. Then, in this approach we aim to identify \\(\\pi^*\\in \\Pi\\) (and its parameters) so that for any $\\pi’\\in \\Pi$ and small enough $\\delta&gt;0$ so that \\(\\pi^*+\\delta (\\pi'-\\pi^*)\\in \\Pi\\), \\(J(\\pi^*+\\delta (\\pi'-\\pi^*))\\le J(\\pi^*)\\). For $\\delta$ small, \\(J(\\pi^*+\\delta (\\pi'-\\pi^*))\\approx J(\\pi^*) + \\delta \\langle J'(\\pi^*), \\pi'- \\pi^* \\rangle\\). Plugging this in into the previous inequality, reordering and dividing by $\\delta&gt;0$ gives . \\[\\begin{align} \\langle J'(\\pi^*), \\pi'- \\pi^* \\rangle \\le 0\\,, \\qquad \\pi' \\in \\Pi\\,. \\label{eq:stp} \\end{align}\\] Here, $J’(\\pi)$ denotes the derivative of $J$. What remains to be seen is whether (1) relaxing the goal to computing \\(\\pi^*\\) helps with the computation (and when) and (2) whether we can get some guarantees for how well $\\pi^*$ satisfying \\eqref{eq:stp} will do compared to \\(J^* = \\max_{\\pi\\in \\Pi} J(\\pi)\\), that is obtaining some approximation guarantees. For the latter we seek for some function $\\varepsilon$ of the MDP $M$ and $\\Pi$ (or $\\phi$, when $\\Pi$ is based on some featuremap) so that . \\[\\begin{align*} J(\\pi^*) \\ge J^* - \\varepsilon(M,\\Pi) \\end{align*}\\] As to the computational approaches, we will consider a simple approach based on (approximately) following the gradient of $\\theta \\mapsto J(\\pi_\\theta$. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec15/#potential-remedy-local-search",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec15/#potential-remedy-local-search"
  },"344": {
    "doc": "15. From policy search to policy gradients",
    "title": "Notes",
    "content": "Access models . The reader may be wondering about what is the appropriate “access model” when $\\pi_\\theta$ is not restricted to the form given in \\eqref{eq:boltzmannpp}. There are many possibilities. One is to develop planners for specific parametric forms. A more general approach is to let the planner access \\(\\pi_{\\theta}(\\cdot\\vert s)\\) and $\\frac{\\partial}{\\partial\\theta}\\pi_{\\theta}(\\cdot \\vert s)$ for any $s$ it has encountered and any value of $\\theta\\in \\mathbb{R}^d$ it chooses. This is akin to the first-order black-box oracle model familiar from optimization theory. From function approximation to POMDPs . The hardness result for policy search is taken from a paper of Vlassis, Littman and Barber, who actually were interested in the computational complexity of planning in partially observable Markov Decision Problems (POMDPs). It is in fact an important observation that with function approximation, planning in MDPs becomes a special case planning in POMDPs: In particular, if policies are restricted to depend on the states through a feature-map $\\phi:\\mathcal{S}\\to \\mathbb{R}^d$ (any two states with identical features will get the same action distribution assigned to them), then planning to achieve high reward with this restricted class is almost the same as planning to achieve high reward in a partially observable MDP where the observation function is $\\phi$. Planners for the former problem could still have some advantage though if they can also access the states: In particular, a local planner which is given a feature-map to help its search but is also given access to the states is in fact not restricted to return actions whose distribution follows a policy from the feature-restricted class of policies. In machine learning, in the analogue problem of competing with a best predictor within a class but using predictors that do not respect the restrictions put on the competitors are called improper and it is known that improper learning is often more powerful than proper learning. However, when it comes to learning online or in a batch fashion then feature-restricted learning and learning in POMDPs become exact analogs. Finally, we note in passing that Vlassis et al. (2012) also add an argument that show that it is not likely that policy search is in NP. Open problem: Hardness of approximate policy search . The result almost implies that the approximate version of policy search is also NP-hard (Theorem 11.15, Arora, Barak 2009). In particular, it is not hard to see with the same construction that if one has an efficient method find a policy with $J(\\pi) \\ge \\max_\\pi J_\\pi - \\varepsilon$ then this gives an efficient method to find an independent set of size $\\alpha(G)/c$ for the said $3$-regular graphs where . \\[c = 1 + \\frac{1-\\gamma}{\\gamma} \\varepsilon \\alpha(G) \\le 1+ \\frac{1-\\gamma}{\\gamma} \\varepsilon n \\le 1+\\varepsilon n \\,,\\] where the last inequality follows if we choose $\\gamma=0.5$. Now, while there exist results that show that the maximum independent set is hard to approximate (i.e., for any fixed $c&gt;1$ finding an independent set of size $\\alpha(G)/c$ is hard), this would only imply hardness of approximate policy search if the hardness result also uses $3$-regular graphs. Also, the above bound on $c$ may be too naive: For example, to get $2$-approximations, one needs $\\varepsilon\\le 1/n$, which is small range for $\\varepsilon$. To get a hardness result for a “constant” $\\varepsilon$ (independent of $n$) needs significantly more work. Dealing with large action spaces . A common reason to consider policy search is because working with a restricted parametric family of policies holds the promise of decoupling the computational cost of learning and planning from the cardinality of the action-space. Indeed, with action-value functions, one usually needs an efficient way of computing greedy actions (with respect to some fixed action-value function). Computing $\\arg\\max_{a\\in \\mathcal{A}} q(s,a)$ in the lack of extra structure of the action-space and the function $q(s,\\cdot)$ takes linear time in the size of $\\mathcal{A}$, which is highly problematic unless $\\mathcal{A}$ has a small cardinality. In many applications of practical interest this is not the case: The action space can be “combinatorially sized”, or even a subset of some (potentially multidimensional) continuous space. If sampling from $\\pi_{\\theta}(\\cdot\\vert s)$ can be done efficiently, one may then potentially avoid the above expensive calculation. Thus, policy search is often proposed as a remedy to extend algorithms to work with large action spaces. Of course, this only applies if the sampling problem can indeed be efficiently implemented, which adds an extra restriction on the policy representation. Nevertheless, there are a number of options to achieve this: One can use for example an implicit representation (perhaps in conjunction with a direct one that uses probabilities/densities) for the policy. For example, the policy may be “represented” as a map $f_\\theta: \\mathcal{S} \\times \\mathcal{R} \\to \\mathcal{A}$ so that sampling from $\\pi_\\theta(\\cdot\\vert s)$ is accomplished by drawing a sample $R\\sim P$ from a fixed distribution over the set $\\mathcal{R}$ and then returning $f(s,R)\\in \\mathcal{A}$. Clearly, this is efficient as long as $f_\\theta$ can be efficiently evaluated at any of its inputs and the random value $R$ can be efficiently produced. If $f_\\theta$ is sufficiently flexible, one can in fact choose a very simple distribution for $P$, such as the standard normal distribution, or the uniform distribution. Note that when $\\mathcal{A}$ is continuous and the policies are deterministic is a special case: The key is still to be able to efficiently produce a sample from $\\pi_\\theta(\\cdot\\vert s)$, just in this case this means a deterministic computation. The catch is that one may also still need the derivatives of $\\pi_{\\theta}(\\cdot\\vert s)$ with respect to the parameter $\\theta$ and with an implicit representation as described above, it is unclear whether these derivatives can be efficiently obtained. As it turns out, this can be arranged if $f_{\\theta}(\\cdot\\vert s)$ is made of composition of elementary (invertible, differentiable) transformations with this property (by the chain rule). This observation is the basis of various approaches to “neural” density estimation (e.g., Tabak and Vanden-Eijnden, 2010, Rezende, Mohamed, 2015, or Jaini et al. 2019). ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec15/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec15/#notes"
  },"345": {
    "doc": "15. From policy search to policy gradients",
    "title": "References",
    "content": ". | Vlassis, Nikos, Michael L. Littman, and David Barber. 2012. “On the Computational Complexity of Stochastic Controller Optimization in POMDPs.” ACM Trans. Comput. Theory, 12, 4 (4): 1–8. | Esteban G. Tabak. Eric Vanden-Eijnden. “Density estimation by dual ascent of the log-likelihood.” Commun. Math. Sci. 8 (1) 217 - 233, March 2010. | Rezende, Danilo Jimenez, and Shakir Mohamed. 2015. “Variational Inference with Normalizing Flows” link. | Rezende, D. J., and S. Mohamed. 2014. “Stochastic Backpropagation and Approximate Inference in Deep Generative Models.” ICML. link. | Jaini, Priyank, Kira A. Selby, and Yaoliang Yu. 2019. “Sum-of-Squares Polynomial Flow.” In Proceedings of the 36th International Conference on Machine Learning, edited by Kamalika Chaudhuri and Ruslan Salakhutdinov, 97:3009–18. Proceedings of Machine Learning Research. PMLR. | Arora, Sanjeev, and Boaz Barak. 2009. Computational Complexity. A Modern Approach. Cambridge: Cambridge University Press. | . The hardness of the maximum independent set problem is a classic result; see, e.g., Theorem 2.15 in the book of Arora and Barak (2009) above, though this proof does not show that the hardness also applies to the case of 3-regular graphs. According to a comment by Gamow on stackexchange, a “complete NP-completeness proof for this problem is given right after Theorem 4.1 in the following paper”: . | Bojan Mohar: “Face Covers and the Genus Problem for Apex Graphs” Journal of Combinatorial Theory, Series B 82, 102-117 (2001) | . On the same page, Yixin Cao notes that there is a way to remove vertices of degree larger than three (presumable without changing the independence number) and refers to another stackexchange page. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec15/#references",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec15/#references"
  },"346": {
    "doc": "15. From policy search to policy gradients",
    "title": "15. From policy search to policy gradients",
    "content": "In the previous lectures we attempted to reduce the complexity of planning by assuming that value functions over the large state-action spaces can be compactly represented with a few parameters. While value-functions are an indispensable component of poly-time MDP planners (see Lectures 3 and 4), it is far from clear whether they should also be given priority when working with larger MDPs. Indeed, perhaps it is more natural to consider sets of policies with a compact description. Formally, in this problem setting the planner will be given a black-box simulation access to a (say, $\\gamma$-discounted) MDP $M=(\\mathcal{S},\\mathcal{A},P,r)$ as before, but the interface also provides access to a parameterized family of policies over $(\\mathcal{S},\\mathcal{A})$, \\(\\pi = (\\pi_\\theta)_{\\theta\\in \\mathbb{R}^d}\\), where for any fixed parameter $\\theta\\in \\mathbb{R}^d$, $\\pi_\\theta$ is a memoryless stochastic policy: $\\pi_\\theta:\\mathcal{S} \\to \\mathcal{M}_1(\\mathcal{A})$. For example, $\\pi_\\theta$ could be such that for some feature-map $\\phi: \\mathcal{S}\\times \\mathcal{A} \\to \\mathcal{R}^d$, . \\[\\begin{align} \\pi_\\theta(a|s) = \\frac{\\exp( \\theta^\\top \\varphi(s,a))}{\\sum_{a'} \\exp(\\theta^\\top \\varphi(s,a'))}\\,, \\qquad (s,a)\\in \\mathcal{S}\\times \\mathcal{A}\\,. \\label{eq:boltzmannpp} \\end{align}\\] In this case “access” to $\\pi_\\theta$ means access to $\\varphi$, which can be either global (i.e., the planner is given the “whole” of $\\varphi$ and can run any preprocessing on it), or local (i.e., $\\varphi(s’,a)$ is returned by the simulator for the “next states” $s’\\in \\mathcal{S}$ and for all actions $a$). Of course, the exponential function can be replaced with other functions, or, one can just use a neural network to output “scores”, which are turned into probabilities in some way. Dispensing with stochastic policies, a narrower class is the class of policies that are greedy with respect to action-value functions that belong to some parametric class. One special case that is worthy of attention due to its simplicity is the case when $\\mathcal{S}$ is partitioned into $m$ (disjoint) subsets $\\mathcal{S}_1,\\dots,\\mathcal{S}_m$ and for $i\\in [m]$, we have $\\mathrm{A}$ basis functions defined as follows: . \\[\\begin{align} \\phi_{i,a'}(s,a) = \\mathbb{I}( s\\in \\mathcal{S}_i, a= a' )\\,, \\qquad s\\in \\mathcal{S}, a,a'\\in \\mathcal{A}, i\\in [m]\\,. \\label{eq:stateagg} \\end{align}\\] Here, to minimize clutter, we allow the basis functions to be indexed by pairs and identified $\\mathcal{A}$ with ${ 1,\\dots,\\mathrm{A}}$, as usual. Then, the policies are given by $\\theta = (\\theta_1,\\dots,\\theta_m)$, the collection of $m$ probability vectors $\\theta_1,\\dots,\\theta_m\\in \\mathcal{M}_1(\\mathcal{A})$: . \\[\\begin{align} \\pi_\\theta(a|s) = \\sum_{i=1}^m \\sum_{a'} \\phi_{i,a'}\\theta_{i,a'}\\,. \\label{eq:directpp} \\end{align}\\] Note that because of the special choice of $\\phi$, $\\pi_{\\theta}(a|s) = \\theta_{i,a}$ for the unique index $i\\in [m]$ such that $s\\in \\mathcal{S}_i$. This is known as state-aggregretion: States belonging to the same group give rise to the same probability distribution over the actions. We say that the featuremap $\\varphi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ is of the state-aggregation type if it takes the form \\eqref{eq:stateagg} with an appropriate reindexing of the basis functions. Fix now a state-aggregation type featuremap. We can consider both the direct parameterization of policies given in \\eqref{eq:directpp}, or the “Boltzmann” parameterization given in \\eqref{eq:boltzmannpp}. As it is easy to see the set of possible policies that can be expressed with the two parameterizations are nearly identical. Letting $\\Pi_{\\text{direct}}$ be the set of policies that can be expressed using $\\varphi$ and the direct parameterization and letting $\\Pi_{\\text{Boltzmann}}$ be the set of policies that can be expressed using $\\varphi$ but with the Boltzmann parameterization, first note that \\(\\Pi_{\\text{direct}},\\Pi_{\\text{Boltzmann}} \\subset \\mathcal{M}_1(\\mathcal{A})^{\\mathcal{S}} \\subset ([0,1]^{\\mathrm{A}})^{\\mathrm{S}}\\), and if we take the closure, $\\text{clo}(\\Pi_{\\text{Boltzmann}})$ of $\\Pi_{\\text{Boltzmann}}$ then we can notice that . \\[\\text{clo}(\\Pi_{\\text{Boltzmann}}) = \\Pi_{\\text{direct}}\\,.\\] In particular, the Boltzmann policies cannot express point-mass distributions with finite parameters, but letting the parameter vectors grow without bound, any policy that can be expressed with the direct parameterization can also be expressed by the Boltzmann parameterization. There are many other possible parameterizations, as also mentioned earlier. The important point to notice is that while the parameterization is necessary so that the algorithms can work with a compressed representation, different representations may describe an identical set of policies. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec15/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec15/"
  },"347": {
    "doc": "16. Policy gradients",
    "title": "The policy gradient theorem",
    "content": "Fix an MDP $M=(\\mathcal{S},\\mathcal{A},P,r)$ and a discount factor $0\\le \\gamma &lt;1$. Continuing from the last lecture for $\\theta\\in \\mathbb{R}^d$ let $\\pi_\\theta$ be a stochastic policy: $\\pi_\\theta:\\mathcal{S}\\to \\mathcal{M}_1(\\mathcal{A})$. Further, fix a distribution $\\mu \\in \\mathcal{M}_1(\\mathcal{S})$ over the states and for a policy $\\pi:\\mathcal{S}\\to \\mathcal{M}_1(\\mathcal{A})$ let . \\[J(\\pi) = \\mu v^\\pi\\] denote the expected value of using policy $\\pi$ in $M$ from an initial state randomly chosen from $\\mu$. The policy gradient theorem gives sufficient conditions under which the map $\\theta \\mapsto J(\\pi_\\theta)$ is differentiable at some parameter $\\theta=\\theta_0$ and gives a “simple” expression for the gradient as a function of $\\frac{d}{dx} M_{\\pi_x} q^{\\pi_{\\theta_0}}$. Just to demistify this, for finite (or discrete) action spaces, for a memoryless policy $\\pi$ and function $q:\\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}} \\to \\mathbb{R}$, $M_{\\pi} q$ is a function mapping states to reals defined via . \\[(M_{\\pi} q)(s) = \\sum_a \\pi(a\\vert s) q(s,a)\\,.\\] Hence, the derivative, $\\frac{d}{dx} M_{\\pi_x} q$ is actually quite simple. It is a function mapping states to $d$ dimensional vectors which satisfies . \\[\\frac{d}{dx} (M_{\\pi_x} q)(s) = \\sum_a \\frac{d}{dx}\\pi_x(a\\vert s) q(s,a)\\,.\\] The theorem we give though is not limited to this case and also applies to when the action space is infinite and even when the policy is deterministic. For the theorem statement, recall that for a policy $\\pi$ we used $\\tilde \\nu_\\mu^\\pi$ to denote its discounted state occupancy measure. Also, for a function $f$, we use $f’$ to denote its derivative. Theorem (Policy Gradient Theorem): Fix an MDP $(\\mathcal{S},\\mathcal{A},P,r)$. For $x\\in \\mathbb{R}^d$, define the maps $f_\\pi: x\\mapsto \\tilde\\nu_\\mu^\\pi M_{\\pi_x} q^\\pi$ and $g_\\pi: x \\mapsto \\tilde \\nu_\\mu^{\\pi_x} v^\\pi$. Fix $\\theta_0\\in \\mathbb{R}^d$. Assume that at least one of the following two conditions is met: . | $\\theta \\mapsto f_{\\pi_\\theta}’(\\theta_0)$ exists and is continuous in a neighborhood of $\\theta_0$ and $g_{\\pi_{\\theta_0}}’(\\theta_0)$ exists; | $\\theta \\mapsto g_{\\pi_\\theta}’(\\theta_0)$ exists and is continuous in a neighborhood of $\\theta_0$ and $f_{\\pi_{\\theta_0}}’(\\theta_0)$ exists; | . Then, $x\\mapsto J(\\pi_x)$ is differentiable at $x=\\theta_0$ and . \\[\\begin{align} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} &amp;= \\frac{d}{dx} \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} M_{\\pi_x} q^{\\pi_{\\theta_0}}|_{x=\\theta_0} = \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} \\frac{d}{dx} M_{\\pi_x} q^{\\pi_{\\theta_0}}|_{x=\\theta_0}\\,, \\label{eq:pgt} \\end{align}\\] where the last equality holds if $\\mathcal{S}$ is finite. For the second expression, we treat $\\frac{d}{dx} M_{\\pi_x} q^{\\pi_{\\theta_0}}$ as an $\\mathrm{S}\\times d$ matrix. Note that this fits well with our convention of treating functions as “column vectors” (hence $M_{\\pi_x}) q^{\\pi_{\\theta_0}}$ is a vector of dimension $\\mathrm{S}$) and with the standard convention that a “vector derivative” creates “row vectors”. Above, the second expression where we moved the derivative with respect to the parameter inside the expression will only be valid in infinite state spaces when some additional regularity assumption is met. One such assumption is that \\(s\\mapsto\\|\\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s)|_{x=\\theta_0}\\|\\) is $\\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}}$-integrable. In words, the theorem shows that the derivative of the performance of a policy can be obtained by integrating a simple derivative that involves the action-value function of the policy. Of the two conditions of the theorem, the first condition is the one that is generally easier to verify. In particular, the condition on the continuous differentiability of $x\\mapsto f_{\\pi_x}$ at $x=\\theta_0$ is usually easy to verify. To show the differentiability of $x\\mapsto g_{\\pi_x}$ at $x=\\theta_0$ just recall that if the partial derivatives of a function exist and are continuous the function is differentiable. Then recall that $\\tilde \\nu_\\mu^{\\pi_x} v = \\sum_{t=0}^\\infty \\gamma^t \\nu P_{\\pi_x}^t v$ and hence its differentiability with respect to (say) $x_1$ follows if $x\\mapsto \\nu M_{\\pi_x} P v$ is continuously differentiable at $x=\\theta_0$. In effect, for finite state-action spaces, differentiability at $\\theta_0$ follows (and the conditions of the theorem are satisfied) as long as for any $(s,a)$ state-action pair, the maps $x\\mapsto \\pi_x(a|x)$ have continuous partial derivatives at $x=\\theta_0$. Proof: The proof is based on a short calculation that starts with writing the value difference identity for $v^{\\pi_x}-v^{\\pi_{\\theta_0}}$, multiplied from the right by $\\mu$, taking derivatives and then letting $x=\\theta_0$. The details are as follows: Recall from Calculus 101 the following result: Assume that $f = f(u,v)$ satisfies at least one of the two conditions: . | \\(z\\mapsto \\frac{\\partial}{\\partial v} f(z,x)\\) exists and is continuous in a neighborhood of $z=x$ and \\(\\frac{\\partial}{\\partial u} f(u,x)\\vert_{u=x}\\) exists; | \\(z\\mapsto \\frac{\\partial}{\\partial u} f(x,z)\\) exists and is continuous in a neighborhood of $z=x$ and \\(\\frac{\\partial}{\\partial v} f(x,v)\\vert_{v=x}\\) exists. | . Then $z \\mapsto f(z,z)$ is differentiable at $z=x$ and . \\[\\begin{align} \\frac{d}{dx} f(x,x) = \\frac{\\partial}{\\partial u} f(x,x) + \\frac{\\partial}{\\partial v} f(x,x)\\,. \\label{eq:c101} \\end{align}\\] Let $\\pi’,\\pi$ be two memoryless policies. By the value difference identity, . \\[\\begin{align*} v^{\\pi'}- v^{\\pi} &amp; = (I-\\gamma P_{\\pi'})^{-1} [ T_{\\pi'} v^\\pi - v^\\pi] \\\\ &amp; = (I-\\gamma P_{\\pi'})^{-1} [ M_{\\pi'} q^\\pi - v^\\pi]\\,, \\end{align*}\\] where the last equality just used that that $T_{\\pi’} v^\\pi = M_{\\pi’} (r+\\gamma P v^\\pi) = M_{\\pi’} q^{\\pi}$. Now let $\\pi’ = \\pi_x$ and $\\pi = \\pi_{\\theta_0}$ and multiply the value difference identity from the left by $\\mu$ to get . \\[\\begin{align} \\mu( v^{\\pi_x}- v^{\\pi_{\\theta_0}}) = \\tilde \\nu_\\mu^{\\pi_x} [ M_{\\pi_x}q^{\\pi_{\\theta_0}} - v^{\\pi_{\\theta_0}}]\\,. \\label{eq:vdi2} \\end{align}\\] Now, focusing on the first term on the right-hand-side, let . \\[\\begin{align} f(u,v) = \\tilde \\nu_\\mu^{\\pi_u} M_{\\pi_v}q^{\\pi_{\\theta_0}}\\,. \\label{eq:fdef} \\end{align}\\] Provided that $f$ is sufficiently regular in a neighborhood of $(x,x)$ (to be discussed later), \\eqref{eq:c101} gives that . \\[\\begin{align*} \\frac{d}{dx} f(x,x) = \\frac{d}{du} \\tilde \\nu_\\mu^{\\pi_u} M_{\\pi_x}q^{\\pi_{\\theta_0}}|_{u=x} + \\frac{d}{dv} \\tilde \\nu_\\mu^{\\pi_x} M_{\\pi_v}q^{\\pi_{\\theta_0}}|_{v=x} \\end{align*}\\] Taking the derivative of both sides of \\eqref{eq:vdi2} with respect to $x$ and using the above display we get . \\[\\begin{align*} \\frac{d}{dx} J(x) = \\frac{d}{dx} \\mu( v^{\\pi_x}- v^{\\pi_{\\theta_0}}) = \\frac{d}{du} \\tilde \\nu_\\mu^{\\pi_u} M_{\\pi_x}q^{\\pi_{\\theta_0}}|_{u=x} + \\frac{d}{dv} \\tilde \\nu_\\mu^{\\pi_x} M_{\\pi_v}q^{\\pi_{\\theta_0}}|_{v=x} + \\frac{d}{dx} \\tilde \\nu_\\mu^{\\pi_x} v^{\\pi_{\\theta_0}}\\,. \\end{align*}\\] Now let $x = \\theta_0$. Then, $M_{\\pi_x}q^{\\pi_{\\theta_0}} = M_{\\pi_{\\theta_0}}q^{\\pi_{\\theta_0}} = v^{\\pi_{\\theta_0}}$. Hence, the first and the third term of the above display cancel each other and we get . \\[\\begin{align*} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} = \\frac{d}{dv} \\tilde \\nu_\\mu^{\\pi_{\\theta_0}} M_{\\pi_v} q^{\\pi_{\\theta_0}}|_{v=\\theta_0}\\,. \\end{align*}\\] Finally, the conditions to apply \\eqref{eq:c101} to our $f$ in \\eqref{eq:fdef} are met by our assumption on $f_\\pi$ and $g_\\pi$, finishing the proof. \\(\\qquad \\blacksquare\\) . When the action-space is discrete and $\\pi_\\theta$ are stochastic policies, we can further manipulate the expression we obtained. In particular, in this case . \\[\\begin{align*} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\pi_x(a\\vert s) q^{\\pi_{\\theta_0}}(s,a) \\end{align*}\\] and thus, for finite $\\mathcal{A}$, . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\frac{d}{dx} \\pi_x(a\\vert s) q^{\\pi_{\\theta_0}}(s,a)\\,. \\label{eq:basicpg} \\end{align}\\] While this can be used as the basis of evaluating (or approximating) gradient, it may be worthwhile to point out an alternate form which is available when $\\pi_x(a\\vert s)&gt;0$. In this case, using the chain rule we get . \\[\\begin{align*} \\frac{d}{dx} \\log \\pi_x(a\\vert s) = \\frac{\\frac{d}{dx} \\pi_x(a\\vert s)}{\\pi_x(a\\vert s)}\\,. \\end{align*}\\] Using this in \\eqref{eq:basicpg} we get . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) q^{\\pi_{\\theta_0}}(s,a)\\,, \\label{eq:basicpga} \\end{align}\\] which has the pleasant property that it takes the form of an expected value over the actions of the score function of the policy map correlated with the action-value function. Before moving on it is worth pointing out that an equivalent expression is obtained if $q^{\\pi_{\\theta_0}}(s,a)$ above is shifted by an arbitrary constant which may depend on $\\theta_0$ or $s$ but not $a$. Indeed, since $\\sum_a \\pi_x(a\\vert s) b(s,\\theta_0) = b(s,\\theta_0)$, differentiating both sides with respect to $x$ gives $\\sum_a \\frac{d}{dx}\\pi_x(a\\vert a) b(s,\\theta_0)=0$. Hence, we also have . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\sum_{a} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) (q^{\\pi_{\\theta_0}}(s,a)-b(s,\\theta_0))\\,. \\label{eq:basicpga2} \\end{align}\\] This may have significance when using simulation to evaluate derivatives: One may attempt to use an appropriate “bias” term to reduce the variance of the estimate of the gradient. Before discussing simulation any further, it may be also worthwhile to discuss what happens when the action-space is infinite. For countable infinite action spaces, the only difference is that \\eqref{eq:basicpg} may not always hold. An easy sufficient condition for this to hold is that \\(\\sum_{a} \\|\\frac{d}{dx} \\pi_x(a\\vert s)\\|\\, |q^{\\pi_{\\theta_0}}(s,a)|\\) is summable, or equivalently, \\(\\|\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\|\\, |q^{\\pi_{\\theta_0}}(s,a)|\\) is $\\pi_x(\\cdot\\vert s)$-summable/integrable. For uncountably infinite action spaces, this argument works with the minimal necessary changes. In the most general case, $\\pi_\\theta(\\cdot\\vert s)$ is a probability measure over $\\mathcal{A}$ and its derivative is a vector-valued measure. The formulae derived above (e.g., \\eqref{eq:basicpga2}) remain valid if we replace the sum with an integral when $\\pi_\\theta(\\cdot\\vert s)$ is given in the form of a density with respect to some fixed measure $\\lambda$ over $\\mathcal{A}$: . \\[\\begin{align} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\int_{\\mathcal{A}} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) (q^{\\pi_{\\theta_0}}(s,a)-b(s,\\theta_0)) \\lambda(da)\\,. \\label{eq:basicpga3} \\end{align}\\] In fact, this is a strictly more general form: \\eqref{eq:basicpga2} is a special case of \\eqref{eq:basicgpa3} when $\\lambda$ is set to the counting measure over $\\mathcal{A}$. In the special case when \\(\\pi_{\\theta}(\\cdot\\vert s) = \\delta_{f_{\\theta}(s)}(\\cdot)\\) (a Dirac at $f_{\\theta}(s)$), in words, when we have a deterministic policy map and $f$ is differentiable with respect to $\\theta$, it is better to start from the formula given in the theorem. Indeed, in this case, . \\[\\begin{align*} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = q^{\\pi_{\\theta_0}}(s,f_\\theta(s)) \\end{align*}\\] and hence . \\[\\begin{align*} \\frac{d}{dx} (M_{\\pi_x} q^{\\pi_{\\theta_0}})(s) = \\frac{d}{dx} q^{\\pi_{\\theta_0}}(s,f_x(s)) \\end{align*}\\] and thus, if either $\\mathcal{S}$ is finite or an appropriate regularity condition holds, . \\[\\begin{align*} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} &amp;= \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} \\frac{d}{dx} q^{\\pi_{\\theta_0}}(\\cdot,f_x(\\cdot))|_{x=\\theta_0}\\,. \\end{align*}\\] If $a\\mapsto q^{\\pi_{\\theta_0}}(s,a)$ is differentiable and $x\\mapsto f_x(s)$ is also differentiable at $x=\\theta_0$ for every $s$ then . \\[\\begin{align*} \\frac{d}{dx} J(\\pi_x)|_{x=\\theta_0} &amp;= \\tilde\\nu_{\\mu}^{\\pi_{\\theta_0}} \\frac{\\partial}{\\partial a} q^{\\pi_{\\theta_0}}(\\cdot,f_{\\theta_0}(\\cdot)) \\frac{d}{dx} f_x(\\cdot)|_{x=\\theta_0}\\,, \\end{align*}\\] which is known as the “deterministic policy gradient formula”. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec16/#the-policy-gradient-theorem",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec16/#the-policy-gradient-theorem"
  },"348": {
    "doc": "16. Policy gradients",
    "title": "Gradient methods",
    "content": "The idea of gradient methods is to make small steps in the parameter space in the direction of the gradient of an objective function that is to be maximized. In the context of policy search, this works as follows: If $x_i\\in \\mathbb{R}^d$ denotes the parameter vector in round $i$, . \\[x_{i+1} = x_i + \\alpha_i \\nabla_x J(\\pi_x)|_{x=x_i}\\,,\\] where for $f$ differentiable, $\\nabla_x f = (\\frac{d}{dx} f)^\\top$ is the “gradient” (transpose of derivative). Above, $\\alpha_i$ is a positive tuning parameter, called the “stepsize” of the update. The idea is that the “gradient” points in the direction where the function is expected to grow. Indeed, since by definition, . \\[f(x') =f(x) + f'(x) (x'-x) + o(\\|x'-x\\|)\\] if $x’ = x+ \\delta (f’(x))^\\top$, . \\[f( x+ \\delta (f'(x))^\\top ) = f(x)+\\delta \\| f'(x)\\|_2^2 + o(|\\delta|)\\,,\\] or . \\[\\frac{f( x+ \\delta (f'(x))^\\top ) - f(x)}{\\delta} = \\| f'(x)\\|_2^2 + o(1)\\,,\\] For **any$$ $\\delta$ sufficiently small so that the $o(1)$ term (in absolute value) is below $| f’(x)|_2^2$, we see that the right-hand side is positive, hence so is the left-hand side, as claimed. This simple observation is the basis of a huge number of algorithmic variants. In the lack of extra structure the best we can hope from a gradient method is that it will end of in the vicinity of a stationary point. In the presence of extra structure (.e.g, concave function to be maximized), convergence to a global maximum can be guaranteed. In all cases the key to the success of gradient methods is the appropriate choice of the stepsizes; these choices are based on a refinement of the above simple argument that shows that moving towards the direction of the gradient helps. There are also ways of “speeding up” convergence; these “acceleration methods” use a refined iteration (two iterates updated simultaneously) and can greatly speed up convergence. As there are many excellent texts that describe various aspects of gradient methods which cover these ideas, we will not delve into them any further, but I will rather give some pointers to this literature in the endnotes. The elephant in the room here is that the gradient of $J$ is not readily available. The next best thing then is to attempt to build an estimate $G$ of $\\nabla_x J(\\pi_x)$. In the planning setting, the question is whether one can get reasonable estimates of this gradient using a simulator. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec16/#gradient-methods",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec16/#gradient-methods"
  },"349": {
    "doc": "16. Policy gradients",
    "title": "Gradient estimation",
    "content": "Generally speaking there are two types of errors when construction an estimate of the gradient: The one that is purely random, and the one that is not. Defining $g(x) = \\mathbb{E}[G]$, $b(x)=\\nabla_x J(\\pi_x) - g(x)$ measures the “bias” of the gradient estimate, while $G-g(x)$ is the noise. Gradient methods with decreasing (or small) stepsizes naturally “average out” the noise. The version of gradient methods that are able to do this are called stochastic gradient methods. Naturally, these methods are slower when the noise is larger and in general cannot converge faster than how fast the noise averages out. In particular, in persistent noise (i.e., noise with nonvanishing variance), the best rate available for stochastic gradient methods is $O(1/\\sqrt{t})$. While this can be slower than what can be achieved without noise, if the iteration cost is polynomial in the relevant quantities, the total cost of achieving an $\\varepsilon&gt;0$ stationary point can be bounded by a polynomial in these quantities and $1/\\varepsilon^2$. When the gradient estimates are biased, the bias will in general put a limit on how close a gradient method can get to a stationary point. While generally a zero bias is preferred to a nonzero bias, a nonzero bias which is positively aligned with the gradient ($\\langle b(x),\\nabla_x J(\\pi_x) \\rangle\\ge 0$) does not hurt (again, for small stepsizes). When there is no way to guarantee that the bias is positively aligned with the gradient, one may get back into control by making sure that the magnitude of the bias is small relative to the magnitude of the gradient. The next question is of course, how to estimate the gradient. For this many approaches have been proposed in the literature. When a simulator is available, as in our case, a straightforward approach is to start from the policy gradient theorem. Indeed, under mild regularity conditions (e.g., if there are finitely many states) \\eqref{eq:pgt} together with \\eqref{eq:basicpga3} gives . \\[\\begin{align} \\frac{d}{dx} J(\\pi_x) = \\int_{\\mathcal{S}} \\tilde \\nu_\\mu^{\\pi_x}(ds) \\int_{\\mathcal{A}} \\pi_x(a\\vert s) \\left(\\frac{d}{dx} \\log \\pi_x(a\\vert s)\\right) (q^{\\pi_{x}}(s,a)-b(s,x)) \\lambda(da)\\,. \\end{align}\\] Now note that $(1-\\gamma)\\tilde \\nu_\\mu^{\\pi_x}$ is a probability measure over $\\mathcal{S}$. Let $S_0,A_0,S_1,A_1,\\dots$ be an infinite sequence of state-action pairs obtained by simulating policy $\\pi_x$ starting from $S_0\\sim \\mu$. In particular, $A_t \\sim \\pi_x(\\cdot|S_t)$ and $S_{t+1}\\sim P_{A_t}(S_t)$ for any $t\\ge 0$. In addition, define $T_1,T_2$ to be independent of each other and from the trajectory $S_0,A_0,S_1,A_1,\\dots$ and have a geometric distribution with parameter $1-\\gamma$. Then, . \\[G = \\frac{1}{1-\\gamma} \\frac{d}{dx} \\log \\pi_x(A_{T_1}\\vert S_{T_1}) \\left(\\sum_{t=0}^{T_2-1} r_{A_{T_1+t}}(S_{T_1+t}) -b(S_{T_1},x)\\right)\\] is an unbiased estimate of $\\frac{d}{dx} J(\\pi_x)$: . \\[\\mathbb{E}[G] = \\frac{d}{dx} J(\\pi_x)\\,.\\] The argument to show this has partially be given earlier in Lecture 8. One can also show that $G$ has a finite covariance matrix, as well as that the expected effort to obtain $G$ is $O(\\frac{1}{1-\\gamma})$. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec16/#gradient-estimation",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec16/#gradient-estimation"
  },"350": {
    "doc": "16. Policy gradients",
    "title": "Vanilla policy gradients (PG) with some special policy classes",
    "content": "Given the hardness result presented in the previous lecture, there is no hope that gradient methods or any other method will find the global optima of the objective function in policy search in a policy-class agnostic manner. To guarantee computational efficiency, one then . | either needs to give up on convergence to a global optima, or | give up on generality, i.e., give up on that the method should work for any policy class and/or policy parameterization. | . Gradient ascent to find a good policy (“vanilla policy gradients”) is one possible approach to take even if it faces these restrictions. In fact, gradient ascent in some cases will find a globally optimal policy. In particular, it has been long known that with small enough stepsizes gradient ascent converges at a reasonable speed to a global optimum provided that two conditions hold: . | The objective function $f$ is smooth (its derivative is Lipschitz continuous); | The objective function is gradient dominated, i.e., with some constants $c&gt;0$, $p \\ge 1$, $f$ satisfies \\(\\sup_x f(x)-f(x')\\le c \\| f'(x') \\|_2^p\\) for any $x’\\in \\mathbb{R}^d$. | . An example when both of these conditions are met is the direct policy parameterization, which does not allow any compression and is thus not helpful per se, but can serve as a test-case to see how far policy gradient (PG) methods can be pushed. In this case, the parameter vector $\\theta$ is $\\mathrm{S}\\mathrm{A}$ dimensional. By allowing “two-dimensional index”, $\\pi_{\\theta}(a\\vert s)=\\theta_{s,a}$, that is, the parameters encode the action selection probabilities in a direct manner. In this case, since the components of $\\theta$ represent probabilities, they need to be nonnegative and the appropriate components needs to sum to one. Hence, $\\theta\\in \\Theta$ for an appropriate set $\\Theta \\subset [0,1]^{\\mathrm{S}\\mathrm{A}}$. Accordingly, one needs to change gradient ascent. This is done as follows: When a proposed update moves the parameter vector outside of $\\Theta$, the proposed updated parameter vector is “back-projected” to $\\Theta$. For the projection there are a number of reasonable options, such as choosing the point within $\\Theta$ which is closest to the proposed point in the standard Euclidean distance. With this modification, gradient ascent can be shown to converge at a reasonable speed in this case. This parallels the methods that were developed for the tabular case (policy iteration, value iteration). In fact, the algorithm can be seen as a “smoother”, incremental version of policy iteration, which gradually adjusts the probabilities assigned to the individual actions. Using $\\pi_i$ to denote the $i$th policy, from the policy gradient theorem one gets . \\[\\tilde \\pi_{i+1}(a\\vert s ) = \\pi_i(a\\vert s) + \\alpha_i \\tilde \\nu_\\mu^{\\pi_i}(s) q^{\\pi_i}(s,a) \\,,\\] and . \\[\\pi_{i+1}(\\cdot\\vert s) = \\arg\\min_{p\\in \\mathcal{M}_1(\\mathcal{A})} \\| p - \\pi_{i+1}(\\cdot\\vert s) \\|_2, \\qquad s\\in \\mathcal{S}\\,.\\] Thus, the probability of an action in a state is increased in proportion to the value of that state. That the action-value of action $a$ at state $s$ is multiplied with the discounted occupancy at $s$ induced by using policy $\\pi_i$ started from $\\mu$ is a bit of a surprise. In particular, if a state is inaccessible under policy $\\pi_i$, the corresponding probabilities will not be updated. In fact, because this, the above iteration may get stuck at a suboptimal policy. The reader is invited to construct an example when this holds. To prevent this, it turns out to be sufficient if there is a constant $C&gt;0$ such that it holds that . \\[\\begin{align} \\tilde \\nu_\\mu^{\\pi^*}(s)\\ge C \\mu(s)\\,, \\qquad \\text{for all } s\\in \\mathcal{S}\\,, \\label{eq:exploinit} \\end{align}\\] where \\(\\pi^*\\) is an optimal policy. Since $\\mu$ appears on both sides and \\(\\pi^*\\) is unknown, this condition does not look to helpful. However, if one chooses $\\mu$ to be positive everywhere, the condition is clearly met. In any case, when \\eqref{eq:exploinit} holds, gradient dominance and smoothness can be both verified, which in turn implies that the above update will converge at a geometric speed, the geometric speed involves an instance dependent constant which has no polynomial bound in terms of $H_\\gamma = 1/(1-\\gamma)$ and the size of the state-action space. Needless to say this is quite unattractive. Policy gradient methods can be sensitive to how policies are parameterized. For illustration, consider still the “tabular case”, just now change the way the memoryless policies are represented. One possibility is to use the Boltzmann, also known as the softmax representation. In this case $\\theta\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ and . \\[\\begin{align*} \\pi_{\\theta}(a\\vert s) = \\frac{\\exp( \\theta_{s,a})}{ \\sum_{a'} \\exp( \\theta_{s,a'})}\\,, \\qquad (s,a)\\in \\mathcal{S}\\times\\mathcal{A}\\,. \\end{align*}\\] A straightforward calculation gives . \\[\\begin{align*} \\frac{\\partial}{\\partial \\theta_{s,a}} \\log \\pi_\\theta( a'\\vert s' ) = \\mathbb{I}(s=s',a=a') - \\pi_{\\theta}(a\\vert s) \\mathbb{I}(s=s') \\end{align*}\\] and hence . \\[\\begin{align*} \\frac{\\partial}{\\partial\\theta_{(s,a)}} J(\\pi_{\\theta}) &amp; = \\sum_{s'} \\tilde \\nu_\\mu^{\\pi_\\theta}(s') \\sum_{a'}\\pi_\\theta(a'\\vert s) \\frac{\\partial}{\\partial \\theta_{s,a}} \\log \\pi_\\theta( a'\\vert s' ) q^{\\pi_\\theta}(s',a')\\\\ &amp; = \\nu_\\mu^{\\pi_\\theta}(s,a) \\left( q^{\\pi_\\theta}(s,a)-v^{\\pi_\\theta}(s) \\right)\\,, \\end{align*}\\] where recall that $\\nu_\\mu^{\\pi}$ is the discounted state-occupancy measure over the state-action pairs of policy $\\pi$ when the initial state distribution is $\\mu$. The difference in the bracket on the right-hand side is known as the advantage of action $a$ and, accordingly, the function . \\[\\mathfrak{a}^{\\pi} = q^\\pi-v^\\pi\\,,\\] which is a function mapping state-action pairs to reals, is called the advantage function underlying policy $\\pi$. To justify the terminology, note that policy iteration can be seen as choosing in each state the action that maximizes the “advantage”. Thus, we expect that we get a better policy if the “probability mass” in the action distribution is shifted towards actions with a larger advantage. Note though that advantages (as defined above) can also be negative and in fact if $\\pi$ is optimal, all actions have nonnegative advantages only. The gradient ascent rule prescribes that . \\[\\theta_{i+1} = \\theta_i + \\alpha_i \\nu_\\mu^{\\pi_{\\theta_i}} \\circ \\mathfrak{a}^{\\pi_{\\theta_i}}\\,,\\] where $\\circ$ denotes componentwise product. While this is similar to the previous update, now the meaning of parameters is quite different. In fact, just because a parameter is increased does not necessarily mean that the probability of the corresponding action is increased: This will only happen if the increase of this parameter exceeds that of the other parameters “at the same state”. By slightly abusing notation with defining $\\pi_i = \\pi_{\\theta_i}$, we have . \\[\\begin{align} \\pi_{i+1}(a\\vert s) \\propto \\pi_i(a\\vert s) \\exp( \\alpha_i \\nu_\\mu^{\\pi_i}(s,a) \\mathfrak{a}^{\\pi_i}(s,a))\\,. \\label{eq:softmaxtab} \\end{align}\\] Just like in the previous update rule, we also see the occupancy measure “weighting” the update. This is again not necessarily helpful and if anything, again, speaks to the arbitrariness of gradient methods. And while this does not entirely stop policy gradient to find an optimal policy, and again, one can even show that the speed is geometric, though, as before, the algorithm altogether fails to run in polynomial time in the relevant quantities. For this theorem which we give without proof recall that $H_\\gamma = 1/(1-\\gamma)$. Theorem (PG is slow with Boltzmann policies): There exists universal constants $\\gamma_0,c,C&gt;0$ such that for any $\\gamma_0&lt;\\gamma&lt;1$, if $\\mathrm{S}&gt;C H_\\gamma^6$ then one can find a discounted MDP with $\\mathrm{S}$ states and $3$ actions, setting $\\mu$ to be the uniform distribution and initializing the parameters so that $\\pi_0$ is the uniform random policy, softmax PG with a constant stepsize of $\\alpha&gt;0$ takes at least . \\[\\frac{c}{\\alpha} \\mathrm{S}^{2^{\\Omega({H_\\gamma})}}\\] iterations. As one expects that without any compression, the chosen planner should behave reasonably, this rules out the “vanilla” version of policy gradient. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec16/#vanilla-policy-gradients-pg-with-some-special-policy-classes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec16/#vanilla-policy-gradients-pg-with-some-special-policy-classes"
  },"351": {
    "doc": "16. Policy gradients",
    "title": "Natural policy gradient (NPG) methods",
    "content": "In fact, a quite unsatisfactory property of gradient ascent that the speed at which it converges can greatly depend on the parameterization used. Thus, for the same policy class, there are many possible “gradient directions”, depending on the parameterization chosen. What is a gradient direction for one parameterization is not necessarily a gradient direction for another one. But what is common about these directions that an infinitesimal step along them is guaranteed increase the objective. One can in fact take a direction obtained with a parameterization and look at what direction it gives with another parameterizations. To get some order, consider transforming all these directions into the space that corresponds to the direct parameterization. It is not hard to see that all possible directions that are within 90 degrees of the gradient direction with this parameterization can be obtained by considering an appropriate parameterization. More generally, regardless of parameterization, all directions within 90 degrees of the gradient direction are ascent directions. This motivates changing the stepsize $\\alpha_i$ from a scalar to a matrix $A_i$. Clearly, to keep the angle between the original gradient direction $g$ and the transformed direction $A_i g$ below 90 degrees, $g^\\top A_i g\\ge 0$ has to hold. For $A_i$ symmetric, this restricts the set of matrix “stepsizes” to the set of positive definite matrices (still, a large set). There are many ways to choose a matrix stepsize. Newton’s method is to choose it so that the direction is the “best” if the function is replaced by its local quadratic approximation. This provably helps to reduce the number of iterations when the objective function is “ill-conditioned”, though all matrix stepsize methods incur additional cost per each iteration, which will often offset the gains. Another idea, which comes from statistical problems where one often works with distributions is to find the direction of update which coincides with the direction one would obtain if one used the steepest descent direction directly in the space of distributions where distances are measured with respect to relative entropy. In some cases, this approach, which was coined the “natural gradient” approach, has been shown to give better results, though the evidence is purely empirical. As it turns out, the matrix stepsize to be used with this approach is the (pseudo)inverse of the so-called Fischer information matrix. In our context, for every state, we have distributions over the actions. Fixing a state $s$, the Fischer information matrix becomes . \\[F_x(s) = \\frac{d}{dx} \\log \\pi_x(\\cdot\\vert s)\\, \\frac{d}{dx} \\log \\pi_x(\\cdot\\vert s)^\\top\\,.\\] To get the “information rate” over the states, one can sum these matrices up, weighted by the discounted state occupancy measure underlying $\\mu$ and $\\pi_x$ to get . \\[F(x) := \\nu_\\mu^{\\pi_x} F_x \\,.\\] The update rule then takes the form . \\[x_{i+1} = x_i + \\alpha_i F(x_i)^{\\dagger} \\nabla_x J(\\pi_x)\\,,\\] where for a square matrix $A$, $A^{\\dagger}$ denotes the pseudoinverse of $A$. Interestingly, the update direction can be obtained without calculating $F$ and inverting it: . Proposition: We have . \\[\\begin{align*} (1-\\gamma) F(x)^{\\dagger} \\nabla_x J(\\pi_x) = \\arg\\min_{w\\in \\mathbb{R}^d} \\nu_\\mu^{\\pi_x} \\left( w^\\top \\nabla_x \\log \\pi_x(\\cdot\\vert \\cdot)- \\mathfrak{a}^{\\pi_x}\\right)^2\\,, \\end{align*}\\] where $\\mathfrak{a}^{\\pi_x} =q^{\\pi_x}-v^{\\pi_x}$ and $\\arg\\min$ chooses the minimum \\(\\|\\cdot\\|_2\\)-norm solution if multiple minimizers exist. Proof: Just recall the formula that gives the solution to a least-squares problem. The details are left to the reader. \\(\\qquad \\blacksquare\\) . As an example of how things look like consider the case when $\\pi_x$ takes the form of a Boltzmann policy: . \\[\\pi_x(a\\vert s) \\propto \\exp(x^\\top \\phi(s,a))\\,,\\] where $\\phi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ is a feature-map. Then, assuming that there are finitely many actions, . \\[\\nabla_x \\log \\pi_x(a\\vert s) = \\underbrace{\\phi(s,a)- \\sum_{a'} \\pi_x(a'\\vert s) \\phi(s,a')}_{\\psi_x(s,a)}\\,.\\] Then, the natural policy gradient update takes the form . \\[x_{i+1} = x_i + \\alpha_i w_i\\,,\\] where . \\[w_i = \\arg\\min_{w\\in \\mathbb{R}^d} \\nu_\\mu^{\\pi_x} \\left( w^\\top \\psi_x - \\mathfrak{a}^{\\pi_{x_i}}\\right)^2\\] In the tabular case $(d=\\mathrm{S}\\mathrm{A}$, no compression), . \\[w_i(s,a) = \\mathfrak{a}^{\\pi_{x_i}}(s,a)\\] and thus . \\[\\pi_{i+1}(a\\vert s) \\propto \\pi_i(a\\vert s) \\exp( \\alpha_i \\mathfrak{a}^{\\pi_i}(s,a) ) = \\pi_i(a\\vert s) \\exp( \\alpha_i q^{\\pi_i}(s,a) )\\,.\\] Note that this update rule eliminates the term $\\nu_\\mu^{\\pi_i}(s,a)$ term that we have previously seen (cf. \\eqref{eq:softmaxtab}). NPG is known to enjoy a reasonable speed of convergence, which gives altogether polynomial planning time. This is promising. No similar results are available for the nontabular case. Note that if we (arbitrarily) change the definition of $w_i$ by replacing $\\psi_x$ above with $\\phi$ and $a^{\\pi_x}$ with $q^{\\pi_x}$, we get what has been called in the literature Q-NPG: . \\[w_i = \\arg\\min_{w\\in \\mathbb{R}^d} \\nu_\\mu^{\\pi_x} \\left( w^\\top \\phi - q^{\\pi_x}\\right)^2\\,.\\] Note that the only difference between Q-NPG and Politex is that in Politex one uses . \\[w_i = \\arg\\min_{w\\in \\mathbb{R}^d} \\hat \\nu \\left( w^\\top \\phi - q^{\\pi_x}\\right)^2\\,,\\] where $ \\hat \\nu$ is the measure obtained from solving the G-optimal design problem. The price of not using $\\hat \\nu$ but using $\\nu_\\mu^{\\pi_x}$ in Q-NPG is that the approximation error in Q-NPG becomes . \\[\\frac{C\\varepsilon}{(1-\\gamma)^{1.5}}\\] where . \\[C = \\left\\| \\frac{d\\tilde\\nu_\\mu^{\\pi^*} }{d\\mu} \\right\\|_\\infty\\] gives a bound on how much the distribution $\\mu$ differs from that of obtained when the optimal policy $\\pi^*$ is followed from $\\mu$. As was argued before, it is necessary that $C$ is finite for policy gradient methods not to “get stuck” at local optima. However, $C$ can be arbitrarily large even for finite state-action MDPs; an in fact it is the presence of $C$ that makes the policy gradient with the direct parameterization a slow algorithm. In contrast, the same quantity in Politex is . \\[\\frac{\\sqrt{d}\\varepsilon}{1-\\gamma}\\,.\\] Not only the uncontrolled constant $C$ is removed, but the dependence on the planning horizon is also improved. Other than these differences, the results available for Q-NPG are similar to that of Politex and in fact the proof technique to obtain the results is also the same. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec16/#natural-policy-gradient-npg-methods",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec16/#natural-policy-gradient-npg-methods"
  },"352": {
    "doc": "16. Policy gradients",
    "title": "The proof of the Calculus 101 result",
    "content": "For completeness, here is the proof of \\eqref{eq:c101}. For the proof recall that for a function $g:\\mathbb{R}^d \\to \\mathbb{R}$, $\\frac{d}{dx} g(x_0)$ is the unique linear operator (row vector, in the Euclidean case) that satisfies . \\[\\begin{align*} g(x)=g(x_0)+\\frac{d}{dx} g(x_0) (x-x_0) + o( \\|x-x_0\\|) \\text{ as } x\\to x_0\\,. \\end{align*}\\] Hence, it suffices to show that . \\[\\begin{align*} f(x',x') = f(x,x) + \\left( \\frac{\\partial}{\\partial u} f(u,x)\\vert_{u=x} + \\frac{\\partial}{\\partial v} f(x,v)\\vert_{v=x} \\right) (x'-x) + o( \\|x'-x\\|)\\,. \\end{align*}\\] To minimize clutter we will write $\\frac{\\partial}{\\partial u} f(x’,x)$ for $\\frac{\\partial}{\\partial u} f(u,x)\\vert_{u=x’}$ (and similarly we write $\\frac{\\partial}{\\partial v} f(x,x’)$ for $\\frac{\\partial}{\\partial v} f(x,v)\\vert_{v=x’}$). By definition we have . \\[\\begin{align*} f(x',x') = f(x',x) + \\frac{\\partial}{\\partial v} f(x',x) (x'-x) + o( \\| x'-x\\| ) \\end{align*}\\] and . \\[\\begin{align*} f(x',x) = f(x,x) + \\frac{\\partial}{\\partial u} f(x,x) (x'-x) + o( \\| x'-x\\| )\\,. \\end{align*}\\] Putting these together we get . \\[\\begin{align*} f(x',x') &amp; = f(x,x) + \\left( \\frac{\\partial}{\\partial v} f(x',x) + \\frac{\\partial}{\\partial u} f(x,x) \\right) (x'-x) + o( \\|x'-x\\|) \\\\ &amp; = f(x,x) + \\left( \\frac{\\partial}{\\partial v} f(x,x) + \\frac{\\partial}{\\partial u} f(x,x) \\right) (x'-x) \\\\ &amp; \\qquad\\qquad\\,\\,+ \\left( \\frac{\\partial}{\\partial v} f(x',x) - \\frac{\\partial}{\\partial v} f(x,x)\\right) (x'-x) + o( \\|x'-x\\|) \\\\ &amp; = f(x,x) + \\left( \\frac{\\partial}{\\partial v} f(x,x) + \\frac{\\partial}{\\partial u} f(x,x) \\right) (x'-x) + o( \\|x'-x\\|) \\,. \\end{align*}\\] where the last equality follows if $\\frac{\\partial}{\\partial v} f(x’,x) - \\frac{\\partial}{\\partial v} f(x,x) = o(1)$ as $x’\\to x$, i.e., if $x’\\mapsto \\frac{\\partial}{\\partial v} f(x’,x)$ is continuous at $x’=x$. That the result also holds under the assumption that $x’\\mapsto \\frac{\\partial}{\\partial u} f(x,x’)$ is continuous at $x’=x$ follows from a symmetric argument. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec16/#the-proof-of-the-calculus-101-result",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec16/#the-proof-of-the-calculus-101-result"
  },"353": {
    "doc": "16. Policy gradients",
    "title": "Summary",
    "content": "While policy gradient methods remain extremely popular and the idea of directly searching in the set of policies is attractive, at the moment it appears that they not only lack theoretical support, but the theoretical results suggest that it is hard to find any setting where policy gradient methods would be provably competitive with alternatives. At minimum, they need careful choices of policy parameterizations and even in that case the update rule may need to be changed to guarantee efficiency and effectiveness, as we have seen above. As an approach to algorithm design their main advantage is their generality and a strong support through various software libraries. Compared to vanilla “dynamic programming” methods they make generally smaller, more incremental changes to the policies, which seems useful. However, this is also achieved by methods like Politex, which is derived using a “bound minimization” approach. While this may seem more ad hoc than following gradients, in fact, one may argue that following gradients is more ad hoc as it fails to guarantee good performance. However, perhaps the most important point here is that one should not care too much about how a method is derived, or what “interpretation” it may have (is Politex a gradient algorithm? does this matter?). What matters is the outcomes: In this case how the methods perform. It is thus wise to learn about all possible ways of designing algorithms as there remains much room for improving the performance of current algorithms. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec16/#summary",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec16/#summary"
  },"354": {
    "doc": "16. Policy gradients",
    "title": "Notes",
    "content": "TBA . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec16/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec16/#notes"
  },"355": {
    "doc": "16. Policy gradients",
    "title": "References",
    "content": ". | Sutton, R. S., McAllester, D. A., Singh, S. P., and Man-sour, Y. (1999). Policy gradient methods for reinforce-ment learning with function approximation. InNeuralInformation Processing Systems 12, pages 1057–1063. | Silver, David, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. 2014. “Deterministic Policy Gradient Algorithms.” In ICML. http://hal.inria.fr/hal-00938992/. | Bhandari, Jalaj, and Daniel Russo. 2019. “Global Optimality Guarantees For Policy Gradient Methods,” June. https://arxiv.org/abs/1906.01786v1. | Agarwal, Alekh, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. 2019. “On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1908.00261. | Mei, Jincheng, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. 2020. “On the Global Convergence Rates of Softmax Policy Gradient Methods.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2005.06392. | Zhang, Junyu, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. 2020. “Variational Policy Gradient Method for Reinforcement Learning with General Utilities.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2007.02151. | Bhandari, Jalaj, and Daniel Russo. 2020. “A Note on the Linear Convergence of Policy Gradient Methods.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2007.11120. | Chung, Wesley, Valentin Thomas, Marlos C. Machado, and Nicolas Le Roux. 2020. “Beyond Variance Reduction: Understanding the True Impact of Baselines on Policy Optimization.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2008.13773. | Li, Gen, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. 2021. “Softmax Policy Gradient Methods Can Take Exponential Time to Converge.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2102.11270. | . The paper to read about natural gradient methods: . | Martens, James. 2014. “New Insights and Perspectives on the Natural Gradient Method,” December. https://arxiv.org/abs/1412.1193v9. Last update: September, 2020. | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec16/#references",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec16/#references"
  },"356": {
    "doc": "16. Policy gradients",
    "title": "16. Policy gradients",
    "content": "In this last lecture on planning, we look at policy search through the lens of applying gradient ascent. We start by proving the so-called policy gradient theorem which is then shown to give rise to an efficient way of constructing noisy, but unbiased gradient estimates in the presence of a simulator. We discuss at a high level the ideas underlying gradient ascent and stochastic gradient ascent methods (as opposed to more common case in machine learning where the goal is to minimize a loss, or objective function, we are maximizing rewards, hence ascending on the objective rather than descending). We then find out about the limitations of policy gradient even in the presence of “perfect representation” (unrestricted policy classes, tabular case) and perfect gradient information, which motivates the introduction of a variant known as “natural policy gradients” (NPG). We then uncover a close relationship between this method and Politex. The lecture concludes with comparing results for NPG and Politex. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec16/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec16/"
  },"357": {
    "doc": "2. The Fundamental Theorem",
    "title": "Introduction",
    "content": "A Markov decision Process (MDP) is a 5-tuple $M = (\\mathcal{S}, \\mathcal{A}, P, r, \\gamma)$, where $\\mathcal{S}$ represents the state space, $\\mathcal{A}$ represents the action space, $P = (P_a(s))_{s,a}$ collects the next state distributions for each state-action pair (to represents the transition dynamics), \\(r= (r_a(s))_{s,a}\\) gives the immediate rewards incurred for taking a given action in a given state, and $0 \\leq \\gamma &lt; 1$ is the discount factor. As said before, for simplicity, the state set $\\mathcal{S}$ and the action set $\\mathcal{A}$ are finite. A policy $\\pi = (\\pi_t)_{t \\geq 0}$ is an infinite long sequence where for each $t\\ge 0$, $\\pi_t: (\\mathcal{S} \\times \\mathcal{A})^{t-1} \\times \\mathcal{S} \\rightarrow \\mathcal{M}_1(\\mathcal{A})$ assigns a probability distribution to histories of length $t$. Following a policy in an MDP means that the distribution of the actions in each time step $t\\ge 0$ will follow what is prescribed by the policy for whatever the history is at that time. When a policy is used in an MDP, the interconnection of the policy and the MDP, together with a start-state distribution, results in a distribution $\\mathbb{P}$ such that for the infinite long sequence of state-action pairs $S_0,A_0,S_1,A_1,\\dots$, $S_0 \\sim \\mu(\\cdot), A_t \\sim \\pi_t(\\cdot | H_t)$, and $S_{t+1} \\sim P_{A_t}(S_t)$ for all $t \\geq 0$ where $H_t = (S_0,A_0,\\dots,S_{t-1},A_{t-1},S_t)$ is the history at time step $t$. This closed loop interaction (or interconnection) of the policy and the MDP is shown in the figure below. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec2/#introduction",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec2/#introduction"
  },"358": {
    "doc": "2. The Fundamental Theorem",
    "title": "Probabilities over Trajectories",
    "content": "One loose end from the previous lecture was the existence of the probability measures \\(\\mathbb{P}_\\mu^\\pi\\). For this, we have the following result: . Theorem (existence theorem): Fix a finite MDP $M$ with state space $\\mathcal{S}$ and action space $\\mathcal{A}$. Then there exists a measurable space $(\\Omega,\\mathcal{F})$ and a sequence of random elements $S_0, A_0, S_1, A_1, \\ldots$ over this space, $S_t\\in \\mathcal{S}$, $A_t\\in \\mathcal{A}$ for $t\\ge 0$, such that for any policy $\\pi = (\\pi_t)_{t \\geq 0}$ of the MDP $M$ and any probability measure $\\mu \\in \\mathcal{M}_1(\\mathcal{S})$ over $\\mathcal{S}$, there exist a probability measure $\\mathbb{P}$ over $(\\Omega,\\mathcal{F})$ satisfying the following properties: . | $\\mathbb{P}(S_0 = s) = \\mu(s)$ for all $s \\in \\mathcal{S}$, | $\\mathbb{P}(A_t = a | H_t) = \\pi_t(a | H_t)$ for all $a \\in \\mathcal{A}, t \\geq 0$, and | $\\mathbb{P}(S_{t+1} = s’ | H_t, A_t) = P_{A_t}(S_t, s’)$ for all $s’ \\in \\mathcal{S}$. | . Proof: Use the Ionescu-Tulcea theorem (Theorem 3.3 in the “bandit book”). \\(\\qquad\\blacksquare\\) . The last property 3 is also known as the Markov property and is how MDPs derive their name. Oftentimes, it makes sense to take $\\Omega$ as the set of infinite long trajectories: $\\Omega = (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}$ and demand that $S_t((s_0,a_0,s_1,a_1,\\dots)) = s_t$ and $A_t((s_0,a_0,s_1,a_1,\\dots)) = a_t$ for all $t\\ge 0$. The resulting probability space is called the canonical probability space of the MDP (more precisely, of the given state and action spaces). That, unless we need other sources of randomness, we can take $\\Omega$ to be this set follows because the conditions on $\\mathbb{P}$ are all concerned with probabilities over states and actions. Formally, one needs to take the pushforward measure of $\\mathbb{P}$ under the map $\\omega \\mapsto (S_0(\\omega),A_0(\\omega),S_1(\\omega),A_1(\\omega),\\dots)$. Implicit in the definition that $\\mathcal{S}$ and $\\mathcal{A}$ are endowed with the discrete $\\sigma$-algebra. This is because we want both ${S_t = s}$ and ${A_t = a}$ be events for any $s\\in \\mathcal{S}$ and $a\\in \\mathcal{A}$. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec2/#probabilities-over-trajectories",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec2/#probabilities-over-trajectories"
  },"359": {
    "doc": "2. The Fundamental Theorem",
    "title": "Optimality and Some Notation",
    "content": "As usual, we use $\\mathbb{E}$ to denote the expectation operator underlying a probability measure $\\mathbb{P}$. When the dependence on $\\mu$ or $\\pi$ is important, we use $\\mathbb{E}_\\mu^\\pi$. We may drop any of these, when the dropped quantity is clear from the context. We will pay special attention to start state distributions concentrated on a single state. When this is state $s$, the distribution is denoted by $\\delta_s$: this is the well-known Dirac distribution with an atom at $s$. The reason we pay special attention to these is because these in a way form the basis of all start state distributions (and in fact quantities that depend linearly on start state distributions). We will use the shorthand \\(\\mathbb{P}_{s}^{\\pi}\\) for \\(\\mathbb{P}_{\\delta_s}^{\\pi}\\). Similarly, we use \\(\\mathbb{E}_{s}^{\\pi}\\) for \\(\\mathbb{E}_{\\delta_s}^{\\pi}\\). Define the return over a trajectory $\\tau = (S_0, A_0, S_1, A_1, \\ldots)$ as . \\[R = \\sum_{t=0}^{\\infty} \\gamma^t r_{A_t}(S_t).\\] The value function $v^\\pi$ of policy maps states to values and in particular for a state $s\\in\\mathcal{S}$, $v^\\pi(s)$ is defined via $v^\\pi(s) = \\mathbb{E}_{s}^{\\pi}[R]$: This is the expected return under the distribution induced by the interconnection of policy $\\pi$ and the MDP when the start state is $s$. Note that $v^\\pi(s)$ is well-defined. This is because it is the expectation of a quantity that is a function of the trajectory $\\tau$; for an explanation see the end-notes. The standard goal in an MDP is to identify a policy that maximizes this value in every state. A policy achieving this is known as an optimal policy. Whether an optimal policy exists at all is not clear at this stage. In any case, if it exist, an optimal policy must satisfy \\(v^\\pi = v^*\\) where $v^*:\\mathcal{S} \\to \\mathbb{R}$ is defined by . \\[v^{*}(s) = \\sup_{\\pi} v^{\\pi}(s)\\,, \\qquad s\\in \\mathcal{S}\\,.\\] By the definition of the optimal value function, we have \\(v^\\pi(s) \\leq v^{*}(s)\\) for all $s \\in \\mathcal{S}$ and any policy $\\pi$. We also use $v^\\pi \\le v^*$ to express this. In general, $f \\le g$ for two functions $f,g$ that are defined over the same domain and take values (say) in the reals, if $f(z)\\le g(z)$ holds for all the possible elements $z$ of their common domain. We similarly define $f\\ge g$. We will also identify functions with vectors and allow vector-space operations on them. All vectors, unless otherwise stated, are column vectors. The symbol $\\boldsymbol{1}$ is defined as a vector of ones. The length of this vector can change depending on the context. In this lecture, it will be $\\mathrm{S}$-dimensional. This symbol will be very useful in a number of calculations. We start with a definition that uses it. Approximately optimal policies . Let $\\varepsilon&gt;0$. A policy $\\pi$ is said to be $\\varepsilon$-optimal if . \\[v^\\pi \\ge v^* - \\varepsilon \\boldsymbol{1}\\,.\\] Finding an $\\varepsilon$-optimal policy with a positive $\\varepsilon$ should intuitively be easier than finding an optimal policy. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec2/#optimality-and-some-notation",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec2/#optimality-and-some-notation"
  },"360": {
    "doc": "2. The Fundamental Theorem",
    "title": "Memoryless Policies (ML)",
    "content": "If optimal policies would need to remember the past of arbitrary length, it would be hopeless to search for efficient algorithms that can compute them as even describing them could take infinite time. Luckily, this is not the case. In finite MDPs, it will turn out to be sufficient to consider policies that use only the most recent state without losing optimality: this is the subject of the fundamental theorem of MDPs, which we will give shortly. We call the policies that take only the most recent state into account memoryless. Formally, a memoryless policy can be identified with a map from the states to probability distributions over the actions: \\(m: \\mathcal{S}\\to \\mathcal{M}_1(\\mathcal{A})\\). Given $m$, the memoryless policy, using our previous policy notation, is $\\pi_t(a|s_0,a_0,\\dots,s_{t-1},a_{t-1},s_t) = m(a|s_t)$, where we abuse notation by using $m(a|s_t)$ in place of $m(s_t)(a)$. Thus, as expected, the policy itself “forgets” the past and just uses the most recent state in assigning probabilities to the individual actions. Under a distribution induced by interconnecting a memoryless policy with an MDP, the sequence of state-action pairs forms a Markov chain. In what follows, by abusing notation further, when it comes to a memoryless policy, we will identify $\\pi$ with $m$ and will just write $\\pi: \\mathcal{S} \\to \\mathcal{M}_1(\\mathcal{A})$. For building up to the proof of the fundamental theorem, we start with the concept of discounted occupancy measures. (Discounted) Occupancy Measure . Given a start state distribution \\(\\mu \\in \\mathcal{M}_1(\\mathcal{S})\\) and a policy \\(\\pi\\), the (discounted) occupancy measure \\(\\nu_\\mu^\\pi \\in \\mathcal{M}_1(\\mathcal{S} \\times \\mathcal{A})\\) induced by \\(\\mu\\) and \\(\\pi\\) and the underlying MDP \\(M\\) is defined as . \\[\\nu_\\mu^\\pi(s, a) = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{P}_\\mu^\\pi (S_t = s, A_t = a).\\] Interestingly, the value function can be represented as an inner product between the immediate reward function $r$ and the occupancy measure $\\nu_\\mu^\\pi$: . \\[\\begin{align*} v^\\pi(\\mu) &amp;= \\mathbb{E}_\\mu^\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r_{A_t}(S_t) \\right] \\\\ &amp;= \\sum_{s, a} \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_\\mu^\\pi \\left[ r_{A_t}(S_t) \\mathbb{I}(S_t = s, A_t = a) \\right] \\\\ &amp;= \\sum_{s, a} r_{a}(s) \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_\\mu^\\pi \\left[ \\mathbb{I}(S_t = s, A_t = a) \\right] \\\\ &amp;= \\sum_{s, a} r_{a}(s) \\sum_{t=0}^\\infty \\gamma^t \\mathbb{P}_\\mu^\\pi(S_t = s, A_t = a) \\\\ &amp;= \\sum_{s, a} r_a(s) \\nu_\\mu^\\pi(s, a) \\\\ &amp;=: \\langle \\nu_\\mu^\\pi, r \\rangle, \\end{align*}\\] where $\\mathbb{I}(S_t = s, A_t = a)$ is the indicator of the event ${S_t=s,A_t=a}$, which gives the value of one when the event holds (i.e., $S_t = s$ and $A_t = a$), and gives zero otherwise. That the summation over $(s,a)$ can be moved outside of the expectation in the first equality follows because expectations are linear. That the infinite sum can be moved outside is more subtle: this follows from Lebesgue’s dominated convergence theorem. See, for example, Chapter 2 of Lattimore &amp; Szepesvári (2020). With the above equation, we see that the problem of maximizing the expected reward for a given initial distribution is the same as choosing a policy that “stirs” the occupancy measure to maximally align with the reward vector $r$. A better alignment will result in a higher value for the policy. This is depicted in the figure below. A key step in proving the sufficiency of memoryless policies for optimal control is the following result: . Theorem: For any policy $\\pi$ and a start state distribution $\\mu \\in \\mathcal{M}_1(\\mathcal{S})$, there exists a memoryless policy $\\pi’$ such that . \\[\\nu_\\mu^{\\pi'} = \\nu_\\mu^{\\pi}.\\] Proof (hint): First define the occupancy measure over the state space \\(\\tilde{\\nu}_\\mu^\\pi(s) := \\sum_a \\nu_\\mu^\\pi(s, a)\\). Then show that the theorem statement holds for the policy $\\pi’$ defined as follows: . \\[\\pi'(a | s) = \\begin{cases} \\frac{\\nu_\\mu^\\pi(s, a)}{\\tilde{\\nu}_\\mu^\\pi(s)} &amp; \\text{if } \\tilde{\\nu}_\\mu^\\pi(s) \\neq 0 \\\\ \\pi_0(a) &amp; \\text{otherwise,} \\end{cases}\\] where \\(\\pi_0(a) \\in \\mathcal{M}_1(\\mathcal{A})\\) is an arbitrary distribution. To do this, expand $\\tilde \\nu_\\mu^\\pi$ using the definition of discounted occupancy measures and use algebra. \\[\\tag*{$\\blacksquare$}\\] Note that it is crucial that the memoryless policy obtained depends on the start state distribution: The reader should try to convince themselves that there are non-memoryless policies whose value function cannot be reproducec by a memoryless policy at every state. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec2/#memoryless-policies-ml",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec2/#memoryless-policies-ml"
  },"361": {
    "doc": "2. The Fundamental Theorem",
    "title": "Bellman Operators, Contractions",
    "content": "The last definitions and results that we need before stating the fundamental theorem concern what are known as Bellman operators. Fix a memoryless policy $\\pi$. Recall that $\\mathrm{S}$ is the cardinality (size) of $\\mathcal{S}$. First, define $r_\\pi(s) = \\sum_a \\pi(a|s) r_a(s)$ to be the expected reward under policy $\\pi$ for a given state $s$. Again, we overload the notation and let $r_\\pi \\in \\mathbb{R}^{\\mathrm{S}}$ denote a vector whose $s$th element \\((r_\\pi)_s = r_\\pi(s)\\). Similarly, we define $P_\\pi(s, s’) := \\sum_a \\pi(a|s) P_a(s, s’)$ and let $P_\\pi \\in [0, 1]^{\\mathrm{S} \\times \\mathrm{S}}$ denote the stochastic transition matrix where the element in the \\(s\\)th row and \\(s'\\)th column \\((P_\\pi)_{s, s'} = P_\\pi(s, s')\\). Note that each row of $P_\\pi$ sums to one: . \\[P_\\pi \\mathbf{1} = \\mathbf{1}\\,.\\] The Bellman/policy evaluation operator underlying $\\pi$, $T_\\pi: \\mathbb{R}^{\\mathrm{S}} \\rightarrow \\mathbb{R}^{\\mathrm{S}}$, is defined as . \\[\\begin{align*} T_\\pi v(s) &amp;= \\sum_a \\pi(a|s) \\left \\{r_a(s) + \\gamma \\sum_{s'} P_a(s, s') v(s') \\right \\} \\\\ &amp;= \\sum_a \\pi(a|s) \\left \\{r_a(s) + \\gamma \\langle P_a(s), v \\rangle \\right \\} \\end{align*}\\] or, in short, . \\[T_\\pi v = r_\\pi + \\gamma P_\\pi v,\\] where $v \\in \\mathbb{R}^{\\mathrm{S}}$. The Bellman operator performs a one-step lookahead (also called a Bellman lookahead) on the value function. We will use the notations $(T_\\pi(v))(s)$, $T_\\pi v(s)$, and \\((T_\\pi v)_s\\) interchangeably. $T_\\pi$ is also known as the policy evaluation operator for the policy $\\pi$. The Bellman optimality operator $T: \\mathbb{R}^{\\mathrm{S}} \\rightarrow \\mathbb{R}^{\\mathrm{S}}$ is defined as . \\[T v(s) = \\max_a \\{ r_a(s) + \\gamma \\langle P_a(s), v \\rangle \\}.\\] We use \\(\\|\\cdot\\|_\\infty\\) to denote the maximum-norm: \\(\\| v \\|_{\\infty} = \\max_i |v_i|\\). The maximum-norm is a “good friend” of the operators we just defined. This is because stochastic matrices, viewed as operators and “maximizing” are “good friends” of this norm. All this results in the following proposition: . Proposition ($\\gamma$-contraction of the Bellman Operators): Given any two vectors $u, v \\in \\mathbb{R}^{\\mathrm{S}}$ and any memoryless policy \\(\\pi\\), . | \\(\\|T_\\pi u - T_\\pi v\\|_\\infty \\leq \\gamma \\|u - v\\|_\\infty\\), and | \\(\\|T u - T v\\|_\\infty \\leq \\gamma \\|u - v\\|_\\infty\\). | . The proposition can be proved by elementary algebra and the complete proof can be found in Appendix A.2 of Szepesvári (2010). For action $a\\in \\mathcal{A}$, we will find it useful to also define the operator $T_a: \\mathbb{R}^{\\mathrm{S}} \\to \\mathbb{R}^{\\mathrm{S}}$ which matches $T_\\pi$ with the memoryless policy which in every state chooses action $a$. Of course, this operator, being a special case, satisfies the above contraction property as well. This can be seen as performing a one-step lookahead with a fixed action. From Banach’s fixed point theorem, we get the following corollary: . Proposition (Fixed-point iteration): Given any $u \\in \\mathbb{R}^{\\mathrm{S}}$ and any memoryless policy $\\pi$, . | \\(v^\\pi = \\lim_{k\\to\\infty} T_\\pi^k u\\) and in particular for any $k\\ge 0$, \\(\\| v^\\pi - T_\\pi^k u \\|_\\infty \\le \\gamma^k \\| u - v^\\pi \\|_\\infty\\) where \\(v^\\pi\\) is the unique vector/function that satisfies \\(T_\\pi v^\\pi = v^\\pi\\); | \\(v_\\infty=\\lim_{k\\to\\infty} T^k u\\) is well-defined and in particular for any $k\\ge 0$, \\(\\| v_\\infty - T^k u \\|_\\infty \\le \\gamma^k \\| u - v_\\infty \\|_\\infty\\). Furthermore, \\(v_\\infty\\) is the unique vector/function that satisfies \\(Tv_\\infty = v_\\infty\\). | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec2/#bellman-operators-contractions",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec2/#bellman-operators-contractions"
  },"362": {
    "doc": "2. The Fundamental Theorem",
    "title": "The Fundamental Theorem",
    "content": "Definition: A memoryless policy $\\pi$ is greedy w.r.t. to a value function $v: \\mathcal{S} \\rightarrow \\mathbb{R}$ if in every state $s\\in \\mathcal{S}$, with probability one $\\pi$ chooses actions that maximize $(T_a v)(s)=r_a(s) + \\gamma \\langle P_a(s), v \\rangle$. Note that there can be more than one action that maximizes the (one-step) Bellman lookahead $(T_a v)(s)$ at any given state (in case there are ties). In fact, ties can be extremely common: Just imagine “duplicating an action” in every state (i.e., the new action has the same associated transitions and rewards as the copied one). If the copied one was maximizing the Bellman lookahead at some state, the new action will do the same. Because we have finitely many actions, a maximizing action always exist. Thus, we can always “take” a greedy policy w.r.t. any $v\\in \\mathbb{R}^{\\mathrm{S}}$. Proposition (Characterizing greedyness): A memoryless policy $\\pi$ is greedy w.r.t. $v\\in \\mathbb{R}^{\\mathrm{S}}$ if and only if . \\[T_\\pi v = T v\\,.\\] With this, we are ready to state what I call the Fundamental Theorem of MDPs: . Theorem (Fundamental Theorem of MDPs): The following hold true in any finite MDP: . | Any policy $\\pi$ that is greedy with respect to $v^*$ is optimal: \\(v^\\pi = v^*\\); | It holds that \\(v^* = T v^*\\). | . The equation $v=Tv$ is known as the Bellman optimality equation and the second part of the result can be stated in words by saying that the optimal value function satisfies the Bellman optimality equation. Also, our previous proposition on fixed-point iteration, where we already came across the Bellman optimality equation, foreshadows a way of approximately computing \\(v^*\\) that we will get back to after the proof. Proof: The proof would be easy if we only considered memoryless policies when defining $v^*$. In particular, letting $\\text{ML}$ stand for the set of memoryless policies of the given MDP, define . \\[\\tilde{v}^*(s) = \\sup_{\\pi \\in \\text{ML}} v^\\pi(s) \\quad \\text{for all } s \\in \\mathcal{S}\\,.\\] As we shall see soon, it is not hard to show the theorem just with \\(v^*\\) replaced everywhere with \\(\\tilde{v}^*\\). That is: . | Any policy $\\pi$ that is greedy with respect to $\\tilde{v}^*$ satisfies \\(v^\\pi = \\tilde{v}^*\\); | It holds that \\(\\tilde{v}^* = T \\tilde{v}^*\\). | . This is what we will show in Part 1 of the proof, while in Part 2 we will show that \\(\\tilde{v}^*=v^*\\). Clearly, the two parts together establish the desired result. Part 1: The idea of the proof is to first show that \\(\\begin{align} \\tilde{v}^*\\le T \\tilde{v}^* \\label{eq:suph} \\end{align}\\) and then show that for any greedy policy $\\pi$, \\(v^\\pi \\ge \\tilde{v}^*\\). The displayed equation follows by noticing that \\(v^\\pi \\le \\tilde{v}^*\\) holds for all memoryless policies $\\pi$ by definition. Applying $T_\\pi$ on both sides, using $v^\\pi = T_\\pi v^\\pi$, we get $v^\\pi \\le T_\\pi \\tilde{v}^*$. Taking the supremum of both sides over $\\pi$ and noticing that $T v = \\sup_{\\pi \\in \\text{ML}} T_\\pi v$ for any $v$, together with the definition of \\(\\tilde{v}^*\\) gives \\(\\eqref{eq:suph}\\). Now, take any memoryless policy $\\pi$ that is greedy w.r.t. \\(\\tilde{v}^*\\). Thus, \\(T_\\pi \\tilde{v}^* = T \\tilde{v}^*\\). Combined with \\(\\eqref{eq:suph}\\), we get . \\[\\begin{align} \\label{eq:start} T_\\pi \\tilde{v}^* \\ge \\tilde{v}^*\\,. \\end{align}\\] Applying $T_\\pi$ on both sides and noticing that $T_\\pi$ keeps the inequality intact (i.e., for any $u,v$ such that $u\\le v$ we get $T_\\pi u \\le T_\\pi v$), we get . \\[T_\\pi^2 \\tilde{v}^* \\ge T_\\pi \\tilde{v}^* \\ge \\tilde{v}^*\\,,\\] where the last inequality follows from \\(\\eqref{eq:start}\\). With the same reasoning we get that for any $k\\ge 0$, . \\[T_\\pi^k \\tilde{v}^* \\ge T_\\pi^{k-1} \\tilde{v}^* \\ge \\dots \\ge \\tilde{v}^*\\,,\\] Now, by our proposition, the fixed-point iteration $T_\\pi^k \\tilde{v}^*$ converges to $v^\\pi$. Hence, taking the limit above, we get . \\[v^\\pi \\ge \\tilde{v}^*.\\] This, together with \\(v^\\pi \\le \\tilde{v}^*\\) shows that \\(v^\\pi = \\tilde{v}^*\\). Finally, $T \\tilde{v}^* = T_\\pi \\tilde{v}^* = T_\\pi v^\\pi = v^\\pi = \\tilde{v}^*$. Part 2: It remains to be shown that \\(\\tilde{v}^* = v^*\\). Let $\\Pi$ be the set of all policies. Because $\\text{ML}\\subset \\Pi$, \\(\\tilde{v}^*\\le v^*\\). Thus, it remains to show that . \\[\\begin{align} \\label{eq:mlbigger} v^* \\le \\tilde{v}^*\\,. \\end{align}\\] To show this, we will use the theorem that guaranteed that for any state-distribution $\\mu$ and policy $\\pi$ (memoryless or not) we can find a memoryless policy, which we will call for now $\\text{ML}(\\pi)$, such that $\\nu_\\mu^\\pi = \\nu_\\mu^{\\text{ML}}$. Fix a state $s\\in \\mathcal{S}$. Applying this result with $\\mu = \\delta_s$, we get . \\[\\begin{align*} v^\\pi(s) &amp; = \\langle \\nu_s^\\pi, r \\rangle \\\\ &amp; = \\langle \\nu_s^{\\text{ML}(\\pi)}, r \\rangle \\\\ &amp; \\le \\sup_{\\pi'\\in \\text{ML}} \\langle \\nu_s^{\\pi'}, r \\rangle \\\\ &amp; = \\sup_{\\pi'\\in \\text{ML}} v^{\\pi'}(s) = \\tilde{v}^*(s)\\,. \\end{align*}\\] Taking the supremum of both sides over $\\pi$, we get \\(v^*(s)= \\sup_{\\pi\\in \\Pi} v^\\pi(s) \\le \\tilde{v}^*(s)\\). Since $s\\in \\mathcal{S}$ was arbitrary, we get \\(v^*\\le \\tilde{v}^*\\), finishing the proof. \\(\\qquad\\blacksquare\\) . A property that came up during the proof that we will repeatedly use is that $T_\\pi$ is monotone as an operator. The same holds for $T$. For the record, we state these as a proposition: . Proposition (monotonicity of Bellman operators): For any memoryless policy $\\pi$, $T_\\pi u \\le T_\\pi v$ holds for any $u,v\\in \\mathbb{R}^{\\mathrm{S}}$ such that $u\\le v$. The same also holds for $T$, the Bellman optimality operator. According to the Fundamental Theorem of MDPs, if we have access to the optimal value function \\(v^*\\), then we can find the optimal policy in an efficient and effective way. We just have to greedify it w.r.t. to the value function: (abusing the policy notation) \\(\\pi(s) = \\arg\\max_{a \\in \\mathcal{A}} \\{r_a(s) + \\gamma \\langle P_a(s), v^* \\rangle \\} \\quad \\forall s \\in \\mathcal{S}\\). Such a greedy policy can be found in $O(\\mathrm{S}^2 \\mathrm{A})$ time. Hence, if we can efficiently find the optimal value function, we will get an efficient way of computing an optimal policy. This is to be contrasted with the naive approach to finding an optimal policy, which is to enlist all the policies and compare their value functions to find a policy whose value function dominates the value functions of all the other policies. However, even if we restrict ourselves to just the set of deterministic policies, there are $\\Theta(\\mathrm{A}^{\\mathrm{S}})$ such policies and thus this can be a costly procedure. As it turns out, for finite MDPs, there is a way to calculate optimal policies in time that is polynomial in $\\mathrm{S}$, $\\mathrm{A}$, and $1/(1-\\gamma)$, avoiding the exponential growth of the naive approach with the size of the state space. Algorithms that can do this belong to the family of dynamic programming algorithms. For our purposes, we call any algorithm a dynamic programming algorithm that uses the idea of keeping track of value of states (that is, uses value functions) while doing its calculations. The Fundamental Theorem is somewhat surprising: how come that we can find policies whose value function dominates that of all other policies? In a way, the Fundamental Theorem tells us that the set of value functions of all policies in some MDP (as a set in $\\mathbb{R}^{\\mathrm{S}}$) is very special: It has a “vertex” which dominates all the other value functions. This is quite fascinating. Of course, the key was the Markov property as this gave us the tool to show the result that allowed us to switch from arbitrary policies to memoryless ones. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec2/#the-fundamental-theorem",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec2/#the-fundamental-theorem"
  },"363": {
    "doc": "2. The Fundamental Theorem",
    "title": "Value Iteration",
    "content": "By the Fundamental Theorem, \\(v^*\\) is the fixed point of $T$. By our earlier proposition, which built on the Banach’s fixed point theorem, the sequence \\(\\{T^k v\\}_{k\\ge 0}\\) converges to \\(v^*\\) at a geometric rate. In the context of MDPs, the process of repeatedly applying $T$ to some function is called value iteration. The initial function is usually taken to be the all-zero function, which we denote by $\\mathbf{0}$, but, of course, if there is a better initial guess on \\(v^*\\), that guess can also be used at initialization. The next result gives a bound on the number of iterations required to reach an $\\varepsilon$-neighborhood (in the max-norm sense) of $v^*$: . Theorem (Value Iteration): Consider an MDP with immediate rewards in the $[0,1]$ interval. Pick an arbitrary positive number $\\varepsilon&gt;0$. Let $v_0 = \\boldsymbol{0}$ and set . \\[v_{k+1} = T v_k \\quad \\text{for } k = 0, 1, 2, \\ldots\\] Then, for $k\\ge \\ln(1/(\\varepsilon(1-\\gamma))/\\ln(1/\\gamma)$, \\(\\|v_k -v^*\\|_\\infty \\le \\varepsilon\\). Before the proof recall that . \\[H_{\\gamma,\\varepsilon}:= \\frac{\\ln(1/(\\varepsilon(1-\\gamma)))}{1-\\gamma} \\ge \\frac{\\ln(1/(\\varepsilon(1-\\gamma)))}{\\ln(1/\\gamma)}\\,.\\] Thus, the effective horizon, $H_{\\gamma,\\varepsilon}$, whom we met in the first lecture, appeared again. Of course, this is no coincidence. Proof: By our assumptions on the rewards, $\\mathbf{0} \\le v^\\pi \\le \\frac{1}{1-\\gamma} \\mathbf{1}$ holds for any policy $\\pi$. Hence, \\(\\|v^*\\|_\\infty \\le \\frac{1}{1-\\gamma}\\) also holds. By our fixed-point iteration proposition, we get . \\[\\begin{align*} \\|v_k - v^*\\|_\\infty &amp;\\leq \\gamma^k \\|v^* - \\mathbf{0}\\|_\\infty = \\gamma^k \\|v^*\\|_\\infty \\leq \\frac{\\gamma^k}{1 - \\gamma} \\,. \\end{align*}\\] Solving for the smallest $k$ such that \\(\\gamma^k/(1-\\gamma)\\le \\varepsilon\\) gives the result. \\[\\tag*{$\\blacksquare$}\\] For fixed $\\gamma&lt;1$, note the mild dependence of the iteration complexity on the target accuracy $\\varepsilon$: we can expect with only a handful iterations to get in a small vicinity of $v^*$. Note also that the total computation cost is $O(\\mathrm{S}^2 \\mathrm{A}k)$ and the space required is at most $O(\\mathrm{S})$, all assuming each value takes up $O(1)$ memory and arithmetic and logic operations also require $O(1)$ time. Note that accuracy requirement was set up in the form of additive errors. If the value function \\(v^*\\) is of order $1/(1-\\gamma)$ (the maximum possible order), a relative accuracy of order $2$ means setting $\\epsilon=0.5/(1-\\gamma)$, making the iteration complexity to be $\\ln(2)/(1-\\gamma)$. However, for controlling the relative error, the more interesting case is when \\(v^*\\) takes on small values. Here, we see that the complexity may grow unbounded. Later, we will see that in a way this lack of fine-grained error control of value iteration will mean that value iteration is not ideal for calculating exactly optimal policies. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec2/#value-iteration",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec2/#value-iteration"
  },"364": {
    "doc": "2. The Fundamental Theorem",
    "title": "Notes",
    "content": "Value functions are well-defined . As noted in the text, value functions are well-defined despite that the probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ is not uniquely defined. In fact, for any $f: (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}} \\to \\mathbb{R}$ (measurable) function and for any $(\\Omega,\\mathcal{F},\\mathbb{P})$ and $(\\Omega’,\\mathcal{F}’,\\mathbb{P}’)$ probability spaces, as long as both $\\mathbb{P}$ and $\\mathbb{P}’$ satisfy the requirements postulated in the existence theorem, \\(\\int f(\\tau(\\omega)) \\mathbb{P}(d\\omega)=\\int f(\\tau(\\omega)) \\mathbb{P}'(d\\omega)\\), or, introducing $\\mathbb{E}$ ($\\mathbb{E}’$) to denote the expectation operator underlying $\\mathbb{P}$ (respectively, $\\mathbb{P}’$), \\(\\mathbb{E}[f(\\tau)]=\\mathbb{E}'[f(\\tau)]\\). It also follows that if we only need probabilities and expectations over trajectories, it suffices to choose $(\\Omega,\\mathcal{F},\\mathbb{P})$ as the canonical probability space induced by the state-action space of the MDP at hand. Other types of MDPs . The obvious question is what survives of all this in other types of MDPs, such as finite-horizon homogenous or inhomogeneous, with or without discounting, total cost (i.e. negative rewards only), or of course the average cost setting? The story is that the arguments can be usually made to work, but this is not entirely automatic. The subject is well-studied and we will give some references and hints later, perhaps even answer some of these questions. Infinite spaces anyone? . The first thing that changes when we switch to infinite spaces is that we cannot take the assumption that the immediate rewards are bounded for granted. This can cause quite a bit of trouble: $v^\\pi$ for some policies can be unbounded, and the same holds for $v^*$. Negative infinite values could be especially “hurtful”. (LQR control is the simplest example where this comes up.) . Another issue is that we cannot take the existence of greedy policies for granted. This happens already when the number of actions is infinite (what is the action that maximizes the reward $r_a(s)=1-1/a$ where $a&gt;0$?). Oftentimes compactness of the action space and continuity assumptions help with this. Though it is not entirely clear why approximate greediness is not sufficient. In fact, as we shall see later, it is often sufficient when we care about approximate computations. Finally, when either the state or action space is uncountably infinite, one has to be careful even with the definition of policies. Now, these will be restricted to probability kernels. This requires putting measurability structures over both the state and action spaces (this is only crucial when either respective set has a larger than countable cardinality). The main change here is that with such a mapping, $h_t \\mapsto \\pi_t(U|h_t)$ must be measurable regardless the choice of $U$ (a measurable subset of $\\mathcal{A}$). This allows us to use the Ionescu-Tulcea theorem and at least the definitions can be made to work. A second difficulty in this case is that “greedification” may lead to outside of the set of these “measurable policies”. There is a large literature concerning these issues. From infinite trajectories to their finite prefixes . Since trajectories are allowed to be infinitely long, we have a nonconstructive result only for the existence of the probability measures induced by the interconnection of policies and MDPs. Oftentimes we need to check whether two probability measures over these infinitely long trajectories coincide. How can this be done? A general result from measure theory says that two measures agree, if they agree of a generator of the underlying $\\sigma$-algebra. A convenient generator system for the $\\sigma$-algebra over the trajectories (for the canonical probability space) is the system whose elements take the form . \\[\\{s_0\\} \\times \\{a_0\\} \\times \\dots\\times \\{ s_t \\} \\times \\mathcal{A} \\times (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}\\] and . \\[\\{s_0\\} \\times \\{a_0\\} \\times \\dots\\times \\{ s_t \\} \\times \\{a_t\\} \\times (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}\\] for some $s_0,a_0,\\dots,s_t,a_t,\\dots$. That is, if $\\mathbb{P}$ and $\\mathbb{P}’$ agree on the probabilities assigned to these sets, they agree everywehere. This makes things a full circle: what this result says is that we only need to check the probabilities assigned to finite prefixes of the infinitely long trajectories. Phew. Since the probabilities assigned to these finite prefixes are a function of $\\mu$, $P$ and $\\pi$ alone, it follows that there is a unique probability measure over the trajectory space $ (\\mathcal{S}\\times \\mathcal{A})^{\\mathbb{N}}$ that satisfies the requirements postulated in the existence theorem. That is, the canonical probability space is uniquely defined. Fundamental Theorem . I think I have seen Bertsekas and Shreve call the theorem I call fundamental also by the same name. However, this is not quite a standard name. Nevertheless, the result is important and many other things follow from it. In a way, this is the result that is at the heart of all the theory. I think it deserves this name. I have probably read the proof presented here somewhere, but this was a while ago and the source escapes me. In the RL literature people often start with memoryless policies and work with \\(\\tilde{v}^*\\) rather than with \\(v^*\\). The question whether \\(\\tilde{v}^*=v^*\\) is well-studied and understood, mostly in the control and operations research literature. Banach’s fixed point theorem . This theorem can be found in Appendix A.1 of my short RL book (Szepesvári, 2010). However, of course, it can be found in many places (the Wikipedia article is also OK). It is worthwhile to spend some time with this theorem to understand its conditions, going back to concepts like Cauchy-sequences (which should perhaps be called sequences with vanishing oscillations) and completeness of the set of real numbers. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec2/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec2/#notes"
  },"365": {
    "doc": "2. The Fundamental Theorem",
    "title": "References",
    "content": "The references mentioned before: . | Lattimore, T., &amp; Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press. | Szepesvári, C. (2010). Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence and machine learning, 4(1), 1-103. | . The next work (a book chpater) gives a concise yet relatively thorough introduction. The chapter also gives a proof of the fundamental theorem; through the sufficiency of Markov policies. This is done for the discounted and also for a number of alternate criteria. | Garcia, Frédérick, and Emmanuel Rachelson. 2013. “Markov Decision Processes.” In Markov Decision Processes in Artificial Intelligence, 1–38. Hoboken, NJ USA: John Wiley &amp; Sons, Inc. | . A summary of basic results for countable and Borel state-space, and Borel action spaces, with potentially unbounded (from below) reward functions can be found in the next (excellent) paper, which also gives a concise overview of the history of these results: . | Feinberg, Eugene A. 2011. Total Expected Discounted Reward MDPS: Existence of Optimal Policies. In Wiley Encyclopedia of Operations Research and Management Science. Hoboken, NJ, USA: John Wiley &amp; Sons, Inc. | . An argument showing the fundamental theorem for the finite-horizon case derived from a general result of David Blackwell can be found in a blog-post of Maxim Raginsky, who gives further pointers, most notable this. David Blackwell has contributed in numerous ways to the foundations of statistics, decision theory, probability theory, and many many other subjects and the importance of his work cannot be overstated. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec2/#references",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec2/#references"
  },"366": {
    "doc": "2. The Fundamental Theorem",
    "title": "2. The Fundamental Theorem",
    "content": "We start by recapping the definition of MDPs and then firm up the loose ends from the previous lecture: why do the probability distributions \\(\\mathbb{P}_\\mu^\\pi\\) exist and how are they defined? We then continue with the introduction of what we call the Fundamental Theorem of Dynamic Programming and end with the discussion of value iteration. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec2/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec2/"
  },"367": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "Finding a Near-Optimal Policy using Value Iteration",
    "content": "In the previous lecture we found that the iterative computation that starts with some $v_0\\in \\mathbb{R}^{\\mathrm{S}}$ and then obtains $v_{k+1}$ using the “Bellman update” . \\[\\begin{align} v_{k+1} = T v_k \\label{eq:vi} \\end{align}\\] leads to a sequence \\(\\{v_k\\}_{k\\ge 0}\\) whose \\(k\\)th term approaches \\(v^*\\), the optimal value function, at a geometric rate: . \\[\\begin{align} \\| v_k - v^* \\|_\\infty \\le \\gamma^k \\| v_0 - v^* \\|_\\infty\\,. \\label{eq:vierror} \\end{align}\\] While this is reassuring, our primary goal is to obtain an optimal, or at least a near-optimal policy. Since any policy that is greedy with respect to (w.r.t) \\(v^*\\) is optimal, a natural idea is to stop the value iteration after some finite number of iteration steps and return a policy that is greedy w.r.t. the approximation of \\(v^*\\) that was just obtained. If we stop the process after the \\(k\\)th step, this defines a policy \\(\\pi_k\\) such that \\(\\pi_k\\) is greedy w.r.t. \\(v_k\\): \\(T_{\\pi_k} v_k = T v_k\\). The hope is that as \\(v_k\\) approaches \\(v^*\\), the policies \\(\\{\\pi_k\\}\\) will also get better in the sense that \\(\\|v^*-v^{\\pi_k}\\|_\\infty\\) decreases. The next theorem guarantees that this will indeed be the case. Theorem (Policy Error Bound): Let \\(v: \\mathcal{S} \\to \\mathbb{R}\\) be arbitrary and \\(\\pi\\) be the greedy policy w.r.t. \\(v\\): \\(T_\\pi v = T v\\). Then, . \\[v^\\pi \\geq v^* - \\frac{2 \\gamma \\|v^*-v\\|_\\infty}{1 - \\gamma} \\boldsymbol{1}.\\] . In words, the theorem states that the policy error (\\(\\|v^*-v^{\\pi}\\|_\\infty\\)) of a policy that is greedy with respect to a function \\(v\\) is controlled by the distance of $v$ to \\(v^*\\). Let $\\Gamma$ be an operator that maps functions over $\\mathcal{S}$ to policies by assigning to a function $v$ the policy that is greedy w.r.t. $v$. With this, a shorter form of the theorem statement is that for any $v\\in \\mathbb{R}^{\\mathcal{S}}$, . \\[\\| v^*-v^{\\Gamma(v)}\\|_\\infty \\le \\frac{2\\gamma \\|v^*-v\\|_\\infty}{1-\\gamma}\\,.\\] Alternatively, the theorem can be seen as stating that the map \\(v \\mapsto \\|v^*-v^{\\Gamma(v)}\\|_\\infty\\) is “\\(2\\gamma/(1-\\gamma)\\)-smooth” with respect to the maximum norm distance at \\(v=v^*\\). The proof is an archetypical example of proofs of using contraction and monotonicity arguments to prove error bounds. We will see variations of this proof many times. Before the proof, let us introduce the notation $|x|$ for a vector $\\mathbb{R}^d$ to mean the componentwise absolute value of the vector: \\(|x|_i = |x_i|\\), \\(i\\in [d]\\). As a way of using this notation, note that for any memoryless policy $\\pi$, \\(\\begin{align} |P_\\pi x |\\le P_\\pi |x| \\le \\|x\\|_\\infty P_\\pi \\boldsymbol{1} = \\|x\\|_\\infty \\boldsymbol{1}\\,, \\label{eq:ppineb} \\end{align}\\) and hence \\(\\begin{align} \\|P_\\pi x \\|_\\infty \\le \\|x\\|_\\infty\\,. \\label{eq:stochmxne} \\end{align}\\) In Eq. \\(\\eqref{eq:ppineb}\\) the first inequality follows because $P_\\pi$ is monotone and \\(x\\le |x| \\le \\|x\\|_\\infty \\boldsymbol{1}\\). For the proof it will also be useful to recall that we also have . \\[\\begin{align} T_\\pi (v+c \\boldsymbol{1}) &amp;= T_\\pi v \\,\\, + c \\gamma \\boldsymbol{1}\\,, \\label{eq:tpiadd1} \\\\ T (v+c \\boldsymbol{1}) &amp;= T v \\,\\, + c \\gamma \\boldsymbol{1}\\,, \\label{eq:tadd1} \\end{align}\\] for any \\(v\\in \\mathbb{R}^{\\mathrm{S}}\\), \\(c\\in \\mathbb{R}\\) and memoryless policy \\(\\pi\\). These two identities follow just by the definitions of $T$ and $T_\\pi$, as the reader can easily verify them. Proof: Let \\(v,v^*,\\pi\\) be as in the theorem statement and let \\(\\varepsilon = \\|v^*-v\\|_\\infty\\). Let \\(\\delta = v^*-v^\\pi\\). The result follows by algebra once we prove that \\(\\|\\delta\\|_\\infty \\le \\gamma \\|\\delta\\|_\\infty + 2\\gamma \\varepsilon\\). Hence, we only need to prove this inequality. By our assumptions on \\(v\\) and \\(v^*\\), \\(-\\varepsilon\\boldsymbol{1}\\le v^*-v \\le \\varepsilon\\boldsymbol{1}\\). Now, . \\[\\begin{align*} \\delta &amp; = v^*-v^\\pi \\\\ &amp; = \\textcolor{red}{T} v^* - \\textcolor{red}{T_\\pi} v^\\pi &amp; \\text{(Fundamental Theorem, $T_\\pi v^\\pi = v^\\pi$)}\\\\ &amp; \\le T(v+\\textcolor{red}{\\varepsilon\\boldsymbol{1}})-T_\\pi v^\\pi &amp; \\text{($T$ monotone)}\\\\ &amp; = Tv-T_\\pi v^\\pi +\\textcolor{red}{\\gamma\\varepsilon\\boldsymbol{1}} &amp; \\text{(Eq. \\eqref{eq:tadd1})}\\\\ &amp; = \\textcolor{red}{T_\\pi} v-T_\\pi v^\\pi +\\gamma\\varepsilon\\boldsymbol{1} &amp; \\text{($\\pi$ def.)}\\\\ &amp; \\le T_\\pi(v^*+\\textcolor{red}{\\varepsilon\\boldsymbol{1}})-T_\\pi v^\\pi + \\gamma \\varepsilon \\boldsymbol{1} &amp; \\text{($T_\\pi$ monotone)}\\\\ &amp; = T_\\pi v^* - T_\\pi v^\\pi + \\textcolor{red}{2}\\gamma \\varepsilon\\boldsymbol{1} &amp; \\text{(Eq. \\eqref{eq:tpiadd1})}\\\\ &amp; = \\textcolor{red}{\\gamma P_\\pi}(v^*-v^\\pi)+2\\gamma \\varepsilon\\boldsymbol{1} &amp; \\text{($T_\\pi$ def.)}\\\\ &amp; = \\gamma P_\\pi \\textcolor{red}{\\delta}+2\\gamma \\varepsilon\\boldsymbol{1}\\,. &amp; \\text{($\\delta$ def.)} \\end{align*}\\] Taking the (pointwise) absolute value of both sides and using the triangle inequality, taking and Eq. \\(\\eqref{eq:stochmxne}\\) we find that \\(\\begin{align*} |\\delta| \\le \\gamma \\|\\delta\\|_\\infty \\boldsymbol{1} + 2\\gamma \\varepsilon\\boldsymbol{1}\\,. \\end{align*}\\) The proof is finished by taking the maximum over the components, noting that \\(\\max_s |\\delta|_s = \\|\\delta\\|_\\infty\\). \\(\\qquad \\blacksquare\\) . An alternative way of finishing the proof is to note that from $\\delta = \\gamma P_\\pi \\delta + 2\\gamma \\varepsilon \\boldsymbol{1}$, by reordering and using that $(I-\\gamma P_\\pi)^{-1} = \\sum_{i\\ge 0} \\gamma^i P_\\pi^i$ is a monotone operator, $\\delta \\le 2\\gamma \\varepsilon \\sum_{i\\ge 0} \\gamma^i P_\\pi \\boldsymbol{1} = 2\\gamma \\varepsilon/(1-\\gamma) \\boldsymbol{1}$. Taking the max-norm of both sides, we get \\(\\|\\delta\\|_\\infty \\le 2\\gamma \\varepsilon/(1-\\gamma)\\). ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec3/#finding-a-near-optimal-policy-using-value-iteration",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec3/#finding-a-near-optimal-policy-using-value-iteration"
  },"368": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "Value Iteration as an Approximate Planning Algorithm",
    "content": ". From Eq. \\(\\eqref{eq:vierror}\\) we see for \\(k \\geq H_{\\gamma, \\varepsilon} = \\frac{\\ln(1 / (\\varepsilon (1 - \\gamma)))}{1 - \\gamma}\\), started with \\(v_0 =0\\), value iteration yields \\(v_k\\) such that \\(\\|v_k - v^*\\|_\\infty \\leq \\varepsilon\\) and consequently, for a policy \\(\\pi_k\\) that is greedy w.r.t. \\(v_k\\), \\(v^{\\pi_k} \\geq v^* - \\frac{2 \\gamma \\varepsilon}{1 - \\gamma} \\boldsymbol{1}\\). Now, for a fixed $\\delta&gt;0$ setting $\\varepsilon$ so that \\(\\delta = \\frac{2 \\gamma \\varepsilon}{1 - \\gamma}\\) holds, we see that after \\(k \\geq H_{\\gamma, \\frac{\\delta(1 - \\gamma)}{2\\gamma}}\\) iterations, we get a \\(\\delta\\)-optimal policy \\(\\pi_k\\): \\(v^{\\pi_k} \\geq v^* - \\delta \\boldsymbol{1}\\). Computing \\(v_{k+1}\\) using \\(\\eqref{eq:vi}\\) takes \\(O(\\mathrm{S}^2 \\mathrm{A})\\) elementary arithmetic (and logic) operations. Putting things together we get the following result: . Theorem (Runtime of Approximate Planning with Value Iteration): Fix a finite discounted MDP and a target accuracy $\\delta&gt;0$. Then, after . \\[O \\left(\\mathrm{S}^2 \\mathrm{A} H_{\\gamma, \\frac{\\delta(1 - \\gamma)}{2\\gamma}} \\right) = \\tilde O\\left( \\frac{\\mathrm{S}^2 \\mathrm{A} }{1 - \\gamma}\\, \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\\] elementary arithmetic operations, value iteration produces a policy $\\pi$ that is $\\delta$-optimal: $v^\\pi \\ge v^* - \\delta \\boldsymbol{1}$, where the $\\tilde{O}(\\cdot)$ result holds when $\\delta \\le 1/e$ is fixed and $\\tilde{O}(\\cdot)$ hides $\\log(2/(1-\\gamma))$. Note that the number of operations needed depends very mildly on the target accuracy. However, accuracy here means an additive error. While the optimal value could be as high as \\(1/(1-\\gamma)\\), it can easily happen that the best value that can be achieved, \\(\\|v^*\\|_\\infty\\), is significantly smaller than \\(1/(1-\\gamma)\\). It may be for example that \\(\\|v^*\\|_\\infty = 0.01\\), in which case a guarantee with \\(\\delta = 0.5\\) is vacuous. By a careful inspection of \\(\\eqref{eq:vierror}\\) we can improve the previous result so that this problem is avoided: . Theorem (Runtime when Controlling for the Relative Error): Fix a finite discounted MDP and a target accuracy \\(\\delta_{\\text{rel}}&gt;0\\). Then, stopping value iteration after $k \\ge H_{\\gamma,\\frac{\\delta_{\\text{rel}}}{2\\gamma}}$ iterations, the policy $\\pi$ produced satisfies the relative error bound . \\[v^\\pi \\ge v^* - \\delta_{\\text{rel}} \\|v^*\\|_\\infty \\boldsymbol{1}\\,,\\] while the total number of elementary arithmetic operations is . \\[O \\left(\\mathrm{S}^2 \\mathrm{A} H_{\\gamma, \\frac{\\delta_{\\text{rel}}}{2\\gamma}} \\right) = \\tilde O\\left( \\frac{\\mathrm{S}^2 \\mathrm{A} }{1 - \\gamma}\\, \\ln\\left(\\frac{1}{\\delta_{\\text{rel}}}\\right)\\right)\\] where $\\tilde{O}(\\cdot)$ hides $\\log(1/(1-\\gamma))$. Notice that the runtime required to achieve a fixed relative accuracy appears to be the same as the runtime required to achieve the same level of absolute accuracy. In fact, the runtime slightly decreases. This should make sense: The worst-case for the fixed absolute accuracy is when \\(\\|v^*\\|_\\infty=1/(1-\\gamma)\\), and in this case the relative accuracy is significantly less demanding: With \\(\\delta_{\\text{rel}}=0.5\\), value iteration can stop after guaranteeing values of \\(0.5/(1-\\gamma)\\), which, as a value, is much smaller than \\(1/(1-\\gamma)-0.5\\), the target with the absolute accuracy level of \\(\\delta = 0.5\\). Note that the relative error bound is not without problems either: It is possible that for some states $s$, \\(v^*(s)-\\delta_{\\text{rel}} \\|v^*\\|_\\infty\\) is negative, a vacuous guarantee. A reasonable stopping criteria would be top stop when the policy that we read out satisfies . \\[v^{\\pi_k} \\ge (1-\\delta_{\\text{rel}}) v^*\\,.\\] Since \\(v^*\\) is not available, to arrive at a stopping condition that can be verified and which implies the above inequality, one can replace $v^*$ above with an upper bound on it, such as \\(v_k +\\gamma^k \\|v_k\\|_\\infty/(1-\\gamma^k) \\boldsymbol{1}\\). In this imagined procedure, in each iteration, one also needs to compute the value function of policy $\\pi_k$ to verify whether the stopping condition is met. If we do this much computation, we may as well replace $v_k$ with $v^{\\pi_k}$ in the update equation \\(\\eqref{eq:vi}\\) hoping that this will further speed up convergence. This results in what is known as policy iteration, which is the subject of the next lecture. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec3/#value-iteration-as-an-approximate-planning-algorithm",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec3/#value-iteration-as-an-approximate-planning-algorithm"
  },"369": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "The Computational Complexity of Planning in MDPs",
    "content": "Now that we have our first results for the computation of approximately optimal policies, it is time to ask whether the algorithm we discovered is doing unnecessary work. That is, what is the minimax computational cost of calculating an optimal, or approximately optimal policy? . To precisely formulate this problem, we need to specify the inputs and the outputs of the algorithms considered. The simplest setting is when the inputs to the algorithms are arrays, describing the transition probabilities and the rewards for each state action pair with some ordering of state-action pairs (and next states in the case of transition probabilities). The output, by the Fundamental Theorem, can be a memoryless policy, either deterministic or stochastic. To describe such a policy, the algorithm could write a table. Clearly, the runtime of the algorithm will be at least the size of the table that needs to be written, so the shorter the output, the better the runtime can be. To be nice with the algorithms, we should allow them to output deterministic policies. After all, the Fundamental Theorem also guarantees that we can always find a deterministic memoryless policy which is optimal. Further, greedy policies can also be chosen to be deterministic, so the value-iteration algorithm would also satisfy this requirement. The shortest specification for a deterministic policy is an array of the size of the state space that has \\(\\mathrm{S}\\) entries. Thus, the runtime of any algorithm that needs to “produce” a fully specified policy is at least \\(\\Omega(\\mathrm{S})\\). This is quite bad! As was noted before, \\(\\mathrm{S}\\), the number of states, in typical problems is expected to be gigantic. But by this easy argument we see that if we demand algorithms to produce fully specified policies then without any further help, they have to do as much work as the number of states. However, things are a bit even worse. In Homework 0, we have seen that no algorithm can find a given value in an array without looking at all entries of the array (curiously, we saw that if we allow randomized computation, that on expectation it is enough to check half of the entries). Based on this, it is not hard to show the following result: . Theorem (Computation Complexity of Planning in MDPs): . Let $0\\le \\delta &lt; \\gamma/(1-\\gamma)$. Any algorithm that is guaranteed to produce $\\delta$-optimal policies in any finite MDP described with tables, with a fixed discount factor $0\\le \\gamma &lt;1$ and rewards in the $[0,1]$ interval needs at least \\(\\Omega(\\mathrm{S}^2\\mathrm{A})\\) elementary arithmetic operations on some MDP with the above properties and whose state space is of size $\\mathrm{S}$ and action space is of size $\\mathrm{A}$. Proof sketch: We construct a family of MDPs such that no matter the algorithm, the algorithm will need to perform the said number of operations in at least one of the MDPs. One-third of the states is reserved for “heaven”, one One-third is reserved for “hell” states. The remaining one-third set of states, call them $R$, is where the algorithms will need to make some nontrivial amount of work. The MDPs are going to be deterministic. In the tables given to the algorithms as input, we (conveniently for the algorithms) order the states so that the “hell” states come first, followed by the “heaven” states, followed by the states in $R$. In the “heaven” class, all states self-loop under all actions and give a reward of one. The optimal value of any of these states is $1/(1-\\gamma)$. In the “hell” class, states also self-loops under all actions but give a reward of zero. The optimal value of these states is $0$. For the remaining states, all actions except one lead to some hell state, while the chosen special action leads to some state in the heaven class. The optimal value of all states in set $R$ have a value of $\\gamma/(1-\\gamma)$ and the value of a policy that in a state in $R$ does not choose the special optimal action gets the value of $0$ in that state. It follows that any algorithm that is guaranteed to be $\\delta$ optimal needs to identify the unique optimal action at every state in $R$. In particular, for every state $s\\in R$ and action $a\\in \\mathcal{A}$, the algorithm needs to read $\\Omega(\\mathrm{S})$ entries of the transition probability vector $P_a(s)$ or it can’t find out whether $a$ leads to a state in the heaven class or the hell class: The probability vector $P_a(s)$ will have a single one at such an entry, either among the $\\mathrm{S}/3$ entries representing the hell, or the $\\mathrm{S}/3$ entries representing the heaven states. By the aforementioned homework problem, any algorithm that needs to find this “needle” requires to check $\\Omega(\\mathrm{S})$ entries. Since the number of states in $R$ is also $\\Omega(\\mathrm{S})$, we get that the algorithm needs to do $\\Omega( \\mathrm{S}\\times \\mathrm{A}) \\mathrm{S}) = \\Omega( \\mathrm{S}^2 \\mathrm{A})$ work. \\(\\qquad \\blacksquare\\) . We immediately see two differences between the lower bound and our previous upper bound(s): In the lower bound there is no dependence on $1/(1-\\gamma)$ (the effective horizon at a constant precision). Furthermore, there is no dependence on $1/\\delta$, the inverse accuracy. As it turns out, the dependence on $1/\\delta$ of value-iteration is superfluous and can be removed. The algorithm that achieves this is policy iteration, which was mentioned earlier. However, this result is saved for the next lecture. After this, the only remaining gap will be the order of the polynomials and the dependence on $1/(1-\\gamma)$, which is closely related to the said polynomial order. And of course, we save for later the most pressing issue that we need to somehow be able to avoid the situation when the runtime depends on the size of the state space (forgetting about the action space for a moment). By the lower bound just presented we already know that this will require changing the problem setting. Just how to do this will be the core question that we will keep returning to in the class. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec3/#the-computational-complexity-of-planning-in-mdps",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec3/#the-computational-complexity-of-planning-in-mdps"
  },"370": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "Notes",
    "content": "Value iteration . The idea of value iteration is probably due to Richard Bellman. Error bound for greedification . This theorem is due to Singh &amp; Yee, 1994. Computational complexity lower bound . The last theorem is due to Chen and Wang (2017), but the construction is also (unsurprisingly) similar to one that appeared in an earlier paper that studied query complexity in the setting when the access to the MDP is provided by a simulation model. In fact, we will present this lower bound later in a lecture where we study batch RL. According to this result, the query-complexity (also known as sample-complexity) of finding a $\\delta$-optimal policy with constant probability in discounted MDPs accessible through a random access simulator, apart from logarithmic factors, is $SA H^3/\\delta^2$, where $H=1/(1-\\gamma)$. Representations matter . We already saw that in order to just clearly define the computational problems (which is necessary for being able to talk about lower bounds), we need to be clear about the inputs (and the outputs). The table representation of MDPs is far from being the only possibility. We just mentioned the “simulation model”. Here the algorithm “learns” about the MDP by issuing next state and reward queries to the simulator at some state-action pair $(s,a)$ of its choice to which the simulator responds with a random next state (drawn fresh) and the $r_a(s)$. Interestingly, this can provably reduce the number of queries compared to the table representation. Another alternative, which still keeps tables, is to give the algorithm a cumulative probability representation. In this representation, the states are identified with ${1,\\dots,\\mathrm{S}}$ as before but instead of giving the algorithm the tables $[P_a(s,1), \\dots, P_a(s,\\mathrm{S})]$ for fixed $(s,a)$, the algorithm is given . \\[[P_a(s,1), P_a(s,1)+P_a(s,2), \\dots, 1]\\] (the last entry could be saved, because it is always equal to one, but in the grand scheme of things, of course, this does not matter). Now, it is not hard to see that if the original probability vector had a single one and zeroes everywhere else, the “needle in the haystack problem” used in the lower bound, with the integral representation above, a clever algorithm can find the entry with the one with at most $O(\\log( \\mathrm{S})) $ queries. As it turns out, with this representation, the query complexity (number of queries required) of producing a good policy can indeed be reduced from the quadratic dependence on the size of the state-space to a log-linear dependence. Hence, we see that the input representation crucially matters. Chen and Wang (2017) also make this point and they discuss yet another, “tree” representation, which leads to a similar speedup. MDPs with short descriptions . The simulator model assumption addresses the problem that just reading the input may be the bottleneck. This is not the only possibility. One can imagine various classes of MDPs that have a short description, which may raise the hope that one can find out a good policy in them without touching each state-action pair. There are many examples of classes of MDPs that belong to this category. These include . | factored MDPs: The transition dynamics have a short, structured (factored) representation, and the same applies to the reward | parametric MDPs: The transition dynamics and the rewards have a short, parametric representation. Examples include linear-quadratic regulation (linear dynamics, quadratic reward, Euclidean state and action spaces, Gaussian noise in the transition dynamics), robotic systems, various operations research problems. | . For factored MDPs one is out of luck: In these, planning is provably “very hard” (computationally). For linear-quadratic regulation, on the other hand, planning is “easy”; once the data is read, all one has to do is to solve some algebraic equations, for which efficient solution methods have been worked out. Query vs. computational complexity . The key idea of the lower bound crucially hinges upon that good algorithms need to “learn” about their inputs: The number of arithmetic and logic operations of any algorithm is at least as large as the number of “read” operations it issues. The minimum number of required read operations to produce an input of some desired property is often called the problems query complexity and by the above reasoning we see that the computational complexity is lower bounded by the query complexity. As it happens, query complexity is much easier to bound than computational complexity in the sense that it is rare to see computational complexity lower bounds strictly larger than the query complexity (the exceptions to this come when a “compact” representation of the MDP is available, such as in the case of factored MDPs). At the heart of query complexity lower bounds is often the needle in the haystack problem. This seems to be generally true when the inputs are “deterministic”. When querying results in stochastic (random) outcomes, multiple queries may be necessary to “reject”, “reduce”, or “filter out” the noise and then new considerations appear. In any case, query complexity is a question about quickly determining the information crucial to arrive at a good decision early and is in a way about “learning”: Before a table is read, the algorithm does not know which MDP it faces. Hence, query complexity is essentially an “information” question and is also sometimes called information complexity and we can think of query complexity as the most basic information theory question. This is a bit different though than mainstream information theory, which is somehow tied up in dealing with reducing the effect of random responses (random “corruptions” of the clean information). Query complexity everywhere . Query complexity is widely studied in a number of communities which, sadly, are almost entirely disjoint. Information-theory, mentioned above is one of them, though as was noted, here the problems are often tied to studying the speed of gaining information in the presence of noise. Besides information theory, there is the whole field of information-based complexity, which has its own journal, multiple books and more. Also notable is the theory community that studies the complexity of evolutionary algorithms. Besides these, of course, query complexity made appearances in the optimization literature (with or without noise), operations research, and of course in the machine learning and statistics community. In particular, in the machine learning and statistics community, when the algorithm is just handed over noisy data, “the sample”, one can ask how large this sample needs to be to achieve some good outcome (e.g., good predictions on unseen data). This leads to the notion of sample complexity, which is the same as our query complexity except that the queries are of the “dull”, “passive” nature of “give me the next datapoint”. As opposed to this, “active learning” refers to the case when the algorithms themselves control some aspects of how the data is collected. Free lunches, needles and a bit of philosophy . Everyone after going to a few machine learning conferences or reading their first book, or blog posts would have heard about David Wolpert’s “no-free lunch theorems”. Yet, I find that to most people the exact nature (or significance) of these theorems remain elusive. Everyone heard that these theorem essentially state that “in the lack of bias, all algorithms are equal” (and therefore there is no free lunch), from which we should conclude that the only way to choose between algorithms is by introducing bias. But what does bias means? If one reads these results carefully (and the theory community of evolutionary computation made a good job of making them accessible) one finds that the results are nothing more that describing some corollaries that to find a needle in a haystack (the special entry in a long array), one needs to search the whole haystack (query almost all entries of the array). Believers of the power of data like to dismiss the significance of the no-free lunch result by claiming that it is ridiculous in that it assumes no structure at all. I find these arguments weak. The main problem is that they are evasive. The evasiveness comes from the reluctance to be clear about what we expect the algorithms to achieve. The claim is that once we are clear about this, that is, clear about the goals, or just the problem specification, we can always hunt for the “needle in the haystack” subproblems within the problem class. This is about figuring out the symmetries (as symmetry equals no structure) that sneakily appear in pretty much any reasonable problem we think of worth studying. The only problems that do not have “needle in the haystack” situations embedded into them are the ones that are not specified at all. What is the upshot of all this? In a way, the real problem is to be clear about what the problem we want to solve is. This is the problem that most theoreticians in my field struggle with every day. Just because this is hard, we cannot give up on this before even starting, or this will just lead to chaos. As we shall see in this class, how to specify the problem is also at the very heart of reinforcement learning theory research. We constantly experiment with various problem definitions, tweaking them in various ways, trying to separate hopelessly hard problems from the easy, but reasonably general ones. Theoreticians like to build a library of various problem settings that they can classify in various ways, including relating the problem settings to each other. While algorithm design is the constructive side of RL (and computer science, more generally), understanding the relationship between the various problem settings is just as equally important. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec3/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec3/#notes"
  },"371": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "References",
    "content": ". | Chen, Y., &amp; Wang, M. (2017). Lower bound on the computational complexity of discounted markov decision problems. arXiv preprint arXiv:1705.07312. [link] | Singh, S. P., &amp; Yee, R. C. (1994). An upper bound on the loss from approximate optimal-value functions. Machine Learning, 16(3), 227-233. [link] | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec3/#references",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec3/#references"
  },"372": {
    "doc": "3. Value Iteration and Our First Lower Bound",
    "title": "3. Value Iteration and Our First Lower Bound",
    "content": "Last time, we discussed the Fundamental Theorem of Dynamic Programming, which then lead to the efficient “value iteration” algorithm for finding the optimal value function. And then we could find the optimal policy by greedifying w.r.t. the optimal value function. In this lecture we will do two things: . | Elaborate more on the the properties of value iteration as a way of obtaining near-optimal policies; | Discuss the computational complexity of planning in finite MDPs. | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec3/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec3/"
  },"373": {
    "doc": "4. Policy Iteration",
    "title": "Policy Iteration",
    "content": "Policy iteration starts with an arbitrary deterministic (memoryless) policy \\(\\pi_0\\). Then, in step $k=0,1,2,\\dots$, the following computations are done: . | calculate \\(v^{\\pi_k}\\), and | obtain \\(\\pi_{k+1}\\), another deterministic memoryless policy, by “greedifying” w.r.t. \\(v^{\\pi_k}\\). | . How do we calculate $v^{\\pi_k}$? Recall that $v^{\\pi}$, for an arbitrary memoryless policy $\\pi$, is the fixed-point of the operator $T_\\pi$: $v^\\pi = T_\\pi v^\\pi$. Also, recall that $T_\\pi v = r_\\pi + \\gamma P_\\pi v$ for any $v\\in \\mathbb{R}^{\\mathcal{S}}$. Thus, $v^\\pi = T_\\pi v^\\pi$ is just a linear equation in $v^\\pi$, which we can solve explicitly. In the context of policy iteration from this we get . \\[\\begin{align} v^{\\pi_k} = (I - \\gamma P_{\\pi_k})^{-1} r_{\\pi_k}\\,. \\label{eq:vpiinv} \\end{align}\\] The careful reader will think of why the inverse of the matrix $I-\\gamma P_{\\pi_k}$ exist. There are many tools we have at this stage to argue that the above is well-defined. One approach is to note that $(I-A)^{-1} = \\sum_{i\\ge 0} A^i$ holds whenever all eigenvalues of the square matrix $A$ lie strictly within the unit circle on the complex plain (see homework 0). This is known as the von Neumann series expansion of $I-A$, but these big words just hide that at the heart of this is the elementary geometric series formula, $1/(1-x) = \\sum_{i\\ge 0} x^i$, which holds for all $|x|&lt;1$, as we have all learned in high school. Based on Eq. \\(\\eqref{eq:vpiinv}\\) we see that \\(v^{\\pi_k}\\) can be obtained with at most \\(O( \\mathrm{S}^3 )\\) (and in fact with at most \\(O( \\mathrm{S}^{2.373\\dots})\\) ) arithmetic and logic operations. In particular, the cost of computing $r_{\\pi_k}$ is $O(\\mathrm{S})$ (since $\\pi_k$ is deterministic), the cost of computing $P_{\\pi_k}$, with the table representation of the MDP and “random access” to the tables, is $O(\\mathrm{S}^2)$. Note that all these are independent of the number of actions. Computationally, the “greedification step” above just means to compute for each state $s\\in \\mathcal{S}$ an action that maximizes the one-step Bellman lookahead values w.r.t. $v^{\\pi_k}$. Writing this out, we see that we need to solve the maximization problem . \\[\\max_{a\\in \\mathcal{A}} r_a(s) + \\gamma \\langle P_a(s),v^{\\pi_k} \\rangle\\] and store the result as the action that will be selected by $\\pi_{k+1}$. Since we agreed that all these policies will be deterministic, we may remove a bit of the storage redundancy, if we allow the algorithm just to store the action chosen by $\\pi_{k+1}$ at every state (and eventually produce the output in this form), rather than requiring it to produce a probability vector for each state, which would have a lot of redundant zero entries in it. Correspondingly, we will further abuse notation and will allow deterministic memoryless policies to be identified with $\\mathcal{S} \\to \\mathcal{A}$ maps. Thus, $\\pi_{k+1}: \\mathcal{S} \\to \\mathcal{A}$. Given $v^{\\pi_k}$, a vector of length $\\mathrm{S}$, the cost of evaluating the argument of the maximum is $O(\\mathrm{S})$. Thus, the cost of computing the maximum is $O(\\mathrm{S}\\mathrm{A})$: This is where the number of actions appears (in these steps) in the runtime. Our main result will be a theorem that states that after $\\tilde O( \\mathrm{SA}/(1-\\gamma))$ iterations, the policy computed by policy iteration is necessarily optimal (and not only approximately optimal!). The proof of this result hinges up on two key observations: . | Policy iteration converges geometrically | After every $H_{\\gamma,1}$ iterations, it eliminates at least one suboptimal action at some state. | . The first result follows from comparing policy iteration with value iteration. We know that value iteration converges at a geometric rate regardless of its initialization. Hence, if we can prove that \\(\\| v^{\\pi_k}-v^* \\|_\\infty \\le \\| T^k v^{\\pi_0}-v^* \\|_\\infty\\) then we will be done. In the so-called “policy improvement lemma”, we will in fact prove a result that implies . \\[\\begin{align} T^k v^{\\pi_0} \\le v^{\\pi_{k}}\\,, \\qquad k=0,1,2,\\dots\\, \\label{eq:pilk} \\end{align}\\] which is stronger than the geometric convergence result. Lemma (Geometric Progress Lemma): Let $\\pi,\\pi’$ be memoryless policies such that $\\pi’$ is greedy w.r.t. $v^\\pi$. Then, . \\[\\begin{align*} v^\\pi \\le T v^{\\pi} \\le v^{\\pi'}\\,. \\end{align*}\\] . Proof: By definition, $T v^\\pi = T_{\\pi’} v^\\pi$. We also have $v^\\pi = T_\\pi v^\\pi \\le T v^\\pi$. Chaining these, we get . \\[\\begin{align} v^\\pi \\le T v^{\\pi} = T_{\\pi'} v^{\\pi}\\,. \\label{eq:pilemmabase} \\end{align}\\] We prove by induction on $i\\ge 1$ that . \\[\\begin{align} v^\\pi \\le T v^{\\pi} \\le T_{\\pi'}^i v^{\\pi}\\,. \\label{eq:pilemmainduction} \\end{align}\\] From this, the result will follow by taking $i\\to \\infty$ of both sides. The base case of induction $i=1$ has just been established. For the general case, assume that the required inequality holds for $i\\ge 1$. We show that it also holds for $i+1$. For this, apply $T_{\\pi’}$ on both sides of Eq. \\(\\eqref{eq:pilemmainduction}\\). Since $T_{\\pi’}$ is monotone, we get . \\[\\begin{align*} T_{\\pi'} v^\\pi \\le T_{\\pi'}^{i+1} v^{\\pi}\\,. \\end{align*}\\] Chaining this with Eq. \\(\\eqref{eq:pilemmabase}\\), we get . \\[\\begin{align*} v^\\pi \\le T v^\\pi = T_{\\pi'} v^\\pi \\le T_{\\pi'}^{i+1} v^{\\pi}\\,, \\end{align*}\\] finishing the inductive step, and hence the proof. \\(\\qquad \\blacksquare\\) . The lemma shows that the value functions are monotonically increasing. Applying this lemma $k$ times starting with $\\pi = \\pi_0$ gives Eq. \\(\\eqref{eq:pilk}\\) and this implies the promised result: . Corollary (Geometric convergence): Let \\(\\{\\pi_k\\}_{k\\ge 0}\\) be the sequence of policies produced by policy iteration. Then, for any \\(k\\ge 0\\), . \\[\\begin{align} \\|v^{\\pi_k} - v^*\\|_\\infty \\leq \\gamma^k \\|v^{\\pi_0} - v^*\\|_\\infty\\,. \\label{eq:pig} \\end{align}\\] . Proof: By \\(\\eqref{eq:pilk}\\), . \\[T^k v^{\\pi_0} \\le v^{\\pi_k} \\le v^*\\,, \\qquad k=0,1,2,\\dots\\,.\\] Hence, . \\[v^* - v^{\\pi_k} \\le v^* - T^k v^{\\pi_0}\\,, \\qquad k=0,1,2,\\dots\\,.\\] Taking componentwise absolute values and then the maximum over the states, we get that . \\[\\|v^* - v^{\\pi_k}\\|_\\infty \\le \\|v^* - T^k v^{\\pi_0}\\|_\\infty = \\|T^k v^* - T^k v^{\\pi_0}\\|_\\infty \\le \\gamma^k \\|v^* - v^{\\pi_0}\\|_\\infty\\,,\\] which is the desired statement. In the equality above we used the Fundamental Theorem and in the last inequality we used that $T$ is a $\\gamma$-contraction. \\(\\qquad\\blacksquare\\) . We now set out to finish by showing the “strict progress lemma”. The lemma uses the corollary we just obtained, but it will also require some truly novel ideas. Lemma (Strict progress lemma): Fix an arbitrary suboptimal memoryless policy $\\pi_0$ and let \\(\\{\\pi_k\\}_{k\\ge 0}\\) be the sequence of policies produced by policy iteration. Then, there exists a state $s_0\\in \\mathcal{S}$ such that for any $k\\ge k^*:= \\lceil H_{\\gamma,1} \\rceil +1$, . \\[\\pi_k(s_0)\\ne \\pi_0(s_0)\\,.\\] . The lemma shows that after every \\(k^* = \\tilde O \\left( \\frac{1}{1-\\gamma}\\right)\\) iterations, policy iteration eliminates one action-choice at one state until there remains no suboptimal action to be eliminated. This can only be continued for at most $SA - S$ times: In every state, at least one action must be optimal. As an immediate corollary of the progress lemma, we get the main result of this lecture: . Theorem (Runtime Bound for Policy Iteration): Consider a finite, discounted MDP with rewards in $[0,1]$. Let \\(k^*\\) be as in the progress lemma, \\(\\{\\pi_k\\}_{k\\ge 0}\\) the sequence of policies obtained by policy iteration starting from an arbitrary initial policy $\\pi_0$. Then, after at most \\(k= k^* (\\mathrm{S}\\mathrm{A}-\\mathrm{S}) = \\tilde O\\left( \\frac{\\mathrm{S}\\mathrm{A}-\\mathrm{S} }{1-\\gamma } \\right)\\) iterations, the policy $\\pi_k$ produced by policy iteration is optimal: $v^{\\pi_k}=v^*$. In particular, policy iteration computes an optimal policy with at most \\(\\tilde O\\left( \\frac{ \\mathrm{S}^4 \\mathrm{A} +\\mathrm{S}^3{\\mathrm{A}^2} }{1-\\gamma} \\right)\\) arithmetic and logic operations. It remains to prove the progress lemma. We start with an identity which will be useful beyond the proof of this lemma. The identity is called the value difference identity and it gives us an alternate form of the difference of values functions of two memoryless policies. Let $\\pi,\\pi’$ be two memoryless policies. Recalling that $v^{\\pi’} = (I-\\gamma P_{\\pi’})^{-1} r_{\\pi’}$, by algebra, we find that . \\[\\begin{align*} v^{\\pi'} - v^{\\pi} &amp; = (I-\\gamma P_{\\pi'})^{-1} [ r_{\\pi'} - (I-\\gamma P_{\\pi'}) v^\\pi] \\\\ &amp; = (I-\\gamma P_{\\pi'})^{-1} [ T_{\\pi'} v^\\pi - v^\\pi]\\,. \\end{align*}\\] Introducing . \\[g(\\pi',\\pi) = T_{\\pi'} v^\\pi - v^\\pi\\,,\\] which we can think of the “advantage” of $\\pi’$ relative to $\\pi$, we get the following lemma: . Lemma (Value Difference Identity): For all memoryless policies \\(\\pi, \\pi'\\), . \\[v^{\\pi'} - v^\\pi = (I - \\gamma P_{\\pi'})^{-1} g(\\pi',\\pi)\\,.\\] . Of course, a symmetric relationship also holds. With this, we are now ready to prove the progress lemma. Note that if \\(\\pi^*\\) is an optimal memoryless policy then for any other memoryless policy $\\pi$, \\(g(\\pi,\\pi^*)\\le 0\\). In fact, the reverse statement also holds: if the above holds for any $\\pi$, $\\pi^*$ must be optimal. This makes it \\(-g(\\pi_k,\\pi^*)\\) an ideal target to track the progress that policy iteration makes. We expect this to start at a high value and decrease as $k$ increases. Note, in particular, that if . \\[\\begin{align} -g(\\pi_k,\\pi^*)(s_0)&lt;-g(\\pi_0,\\pi^*)(s_0) \\label{eq:strictprogress} \\end{align}\\] for some state $s_0\\in \\mathcal{S}$ then, by algebra, . \\[r_{\\pi_k(s_0)}(s_0) + \\gamma \\langle P_{\\pi_k(s_0)} , v^* \\rangle &gt; r_{\\pi_0(s_0)}(s_0) + \\gamma \\langle P_{\\pi_0(s_0)} , v^* \\rangle\\] which means that $\\pi_k(s_0)\\ne \\pi_0(s_0)$. Hence, the idea of the proof is to show that Eq. \\(\\eqref{eq:strictprogress}\\) holds for any $k\\ge k^*$. Proof (of the progress lemma): Fix $k\\ge 0$ and \\(\\pi_0\\) such that \\(\\pi_0\\) is not optimal. Let \\(\\pi^*\\) be an arbitrary memoryless optimal policy. Then, for policy \\(\\pi_k\\), by the value difference identity and since \\(\\pi^*\\) is optimal, . \\[- g(\\pi_k,\\pi^*) = (I - \\gamma P_{\\pi_k}) (v^* - v^{\\pi_k}) = (v^* - v^{\\pi_k}) - \\gamma P_{\\pi_k} (v^* - v^{\\pi_k}) \\leq v^* - v^{\\pi_k}\\,,\\] where the last inequality follows because $P_{\\pi_k}$ is stochastic and hence monotone and because \\(v^* - v^{\\pi_k}\\ge 0\\). Our goal is to relate the right-hand side to \\(-g(\\pi_0,\\pi^*)\\). Since Eq. \\(\\eqref{eq:pig}\\) allows us to relate the right-hand side to \\(v^*-v^{\\pi_0}\\), and the value difference identity then lets us bring in \\(-g(\\pi_0,\\pi^*)\\), preparing to use Eq. \\(\\eqref{eq:pig}\\), we first take the max-norm of both sides of the above inequality, noting that this keeps the inequality by the definition of the max-norm. Then, as planned, we use Eq. \\(\\eqref{eq:pig}\\) and the value difference identity to get . \\[\\begin{align} \\|g(\\pi_k,\\pi^*)\\|_\\infty &amp; \\leq \\|v^* - v^{\\pi_k}\\|_\\infty \\leq \\gamma^k \\|v^* - v^{\\pi_0}\\|_\\infty = \\gamma^k \\|(I - \\gamma P_{\\pi_0})^{-1} (-g(\\pi_0,\\pi^*))\\|_\\infty \\nonumber \\\\ &amp; \\leq \\frac{\\gamma^k}{1 - \\gamma} \\|g(\\pi_0,\\pi^*)\\|_\\infty\\,, \\label{eq:plmain} \\end{align}\\] where the last inequality follows by noting that \\((I - \\gamma P_{\\pi_0})^{-1} = \\sum_{i\\ge 0} \\gamma^i P_{\\pi_0}^i\\) and thus from the triangle inequality and because \\(P_{\\pi_0}\\) is a max-norm non-expansion, \\(\\| (I - \\gamma P_{\\pi_0})^{-1} x \\|_\\infty \\le \\frac{1}{1-\\gamma}\\| x \\|_\\infty\\) holds for any \\(x\\in \\mathbb{R}^{\\mathrm{S}}\\). Now, define $s_0\\in \\mathcal{S}$ to be the state that satisfies \\(-g(\\pi_0,\\pi^*)(s_0) = \\| g(\\pi_0,\\pi^*)(s_0)\\|_\\infty\\). Since $\\mathcal{S}$ is finite, this exists. Noting that \\(0\\le -g(\\pi_k,\\pi^*)(s_0)\\le \\| g(\\pi_k,\\pi^*)\\|_\\infty\\), we get from Eq. \\(\\eqref{eq:plmain}\\) that . \\[-g(\\pi_k,\\pi^*)(s_0) \\leq \\|g(\\pi_k,\\pi^*)\\|_\\infty \\leq \\frac{\\gamma^k}{1 - \\gamma} (-g(\\pi_0,\\pi^*)(s_0)).\\] Now when \\(k\\ge k^*\\), \\(\\frac{\\gamma^k}{1 - \\gamma} &lt; 1\\). Since \\(\\pi_0 \\neq \\pi^*\\), \\(0&lt;\\|g(\\pi_0,\\pi^*)\\|_\\infty = -g(\\pi_0,\\pi^*)(s_0)\\) and thus, . \\[\\begin{align*} -g(\\pi_k,\\pi^*)(s_0) \\leq \\frac{\\gamma^k}{1 - \\gamma} (-g(\\pi_0,\\pi^*)(s_0)) &lt; -g(\\pi_0,\\pi^*)(s_0)\\,, \\end{align*}\\] which is Eq. \\(\\eqref{eq:strictprogress}\\), and thus, by our earlier discussion, \\(\\pi_k(s_0)\\ne \\pi_0(s_0)\\). The proof is done because this holds for any \\(k\\ge k^*\\). \\(\\qquad\\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec4/#policy-iteration",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec4/#policy-iteration"
  },"374": {
    "doc": "4. Policy Iteration",
    "title": "Is Value Iteration Inferior?",
    "content": "Our earlier result on the runtime of value iteration involves a $\\log(1/\\delta)$ term which grows without bounds as $\\delta$, the required precision level, decreases towards zero. However, at this stage it is not clear whether this extra term is the result of a loose analysis or whether it is a property of value-iteration. Can value iteration be guaranteed to find an optimal policy with computation which is polynomial in $\\mathrm{S}$, $\\mathrm{A}$ and the planning horizon $1/(1-\\gamma)$, assuming all value functions takes values in $[0,1/(1-\\gamma)]$? . Calling any algorithm that achieves the above strongly polynomial, we see that with this terminology we can say that policy iteration is strongly polynomial. Note that in the above definition rather than assuming that the rewards lie in $[0,1]$, we use the assumption that the value functions for all policies take values in $[0,1/(1-\\gamma)]$. This is a weaker assumption, but checking our proof for the runtime on policy iteration we see that it only needed this assumption. However, as it turns out, value-iteration is not strongly polynomial: . Proposition: There exists a family of MDPs with deterministic transitions, three states, two actions and value functions for all policies taking values in $[0,1/(1-\\gamma)]$ such that the worst-case iteration complexity of value iteration over this set of MDPs to find an optimal policy is infinite. Here, iteration complexity means the smallest number of iterations $k$ after which $\\pi_k$, as computed by value iteration, is optimal, for any of the MDPs in the family. Of course, an infinite iteration complexity also implies an infinite runtime complexity. Proof: The MDP is depicted in the following figure: . The circles show the states with their names in the circles, the arrows with labels $a_0$ and $a_1$ show the transitions between the states as a result of using the actions. The label $r=\\cdot$ shows how much reward is incurred along a transition. On the figure, $R$ is not a return, but a free parameter, which is chosen in the interval $[0,\\gamma/(1-\\gamma)]$ and which will govern the iteration complexity of value iteration. We consider value iteration initialized at $v_0 = \\boldsymbol{0}$. It is easy to see that the unique optimal action at $s_1$ is $a_0$, incurring a value of $\\gamma/(1-\\gamma)$ at this state. It is also easy to see that $\\pi_0(s_1)=a_1\\ne a_0$. We will show that value iteration can “hug” action $a_1$ at state $s_0$ indefinitely as $R$ approaches $\\gamma/(1-\\gamma)$ from below. For this, just note that $v_k(s_0)=0$ and that $v_k(s_2) =\\frac{\\gamma}{1-\\gamma}(1-\\gamma^k)$ for any $k\\ge 0$. Then, a little calculation shows that $\\pi_k(s_1)=a_1$ as long as $R&gt;v_k(s_2)$. If we want value iteration to spend more than $k_0$ iterations, all we have to do is to choose $R = \\frac{v^*(s_2)+v_{k_0}(s_2)}{2}&lt;\\gamma/(1-\\gamma)$. \\(\\blacksquare\\) . It is instructive to note how policy iteration avoids the blow-up of the iteration-counts. This result shows that value-iteration, as far as we are concerned with calculating an optimal policy, exactly, is clearly inferior to policy iteration. However, we also had our earlier positive result for value iteration that showed that the cost of achieving $\\delta$-suboptimal policies is at most $\\log(1/\\delta)$ (and polynomial in the remaining quantities). What does this all mean? Should we really care about that value-iteration is not finite for exact computation? We have many reasons to not to care much about exact calculations. In the end, we will do sampling, learning, all of which make exact calculations impossible. Also, recall that our models are just models: The models themselves introduce errors. Why would we want to care about exact optimality? In summary: . Exact optimality is nice to have, but approximate computations with runtime growing mildly with the required precision should be almost equally acceptable. Yet, it remains intriguing to think of how policy iteration can just “snap” into the right solution and how by changing just a few lines of code, a drastic improvement in runtime may be possible. We will keep returning to the question of whether an algorithm has some provable advantage over some others. When this can be shown, it is a true win: We do not need to bother with the inferior algorithm anymore. While this is great, remember that all this depends on how the problems are defined. As we have seen before, and we will see many more times, changing the problem definition can drastically change the landscape of what works and what does not work. And who knows, some algorithm may be inferior in some context, and be superior in some other. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec4/#is-value-iteration-inferior",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec4/#is-value-iteration-inferior"
  },"375": {
    "doc": "4. Policy Iteration",
    "title": "Notes",
    "content": "The runtime bound on policy iteration . The first result that showed that after $\\text{poly}(\\mathrm{S},\\mathrm{A},\\frac{1}{1-\\gamma})$ arithmetic and logic operations one can compute an optimal policy is due to Yinyu Ye (2011). This was a real breakthrough of the time. The theorem we proved is by Bruno Scherrer (2016) and we followed closely his proof. This proof is much simpler than the first one by Yinyu Ye, though the main ideas can be traced back to the proof of Yinyu Ye. Runtime of value iteration . The example that shows that value iteration is not strongly polynomial is due to Eugene A. Feinberg, Jefferson Huang and Bruno Scherrer (2014). Ties and stopping . More often than one may imagine, two actions may tie for the maximum in the above problem. Which one to use in this case? As it turns out, it matters only if we want to build a stopping condition for the algorithm that stops the first time it detects that $\\pi_{k+1}=\\pi_k$. This stopping condition takes $O(\\mathrm{S})$ operations, so is quite cheap. If we use this stopping condition, we better make sure that when there are ties, the algorithm resolves them in a systematic fashion, meaning that it has a fixed preference relation over the actions that it respects in case of ties. Otherwise, in the case when there are two optimal actions at some state $s$, $\\pi_k$ is an optimal policy, $\\pi_{k+1}$ may choose the optimal action that $\\pi_k$ did not choose, and then $\\pi_{k+2}$ could choose the same action as $\\pi_k$ at the same state, etc. and the stopping condition would fail to detect that all these policies are optimal. Alternatively to resolving ties systematically one may simply change the stopping condition to checking whether $v^{\\pi_k} = v^{\\pi_{k+1}}$. The reader is invited to check that this would work. “In practice”, though, this may be problematic if $v^{\\pi_k}$ and $v^{\\pi_{k+1}}$ are computed with finite precision and somehow the approximation errors that arise in this calculation lead to different answers. Can this happen at all? It can! We may have $v^{\\pi_k} = v^{\\pi_{k+1}}$ (with infinite precision), while $r_{\\pi_k}\\ne r_{\\pi_{k+1}}$ and $I-\\gamma P_{\\pi_k} \\ne I-\\gamma P_{\\pi_{k+1}}$. And so with finite precision calculations, there is no guarantee that we get the same outcomes in the two cases! The only guarantee that we get with finite precision calculations is that with identical inputs, the outputs are identical. An easy way out, of course, is just to use the theorem above and stop after the number of iterations is sufficiently large. However, this may be needlessly, wasteful. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec4/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec4/#notes"
  },"376": {
    "doc": "4. Policy Iteration",
    "title": "References",
    "content": ". | Feinberg, E. A., Huang, J., &amp; Scherrer, B. (2014). Modified policy iteration algorithms are not strongly polynomial for discounted dynamic programming. Operations Research Letters, 42(6-7), 429-431. [link] | Scherrer, B. (2016). Improved and generalized upper bounds on the complexity of policy iteration. Mathematics of Operations Research, 41(3), 758-774. [link] | Ye, Y. (2011). The simplex and policy-iteration methods are strongly polynomial for the Markov decision problem with a fixed discount rate. Mathematics of Operations Research, 36(4), 593-603. [link] | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec4/#references",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec4/#references"
  },"377": {
    "doc": "4. Policy Iteration",
    "title": "4. Policy Iteration",
    "content": "In this lecture we . | formally define policy iteration and | show that with $\\tilde O( \\textrm{poly}(\\mathrm{S},\\mathrm{A}, \\frac{1}{1-\\gamma}))$ elementary arithmetic operations, it produces an optimal policy | . This latter bound is to be contrasted with what we found out about the runtime of value-iteration in the previous lecture. In particular, value-iteration’s runtime bound that we discovered previously grew linearly with $\\log(1/\\delta))$ where $\\delta$ was the targeted suboptimality level. This may appear as a big difference in the limit of $\\delta\\to 0$. Is this difference real? Is value-iteration truly inferior to policy-iteration? We will discuss these at the end of the lecture. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec4/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec4/"
  },"378": {
    "doc": "5. Local Planning - Part I.",
    "title": "What is Local Planning?",
    "content": "In a previous lecture we have seen that in discounted MDP with $S$ states and $A$ actions, no algorithm can output a $\\delta\\le \\gamma/(1-\\gamma)$ optimal or better policy with a computation cost less than $\\Omega( S^2 A )$ provided that the MDP is given with a table representation. One of the $SA$ factors here comes from that to specify a policy one needs to compute (and output) what action to take in every state. The additional $S$ factor comes from because to figure out whether an action is any good, one needs to read almost all entries of the next-state distribution vector. An unpleasant tendency of the world is that if a problem is modelled as an MDP (that is, the Markov assumption is faithfully observed), the size of the state space tends to blow up. Bellman’s curse of dimensionality is one reason why this happens. To be able to deal with such large MDPs, we expect our algorithm’s runtime to be independent of the size of the state space. However, our lower bound tells us that this is a pipe dream. But why did we require the planner to output a full policy? And why did we assume that the only way to get information about the MDP is to read big tables of transition probabilities? In fact, if the planner is used inside an “agent” that is embedded in an environment, there is no need for the planner to output a full policy: In every moment, the planner just needs to calculate the action to be taken in the state corresponding to the current circumstances of the environment. In particular, there is no need to specify what action to take under any other circumstances than the current one! . As we usually do in these lectures, assume assume that the environment is an MDP and the agent gets access to the state in every step when it needs to make a decision. Further, assume that the agent is lucky to also have access to a simulator of the MDP that describes its environment. Just think of the simulator as a black box that can be, fed with a state-action pair and responds with the immediate reward and a random next state from the correct next-state distribution. One can then perhaps build a planner that uses this black box with a “few” queries and quickly returns an action, to be taken by the agent, moving the environment to a random next state, from where the process continues. Now, the planner does not need to output actions at all states and it does not need to spend time on reading long probability vectors. Hence, in theory, the obstacles that led to the lower bound are removed. The question still remains whether in this new situation planner’s can indeed get away with runtime independent of the size of the state space. To break the suspense, the answer is yes and it comes very easily for deterministic environments. For stochastic environments a little more work will be necessary. In the remainder of this lecture we give a formal problem definition for the local planning problem that was described informally above. Next, the result is explained for deterministic environments. This result will be matched with a lower bound. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec5/#what-is-local-planning",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec5/#what-is-local-planning"
  },"379": {
    "doc": "5. Local Planning - Part I.",
    "title": "Local Planning: Formal Definitions",
    "content": "We start with the definition of MDP simulators. We use a language similar to that used to describe optimization problems where one talks about optimization in the presence of various oracles (zeroth-order, first order, noisy, etc.). Because we assume that all MDPs are finite, we identify the state and action spaces with subsets of the natural numbers and for the action set we also require that the action set is $[\\mathrm{A}]$ where $\\mathrm{A}$ is the number of actions. This simplifies the description quite a bit. Definition (MDP simulator): A simulator implementing an MDP \\(M=(\\mathcal{S},\\mathcal{A},P,r)\\) is a “black-box oracle” that when queried with a state action pair \\((s,a)\\in \\mathcal{S}\\times\\mathcal{A}\\) returns the reward $r_a(s)$ and a random state \\(S' \\sim P_a(s)\\), where \\(r=(r_a(s))_{s,a}\\) and \\(P = (P_a(s))_{s,a}\\). User’s of the black-box must pay attention avoid querying it for state-action pairs outside of $\\mathcal{S}\\times \\mathcal{A}$. Our next notion is that of a local planner: . Definition (Local Planner): A local planner takes as input the number of actions $\\mathrm{A}$, a state $s\\in \\mathbb{N}$, an MDP simulator “access point”. After querying this simulator finitely many times, the planner needs to return an action from $[\\mathrm{A}]$. (Local) planners may randomize their calculation. Even if they do not randomize, the action returned by a planner is in general random due to the randomness of the simulator that the planner uses. A planner is well-formed if no matter what MDP it interfaces with through a simulator, it returns an action after querying the simulator finitely many times. This also means that the planner can never feed the simulator with state-action pair outside of the set of such pairs. If a local planner is given access to a simulator of $M$, the planner and the MDP $M$ together induce a policy of the MDP. We will just refer to this policy as the planner-induced policy $\\pi$ when the MDP is clear from the context. Yet, this policy depends on the MDP implemented by the simulator. If a local planner is well-formed, this policy is well-defined no matter the MDP that is implemented by the simulator. Local planners are expected to produce good policies: . Definition ($\\delta$-sound Local Planner): We say that a local planner is $\\delta$-sound if it is well-formed and for any MDP $M$, the policy $\\pi$ induced by it and a simulator implementing $M$ is $\\delta$-optimal in $M$. In particular, . \\[v^\\pi \\ge v^* - \\delta \\boldsymbol{1}\\] must hold where $v^*$ is the optimal value function in $M$. The (per-state, worst-case) query-cost of a local planner is the maximum number of queries it submits to the simulator where the maximum is over both the MDPs and the initial states. The following vignette summarizes the problem of local planning: . | Model: | Any finite MDP $M$ | . | Oracle: | Black-box simulator of $M$ | . | Local input: | State $s$ | . | Local output: | Action $A$ | . | Outcome: | Policy $\\pi$ | . | Postcondition: | \\(v^\\pi_M \\ge v^*_M-\\delta \\boldsymbol{1}\\) | . As an optimization, we let local planners also take as input $\\delta$, the target suboptimality level. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec5/#local-planning-formal-definitions",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec5/#local-planning-formal-definitions"
  },"380": {
    "doc": "5. Local Planning - Part I.",
    "title": "Local Planning through Value Iteration and Action-value Functions",
    "content": "Recall value iteration: . | Let \\(v_0 = \\boldsymbol{0}\\) | For \\(k=1,2,\\dots\\) let \\(v_{k+1} = Tv_k\\) | . As we have seen, if the iteration is stopped so that $k\\ge H_{\\gamma,\\delta(1-\\gamma)/(2\\gamma)}$, the policy $\\pi_k$ defined via . \\[\\pi_k(s) = \\arg\\max_a r_a(s) + \\gamma \\langle P_a(s),v_k \\rangle\\] is guaranteed to be $\\delta$-optimal. Can this be used for local planning? As we shall see, in a way, yes. But before showing this, it will be wortwhile to introduce some additional notation that, in the short term, will save us some writing. More importantly, the new notation will also be seen to influence algorithm design. The observation is that to decide about what action to take, we need to calculate the one-step lookahead value of the various actions. Rather than doing this in a separate step as shown above, we could have as well chosen to keep track of these lookahead values throughout the whole procedure. Indeed, define \\(\\tilde T: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}\\) as . \\[\\tilde T q = r + \\gamma P M q, \\qquad (q \\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}})\\,,\\] where $r\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ and the operators $P: \\mathbb{R}^{\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ and $M: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}}$ are defined via . \\[\\begin{align*} r(s,a) = r_a(s)\\,, \\quad (P v)(s,a) = \\langle P_a(s), v \\rangle\\,, \\quad (M q)(s) = \\max_{a\\in \\mathcal{A}} q(s,a) \\end{align*}\\] with \\(s\\in \\mathcal{S}\\), \\(a\\in \\mathcal{A}\\), \\(v\\in \\mathbb{R}^{\\mathcal{S}}\\), \\(q\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}\\). Then the definition of $\\pi_k$ can be shortened to . \\[\\pi_k(s) = \\arg\\max_a (\\tilde T^{k+1} \\boldsymbol{0})(s,a)\\,.\\] It is instructive to write the above computation in a recursive, algorithmic form. Let . \\[q_k = \\tilde T^k \\boldsymbol{0}.\\] Using a Python-like pseudocode, our function to calculate the values $q_k(s,\\cdot)$ looks as follows: . 1. define q(k,s): 2. if k = 0 return 0 # base case 3. return [ r(s,a) + gamma * sum( [P(s,a,s') * max(q(k-1,s')) for s' in S] ) for a in A ] 4. end . Line 3, which is where the recursive call happens uses Python’s list comprehensions: the brackets create lists and the function itself returns a list. This is a recursive function (since it calls itself in line 3. The runtime is easily seen to be $(\\mathrm{A}\\mathrm{S})^k$, which is not very hopeful until we notice that if the MDP was deterministic, that is, $P(s,a,\\cdot)$ has a single one entry, and we have a way of looking up which entry is this without going through all the states, say, $g: \\mathcal{S}\\times \\mathcal{A} \\to \\mathcal{S}$ is a function that gives the next states, we can rewrite the above as . 1. define q(k,s): 2. if k = 0 return 0 # base case 3. return [ r(s,a) + gamma * max(q(k-1,g(s,a))) for a in A ] 4. end . As in line 3 there is no loop over the next states (no summing up over these), the runtime becomes . \\[O(A^k)\\,\\] which is the first time we see that a good action can be calculated with effort regardless of the size of the state space! And of course, if one is given a simulator of the underlying MDP, which is deterministic, calling $g$ is the same as calling the simulator (once). But will this idea extend to the stochastic case? The answer is yes, but the details will be given in the next lecture. Instead, in this lecture we take a brief look at whether there is any possibility to do better than the above recursive procedure. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec5/#local-planning-through-value-iteration-and-action-value-functions",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec5/#local-planning-through-value-iteration-and-action-value-functions"
  },"381": {
    "doc": "5. Local Planning - Part I.",
    "title": "Lower Bound",
    "content": ". Theorem (local planning lower bound): Take any local planner $p$ that is $\\delta$-sound with $\\delta&lt; 1$ for discounted MDPs with rewards in $[0,1]$. Then there exist some MDPs on which $p$ uses at least \\(\\Omega(\\mathrm{A}^{k})\\) queries at some state with . \\[\\begin{align} k=\\left\\lceil \\frac{\\ln( 1/(\\delta(1-\\gamma)) )}{\\ln(1/\\gamma)}\\right\\rceil, \\label{eq:kdeflb} \\end{align}\\] where \\(\\mathrm{A}\\) is the number of actions in the MDP. Denote by $k_\\gamma$ the value defined in \\eqref{eq:kdeflb}. Then, for $\\gamma\\to 1$, $k_\\gamma =\\Omega( H_{\\gamma,\\delta} )$. Proof: This is a typical needle-in-the-haystack argument. We saw in homework one that no algorithm can find out which element of a binary array of length $m$ is one with less than $\\Omega(m)$ queries. Take a rooted regular $\\mathrm{A}$-ary tree of depth $k$. The tree has exactly $\\mathrm{A}^k$ leafs. Consider an MDP with states corresponding to the nodes of this tree and an extra absorbing state. Call the root $s_0$. Let the dynamics be deterministic: Taking an action at a node (of the tree makes the next state the child of that node. Taking an action at a leaf node makes the next state the absorbing state. Any action at the absorbing state makes the next state the absorbing state. Let all the rewards be zero except at exactly one of the leaf nodes for exactly one action set the reward to be one. If a planner is $\\delta$-sound, we claim that it must find the optimal action at $s_0$. This holds because the value of this action is $\\sum_{i=k}^\\infty \\gamma^i=\\gamma^k/(1-\\gamma)$ and, by our choice of $k$, $\\gamma^k/(1-\\gamma) \\ge \\delta$. It follows that the planner needs to be able to identify the unique action at the unique leaf node whose reward is one, which, by the homework problem, needs at least $\\Omega(\\mathrm{A}^{k})$ queries. \\(\\qquad \\blacksquare\\) . With a little extra work, the value of $k_\\gamma$ can be improved to $k=\\Omega( H_{\\gamma,\\delta(1-\\gamma)} )$, which matches the upper bound. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec5/#lower-bound",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec5/#lower-bound"
  },"382": {
    "doc": "5. Local Planning - Part I.",
    "title": "Notes",
    "content": "Dealing with larger state spaces . For a fully formal specification the reader may worry about how a state is described to a local planner, especially, if we allowed uncountably many states. Because the local planner will only have access to the state that it receives as its input and the other states that are returned from the simulator, for the purpose of communication between the local planner and its environment and the simulator, all these states can just be assigned unique numbers to identify them. Gap between the lower and upper bound . There is an obvious gap between the lower and the upper bound that should be closed. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec5/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec5/#notes"
  },"383": {
    "doc": "5. Local Planning - Part I.",
    "title": "5. Local Planning - Part I.",
    "content": "In this lecture we . | introduce local planning; | show that for deterministic MDPs there is a local planner whose runtime per call is independent of the size of the state space; | show that this local planner has in fact a near-optimal runtime in a worst-case sense. | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec5/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec5/"
  },"384": {
    "doc": "6. Local Planning - Part II.",
    "title": "Sampling May Save the Day?",
    "content": "Assume now that the MDP is stochastic. Recall the pseudocode of the recursive form of value iteration from the last lecture that computes $(T^k \\boldsymbol{0})(s,\\cdot)$: . 1. define q(k,s): 2. if k = 0 return 0 # base case 3. return [ r(s,a) + gamma * sum( [P(s,a,s') * max(q(k-1,s')) for s' in S] ) for a in A ] 4. end . Obviously, the size of the state space creeps in because in line 3 we need to calculate an expected value over the next state distribution at $(s,a)$. As noted beforehand, in deterministic systems when a simulator is available, the sum over the next-states can be replaced with a single simulator call. But the reader may remember from Probability 101 that sampling allows one to approximate expected values, where the error of approximation is independent of the cardinality of the set over which we average the values. Here, this set is $\\mathcal{S}$, the state space. This is extremely lucky! . To quantify the size of these errors, we recall Hoeffding’s inequality: . Lemma (Hoeffding’s Inequality): Given $m$ independent, identically distributed (i.i.d.) random variables that take values in the $[0,1]$ interval, for any \\(0 \\leq \\zeta &lt; 1\\), with probability at least \\(1 - \\zeta\\) it holds that . \\[\\left| \\frac{1}{m} \\sum_{i=1}^m X_i - \\mathbb{E}[X_1] \\right| \\leq \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} }\\,.\\] . Letting $S_1’,\\dots,S_m’ \\stackrel{\\textrm{i.i.d.}}{\\sim} P_a(s)$ for some state-action pair $(s,a)$ and $v:S \\to [0,v_{\\max}]$, by this result, for any $0\\le \\zeta &lt;1$, with probability $1-\\zeta$, . \\[\\begin{align} \\left|\\frac1m \\sum_{i=1}^m v(S_i') - \\langle P_a(s), v \\rangle\\right| \\le v_{\\max} \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} }. \\label{eq:hoeffbop} \\end{align}\\] This suggests the following approach: For each state action pair $(s,a)$ draw $S_1’,\\dots,S_m’ \\stackrel{\\textrm{i.i.d.}}{\\sim} P_a(s)$ and store it on a list $C(s,a)$. Then, whenever for some function $v$ we need the value of $\\langle P_a(s), v \\rangle$, just use the sample average . \\[\\frac1m \\sum_{s'\\in C(s,a)} v(s')\\,.\\] Plugging this approximation into our previous pseudocode gives the following new code: . 1. define q(k,s): 2. if k = 0 return 0 # base case 3. return [ r(s,a) + gamma/m * sum( [max(q(k-1,s')) for s' in C(s,a)] ) for a in A ] 4. end . The total runtime of this function is now $O( (m\\mathrm{A})^{k+1} )$. What is important is that this will give us a compute time independent of the size of the state space as long as we can show that $m$ can be set independently of $\\mathrm{S}$ while meeting our target for the suboptimality of the induced policy. This pseudocode sweeps under the rug on who creates the lists $C(s,a)$ and when? A simple and effective approach is to use “lazy evaluation” (or memoization): Create $C(s,a)$ at the first time it is needed (and do not create it otherwise). An alternative to the approach we follow here is to avoid storing these lists and just create them on demand. Both procedures are valid, but we will stick to the procedure that creates the lists only once and will comment on the other approach at the end of this note. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec6/#sampling-may-save-the-day",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec6/#sampling-may-save-the-day"
  },"385": {
    "doc": "6. Local Planning - Part II.",
    "title": "Good Action-Value Approximations Suffice",
    "content": "As a first step towards understanding the strength and weaknesses of this approach, let us define $\\hat T: \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}\\times\\mathcal{A}}$ by . \\[(\\hat T q)(s,a) = r_a(s) + \\frac{\\gamma}{m} \\sum_{s'\\in C(s,a)} \\max_{a'\\in \\mathcal{A}} q(s',a')\\,.\\] With the help of this definition, when called with state $s=s_0$, the planner computes . \\[\\begin{align*} A = \\arg\\max_{a\\in \\mathcal{A}} \\underbrace{ (\\hat T^H \\boldsymbol{0})(s_0,a) }_{Q_H(s_0,a)}\\,, \\end{align*}\\] The conciseness of this formulae, if anything, must please everyone! . Let us now turn to the question of whether the policy $\\hat \\pi$ induced by this planners is a good one. We start with a lemma that parallels our earlier result that bounded the suboptimality of a policy that is greedy w.r.t. a function over the states as a function of how well the function approximates the optimal value function. To state the lemma, we need the analog of optimal value functions but with action values. Suboptimality of $\\epsilon$-optimizing policies . Define . \\[q^*(s,a) = r_a(s) + \\gamma \\langle P_a(s), v^* \\rangle\\,.\\] We call this function $q^*$ the optimal action-value function (in our MDP). The function \\(q^*\\) is easily seen to satisfy \\(M q^* = v^*\\) and thus also \\(q^* = T q^*\\). The promised lemma is as follows: . Lemma (Policy error bound - I.): Let $\\pi$ be a memoryless policy and choose a function $q:\\mathcal{S}\\times\\mathcal{A} \\to \\mathbb{R}$ and $\\epsilon\\ge 0$. Then, the following hold: . | If $\\pi$ is $\\epsilon$-optimizing in the sense that \\(\\sum_a \\pi(a\\vert s) q^*(s,a) \\ge v^*(s)-\\epsilon\\) holds for every state $s\\in \\mathcal{S}$ then $\\pi$ is $\\epsilon/(1-\\gamma)$ suboptimal: \\(v^\\pi \\ge v^* - \\frac{\\epsilon}{1-\\gamma} \\boldsymbol{1}\\,.\\) . | If $\\pi$ is greedy with respect to $q$ then $\\pi$ is $2\\epsilon$-optimizing with \\(\\epsilon= \\|q-q^*\\|_\\infty\\) and thus . | . \\[v^\\pi \\ge v^* - \\frac{2\\|q-q^*\\|_\\infty}{1-\\gamma} \\boldsymbol{1}\\,.\\] . For the proof, which is partially left to the reader, we will find some more notation may be useful. In particular, for a memoryless policy, define the operator $M_\\pi: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}}$: . \\[(M_\\pi q)(s) = \\sum_{a\\in \\mathcal{A}} \\pi(a|s) q(s,a)\\,, \\qquad (q\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}, \\, \\, s\\in \\mathcal{S}).\\] With the help of this operator the condition that $\\pi$ is greedy with respect to $q$ can be written as . \\[M_\\pi q = M q\\,.\\] Further, the first claim of the lemma can be stated in the more concise form $M_\\pi q^* \\ge v^* - 2\\epsilon\\boldsymbol{1}$. For future reference, we will also find it useful to define $P_\\pi: \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}} \\to \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$: . \\[P_\\pi = P M_\\pi\\,.\\] Note that here we abused notation as $P_\\pi$ has already been used to denote the operator that maps functions of the states to functions of the state. From the context, the meaning of $P_\\pi$ will always be clear. Proof: The first part of the proof is standard and is left to the reader. For the second part note that . \\[\\begin{align*} M_\\pi q^* &amp; \\ge M_\\pi(q-\\epsilon \\boldsymbol{1}) = M_\\pi q - \\epsilon\\boldsymbol{1}=M q - \\epsilon \\boldsymbol{1} \\ge M(q^* - \\epsilon \\boldsymbol{1}) - \\epsilon \\boldsymbol{1} = M q^* - 2\\epsilon \\boldsymbol{1} = v^* - 2\\epsilon\\boldsymbol{1}\\,. \\end{align*}\\] Then use the first part. \\(\\qquad \\blacksquare\\) . Suboptimality of almost $\\epsilon$-optimizing policies . There are two issues that need to be taken care of. One is that the planner is randomizing when computing the values $Q_H(s_0,\\cdot)$. What happens when the random next states obtained from the simulator are not “representative”? We cannot expect the outcome of this randomized computation to be precise! Indeed, the best we can expect is that the outcome is “accurate” with some probability, hopefully close to one. In fact, from Hoeffding’s inequality, we see that if we want to achieve small errors in the computation for some target probability, we need to increase the sample size. But Hoeffding’s inequality, in all cases, allows errors which are uncontrolled on some failure event. All in all, the best we can hope for is that with each call, $Q_H(s_0,\\cdot)$ is a good approximation to \\(q^*(s_0,\\cdot)\\) outside of some “failure event” whose probability we will try to separately control. Let us say the probability of this bad event is at most $\\zeta$. Then, outside of this bad event, we hope that the error . \\[\\begin{align} \\delta_H = \\| Q_H(s_0,\\cdot) - q^*(s_0,\\cdot)\\|_\\infty \\label{eq:d0def} \\end{align}\\] is small. Then, using the same proof as for part one of the above lemma, outside of this bad event, the action $A$ returned by the planner is $2\\epsilon$ optimizing at state $s_0$: . \\[q^*(s_0, A) \\ge v^*(s_0) - 2\\epsilon\\,.\\] Let $\\hat \\pi(a \\vert s_0)$ denote the probability that the action $A$ return is $a$: \\(\\hat \\pi(a \\vert s_0)=\\mathbb{P}_{s_0}(A=a)\\), where $\\mathbb{P}_{s_0}$ is the probability measure induced by the interaction of the planner and the MDP simulator. Then, . \\[\\sum_{a} \\hat \\pi(a \\vert s_0) \\mathbb{I}( q^*(s_0, a) \\ge v^*(s_0) - 2\\epsilon ) = \\mathbb{P}_{s_0}( q^*(s_0, A) \\ge v^*(s_0) - 2\\epsilon ) \\ge 1-\\zeta\\,.\\] In words, with probability at least $1-\\zeta$, $\\hat \\pi$ chooses $2\\epsilon$-optimizing actions: The policy is almost $2\\epsilon$-optimizing. While this is not as good as always choosing $2\\epsilon$-optimizing actions, we expect that as $\\zeta\\to 0$ the difference in performance between $\\hat \\pi$ and a policy that always chooses $2\\epsilon$-optimizing actions disappears because performance is expected to depend on action probabilities in a continuous fashion. The next lemma makes this precise: . Lemma (Policy error bound II): Let $\\zeta\\in [0,1]$, $\\pi$ be a memoryless policy that selects $\\epsilon$-optimizing actions with probability at least $1-\\zeta$ in each state. Then, . \\[v^\\pi \\ge v^* - \\frac{\\epsilon+2\\zeta \\|q^*\\|_\\infty}{1-\\gamma} \\boldsymbol{1}\\,.\\] . Proof: By Part 1 of the previous lemma, it suffices to show that $\\pi$ is \\(\\epsilon+2\\zeta \\|q^*\\|_\\infty\\)-optimizing in every state. This follows from algebra and is left to the reader. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec6/#good-action-value-approximations-suffice",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec6/#good-action-value-approximations-suffice"
  },"386": {
    "doc": "6. Local Planning - Part II.",
    "title": "Error control",
    "content": "What remains is to show that with high probability, the error $\\delta_H$, defined in \\(\\eqref{eq:d0def}\\) is small. Intuitively, $\\hat T \\approx T$. To firm up this intuition we may note that for any fixed $q\\in \\mathbb{R}^{\\mathcal{S}\\times \\mathcal{A}}$ function over the state-action pairs such that $|q|_\\infty \\le \\frac{1}{1-\\gamma}$ and for any fixed $(s,a)\\in \\mathcal{S}\\times \\mathcal{A}$, by Eq. \\(\\eqref{eq:hoeffbop}\\) and the choice of the sets $\\mathcal{C}(s,a)$, with probability $1-\\zeta$, . \\[\\begin{align} |\\hat T q (s,a)-T q(s,a)| &amp; = \\gamma \\left| \\frac1m \\sum_{s'\\in \\mathcal{C}(s,a)} v(s')\\,\\, - \\langle P_a(s), v \\rangle \\right| \\le \\gamma \\|q\\|_\\infty\\, \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} } \\nonumber \\\\ &amp;\\le \\frac{\\gamma}{1-\\gamma}\\, \\sqrt{\\frac{\\log \\frac{2}{\\zeta}}{2m} } =: \\Delta(\\zeta,m), \\label{eq:basicerror} \\end{align}\\] where, for brevity, we introduced $v = Mq$ in the above formula. Union bounds . So we know that for any fixed state-action pair $(s,a)$, outside of a low probability event, $(\\hat T q)(s,a)$ is close to $( T q)(s,a)$. But can we conclude from this that, outside of some low probability event, $(\\hat T q)(s,a)$ is close to $( T q)(s,a)$ everywhere? . To answer this question, it will be easier to turn it around and ask just try to come up with some event that, on the one hand, has low probability, while, in the other hand, outside of this event, $(\\hat T q)(s,a)$ is close to $( T q)(s,a)$ regardless of $(s,a)$. Denoting by $\\mathcal{E}(s,a)$ the event when $(\\hat T q)(s,a)$ is not close to $( T q)(s,a)$, i.e., . \\[\\mathcal{E}(s,a) = \\{ |(\\hat T q)(s,a) - (\\hat T q)(s,a)|&gt; \\Delta(\\zeta,m) \\}\\,,\\] it is clear that if $\\mathcal{E} = \\cup_{(s,a)} \\mathcal{E}(s,a)$ then outside of $\\mathcal{E}$, none of $\\mathcal{E}(s,a)$ holds and hence . \\[\\max_{(s,a)\\in \\mathcal{S}\\times \\mathcal{A}} |(\\hat T q)(s,a) - (\\hat T q)(s,a)|\\le \\Delta(\\zeta,m)\\,.\\] But how large can the probability of $\\mathcal{E}$ be? For this, recall the following elementary result, which follows directly from the properties of measures: . Lemma (Union Bound): For any probability measure $\\mathbb{P}$ and any countable sequence of events \\(A_1, A_2, \\ldots\\) of the underlying measurable space, . \\[\\mathbb{P}\\left(\\cup_i A_i \\right) \\leq \\sum_i \\mathbb{P}(A_i).\\] . By this result, using that $\\mathcal{S}\\times \\mathcal{A}$ is finite, . \\[\\mathbb{P}(\\mathcal{E}) \\le \\sum_{(s,a)\\in \\mathcal{S}\\times \\mathcal{A}} \\mathbb{P}( \\mathcal{E}(s,a)) \\le \\mathrm{S} \\mathrm{A} \\zeta\\,.\\] If we want this probability to be $0\\le \\zeta’\\le 1$, we can set $\\zeta = \\frac{\\zeta’}{\\mathrm{S}\\mathrm{A}}$ and conclude that with probability $1-\\zeta’$, for any state-action pair $(s,a)\\in \\mathcal{S}\\times \\mathcal{A}$, . \\[\\begin{align} |(\\hat T q)(s,a) - (\\hat T q)(s,a)| \\le \\Delta\\left(\\frac{\\zeta'}{\\mathrm{S}\\mathrm{A}},m\\right) = \\frac{\\gamma}{1-\\gamma} \\, \\sqrt{\\frac{\\log \\frac{2\\mathrm{S}\\mathrm{A}}{\\zeta}}{2m} }\\,. \\label{eq:ropec} \\end{align}\\] The following diagram summarizes the idea of union bounds: . To control the error of some bad event happening, we can break the the bad event into a number of elementary parts. By controlling the probability of each such part, we can control the probability of the bad event, or, alternatively, control the probability of the complementary “good” event. The worst case for controlling the probability of the bad event is if the elementary parts do not overlap, but the argument of course works even in this case. Returning to our calculations, from the last formula we see that the errors grew a little compared to \\(\\eqref{eq:basicerror}\\), but the growth is modest: the errors scale with the logarithm of the number of state-action pairs. While this logarithmic error-growth is mild, it is unfortunate that the number of states appears here. To control the errors, by this formulae we would need to choose $m$ to be proportional to the logarithm of the size of the state space, which is better than a linear dependence, but still. One must wonder whether this dependence is truly necessary? If it was, there would be a big gap between the complexity of planning in deterministic and stochastic MDPs. We should not give in for this just yet! . Avoiding dependence on state space cardinality . The key to avoiding the dependence on the cardinality of the state is to avoid taking union bounds over the whole state-action set. That this may be possible follows from that, thinking back to the recursive implementation of the planner, we can notice that the planner does not necessarily rely on all the sets $\\mathcal{C}(s,a)$. To get a handle on this, it will be useful to introduce a notion of a distance induced by the set $\\mathcal{C}(s):=\\cup_{a\\in \\mathcal{A}}\\mathcal{C}(s,a)$ between the states. This distance between states $s$ and $s’$ (denoted by $\\text{dist}(s,s’)$) will be the smallest number of steps that we can take to get from $s$ to $s’$, if in each step we can choose one a “neighbouring” state to the last state, starting from state $s$. Formally, this is the length $n$ of the shortest sequence $s_0,s_1,\\dots,s_n$ such that $s_0=s$, $s_n = s’$ and for each $i\\in [n]$, $s_i \\in \\mathcal{C}(s_{i-1})$ (this is the distance between states in the directed graph over the states with edges induced by $\\mathcal{C}$). With this, for $h\\ge 0$, define . \\[\\begin{align*} \\mathcal{S}_h &amp;= \\{s \\in \\mathcal{S} | \\text{ dist}(s_0,s) \\leq h \\} \\end{align*}\\] as the set of states accessible from $s_0$ by at most $h$ steps. Note that this is a nested sequence of sets and $\\mathcal{S}_0 = {s_0}$, $\\mathcal{S}_1$ contains $s_0$ and its immediate “neighbors”, etc. We may now observe that in the calculation of $Q_H(s_0,\\cdot)$ when function $q$ is called with a certain value of $0\\le k \\le H$, for the state that appears in the call we have . \\[s\\in \\mathcal{S}_{H-k}\\,.\\] This can be proved by induction on $k$, starting with $k=H$. Click here for the proof. The base case follows because when $q$ calls itself it decrements $k$. Hence, when $q$ is called with $k=H$ and state $s$, $s=s_0$ must be true. Hence, $s\\in \\mathcal{S}_0$. Now, assume that the claim holds for $k=i+1$ with some $0\\le i&lt; H$. Take any state $s'$ that $q$ is called on while $k=i$. Since $i&lt;H$, this call must be a recursive call (from line 3). Going up on the call chain, at the time this recursive call is made, $k=i+1$ (since in the recursive calls the value of $k$ is decremented). This call happens when $s'\\in \\mathcal{C}(s,a)$ for some action $a\\in \\mathcal{A}$ and some state $s$, which, by the induction hypothesis, satisfies $s\\in \\mathcal{S}_{H-(i+1)}$. It follows that $s$ is at a distance of at most $H-i-1$ from $s_0$, while $s'$, a \"neighbour\" of $s$, is at most of a distance of $H-i-1+1=H-i$ from $s_0$. Hence, $s\\in \\mathcal{S}_{H-i}$, finishing the induction. $$\\blacksquare$$ Taking into account that when $q$ is called with $k=0$, the sets $\\mathcal{C}(s,a)$ are not used (line 2), we see that only states $s$ from $\\mathcal{S}_{H-1}$ are such that the calculation ever uses the set $\\mathcal{C}(s,a)$. Since $|\\mathcal{C}(s,a)|=m$, . \\[\\mathcal{S}_h \\le 1 + (mA) + \\dots + (mA)^h \\le (mA)^{h+1}\\] and in particular, $\\mathcal{S}_{H-1}\\le (mA)^H$, which is independent of the size of the state space. Of course, all along, we knew this very well: This is why the total runtime is also independent of the size of the state space. The plan is to take advantage of this to avoid a union bound over all possible state-action pairs. We start with a recursive expression for the errors. Recall that \\(\\delta_H = \\| (\\hat T^H \\boldsymbol{0})(s_0,\\cdot)-q^*(s_0,\\cdot)\\|_\\infty\\). By the triangle inequality, . \\[\\begin{align*} \\delta_H &amp; = \\| (\\hat T^H \\boldsymbol{0})(s_0,\\cdot)-q^*(s_0,\\cdot)\\|_\\infty \\le \\| (\\hat T \\hat T^{H-1} \\boldsymbol{0})(s_0,\\cdot)- \\hat T q^*(s_0,\\cdot)\\|_\\infty + \\| \\hat T q^*(s_0,\\cdot)- q^*(s_0,\\cdot)\\|_\\infty\\,. \\end{align*}\\] Now, observing that . \\[\\vert \\hat T q (s,a)-\\hat T q^* (s,a) \\vert \\le \\frac{\\gamma}{m} \\sum_{s'\\in \\mathcal{C}(s,a)} \\vert Mq - v^* \\vert (s') \\le \\gamma \\max_{s'\\in \\mathcal{C}(s)} \\vert Mq - v^* \\vert (s')\\,,\\] we see that . \\[\\begin{align*} \\delta_H &amp; \\le \\gamma \\max_{s'\\in \\mathcal{C}(s_0),a\\in \\mathcal{A}} | (\\hat T^{H-1} \\boldsymbol{0})(s',a)-q^*(s',a) | + \\| \\hat T q^*(s_0,\\cdot)- q^*(s_0,\\cdot)\\|_\\infty\\,. \\end{align*}\\] In particular, defining . \\[\\delta_{h} = \\underbrace{\\max_{s'\\in \\mathcal{S}_{H-h},a\\in \\mathcal{A}} | \\hat T^{h} \\boldsymbol{0}(s',a)-q^*(s',a)|}_{=:\\| \\hat T^h \\boldsymbol{0}-q^*\\|_{\\mathcal{S}_{H-h}}}\\,,\\] we see that . \\[\\delta_H \\le \\gamma \\delta_{H-1} + \\| \\hat T q^* - q^* \\|_{\\mathcal{S}_0}\\,,\\] where we use the notation \\(\\| q \\|_{\\mathcal{U}} = \\max_{s\\in \\mathcal{U},\\max_{a\\in \\mathcal{A}}} |q(s,a)|\\). More generally, we can prove by induction on $1\\le h \\le H$ (starting with $h=H$) that . \\[\\delta_h \\le \\gamma \\delta_{h-1} + \\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-h}} \\le \\gamma \\delta_{h-1} + \\underbrace{ \\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-1}}}_{=:\\varepsilon'/(1-\\gamma)} \\,,\\] while . \\[\\delta_0 = \\| q^*\\|_{\\mathcal{S}_{H}} \\le \\| q^* \\|_\\infty \\le \\frac{1}{1-\\gamma}\\,,\\] where the last inequality uses that $r_a(s)\\in [0,1]$, which we shall assume for simplicity. Unfolding this recursion for $(\\delta_h)_h$, letting . we get . \\[\\begin{align} \\delta_H &amp;\\leq \\frac{\\gamma^H + \\varepsilon'(1 + \\gamma + \\cdots + \\gamma^{H-1})}{1 - \\gamma} \\leq \\left(\\gamma^H + \\frac{\\varepsilon'}{1 - \\gamma} \\right) \\frac{1}{1 - \\gamma} \\label{eq:delta_H}. \\end{align}\\] We see that the first term in the sum on the right-hand side (in the parenthesis) is controlled by $H$. It remains to show that $\\varepsilon’$ can also be controlled (by choosing $m$ appropriately). In fact, notice that $\\varepsilon’$ is the maximum-norm error with which \\(\\hat T q^*\\) approaximates \\(q^* = T q^*\\), but only for states in \\(\\mathcal{S}_{H-1}\\) we need to control this error. By our earlier argument, this set has at most \\((mA)^H\\) states, hence, it is believable that this error can be controlled even when $m$ is chosen independently of the number of states. Controlling \\(\\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-1}}\\) . Since \\(\\mathcal{S}_{H-1}\\) has only $(mA)^H$ states in it, one’s first instinct is to take a union bound over the error events for the states in this set. The trouble is that the set \\(\\mathcal{S}_{H-1}\\) itself is random. As such, it is not clear, what the failure events should be? And how many failure events are we going to have? The size of this set is also random! Notice that if \\((A_i)_{i\\in [n]}\\) are some events with \\(\\mathbb{P}(A_i)\\le \\delta\\) and \\(I_1,\\dots,I_k\\in [n]\\) are random indices, it does not hold that \\(\\mathbb{P}( \\cup_{j=1}^k A_{I_j}) \\le k \\delta\\): One cannot apply the union bound to randomly chosen events. In fact, in the worst case, \\(\\mathbb{P}( \\cup_{j=1}^k A_{I_j}) = n \\delta\\)! . To exploit that \\(\\mathcal{S}_{H-1}\\) is a small set, we need to use one more time the structure. The reason that the randomness of \\(\\mathcal{S}_{H-1}\\) is not going to matter too much is because of the special way this set is constructed. First of all, clearly, \\(s_0\\in \\mathcal{S}_{H-1}\\) always and at this state the error \\(\\|(\\hat T q^*)(s_0,\\cdot)- Tq^*(s_0,\\cdot)\\|_\\infty\\) is under control by Hoeffding’s inequality. Next, we may consider the neighbors of \\(s_0\\). If \\(S\\in \\mathcal{C}(s_0)\\), either \\(S=s_0\\), in which case we already know that the error at \\(S\\) is under control, or \\(S\\) is a “bona fide neighbor” and we can think of then generating the elements in \\(\\mathcal{C}(S,a)\\) just inside the call of $q$. Ultimately, the error at such a neighbor is under control because, by definition, all the sets \\(\\mathcal{C}(s,a)\\) (with $(s,a)$ sweeping through all possible state-action pairs) are independently chosen. This suggests that we should consider the chronological order in which in the recursive call of function $q$ the states in \\(\\mathcal{S}_{H-1}\\) appear. Let this order be $S_1,S_2,\\dots,S_n$, where $n = 1+(mA)+\\dots+(mA)^{H-1}$, $S_1=s_0$, $S_2$ is the second state that $q$ is called on (necessarily, \\(S_2\\in \\mathcal{S}(s_0)\\)), \\(S_3\\) is the third such state. Note that states may reappear in this sequence multiple times. Furthermore, by construction, \\(\\mathcal{S}_{H-1} = \\{ S_1,\\dots,S_n \\}\\). Also note that the length of this sequence is not random: This length is exactly the number of times $q$ is called, which is clearly not random. That \\(\\| \\hat T q^* - q^* \\|_{\\mathcal{S}_{H-1}}=\\| \\hat T q^* - Tq^* \\|_{\\mathcal{S}_{H-1}}\\) is under control directly follows from the next lemma: . Lemma: Assume that the immediate rewards belong to the $[0,1]$ interval. For any $0\\le \\zeta \\le 1$ with probability $1-\\mathrm{A}n\\zeta$, for any $1\\le i \\le n$, . \\[\\begin{align*} \\| \\hat T q^* (S_i, \\cdot)- q^* (S_i,\\cdot) \\|_{\\infty} \\le \\Delta(\\zeta,m)\\,, \\end{align*}\\] where \\(\\Delta\\) is given by \\(\\eqref{eq:basicerror}\\). Proof: Recall that $\\mathcal{C}(s,a) = (S_1’(s,a),\\dots,S_m’(s,a))$ where (i) the \\((\\mathcal{C}(s,a))_{(s,a)}\\) are mutually independent and (ii) for any $(s,a)$, $(S_i’(s,a))_i$ is an i.i.d. sequence with common distribution $P_a(s)$. For \\(s\\in \\mathcal{S}\\), \\(a\\in \\mathcal{A}\\), \\(C\\in \\mathcal{S}^m\\), let . \\[\\begin{align*} g(s,a,C) =| \\frac{\\gamma}{m} \\sum_{s'\\in C} v^*(s') \\,\\, - \\langle P_a(s), v^* \\rangle | \\end{align*}\\] (as earlier, $s’\\in C$ means that $s’$ is an element of the set composed of the elements in the sequence $C$). Recall that by the definition of $\\hat T$ and the properties of $q^*$, . \\[\\begin{align} |\\hat T q^*(s,a)- q^*(s,a)| = | \\frac{\\gamma}{m} \\sum_{s'\\in \\mathcal{C}(s,a)} v^*(s') \\,\\, - \\langle P_a(s), v^* \\rangle | = g(s,a,\\mathcal{C}(s,a)) \\,. \\label{eq:dub} \\end{align}\\] Fix \\(1 \\le i \\le n\\). Let \\(\\tau = \\min\\{ 1\\le j \\le i\\,:\\, S_j = S_i \\}\\). That is, $\\tau$ is the time when $S_i$ first appears in the sequence \\(\\{S_i\\}_i\\). Fix $a\\in \\mathcal{A}$. We claim that given \\(S_{\\tau}\\), \\((S_j'(S_{\\tau},a))_{j=1}^m\\) is i.i.d. with common distribution \\(P_a(S_{\\tau})\\). That is, for any \\(s,s_1',\\dots,s_m'\\in \\mathcal{S}\\), . \\[\\begin{align} \\mathbb{P}( S_1'(S_{\\tau},a)=s_1',\\dots, S_m'(S_{\\tau},a)=s_m' \\, \\vert\\, S_{\\tau}=s) = \\prod_{j=1}^m P(s,a,s_j') \\label{eq:indep} \\end{align}\\] Note that given this, for any $\\Delta\\ge 0$, by \\(\\eqref{eq:dub}\\), . \\[\\begin{align*} \\mathbb{P}( &amp; |\\hat T q^*(S_i,a)- q^*(S_i,a)| &gt; \\Delta ) = \\mathbb{P}( g(S_i,a,\\mathcal{C}(S_i,a)) &gt; \\Delta ) \\\\ &amp; = \\mathbb{P}( g(S_{\\tau},a,\\mathcal{C}(S_\\tau,a)) &gt; \\Delta ) \\\\ &amp; = \\sum_{s} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, S_{\\tau}=s ) \\\\ &amp; = \\sum_{s} \\sum_{1\\le j \\le i} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, S_j=s, \\tau=j ) \\\\ &amp; = \\sum_{s} \\sum_{1\\le j \\le i} \\sum_{\\substack{s_{1:j-1} \\in \\mathcal{S}^{j-1}:\\\\ s\\not\\in s_{1:j-1}}} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, S_j=s, S_{1:j-1}=s_{1:j-1})\\\\ &amp; = \\sum_{s} \\sum_{1\\le j \\le i} \\sum_{\\substack{s_{1:j-1} \\in \\mathcal{S}^{j-1}:\\\\ s\\not\\in s_{1:j-1}}} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, \\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1 )\\,, \\end{align*}\\] for some binary valued functions $\\phi_1$, $\\dots$, $\\phi_i$ where for $1\\le j \\le i$, $\\phi_j$ is defined so that . \\[\\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1\\] holds if and only if $S_j=s, S_{1:j-1}=s_{1:j-1}$ holds, where $s\\in \\mathcal{S}$ and $s_{1:j-1}\\in \\mathcal{S}^{j-1}$ are arbitrary so that $s\\not\\in s_{1:j-1}$. That such functions exist follows because for any sequence $s_{1:j}$ to verify whether $S_{1:j}=s_{1:j}$ the knowledge of the sets $\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1})$ suffices: The appropriate function should first check $S_1=s_1$, then move on to checking $S_2=s_2$ only if $S_1=s_1$ holds, etc. Now, notice that by our assumptions, for $s\\not\\in s_{1:j-1}$, $\\mathcal{C}(s,a)$ and $\\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1$ are independent of each other. Hence, . \\[\\begin{align*} \\mathbb{P}( &amp; g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta, \\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1 )\\\\ &amp; = \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta) \\cdot \\mathbb{P}(\\phi_j( s,s_{1:j-1},\\mathcal{C}(s_1),\\dots,\\mathcal{C}(s_{j-1}) )=1 )\\,. \\end{align*}\\] Plugging this back into the previous displayed equation, “unrolling” the expansion done using the law of total probability, we find that . \\[\\begin{align*} \\mathbb{P}( |\\hat T q^*(S_i,a)- q^*(S_i,a)| &gt; \\Delta ) &amp; = \\sum_{s} \\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta ) \\mathbb{P}( S_\\tau = s )\\,. \\end{align*}\\] Now, choose $\\Delta = \\Delta(\\zeta,m)$ from \\(\\eqref{eq:basicerror}\\) so that, thanks to $|q^*|_\\infty \\le 1/(1-\\gamma)$, for any fixed $(s,a)$, \\(\\mathbb{P}( g(s,a,\\mathcal{C}(s,a)) &gt; \\Delta(\\zeta,m) )\\le \\zeta\\) Plugging this in into the previous display we get . \\[\\begin{align*} \\mathbb{P}( |\\hat T q^*(S_i,a)- q^*(S_i,a)| &gt; \\Delta(\\zeta,m) ) \\le \\zeta \\sum_{s} \\mathbb{P}( S_\\tau = s ) = \\zeta\\,. \\end{align*}\\] The claim the follows by a union bound over all actions and all $1\\le i \\le n$. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec6/#error-control",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec6/#error-control"
  },"387": {
    "doc": "6. Local Planning - Part II.",
    "title": "Final error bound",
    "content": "Putting everything together, we get that for any $0\\le \\zeta \\le 1$, the policy $\\hat \\pi$ induced by the planner is $\\epsilon(m,H,\\zeta)$-optimal with . \\[\\epsilon(m,H,\\zeta):=\\frac{2}{(1-\\gamma)^2} \\left[\\gamma^H + \\frac{1}{1-\\gamma} \\sqrt{ \\frac{\\log\\left(\\frac{2n\\mathrm{A}}{\\zeta}\\right)}{2m} } + \\zeta \\right]\\,.\\] Thus, to obtain a planner that induces a $\\delta$-optimal policy, we can set $H$, $\\zeta$ and $m$ so that each term above contributes at most $\\delta/3$: . \\[\\begin{align*} \\frac{2\\gamma^H}{1-\\gamma} &amp; \\le (1-\\gamma)\\frac{\\delta}{3}\\,,\\\\ \\zeta &amp; \\le (1-\\gamma)^2\\frac{\\delta}{6}\\, \\qquad \\text{and}\\\\ \\frac{m}{\\log\\left(\\frac{2n\\mathrm{A}}{\\zeta}\\right)} &amp; \\ge \\frac{18}{\\delta^2(1-\\gamma)^6}\\,. \\end{align*}\\] For $H$ we get that we can set $H = \\lceil H_{\\gamma,(1-\\gamma)\\delta/6}\\rceil$. We can also set $\\zeta = (1-\\gamma)^2\\delta/6$. To solve for the smallest $m$ that satisfies the last inequality, recall that $n = (mA)^H$. To find the critical value of $m$ note the following elementary result which we cite without a proof: . Proposition: Let $a&gt;0$, $b\\in \\mathbb{R}$. Let \\(t^*=\\frac{2}{a}\\left[ \\log\\left(\\frac1a\\right)-b \\right]\\). Then, for any positive real $t$ such that $t\\ge t^*$, . \\[\\begin{align*} at+b &gt; \\log(t)\\,. \\end{align*}\\] . From this, defining . \\[c_\\delta = \\frac{18}{\\delta^2(1-\\gamma)^6}\\] and . \\[\\begin{align} m^*(\\delta,\\mathrm{A}) = 2c_\\delta \\left[ H \\log(c_\\delta H) + \\log\\left(\\frac{12}{(1-\\gamma)^2\\delta}\\right) + (H+1) \\log(\\mathrm{A}) \\right] \\label{eq:mstar} \\end{align}\\] if $m \\ge m^*$ then all the inequalities are satisfied. Putting things together, we thus get the following result: . Theorem: Assume that the immediate rewards belong to the $[0,1]$ interval. There is a local planner such that for any $\\delta\\ge 0$, in any discounted MDP with discount factor $\\gamma$, the planner induces a $\\delta$-optimal policy and uses at most \\(O( (m^* \\mathrm{A})^H )\\) elementary arithmetic and logic operations per its calls, where $m^*(\\delta,\\mathrm{A})$ is given by \\(\\eqref{eq:mstar}\\) and $H = \\lceil H_{\\gamma,(1-\\gamma)\\delta/3}\\rceil$. Overall, we see that the runtime did increase compared to the deterministic case (apart from logarithmic factors, in the above result $m = H^8/\\delta^2$ whereas in the deterministic case $m=1$!), but we managed to get a runtime that is independent of the cardinality of the state space. Again, what is troubling is the exponential dependence on the effective horizon, though as we have seen, in the worst-case, this is unavoidable. In the next lectures we will consider proving the planner with extra information so that this exponential dependence can be avoided. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec6/#final-error-bound",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec6/#final-error-bound"
  },"388": {
    "doc": "6. Local Planning - Part II.",
    "title": "Notes",
    "content": "Sparse lookahead trees . The idea of the algorithm that we analyzed comes from a paper by Kearns, Mansour and Ng from 2002. In their paper they consider the version of the algorithm which creates a fresh “new” random set $\\mathcal{C}(s,a)$ in every recursive call. This makes it harder to see their algorithm as approximating the Bellman operator, but in effect, the two approaches are by and large the same. In fact, if we introduce $H$ random operators, $\\hat T_1$, $\\dots$, $\\hat T_H$ which are the same as $\\hat T$ above but $\\hat T_h$ has its own “private” sets $( \\hat C_h(s,a) )_{(s,a)}$, then their algorithm can be written as computing . \\[A = \\arg\\max_{a} (\\hat T_1 \\dots \\hat T_h \\boldsymbol{0})(s_0,a)\\,.\\] It is not hard to modify the analysis given here to accommodate this change. With this, one can also interpret the calculations done by the algorithm as backing up values in a “sparse lookahead tree” built recursively from $s_0$. Much work has been devoted to improving these basic ideas and eventually these ideas led to various Monte-Carlo tree search algorithms, including yours truly’s UCT. In general, these algorithms attempt to improve on the runtime by building the trees when they need to be built. As it turns out, a useful strategy here is to expand nodes which in a way hold the greatest promise to improve the value at the “root”. This is known as the “optimisism in planning”. Note that A* (and its MDP relative, AO) are also based on optimism: A’s admissible heuristic functions in our language correspond to functions that upper bound the optimal value. The definite source on MCTS theory as of today is Remi Munos’s monograph. Measure concentration . Hoeffding’s inequality is a special case of what is known as measure concentration. This phrase refers to that the empirical measure induced by a sample is a good approximation to the whole measure. The simplest case is when one just compares the means of the measures (the empirical and the sample-generating one), giving rise to concentration inequalities around the mean. Hoeffding’s inequality is an example. What we like about Hoeffding’s inequality (besides that it is simple) is that the failure probability, $\\delta$ (later $\\zeta$) appears inside a logarithm. That means, that the price of being more stringent is mild. When the exact dependence is of type that appears in Hoeffding’s inequality (i.e., $\\sqrt{ \\log(1/\\delta)})$), we say that the deviation of the subgaussian type because Gaussian random variables also satisfy an inequality like this. Concentration of measure and concentration inequalities are a central topic in probability theory, with separate books devoted to them. A few favourites are given at the end of this notes . For learning purposes, Pollard’s mini-book is nice (but all these books have pros and cons), or Vershynin’s book. The comparison inequality . The comparison inequality between the logarithm and the linear function is given as Proposition 4 here. The proof is based on two observations: First, it is enough to consider the case when $b=0$. Then, if $a\\ge 1$, the result is trivial, while for $a&lt; 1$, the guess is based on doubling the value where the growth rate of $t\\mapsto at$ matches that of $t\\mapsto \\log(t)$. A model-centered view and random operators . A key idea of this lecture is that $\\hat T$ is a good (random) approximation to $T$, hence, it can be used in place of $T$. One can also tell this story by saying that the data underlying $\\hat T$ gives a random approximation to the MDP; the transition probabilities of this random approximating MDP would be defined using . \\[\\hat P(s,a,s') = \\frac1m \\sum_{s''\\in C(s,a)} \\mathbb{I}\\{ s''=s'\\}\\] It may seem quite miraculous that with only a few elements in $C(s,a)$ (i.e., small $m$) we get a good approximation to the next state distribution. But so is the magic of randomness! Using a random operator (or a sequence of them, if, as outlined above, one uses a fresh set of random next state every time an update is calculated) in a dynamic programming method has been coined empirical dynamic programming by Haskell et al.. A bigger point is that for a model to be a “good” approximation to the “true MDP”, it suffices that the Bellman optimality operator that it induces is a “close” approximation to the Bellman optimality operator of the true MDP. This in fact brings us to our next topic, which is what happens when the simulator is imperfect? . Imperfect simulation model? . We can rarely expect simulators to be perfect. Luckily, not all is lost in this case. As noted above, if the simulator induced an MDP whose Bellman optimality operator is in a way close to the Bellman optimality operator of the true MDP, we expect the outcome of planning to be still a good policy in the true MDP. In fact, the above proof has already all the key elements in place to show this. In particular, it is not hard to show that if $\\hat T$ is a $\\gamma$ max-norm contraction and $\\hat q^*$ is its fixed point then . \\[\\|\\hat q^* - q^*\\|_\\infty \\le \\frac{\\| \\hat T q^* - T q^* \\|_\\infty}{1-\\gamma}\\,,\\] which, combined with the our first lemma of this lecture on the policy error bound gives that the policy that is greedy with respect to $\\hat q^*$ is . \\[\\frac{2\\| \\hat T q^* - T q^* \\|_\\infty }{(1-\\gamma)^2}\\] optimal in the MDP underlying $T$. We will return to this in later lectures. In particular, in batch reinforcement learning, one of the basic methods is to learn a “model” of the environment and as such it is inevitable to study the error that results from modelling errors. See Lecture 17 and Lecture 18. Monte-Carlo methods . We saw in homework 0 that randomization may help a little, and today we saw that it can help in a more significant way. A major lesson again is that representations do matter: If the MDP is not given with a “generative simulator”, getting such a simulator may be really hard. This is good to remember when it comes to learning models: . One should insist on learning models that make the job of planners easier. Generative models are one such case, provably, as we have seen in today’s lecture put together with our previous lower bound that involved the number of states. Randomization, more generally, is a powerful tool in computing science, which brings us to a somewhat philosophical question: What is randomness? Does “true randomness” exist? Can we really build computers to harness this? . True randomness? . What is the meaning of “true” randomness? The margin is definitely not big enough to explain this. Hence, we just leave this there, hanging, for everyone to ponder about. But let’s also note that this is a thoroughly studied question in theoretical computing science, with many beautiful results and even books. Arora and Barak’s book on computational complexity (Chapters 7, 20 and 21) is a good start for exploring this. Can we recycle the sets $C(s,a)$ between the calls? . If simulation is expensive, it may be tempting to recycle the sets between calls of the planner. After all, even if we recycle these sets, $\\hat pi$ will have the property that it selects $\\epsilon$-optimizing actions with high probability at every state. However, this may not be a good idea. The reader is challenged to think about what can go wrong? The proof actually uses that the planner construct a new random operator $\\hat T$ with every call. But where is this used? . The ubiquity of continuity arguments in the MDP literature . All the computations that we do with MDPs tend to be approximate. We evaluate policies approximately. We compute a Bellman back approximately. We have approximate models. We greedify approximately. If any of these operations could enlarge small errors, none of the approximate methods would work. The study of approximate computations (which is a necessity if one faces large MDPs) is a study of the sensitivity of the values of the resulting policies to the errors introduced in the computations. This, in numerical analysis, would be called error analysis. In other areas of mathematics, this is called sensitivity analysis. In fact, sensitivity analysis often involves computing derivatives to see how fast outputs change as the inputs change (which is that data that will be approximated). What should we be taking derivatives with respect to here? Well, it is always the data that is being changed. One can in fact use differentiation based sensitivity analysis everywhere. This has been tried a little in the “older” MDP literature and is also related to policy gradient theorems (that we will learn about laters). However, perhaps there are more nice things to be discovered about this approach. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec6/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec6/#notes"
  },"389": {
    "doc": "6. Local Planning - Part II.",
    "title": "References",
    "content": ". | Kearns, M., Mansour, Y., &amp; Ng, A. Y. (2002). A sparse sampling algorithm for near-optimal planning in large Markov decision processes. Machine learning, 49(2), 193-208. [link] | David Pollard (2015). A few good inequalities. Chapter 2 of a book under preparation with working title “MiniEmpirical”. [link] | Stephane Boucheron, Gabor Lugosi and Pascal Massart (2012). Concentration inequalities: A nonasymptotic theory of indepndence. Clarendon Press – Oxford. [link] | Roman Vershynin (2018). High-Dimensional Probability: An Introduction with Applications in Data Science. [link] | M. J. Wainwright (2019) High-dimensional statistics: A non-asymptotic viewpoint. Cambridge University Press. | Lafferty J., Liu H., &amp; Wasserman L. (2010). Concentration of Measure. [link] | Lattimore, T., &amp; Szepesvári, C. (2020). Bandit algorithms. Cambridge University Press. | William B. Haskell, Rahul Jain, and Dileep Kalathil. Empirical dynamic programming. Mathematics of Operations Research, 2016. | Sanjeev Arora and Boaz Barak (2009). Computational Complexity: A Modern Approach. Cambridge University Press. | Remi Munos (2014). From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning. Foundations and Trends in Machine Learning: Vol. 7: No. 1, pp 1-129. | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec6/#references",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec6/#references"
  },"390": {
    "doc": "6. Local Planning - Part II.",
    "title": "6. Local Planning - Part II.",
    "content": "In the previous lecture local planning was introduced. The main idea is to amortize the cost of planning by asking a planner to produce an action to be taken at a particular state so that the policy induced by repeatedly calling the planner at the states just visited and then using the action returned by the planner is near-optimal. We have seen that with this, the cost of planning can be made independent of the size of the state space – at least for deterministic MDPs. For this, one can use just a recursive implementation of value iteration, which, for convenience, we wrote using action-value functions and the corresponding Bellman optimality operator, $T$, defined by . \\[\\begin{align*} T q(s,a) = r_a(s) + \\gamma \\langle P_a(s), M q \\rangle\\,. \\end{align*}\\] (in the previous lecture we used $\\tilde T$ to denote this operator, but to reduce clutter from now on, we will drop the tilde). We have also seen that no procedure can do significantly better in terms of its runtime (or query cost) than this simple recursive procedure. In this lecture we show that these ideas also extend to the stochastic case. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec6/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec6/"
  },"391": {
    "doc": "7. Function Approximation",
    "title": "Hints on value functions",
    "content": "The hints that we start with will concern the value functions. In particular, they state that either the optimal value, or the value function of all policies are effectively compressible. For motivation, consider the figure on the right. Imagine the state space is an interval of the real line and the optimal value function in an MDP looks like as shown on the figure: It is a nice, smooth function over the interval. As is well known, such relatively slowly changing functions can be well approximated by using the linear combination of a few fixed basis functions, like an appropriate polynomial, or Fourier basis, or using splines. Then, one hopes that even though the state space is large or even infinite as in this example, there could perhaps be a method that calculates the few coefficients needed get a good approximation to \\(v^*\\) with a runtime that depends polynomially on the horizon, the number of actions and the number of coefficients that one needs to calculate. Given the knowledge of $v^*$ and simulator access to the MDP, good actions can then be efficiently obtained by performing one-step lookahead computations. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec7/#hints-on-value-functions",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec7/#hints-on-value-functions"
  },"392": {
    "doc": "7. Function Approximation",
    "title": "Linear function approximation",
    "content": "If the basis functions mentioned are $\\phi_1,\\dots,\\phi_d: \\mathcal{S} \\to \\mathbb{R}$ then, formally, the hope is that with some coefficients $\\theta =(\\theta_1,\\dots,\\theta_d)^\\top\\in \\mathbb{R}^d$, we will have . \\[\\begin{align} v^*(s) = \\sum_{i=1}^d \\theta_i \\phi_i(s)\\, \\qquad \\text{for all } s\\in \\mathcal{S}\\,. \\label{eq:vstr} \\end{align}\\] In the reinforcement learning literature, the vector $(\\phi_1(s), \\dots, \\phi_d(s))^\\top$ is called the feature vector assigned to state $s$. For a more compact notation we also use $\\phi$ to be a map from $\\mathcal{S}$ to $\\mathbb{R}^d$ which assigns the feature vectors to the states: . \\[\\phi(s) = (\\phi_1(s),\\dots,\\phi_d(s))^\\top\\,.\\] Conversely, given $\\phi: \\mathcal{S}\\to \\mathbb{R}^d$, its component are denoted using $\\phi_1,\\dots,\\phi_d$. It will also be useful to introduce a matrix notation: Recall that the number of states is $\\mathrm{S}$ and without loss of generality we may assume that $\\mathcal{S} = [\\mathrm{S}]$. Then, we can treat each of $\\phi_1,\\dots,\\phi_d$ as $\\mathrm{S}$-dimensional vectors: The $i$th component of $\\phi_j$ is $\\phi_j(i)$. Then, we can stack $\\phi_1,\\dots,\\phi_d$ next to each other to form a matrix: . \\[\\Phi = \\begin{pmatrix} | &amp; | &amp; \\dots &amp; | \\\\ \\phi_1 &amp; \\phi_2 &amp; \\dots &amp; \\phi_d \\\\ | &amp; | &amp; \\dots &amp; | \\end{pmatrix} \\in \\mathrm{R}^{\\mathrm{S}\\times d}\\,.\\] That is, $\\Phi$ is a $\\mathrm{S}\\times d$ matrix. The set of real-valued functions over the state space that can be described with the linear combination of the basis functions is . \\[\\mathcal{F} = \\{ f: \\mathcal{S} \\to \\mathbb{R} \\,:\\, \\exists \\theta\\in \\mathbb{R}^d \\text{ s.t. } f(s) = \\langle \\phi(s),\\theta \\rangle \\}\\,.\\] Identifying the space of real-valued functions with the vector space $\\mathbb{R}^{\\mathrm{S}}$ in the natural way, $\\mathcal{F}$ is a $d$-dimensional subspace of $\\mathbb{R}^{\\mathrm{S}}$, which is the same as the “column space”, or the span, or the range space of $\\Phi$: . \\[\\mathcal{F} = \\{ \\Phi \\theta \\,:\\, \\theta\\in \\mathbb{R}^d \\} = \\text{span}(\\Phi)\\] If we need to indicate the dependence of $\\mathcal{F}$ on the choice of features, we will write either \\(\\mathcal{F}_{\\phi}\\) or \\(\\mathcal{F}_{\\Phi}\\). Now, we have three equivalent ways of specifying the “features”, either by specifying the basis functions $\\phi_1,\\dots,\\phi_d$, or the feature-map $\\phi$, or the feature matrix $\\Phi$, and we have a four equivalent way of specifying the functions that can be obtained via the linear combination of features. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec7/#linear-function-approximation",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec7/#linear-function-approximation"
  },"393": {
    "doc": "7. Function Approximation",
    "title": "Delivering the hint",
    "content": "Note that in the above problem description it is tacitly assumed that the feature-map, in some form or another, is available to the planner. In fact, the feature map can be made available in multiple ways. When we argue for lower bounds, especially for query complexity, we often assume that the whole feature-map is available for the algorithm. For upper bounds with local planning, the most natural assumption is that the planner gets from the simulator the feature vector of the states that it encounters. In particular, when it comes to local planning, the natural assumption is that the planner gets the feature vector of the initial state together with the state and with any subsequent calls to the simulator, the simulator returns the feature vector of the next states, together with the next states. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec7/#delivering-the-hint",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec7/#delivering-the-hint"
  },"394": {
    "doc": "7. Function Approximation",
    "title": "Typical hints",
    "content": "In what follows we will study planning under a number of different hints (or assumptions) that connect the MDP and a feature-map. The simplest of this just states that $\\eqref{eq:vstr}$ holds: . Assumption A1 ($v^*$-realizibility): The MDP $M$ and the featuremap $\\phi$ are such that \\(v^* \\in \\mathcal{F}_\\phi\\) . A second variation is when all value functions are realizable: . Assumption A2 (universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(v^\\pi \\in \\mathcal{F}_\\phi\\). Clearly, A2 implies A1, because by the fundamental theorem of MDPs, there exists a memoryless policy \\(\\pi\\) such that \\(v^\\pi = v^*\\). The figure on the right illustrates the set of all finite MDPs with some state space and within those the set of those MDPs that satisfy A1 with a specific feature map $\\phi$ (denoted by A1\\(\\mbox{}_\\phi\\) on the figure), as well as those MDPs that satisfy A2 with the same feature map (denoted by A2\\(\\mbox{}_\\phi\\)). Both of these sets represent a very small fraction of all MDPs. However, of one changes the feature map, the union of all these sets clearly covers the set of all MDPs: The hint is general. There are many variations of these assumptions. Often, we will find it useful to relax the assumption value functions are exactly realizable. Under the modified assumptions the value function does not need to lie in the span of the feature-map, but only in some vicinity of it. The natural error metric to be used is the maximum norm for reasons that will become clear later. To help with stating these assumptions in a compact form, introduce the notation . \\[v\\in_{\\varepsilon} \\mathcal{F}\\] to denote that . \\[\\inf_{f\\in \\mathcal{F}} \\| f - v \\|_\\infty \\le \\epsilon\\,.\\] That is, $v\\in_{\\varepsilon} \\mathcal{F}$ means that the best approximator to $v$ from $\\mathcal{F}$ approximates it within a uniform error of $\\varepsilon$. Fixing $\\varepsilon\\ge 0$ and replacing $\\in$ with $\\in_{\\varepsilon}$ in the above two assumptions gives the following: . Assumption A1$\\mbox{}_{\\varepsilon}$ (approximate $v^*$ realizability): The MDP $M$ and the featuremap $\\phi$ are such that \\(v^* \\in_{\\varepsilon} \\mathcal{F}_\\phi\\) . Assumption A2$\\mbox{}_{\\varepsilon}$ (approximate universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(v^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\phi\\). ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec7/#typical-hints",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec7/#typical-hints"
  },"395": {
    "doc": "7. Function Approximation",
    "title": "Action-value hints",
    "content": "We obtain new variants if we consider feature-maps that map state-action pairs to vectors. Concretely, (by abusing notation) let $\\phi: \\mathcal{S}\\times\\mathcal{A}\\to \\mathbb{R}^d$. Then, the analog of A1 is as follows: . Assumption B1 ($q^*$-realizibility): The MDP $M$ and the featuremap $\\phi$ are such that \\(q^* \\in \\mathcal{F}_\\phi\\) . Here, as expected, $\\mathcal{F}_\\phi$ is defined as the set of functions that lie in the span of the feature-map. The analog of A2 is as follows: . Assumption B2 (universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(q^\\pi \\in \\mathcal{F}_\\phi\\). We can also introduce positive approximation errors $\\varepsilon&gt;0$, which lead to B1\\(_{\\varepsilon}\\) and B2\\(_{\\varepsilon}\\): . Assumption B1$\\mbox{}_{\\varepsilon}$ (approximate $q^*$-realizibility): The MDP $M$ and the featuremap $\\phi$ are such that \\(q^* \\in_{\\varepsilon} \\mathcal{F}_\\phi\\) . Assumption B2$\\mbox{}_{\\varepsilon}$ (approximate universal value function realizibility) The MDP $M$ and the featuremap $\\phi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(q^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\phi\\). One may wonder why not choose one of these assumptions? When one assumption implies another, then clearly there is a preference to choose the weaker assumption. But often, there is going to be a price and sometimes the assumptions are just not comparable. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec7/#action-value-hints",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec7/#action-value-hints"
  },"396": {
    "doc": "7. Function Approximation",
    "title": "Notes",
    "content": "Origin . The idea of using value function approximation in planning dates back to at least the 1960s if not earlier. I include some intriguing early references at the end. That these ideas already appeared at the down of computing where computers hardly even existed is quite intriguing. Infinite spaces . Function approximation is especially appealing when the state space, or the action space, or both are “continuous” (i.e., they are a subset of a Euclidean space). In this case, the compression is “infinite”. Experimental evidence suggests that function approximation can work quite well in the context of MDP planning in a surprisingly large number of different scenarios. When the spaces are infinite, all the “math” will still go through, except that occasionally one has to be a bit more careful. For example, one cannot clearly say that $\\Phi$ is a matrix, but $\\Phi$ can clearly be defined as a linear operator mapping $\\mathbb{R}^d$ to the vector space of all real-valued functions over the (say) state space (when the feature map is also over states). Nonlinear value function approximation . The most successful use of the idea of compressing value functions uses neural networks. Readers are most likely are already familiar with the ideas underlying neural networks. The hope here is that whatever we find in the case of linear function approximation will have implications in how to use nonlinear function approximation in MDP planning. In a way, the very first question is whether one can decouple the design of the planning algorithm from what function approximation technique it is used with. We will study this question by asking for planners that work with any feature map. If we find that we can identify planners that are performant no matter the feature map, the decoupling is successful and we can hope that the ideas will generalize to nonlinear function approximation. However, if we find that successful planners need to use intricate properties of the feature maps, then this is must be taken as a warning that complications may arise when the results are generalized to nonlinear function approximation. In any case, it appears to be a prudent strategy to first investigate the simpler, more straightforward linear case, before considering the nonlinear case. Computation with advice/Non-uniform Computation . Computation with advice is a general approach in computer science where a problem of computing a map is changed to computing a map which has an additional input, the advice. Clearly, the approach taken here can be seen as a special case of computation with advice. There is also the closely related notion of non-uniform computation studied in computability/complexity theory. In non-uniform computation, the Turing machine, in addition to its input, also receives some “advice” string. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec7/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec7/#notes"
  },"397": {
    "doc": "7. Function Approximation",
    "title": "References",
    "content": ". | Richard Bellman, Robert Kalaba and Bella Kotkin. 1963. Polynomial Approximation–A New Computational Technique in Dynamic Programming: Allocation Processes. Mathematics of Computation, 17 (82): 155-161 | Daniel, James W. 1976. “Splines and Efficiency in Dynamic Programming.” Journal of Mathematical Analysis and Applications 54 (2): 402–7. | Schweitzer, Paul J., and Abraham Seidmann. 1985. “Generalized Polynomial Approximations in Markovian Decision Processes.” Journal of Mathematical Analysis and Applications 110 (2): 568–82. | Brattka, Vasco, and Arno Pauly. 2010. Computation with Advice. arXiv [cs.LO]. | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec7/#references",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec7/#references"
  },"398": {
    "doc": "7. Function Approximation",
    "title": "7. Function Approximation",
    "content": "Our lower bound for local planners show that there are no local planners that lead to good policies in all MDPs while satisfying the following three requirements . | the planner induces policies that achieve some positive fraction of the optimal value in all MDPs; | the per-state runtime shows polynomial dependence on the planning horizon $H$ and | it shows a polynomial dependence on the number of actions and | it shows no dependence on the number of states in the MDP. | . Thus, one is left with no choice than to give up on one of the requirements. Since efficiency is clearly nonnegotiable (otherwise the runner just would not be practical), the only requirement that can be replaced is the first one. In what follows we will look at ways of relaxing this requirement. In all the relaxations we will look at, we will essentially restrict the set of MDPs that the planner is expected to work on. However, we will do this in such a way that no MDP will be ever ruled out. We achieve this by giving the planner some extra hint about the MDP and we demand good performance only when the hint is correct. Since the hint will take a general form, some hint is always correct for any MDP. Hence, no MDP is left behind and the planner can again demanded to be efficient and effective. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec7/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec7/"
  },"399": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Approximate Policy Evaluation: Done Well",
    "content": "Recall that in phase $k$ of policy iteration, given a policy $\\pi_k$, the next policy $\\pi_{k+1}$ is obtained as the policy that is greedy with respect to $q^{\\pi_k}$. If we found some coefficients $\\theta_k\\in \\mathbb{R}^d$ such that . \\[\\begin{align*} q^{\\pi_k} \\approx \\Phi \\theta_k\\,, \\end{align*}\\] then when it comes to “using” policy $\\pi_{k+1}$, we could just use $\\arg\\max_{a} \\langle \\theta_k,\\varphi(s,a)\\rangle$ when an action is needed at state $s$. Note that this action can be obtained at the cost of $O(d)$ elementary operations, a small overhead compared to a table lookup (with idealized $O(1)$ access times). Hence, the main question is how to obtain this parameter in an efficient manner. To be more precise, here we want to control the uniform error committed in approximating $q^{\\pi_k}$. To simplify the notation, let $\\pi = \\pi_k$. A simple idea is rolling out with the policy $\\pi$ from a fixed set $\\mathcal{C}\\subset \\mathcal{S}\\times \\mathcal{A}$ to “approximately” measure the value of $\\pi$ at the pairs in $\\mathcal{C}$. For concreteness, let $(s,a)\\in \\mathcal{C}$. Rolling out with policy this pair means using the simulator to simulate what would happen if we used policy $\\pi$ for a number of consecutive time steps when the initial state is $s$, the first action $a$, but for subsequent time steps the actions are chosen using policy $\\pi$ for whatever states are encountered. If the simulation goes on for $H$ steps, this way we get \\(m\\) trajectories starting in \\(z = (s, a)\\). For $1\\le j \\le m$ let the trajectory obtained be \\(\\tau_\\pi^{(j)}(s, a)\\). Thus, . \\(\\begin{align*} \\tau_\\pi^{(j)}(s, a) = \\left( S_0^{(j)}, A_0^{(j)}, S_1^{(j)}, A_1^{(j)}, \\ldots, S_{H-1}^{(j)}, A_{H-1}^{(j)} \\right)\\, \\end{align*}\\), . where \\(S_0^{(j)}=s\\), \\(A_0^{(j)}=a\\), and for $1\\le t \\le H-1$, \\(S_{t}^{(j)} \\sim P_{A_t^{(j)}} ( S_{t-1}^{(j)} )\\), and \\(A_t^{(j)} \\sim \\pi ( \\cdot | S_{t}^{(j)} )\\). The figure on the right illustrates these trajectories. Given these trajectories, the empirical mean of the discounted sum of rewards along these trajectories is used for approximating $q^\\pi(z)$: . \\[\\begin{align} \\hat R_m(z) = \\frac{1}{m} \\sum_{j=1}^m \\sum_{t=0}^{H-1} \\gamma^t r_{A_t^{(j)}}(S_t^{(j)}). \\label{eq:petargetsbiased} \\end{align}\\] Under the usual condition that the rewards are in the $[0,1]$ interval, the expected value of $\\hat{q}^\\pi(z)$ is in the $\\gamma^H/(1-\\gamma)$ vicinity of the $q^\\pi(z)$ and by averaging a large number of independent trajectories, we also achieve that the empirical means are tightly concentrated around their mean. Using a randomization device, it is possible to remove the error (“bias”) introduced by truncating the trajectories at a fixed time. For this, just let $(H^{(j)})_{j}$ be independent geometrically distributed random variables with parameter $1-\\gamma$, which are also independently chosen from the trajectories. By definition \\(H^{(j)}\\) is the number of $1-\\gamma$-parameter Bernoulli trials needed to get one success. With the help of these variables, define now $\\hat R_m(z)$ by . \\[\\begin{align} \\hat R_m(z) = \\frac{1}{m} \\sum_{j=1}^m \\sum_{t=0}^{H^{(j)}-1} r_{A_t^{(j)}}(S_t^{(j)})\\,. \\label{eq:petargetsunbiased} \\end{align}\\] Note that in the expression of \\(\\hat R_m(z)\\) the discount factor is eliminated. To calculate \\(\\hat R_m(z)\\) one can just perform a rollout with policy $\\pi$ as before, just in each time step $t=0,1,\\dots$, after obtaining $r_{A_t^{(j)}}(S_t^{(j)})$, draw a Bernoulli variable with parameter $(1-\\gamma)$ to decide whether the rollout should continue. To see why the above definition works, fix $j$ and note that by definition, for $h\\ge 1$, \\(\\mathbb{P}(H^{(j)}=h) = \\gamma^{h-1}(1-\\gamma)\\) and thus \\(\\mathbb{P}(H^{(j)}\\ge t+1) = \\gamma^t\\). Therefore, . \\[\\begin{align*} \\mathbb{E}[ \\sum_{t=0}^{H^{(j)}-1} r_{A_t^{(j)}}(S_t^{(j)}) ] &amp; = \\sum_{t=0}^\\infty \\mathbb{E}[ \\mathbb{I}\\{ t \\le H^{(j)}-1\\} r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = \\sum_{t=0}^\\infty \\mathbb{E}[ \\mathbb{I}\\{ t \\le H^{(j)}-1\\} ]\\, \\mathbb{E}[ r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = \\sum_{t=0}^\\infty \\mathbb{P}( t+1 \\le H^{(j)} )\\, \\mathbb{E}[ r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}[ r_{A_t^{(j)}}(S_t^{(j)}) ] \\\\ &amp; = q^\\pi(z)\\,. \\end{align*}\\] All in all, this means, that we expect that if we solve for the least-squares problem . \\[\\begin{align} \\hat\\theta = \\arg\\min_{\\theta\\in \\mathbb{R}^d} \\sum_{z\\in \\mathcal{C}} \\left( \\langle \\theta,\\varphi(z) \\rangle - \\hat R_m(z)\\right)^2\\,, \\label{eq:lse} \\end{align}\\] we expect $\\Phi \\hat\\theta$ to be a good approximation to $q^\\pi$. Or at least, we can expect this hold at the points of $\\mathcal{C}$, where we are taking our measurements. The question is what happens outside of $\\mathcal{C}$: That is, what guarantees can we get for extrapolating to points of $\\mathcal{Z}:= \\mathcal{S}\\times \\mathcal{A}$. The first thing to observe that unless we are choosing $\\mathcal{C}$ carefully, there is no guarantee about the extrapolation error will be kept under control. In fact, if the choice of $\\mathcal{C}$ is so unfortunate that all the feature vectors for points in $\\mathcal{C}$ are identical, the least-squares problem will have many solutions. Our next lemma gives an explicit error bound on the extrapolation error. For the coming results we slightly generalize least-squares by introducing a weighting of the various errors in \\(\\eqref{eq:lse}\\). For this, let $\\varrho: \\mathcal{C} \\to (0,\\infty)$ be a weighting function assigning a positive weight to the various error terms and let . \\[\\begin{align} \\hat\\theta = \\arg\\min_{\\theta\\in \\mathbb{R}^d} \\sum_{z\\in \\mathcal{C}} \\varrho(z) \\left( \\langle \\theta,\\varphi(z) \\rangle - \\hat R_m(z)\\right)^2 \\label{eq:wlse} \\end{align}\\] be the minimizer of the resulting weighted squared-loss. A simple calculation gives that provided the (weighted) moment matrix . \\[\\begin{align} G_\\varrho = \\sum_{z\\in \\mathcal{C}} \\varrho(z) \\varphi(z) \\varphi(z)^\\top \\label{eq:mommx} \\end{align}\\] is nonsingular, the solution to the above weighted least-squares problem is unique and is equal to . \\[\\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\hat R_m(z') \\varphi(z')\\,,\\] From this expression we see that there is no loss of generality in assuming that the weights in the weighting function sum to one: \\(\\sum_{z\\in \\mathcal{C}} \\varrho(z) = 1\\). We will denote this by writing $\\varrho \\in \\Delta_1(\\mathcal{C})$ (here, $\\Delta_1$ refers to the fact that we can see $\\varrho$ as an element of a $|\\mathcal{C}|-1$ simplex). To state the lemma recall the notation that for a positive definite, $d\\times d$ matrix $Q$ and vector $x\\in \\mathbb{R}^d$, . \\[\\|x\\|_Q^2 = x^\\top Q x\\,.\\] ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec8/#approximate-policy-evaluation-done-well",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec8/#approximate-policy-evaluation-done-well"
  },"400": {
    "doc": "8. Approximate Policy Iteration",
    "title": "",
    "content": "Lemma (extrapolation error control in least-squares): Fix any \\(\\theta \\in \\mathbb{R}^d\\), \\(\\varepsilon: \\mathcal{Z} \\rightarrow \\mathbb{R}\\), $\\mathcal{C}\\subset \\mathcal{Z}$ and \\(\\varrho\\in \\Delta_1(\\mathcal{C})\\) such that the moment matrix $G_\\varrho$ is nonsingular. Define . \\[\\begin{align*} \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big(\\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z')\\,. \\end{align*}\\] Then, for any \\(z\\in \\mathcal{Z}\\) we have . \\[\\left| \\varphi(z)^\\top \\hat{\\theta} - \\varphi(z)^\\top \\theta \\right| \\leq \\| \\varphi(z) \\|_{G_{\\varrho}^{-1}}\\, \\max_{z' \\in C} \\left| \\varepsilon(z') \\right|\\,.\\] . Before the proof note that what his lemma tells us is that as long as we guarantee that the moment matrix is full rank, the extrapolation errors relative to predicting with some $\\theta\\in \\mathbb{R}^d$ can be controlled by controlling . | the value of \\(g(\\varrho):= \\max_{z\\in \\mathcal{Z}} \\| \\varphi(z) \\|_{G_{\\varrho}^{-1}}\\); and | the maximum deviation of the targets used in the weighted least-squares problem and the predictions with $\\theta$. | . Proof: First, we relate $\\hat\\theta$ to $\\theta$: . \\[\\begin{align*} \\hat{\\theta} &amp;= G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big(\\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z') \\\\ &amp;= G_\\varrho^{-1} \\left( \\sum_{z' \\in C} \\varrho(z') \\varphi(z') \\varphi(z')^\\top \\right) \\theta + G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\varepsilon(z') \\varphi(z') \\\\ &amp;= \\theta + G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\varepsilon(z') \\varphi(z'). \\end{align*}\\] Then for a fixed \\(z \\in \\mathcal{Z}\\), . \\[\\begin{align*} \\left| \\varphi(z)^\\top \\hat{\\theta} - \\varphi(z)^\\top \\theta \\right| &amp;= \\left| \\sum_{z' \\in C} \\varrho(z') \\varepsilon(z') \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') \\right| \\\\ &amp;\\leq \\sum_{z' \\in C} \\varrho(z') | \\varepsilon(z') | \\cdot | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') | \\\\ &amp;\\leq \\Big( \\max_{z' \\in C} |\\varepsilon(z')| \\Big) \\sum_{z' \\in C} \\varrho(z') | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') |\\,. \\end{align*}\\] To get a sense of how to control the sum notice that if $\\varphi(z)$ in the last sum was somehow replaced by $\\varphi(z’)$, using the definition of $G_\\varrho$ could greatly simplify the last expression. To get here, one may further notice that having the term in absolute value squared would help. Now, to get the squares, recall Jensen’s inequality, which states that for any convex function \\(f\\) and probability distribution \\(\\mu\\), \\(f \\left(\\int u \\mu(du) \\right) \\leq \\int f(u) \\mu(du)\\). Of course, this also works when $\\mu$ is a finitely supported, which is the case here. Thus, applying Jensen’s inequality with \\(f(x) = x^2\\), we thus get . \\[\\begin{align*} \\left(\\sum_{z' \\in C} \\varrho(z') | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') |\\right)^2 &amp; \\le \\sum_{z' \\in C} \\varrho(z') | \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') |^2 \\\\ &amp;= \\sum_{z' \\in C} \\varrho(z') \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z') \\varphi(z')^\\top G_\\varrho^{-1} \\varphi(z) \\\\ &amp;= \\varphi(z)^\\top G_\\varrho^{-1} \\left( \\sum_{z' \\in C} \\varrho(z') \\varphi(z') \\varphi(z')^\\top \\right) G_\\varrho^{-1} \\varphi(z) \\\\ &amp;= \\varphi(z)^\\top G_\\varrho^{-1} \\varphi(z) = \\|\\varphi(z)\\|_{G_\\varrho^{-1}}^2 \\end{align*}\\] Plugging this back into the previous inequality gives the desired result. \\(\\qquad \\blacksquare\\) . It remains to be seen of whether \\(g(\\varrho)=\\max_z \\|\\varphi(z)\\|_{G_\\varrho^{-1}}\\) can be kept under control. This is the subject of a classic result of Kiefer and Wolfowitz: . Theorem (Kiefer-Wolfowitz): Let $\\mathcal{Z}$ be finite. Let $\\varphi: \\mathcal{Z} \\to \\mathbb{R}^d$ be such that the underlying feature matrix $\\Phi$ is rank $d$. There exists a set \\(\\mathcal{C} \\subseteq \\mathcal{Z}\\) and a distribution \\(\\varrho: C \\rightarrow [0, 1]\\) over this set, i.e. \\(\\sum_{z' \\in \\mathcal{C}} \\varrho(z') = 1\\), such that . | \\(\\vert \\mathcal{C} \\vert \\leq d(d+1)/2\\); | \\(\\sup_{z \\in \\mathcal{Z}} \\|\\varphi(z)\\|_{G_\\varrho^{-1}} \\leq \\sqrt{d}\\); | In the previous line, the inequality is achieved with equality and the value of $\\sqrt{d}$ is best possible under all possible choices of $\\mathcal{C}$ and $\\rho$. | . We will not give a proof of the theorem, but we give references at the end where the reader can look up the proof. When $\\varphi$ is not full rank (i.e., $\\Phi$ is not rank $d$), one may reduce the dimensionality (and the cardinality of $C$ reduces accordingly). The problem of choosing $\\mathcal{C}$ and $\\rho$ such that $g(\\rho)$ is minimized is called the $G$-optimal design problem in statistics. This is a specific instance of optimal experimental design. Combining the Kiefer-Wolfowitz theorem with the previous lemma shows that least-squares amplifies the “measurement errors” by at most a factor of \\(\\sqrt{d}\\): . Corollary (extrapolation error control in least-squares via optimal design): Fix any $\\varphi:\\mathcal{Z} \\to \\mathbb{R}^d$ full rank. Then, there exists a set $\\mathcal{C} \\subset \\mathcal{Z}$ with at most $d(d+1)/2$ elements and a weighting function \\(\\varrho\\in \\Delta_1(\\mathcal{C})\\) such that for any \\(\\theta \\in \\mathbb{R}^d\\) and any \\(\\varepsilon: \\mathcal{C} \\rightarrow \\mathbb{R}\\), . \\[\\max_{z\\in \\mathcal{Z}}\\left| \\varphi(z)^\\top \\hat{\\theta} - \\varphi(z)^\\top \\theta \\right| \\leq \\sqrt{d}\\, \\max_{z' \\in C} \\left| \\varepsilon(z') \\right|\\,.\\] where $\\hat\\theta$ is given by . \\[\\begin{align*} \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big(\\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z')\\,. \\end{align*}\\] . Importantly, note that $\\mathcal{C}$ and $\\varrho$ are chosen independently of $\\theta$ and $\\epsilon$, that is, they are independent of the target. This suggests that in approximate policy evaluation, one should choose $(\\mathcal{C},\\rho)$ as in the Kiefer-Wolfowitz theorem and use the $\\rho$ weighted moment matrix. This leads to \\(\\begin{align} \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\hat R_m(z') \\varphi(z')\\,. \\label{eq:lspeg} \\end{align}\\) where $\\hat R_m(z)$ is defined by Eq. \\(\\eqref{eq:petargetsbiased}\\) and $G_\\varrho$ is defined by Eq. \\(\\eqref{eq:mommx}\\). We call this procedure least-square policy evaluation based on rollouts from $G$-optimal design points, or LSPE-$G$, for short. Note that we stick to the truncated rollouts, because this allows a simpler probabilistic analysis. That this properly controls the extrapolation error is as attested by the next result: . Lemma (LSPE-$G$ extrapolation error control): Fix any full-rank feature-map $\\varphi:\\mathcal{Z} \\to \\mathbb{R}^d$ and take the set $\\mathcal{C} \\subset \\mathcal{Z}$ and the weighting function \\(\\varrho\\in \\Delta_1(\\mathcal{C})\\) as in the Kiefer-Wolfowitz theorem. Fix an arbitrary policy $\\pi$ and let $\\theta$ and $\\varepsilon_\\pi$ such that $q^\\pi = \\Phi \\theta + \\varepsilon_\\pi$ and assume that immediate rewards belong to the interval $[0,1]$. Let $\\hat{\\theta}$ be as in Eq. \\eqref{eq:lspeg}. Then, for any $0\\le \\delta \\le 1$, with probability $1-\\delta$, . \\[\\begin{align} \\left\\| q^\\pi - \\Phi \\hat{\\theta} \\right\\|_\\infty &amp;\\leq \\|\\varepsilon_\\pi\\|_\\infty (1 + \\sqrt{d}) + \\sqrt{d} \\left(\\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}}\\right). \\label{eq:lspeee} \\end{align}\\] . Notice that that from the Kiefer-Wolfowitz theorem, \\(\\vert C \\vert = O(d^2)\\) and therefore nothing in the above expression depends on the size of the state space. Now, say we want to make the above error bound at most \\(\\|\\varepsilon_\\pi\\|_\\infty (1 + \\sqrt{d}) + 2\\varepsilon\\) with some value of $\\varepsilon&gt;0$. From the above we see that it suffices to choose $H$ and $m$ so that . \\[\\begin{align*} \\frac{\\gamma^H}{1 - \\gamma} \\leq \\varepsilon/\\sqrt{d} \\qquad \\text{and} \\qquad \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}} \\leq \\varepsilon/\\sqrt{d}. \\end{align*}\\] This, together with \\(\\vert\\mathcal{C}\\vert\\le d(d+1)/2\\) gives . \\[\\begin{align*} H \\geq H_{\\gamma, \\varepsilon/\\sqrt{d}} \\qquad \\text{and} \\qquad m \\geq \\frac{d}{(1 - \\gamma)^2 \\varepsilon^2} \\, \\log \\frac{d(d+1)}{\\delta}\\,. \\end{align*}\\] Proof: In a nutshell, we use the previous corollary, together with Hoeffding’s inequality and using that $|q^\\pi-T_\\pi^H \\boldsymbol{0}|_\\infty \\le \\gamma^H/(1-\\gamma)$, which follows since the rewards are bounded in $[0,1]$. Click here for the full proof. Fix $z\\in \\mathcal{C}$. Let us write $\\hat{R}_m(z) = q^\\pi(z) + \\hat{R}_m(z) - q^\\pi(z) = \\varphi(z)^\\top \\theta + \\varepsilon(z)$ where we define $\\varepsilon(z) = \\hat{R}_m(z) - q^\\pi(z) + \\varepsilon_\\pi(z)$. Then $$ \\hat{\\theta} = G_\\varrho^{-1} \\sum_{z' \\in C} \\varrho(z') \\Big( \\varphi(z')^\\top \\theta + \\varepsilon(z') \\Big) \\varphi(z'). $$ Now we will bound the difference between our action-value function estimate and the true action-value function: $$ \\begin{align} \\| q^\\pi - \\Phi \\hat\\theta \\|_\\infty &amp; \\le \\| \\Phi \\theta - \\Phi \\hat\\theta\\|_\\infty + \\| \\varepsilon_\\pi \\|_\\infty \\le \\sqrt{d}\\, \\max_{z\\in \\mathcal{C}} |\\varepsilon(z)|\\, + \\| \\varepsilon_\\pi \\|_\\infty \\label{eq:bound_q_values} \\end{align} $$ where the last line follows from the Corollary above. For bounding the first term above, first note that $\\mathbb{E} \\left[ \\hat{R}_m(z) \\right] = (T_\\pi^H \\mathbf{0})(z)$. Then, $$ \\begin{align*} \\varepsilon(z) &amp;= \\hat{R}_m(z) - q^\\pi(z) + \\varepsilon_\\pi(z) \\nonumber \\\\ &amp;= \\underbrace{\\hat{R}_m(z) - (T_\\pi^H \\mathbf{0})(z)}_{\\text{sampling error}} + \\underbrace{(T_\\pi^H \\mathbf{0})(z) - q^\\pi(z)}_{\\text{truncation error}} + \\underbrace{\\varepsilon_\\pi(z)}_{\\text{fn. approx. error}}. \\end{align*} $$ Since the rewards are assumed to belong to the unit interval, the truncation error is at most $\\frac{\\gamma^H}{1 - \\gamma}$. Concerning the sampling error (first term), Hoeffding's inequality gives that for any given $z\\in \\mathcal{C}$, $ \\left \\vert \\hat{R}_m(z) - (T_\\pi^H \\mathbf{0})(z) \\right \\vert \\leq \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 / \\delta)}{2m}}$ with at least $1 - \\delta$ probability. Applying a union bound, we get that with probability at least $1 - \\delta$, for all $z \\in \\mathcal{C}$, $ \\left \\vert \\hat{R}_m(z) - (T_\\pi^H \\mathbf{0})(z) \\right \\vert \\leq \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}}$. Putting things together, we get that with probability at least $1 - \\delta$, $$ \\begin{equation} \\max_{z \\in \\mathcal{C}} | \\varepsilon(z) | \\leq \\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log(2 \\vert C \\vert / \\delta)}{2m}} + \\|\\varepsilon_\\pi\\|_\\infty\\,. \\label{eq:bound_varepsilon_z} \\end{equation} $$ Plugging this into Eq. \\eqref{eq:bound_q_values} and algebra gives the desired result. \\(\\blacksquare\\) . In summary, what we have shown so far is that if the features can approximate well the action-value function of a policy, then there is a simple procedure (Monte-Carlo rollouts and least-squares estimation based on an optimal experimental design) to produce an reliable estimate of the action-value function of the policy. The question remains whether if we use these estimates in policy iteration, the whole procedure will still give good policies after a sufficiently large number of iterations. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec8/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec8/"
  },"401": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Progress Lemma with Approximation Errors",
    "content": "Here we give a refinement of the geometric progress lemma of policy iteration that allows for “approximate” policy improvement steps. This previous lemma stated that the value function of the improved policy $\\pi’$ is at least as large as the Bellman operator applied to the value function of the policy $\\pi$ to be improved. Our new lemma is as follows: . Lemma (Geometric progress lemma with approximate policy improvement): Consider a memoryless policy \\(\\pi\\) and its corresponding value function \\(v^\\pi\\). Let \\(\\pi'\\) be any policy and define $\\varepsilon:\\mathcal{S} \\to \\mathbb{R}$ via . \\[T v^\\pi = T_{\\pi'} v^{\\pi} + \\varepsilon\\,.\\] Then, . \\[\\|v^* - v^{\\pi'}\\|_\\infty \\leq \\gamma \\|v^* - v^{\\pi}\\|_\\infty + \\frac{1}{1 - \\gamma} \\, \\|\\varepsilon\\|_\\infty.\\] . Proof: First note that for the optimal policy \\(\\pi^*\\), \\(T_{\\pi^*} v^* = v^*\\). We have . \\[\\begin{align} v^* - v^{\\pi'} &amp; = T_{\\pi^*}v^* - T_{\\pi^*} v^{\\pi} + \\overbrace{T_{\\pi^*} v^\\pi}^{\\le T v^\\pi} - T_{\\pi'} v^\\pi + T_{\\pi'} v^{\\pi} - T_{\\pi'} v^{\\pi'} \\nonumber \\\\ &amp;\\le \\gamma P_{\\pi^*} (v^*-v^\\pi) + \\varepsilon + \\gamma P_{\\pi'} (v^\\pi-v^{\\pi'})\\,. \\label{eq:vstar_vpiprime} \\end{align}\\] Using the value difference identity and that $v_\\pi =T_\\pi v^\\pi\\le T v^\\pi$, we calculate . \\[\\begin{align*} v^\\pi - v^{\\pi'} = (I-\\gamma P_{\\pi'})^{-1} [ v^\\pi - T_{\\pi'}v^\\pi] \\le (I-\\gamma P_{\\pi'})^{-1} [ T v^\\pi - (T v^\\pi -\\varepsilon) ] = (I-\\gamma P_{\\pi'})^{-1} \\varepsilon\\,, \\end{align*}\\] where the inequality follows because $(I-\\gamma P_{\\pi’})^{-1}= \\sum_{k\\ge 0} (\\gamma P_{\\pi’})^k$, the sum of positive linear operators, is a positive linear operator itself and hence is also monotone. Plugging the inequality obtained into \\eqref{eq:vstar_vpiprime} gives . \\[\\begin{align*} v^* - v^{\\pi'} \\le \\gamma P_{\\pi^*} (v^*-v^\\pi) + (I-\\gamma P_{\\pi'})^{-1} \\varepsilon. \\end{align*}\\] Taking the maximum norm of both sides and using the triangle inequality and that \\(\\| (I-\\gamma P_{\\pi'})^{-1} \\|_\\infty \\le 1/(1-\\gamma)\\) gives the desired result. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec8/#progress-lemma-with-approximation-errors",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec8/#progress-lemma-with-approximation-errors"
  },"402": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Approximate Policy Iteration",
    "content": "Notice that the progress lemma makes no assumptions about the origin of the errors. This motivates considering a generic version of approximate policy iteration where for $k\\ge 1$ in the $k$th update set, the new policy $\\pi_k$ is approximately greedy with respect to $v^{\\pi_k}$ in that sense that . \\[\\begin{align} T v^{\\pi_k} = T_{\\pi_{k+1}} v^{\\pi_k} + \\varepsilon_k\\,. \\label{eq:apidef} \\end{align}\\] The progress lemma implies that the resulting sequence of policies will have value functions that converge to a neighborhood of $v^*$ where the size of the neighborhood is governed by the magnitude of the error terms \\((\\varepsilon_k)_k\\). ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec8/#approximate-policy-iteration",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec8/#approximate-policy-iteration"
  },"403": {
    "doc": "8. Approximate Policy Iteration",
    "title": "",
    "content": "Theorem (Approximate Policy Iteration): Let \\((\\pi_k)_{k\\ge 0}\\), \\((\\varepsilon_k)_k\\) be such that \\eqref{eq:apidef} holds for all \\(k\\ge 0\\). Then, for any \\(k\\ge 1\\), . \\[\\begin{align} \\|v^* - v^{\\pi_k}\\|_\\infty \\leq \\frac{\\gamma^k}{1-\\gamma} + \\frac{1}{(1-\\gamma)^2} \\max_{0\\le s \\le k-1} \\|\\varepsilon_{s}\\|_\\infty\\,. \\label{eq:apieb} \\end{align}\\] . Proof: Left as an exercise. \\(\\qquad \\blacksquare\\) . Consider now a version of approximate policy iteration where the sequence of policies \\((\\pi_k)_{k\\ge 0}\\) is defined as follows: . \\[\\begin{align} q_k = q^{\\pi_k} + \\varepsilon_k', \\qquad M_{\\pi_k} q_k = M q_k\\,, \\quad k=0,1,\\dots\\,. \\label{eq:apiavf} \\end{align}\\] That is, for each \\(k=0,1,\\dots\\), \\(\\pi_k\\) is greedy with respect to \\(q_k\\). ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec8/#-1",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec8/#-1"
  },"404": {
    "doc": "8. Approximate Policy Iteration",
    "title": "",
    "content": "Corollary (Approximate Policy Iteration with Approximate Action-value Functions): The sequence defined in \\eqref{eq:apiavf} is such that . \\[\\| v^* - v^{\\pi_k} \\|_\\infty \\leq \\frac{\\gamma^k}{1-\\gamma} + \\frac{2}{(1-\\gamma)^2} \\max_{0\\le s \\le k-1} \\|\\varepsilon_{s}'\\|_\\infty\\,.\\] . Proof: To simplify the notation consider policies \\(\\pi,\\pi'\\) and functions \\(q,\\varepsilon'\\) over the state-action space such that \\(M_{\\pi'} q = M q\\) and \\(q=q^\\pi+\\varepsilon'\\). We have . \\[\\begin{align*} T v^\\pi &amp; \\ge T_{\\pi'} v^\\pi = M_{\\pi'} (r+\\gamma P v^\\pi) = M_{\\pi'} q^\\pi = M_{\\pi'} q - M_{\\pi} \\varepsilon' = M q - M_\\pi \\varepsilon'\\\\ &amp; \\ge M (q^\\pi - \\|\\varepsilon'\\|_\\infty \\boldsymbol{1}) - M_\\pi \\varepsilon' \\ge M q^\\pi - 2 \\|\\varepsilon'\\|_\\infty \\boldsymbol{1} = T v^\\pi - 2 \\|\\varepsilon'\\|_\\infty \\boldsymbol{1}\\,, \\end{align*}\\] where we used that \\(M_\\pi\\) is linear, monotone, and that $M$ is monotone, and both are nonexpansions in the maximum norm. Hence, if $\\varepsilon_k$ is defined by \\eqref{eq:apidef} then \\(\\|\\varepsilon_k\\|_\\infty \\le 2 \\|\\varepsilon_k'\\|_\\infty\\) and the result follows from the previous theorem. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec8/#-2",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec8/#-2"
  },"405": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Global planning with least-squares policy iteration",
    "content": "Putting things together gives the following planning method: . | Given the feature map $\\varphi$, find \\(\\mathcal{C}\\) and \\(\\rho\\) as in the Kiefer-Wolfowitz theorem | Let \\(\\theta_{-1}=0\\) | For \\(k=0,1,2,\\dots,K-1\\) do | \\(\\qquad\\) Roll out with policy \\(\\pi:=\\pi_k\\) for $H$ steps to get the targets \\(\\hat R_m(z)\\) where \\(z\\in \\mathcal{C}\\) \\(\\qquad\\) and \\(\\pi_k(s) = \\arg\\max_a \\langle \\theta_{k-1}, \\varphi(s,a) \\rangle\\) | \\(\\qquad\\) Solve the weighted least-squares problem given by Eq. \\(\\eqref{eq:wlse}\\) to get \\(\\theta_k\\). | Return \\(\\theta_{K-1}\\) | . We call this method least-squares policy iteration (LSPI) for obvious reasons. Note that this is a global planning method: The method makes no use of an input state and the parameter vector returned can be used to get the policy $\\pi_{K}$ (as in the method above). Theorem (LSPI performance): Fix an arbitrary full rank feature-map $\\varphi: \\mathcal{S}\\times \\mathcal{A} \\to \\mathbb{R}^d$ and let $K,m,H\\ge 1$. Assume that B2\\(_{\\varepsilon}\\) holds. Then, for any $0\\le \\zeta \\le 1$, with probability at least $1-\\zeta$, the policy $\\pi_{K}$ which is greedy with respect to $\\Phi \\theta_{K-1}$ is $\\delta$-suboptimal with . \\[\\begin{align*} \\delta \\le \\underbrace{\\frac{2(1 + \\sqrt{d})}{(1-\\gamma)^2}\\, \\varepsilon}_{\\text{approx. error}} + \\underbrace{\\frac{\\gamma^{K-1}}{1-\\gamma}}_{\\text{iter. error}} + \\underbrace{\\frac{2\\sqrt{d}}{(1-\\gamma)^2} \\left(\\gamma^H + \\sqrt{\\frac{\\log( d(d+1)K / \\zeta)}{2m}}\\right)}_{\\text{pol.eval. error}} \\,. \\end{align*}\\] In particular, for any $\\varepsilon’&gt;0$, choosing $K,H,m$ so that . \\[\\begin{align*} K &amp; \\ge H_{\\gamma,\\gamma\\varepsilon'/2} \\\\ H &amp; \\ge H_{\\gamma,(1-\\gamma)^2\\varepsilon'/(8\\sqrt{d})} \\qquad \\text{and} \\\\ m &amp; \\ge \\frac{32 d}{(1-\\gamma)^6 (\\varepsilon')^2} \\log( (d+1)^2 K /\\zeta ) \\end{align*}\\] policy $\\pi_K$ is $\\delta$-optimal with . \\[\\begin{align*} \\delta \\le \\frac{2(1 + \\sqrt{d})}{(1-\\gamma)^3}\\, \\varepsilon + \\varepsilon'\\,, \\end{align*}\\] while the total computation cost is $\\text{poly}(\\frac{1}{1-\\gamma},d,\\mathrm{A},\\frac{1}{(\\varepsilon’)^2},\\log(1/\\zeta))$. Thus, with a polynomial cost, LSPI with the specific configuration at the cost of polynomial computation cost, but importantly, with a cost that is independent of the size of the state space, can result in a good policy as long as $\\varepsilon$, the worst-case error of approximating action-value functions of policies using the features provided, is sufficiently small. Proof: Note that B2\\(_\\varepsilon\\) and that $\\Phi$ is full rank implies that for any memoryless policy $\\pi$ there exists a parameter vector $\\theta\\in \\mathbb{R}^d$ such that \\(\\| \\Phi \\theta - q^\\pi \\|_\\infty \\le \\varepsilon\\) (cf. Part 2 of Question 3 of Assignment 2). Hence, we can use the “LSPE extrapolation error bound” (cf. \\(\\eqref{eq:lspeee}\\)). By this result, a union bound and of course by B2$_\\varepsilon$, we get that for any $0\\le \\zeta \\le 1$, with probability at least $1-\\zeta$, for any $0 \\le k \\le K-1$, . \\[\\begin{align*} \\| q^{\\pi_k} - \\Phi \\theta_k \\|_\\infty &amp;\\leq \\varepsilon (1 + \\sqrt{d}) + \\sqrt{d} \\left(\\frac{\\gamma^H}{1 - \\gamma} + \\frac{1}{1 - \\gamma} \\sqrt{\\frac{\\log( d(d+1)K / \\zeta)}{2m}}\\right)\\,, \\end{align*}\\] where we also used that \\(\\vert \\mathcal{C} \\vert \\le d(d+1)\\). Call the quantity on the right-hand side in the above inequality $\\kappa$. Take the event when the above inequalities hold and for now assume this event holds. By the previous theorem, $\\pi_K$ is $\\delta$-optimal with . \\[\\delta \\le \\frac{\\gamma^{K-1}}{1-\\gamma} + \\frac{2}{(1-\\gamma)^2} \\kappa \\,.\\] To obtain the second part of the result, we split $\\varepsilon’$ into two equal parts: $K$ is set to force the iteration error to be at most $\\varepsilon’/2$, while $H$ and $m$ are chosen to force the policy evaluation error to be at most $\\varepsilon’/2$. Here, to choose $H$ and $M$, $\\varepsilon’/2$ is again split into two equal parts. The details of this calculation are left to the reader. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec8/#global-planning-with-least-squares-policy-iteration",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec8/#global-planning-with-least-squares-policy-iteration"
  },"406": {
    "doc": "8. Approximate Policy Iteration",
    "title": "Notes",
    "content": "Approximate Dynamic Programming (ADP) . Value iteration and policy iteration are specific instances of dynamic programming methods. In general, dynamic programming refers to methods that use value functions to calculate good policies. In approximate dynamic programming the methods are modified by introducing “errors” when calculating the values. The idea is that the origin of the errors does not matter (e.g., whether they come due to imperfect function approximation, linear, or nonlinear, or due to the sampling): The analysis is done in a general form. While here we met approximate policy iteration, one can also use the same ideas as shown here to study an approximate version of value iteration. A homework in problem set 2 asks you to study this method, which is usualy called approximate value iteration. In an earlier homework you were asked to study how linear programming can also be used to compute optimal value functions. Adding approximations we then get approximate linear programming. What function approximation technique to use? . We note in passing that fans of neural networks should like that the general, ADP-style results, like the theorem in the middle of this lecture, can be also applied to the case when neural networks are used as the function approximation technique. However, one main lesson of the lecture is that to control extrapolation errors, one should be quite careful in how the training data is chosen. For linear prediction and least-squares fitting, optimal design gives a complete answer, but the analog questions are completely open in the case of nonlinear function approximation, such as neural networks. There is also a sizable literature that connects nonparametric techniques (an analysis friendly relative of neural networks) to ADP methods. Concentrability coefficients and all that jazz . The idea of introducing approximate calculations has been introduced at the same time people got interested in Markov Decision Processes in the 1960s. Hence, the literature is quite enormous. However, the approach taken here which asks for error bounds where the algorithmic (not approximation-) error is uniformly controlled regardless of the MDP is quite recent and where the term that involves the approximation error is also uniformly bounded (for a fixed dimension and discount factor). Earlier literature often presented bounds where the magnification factor of the approximation and the algorithmic error involved terms which depended on the MDP. Often these came in the form of “concentrability coefficients” (and yours truly was quite busy with working on these results a while ago). The main conclusion of this earlier analysis is that more stochasticity in the transitions means less control, less concentrability, which is advantageous for the ADP algorithms. While this makes sense and this indicates that these earlier results are complementary to the results presented here, the issue is that these results are quite pessimistic for example when the MDP is deterministic (as in this case the concentrability coefficients can be as large as the size of the state space). While here we emphasized the importance of using a good design to control the extrapolation errors, in these earlier results, no optimal design was used. The upshot is that this saves the effort of coming up with a good design, but the obvious downside is that the extrapolation error may become uncontrolled. In the batch setting (which we will come back to later), of course, there is no way to control the sample collection, and this is in fact the setting where this earlier analysis was done. The strength of hints . A critical assumption in the analysis of API was that the approximation error is controlled uniformly for all policies. This feels limiting. Yet, there are some interesting sufficient conditions when this assumption is clearly satisfied. In general, these require that the transition dynamics and the reward are both “compressible”. For example, if the MDP is such that $r$, the immediate reward as a function of the state-action pairs satisfies \\(r = \\Phi \\theta_r\\) and the transition matrix, \\(P\\in [0,1]^{\\mathrm{S}\\mathrm{A} \\times \\mathrm{S}}\\) satisfies \\(P = \\Phi H\\) with some matrix \\(H\\in \\mathbb{R}^{d\\times \\mathrm{S}}\\), then for any policy policy \\(\\pi\\), \\(T_\\pi q = r+ \\gamma P M_\\pi q\\) has a range which is a subset of \\(\\text{span}(\\Phi)=\\mathcal{F}_{\\varphi}\\). Since \\(q^\\pi\\) is the fixed-point of \\(T_\\pi\\), i.e., \\(q^\\pi = T_\\pi q^\\pi\\), it follows that \\(q^\\pi\\) is also necessarily in the range space of \\(T_\\pi\\). As such, \\(q^\\pi \\in \\mathcal{F}_{\\varphi}\\) and \\(\\varepsilon_{\\text{apx}}=0\\). MDPs that satisfy the above two constraints are called linear in \\(\\Phi\\) (or sometimes, just “linear MDPs”). Exact linearity can be relaxed: If \\(r = \\Phi \\theta_r + \\varepsilon_r\\) and \\(P = \\Phi H +E\\), then for any policy \\(\\pi\\), \\(q^\\pi\\in_{\\varepsilon} \\mathcal{F}_{\\varphi}\\) with \\(\\varepsilon \\le \\|\\varepsilon_r\\|_\\infty+\\frac{\\gamma}{1-\\gamma}\\|E\\|_\\infty\\). Nevertheless, later we will investigate whether this assumption can be relaxed. The tightness of the bounds . It is not known whether the bound presented in the final result is tight. In fact, the dependence of $m$ on the $1/(1-\\gamma)$ is almost certainly not tight; in similar scenarios it has been shown in the past that replacing Hoeffding’s inequality with Bernstein’s inequality allows the reduction of this factor. It is more interesting whether the amplification factor of the approximation error, $\\sqrt{d}/(1-\\gamma)^2$, is best possible. In the next lecture we will show that the $\\sqrt{d}$ approximation error amplification factor cannot be removed while keeping the runtime under control. In a later lecture, we will show that the dependence on $1/(1-\\gamma)$ cannot be improved either – at least for this algorithm. However, we will see that if the main concern is the amplification of the approximation error, while keeping the runtime polynomial (perhaps with a higher order though) then under B2\\(_{\\varepsilon}\\) better algorithms exist. The cost of optimal experimental design . The careful reader would not miss that to run the proposed method one needs to find the set $\\mathcal{C}$ and the weighting function $\\rho$. The first observation here is that it is not crucial to find the best possible $(\\mathcal{C},\\rho)$ pair. The Kiefer-Wolfowitz theorem showed that with this best possible choice, $g(\\rho) = \\sqrt{d}$. However, if one finds a pair such that $g(\\rho)=2\\sqrt{d}$, the price of this is that wherever $\\sqrt{d}$ appears in the final performance bound, a submultiplicative factor of $2$ will also need to be introduced. This should be acceptable. In relation to this note that by relaxing this optimality requirement, the cardinality of $\\mathcal{C}$ can be reduced. For example, by introducing the factor of $2$ as suggested above allows one to reduce the cardinlity to $O(d \\log \\log d)$; which may actually be a good tradeoff as this can save much on the runtime. However, the question still remains of who computes these (approximately) optimal designs and at what cost. While this calculation only needs to be done once and is independent of the MDP (just depends on the feature map), the value of these methods remains unclear because of this compute cost. General methods to compute approximately optimal designs needed here are known, but their runtime for our case will be proportional to the number of state-action pairs. In the very rare cases when simulating transitions is very costly but the number of state-action pairs is not too high, this may be a viable option. However, these cases are rare. For special choices of the feature-map, optimal designs may be known. However, this reduces the general applicability of the method presented here. Thus, a major question is whether the optimal experimental design can be avoided. What is known is that for linear prediction with least-squares, clearly, they cannot be avoided. One suspects that this is true more generally. Can optimal designs be avoided while keeping the results essentially unchanged? Of particular interest would be if the feature-map would also be only “locally explored” as the planner interacts with the simulator. Altogether, one suspects that two factors contributed here for the appearance of optimal experimental design: One factor is that the planner is global: It comes up with a parameter vector that leads to a policy that can be used regardless of the state. The other (perhaps) factor is that the approach was based on simple “patching up” a dynamic programming algorithm with a function approximator. While this is a common approach, controlling the extrapolation errors in this approach is critical and is likely only possible with something like an optimal experimental design. As we shall see soon, there are indeed approaches that avoid the optimal experimental design step and which are based on local planning and they also deviate from the ADP approach. Policy evaluation alternatives . The policy evaluation method presented here feels unsophisticated. It uses simple Monte-Carlo rollouts, with truncation, averaging and least-squares regression. The reinforcement learning literature offers many alternatives, such as the “temporal difference” learning type methods that are based on solving the fixed point equation $q^\\pi = T_\\pi q^\\pi$. One can indeed try to use this equation to avoid the crude Monte-Carlo approach presented here, in the hope of reducing the variance (which is currently rather crudely upper bounded using the $1/(1-\\gamma)$ term in the Hoeffding bound). Rewriting the fixed point as $(I-\\gamma P_\\pi) q^\\pi = r$, and then plugging in $q^\\phi = \\Phi \\theta + \\varepsilon$, we see that the trouble is that to control the extrapolation errors, the optimal design must likely depend on the policy to be evaluated (because of the appearance of $(I-\\gamma P_\\pi)\\Phi$). Alternative error control: Bellman residuals . Let \\((\\pi_k)_{k\\ge 0}\\) and \\((q_k,\\varepsilon_k)_{k\\ge 0}\\) be so that . \\[\\varepsilon_k = q_k - T_{\\pi_k} q_k\\] Here, \\(\\varepsilon_k\\) is called the “Bellman residual” of \\(q_k\\). The policy evaluation alternatives above aim at controlling these residuals. The reader is invited to derive the analogue of the “approximate policy iteration” error bound in \\eqref{eq:apieb} for this scenario. The role of $\\rho$ in the Kiefer-Wolfowitz result . One may wonder about how critical is the presence of $\\rho$ in the results presented. For this, we can say that it is not critical. Unweighted least-squares does not perform much worse. Least-squares error bound . The error bound presented for least-squares does not use the full power of randomness. When part of the errors $\\varepsilon(z)$ with $z\\in \\mathcal{C}$ are random, some helpful averaging effects can appear, which we ignored for now, but which could be used in a more refined analysis. Optimal experimental design – a field on its own . Optimal exoerimental design is a subfield of statistics. The design considered here is just one possibility. In fact, this design which is called G-optimal design (G stands, uninspiringly, for the word “general”). The Kiefer-Wolfowitz theorem actually also states that this is equivalent to the D-optimal designs. Lack of convergence . The results presented show convergence to a ball around the optimal target. Some people think this is a major concern. While having a convergent method may look more appealing, as long as one controls the size of the ball, I will not be too concerned. Approximate value iteration (AVI) . Similarly to what is done here, one can introduce an approximate version of value-iteration. This is the subject of Question 3 of homework 2. While the conditions are different, the qualitative behavior of AVI is similar to that of approximate policy iteration. In particular, as for approximate policy iteration, there are two steps to this proof: One is to show that the residuals $\\varepsilon_k = q_k - T q_{k-1}$ can be controlled and the second is that if they are controlled then the policy that is greedy with respect to (say) $q_K$ is $\\delta$-optimal with $\\delta$ controlled by \\(\\varepsilon_{1:K}:=\\max_{1\\le k \\le K} \\| \\varepsilon_k \\|_\\infty\\). For this second part, we have the following bound: . \\[\\begin{align} \\delta \\le 2 H^2 (\\gamma^K + \\varepsilon_{1:K})\\,. \\label{eq:lsvibound} \\end{align}\\] where $H=1/(1-\\gamma)$. The procedure that uses least-squares fitting to get the iterates $(q_k)_k$ is known under various names, such as least-squares value iteration (LSVI), fitted Q-iteration (FQI), least-squares Q iteration (LSQI). This proliferation of abbreviations and names is unfortunate, but there is not much that can be done at this stage. To add insult to injury, when neural networks are used to represent the iterates and an incremental stochastic gradient descent algorithm is used for “fitting” the weights of these networks by resampling old data from a “replay buffer”, the resulting procedure is coined “Deep Q-Networks” (training), or DQN for short. Bounds on the parameter vector . The Kiefer-Wolfowitz theorem implies the following: . Proposition: Let $\\phi:\\mathcal{Z}\\to\\mathbb{R}^d$ and $\\theta\\in \\mathbb{R}^d$ be such that $\\sup_{z\\in \\mathcal{Z}}|\\langle \\phi(z),\\theta \\rangle|\\le 1$ and \\(\\sup_{z\\in \\mathcal{Z}} \\|\\phi(z)\\|_2 &lt;+\\infty\\). Then, there exist a matrix $S\\in \\mathbb{R}^{d\\times d}$ such that for $\\tilde \\phi$ . \\[\\begin{align*} \\tilde\\phi(z) &amp; = S\\phi(z)\\,, \\qquad z\\in \\mathcal{Z} \\end{align*}\\] there exists \\(\\tilde \\theta\\in \\mathbb{R}^d\\) such that the following hold: . | \\(\\langle \\phi(z),\\theta \\rangle = \\langle \\tilde \\phi(z),\\tilde \\theta \\rangle\\), \\(z\\in \\mathcal{Z}\\); | \\(\\sup_{z\\in \\mathcal{Z}} \\| \\tilde \\phi(z) \\|_2 \\le 1\\); | \\(\\|\\tilde \\theta \\|_2 \\le \\sqrt{d}\\). | . Proof: Let $\\rho:\\mathcal{Z} \\to [0,1]$ be the $G$-optimal design whose existence is guaranteed by the Kiefer-Wolfowitz theorem. Let \\(M = \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\phi(z)\\phi(z)^\\top\\) be the underlying moment matrix. Then, by the definition of $\\rho$, \\(\\sup_{z\\in \\mathcal{Z}}\\|\\phi(z)\\|_{M^{-1}}^2 \\le d\\). Define \\(S= (dM)^{-1/2}\\) and \\(\\tilde \\theta = S^{-1} \\theta\\). The first property is clearly satisfied. As to the second property, . \\[\\|\\tilde \\phi(z)\\|_2^2 = \\| (dM)^{-1/2}\\phi(z)\\|_2^2 = \\phi(z)^\\top (dM)^{-1} \\phi(z) \\le 1\\,.\\] Finally, for the third property, . \\[\\| \\tilde \\theta \\|_2^2 = d \\theta^\\top \\left( \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\phi(z) \\phi(z)^\\top \\right) \\theta = d \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\underbrace{(\\theta^\\top \\phi(z))^2}_{\\le 1} \\le d\\,,\\] finishing the proof. \\(\\qquad \\blacksquare\\) . Thus, if one has access to the full feature-map then knowing that a function realized is bounded, one may as well assume that the feature map is bounded and the parameter vector is bounded just by $\\sqrt{d}$. Regularized least-squares . The linear least-squares predictor given by a feature-map $\\phi$ and data $(z_1,y_1),\\dots,(z_n,y_n)$ predicts a response at $z$ via $\\langle \\phi(z),\\hat\\theta \\rangle$ where . \\[\\begin{align} \\hat\\theta = G^{-1}\\sum_{i=1}^n \\phi_i y_i\\,, \\label{eq:ridgesol} \\end{align}\\] with . \\[G = \\sum_{i=1}^n \\phi_i \\phi_i^\\top\\,.\\] Here, by abusing notation for the sake of minimizing clutter, we use $\\phi_i=\\phi(z_i)$, $i=1,\\dots,n$. The problem is that $G$ may not be invertible (i.e., $\\hat \\theta$ may not be defined as written above). “By continuity”, it is nearly equally problematic when $G$ is ill-conditioned (i.e., its minimum eigenvalue is “much smaller” than its maximum eigenvalue). In fact, this leads to poor “generalization”. One remedy, often used, is to modify $G$ by shifting it with a small constant multiple of the identity matrix: . \\[G = \\lambda I + \\sum_{i=1}^n \\phi_i \\phi_i^\\top\\,.\\] Here, $\\lambda&gt;0$ is a tuning parameter, whose value is often chosen based on cross-validation or with a similar process. The modification guarantees that $G$ is invertible and it overall improves the quality of predictions, especially when $\\lambda$ is tuned base on data. Above, the choice of the identity matrix, while is common in the literature, is completely arbitrary. In particular, invertibility will be guaranteed if $I$ is replaced with any other positive definite matrix $P$. In fact, the matrix one should use here should be one that makes $|\\theta|_P^2$ small (while, say, keeping the minimum eigenvalue of $P$ at constant). That this is the choice that makes sense can be argued for by noting that with . \\[G = \\lambda P + \\sum_{i=1}^n \\phi_i \\phi_i^\\top\\,.\\] the $\\hat\\theta$ vector defined in \\eqref{eq:ridgesol} is the minimizer of . \\[L_n(\\theta) = \\sum_{i=1}^n ( \\langle \\phi_i,\\theta \\rangle - y_i)^2 \\,\\,+ \\lambda \\| \\theta\\|_P^2\\,,\\] and thus, the extra penalty has the least impact for the choice of $P$ that makes the norm of $\\theta$ the smallest. If we only know that $\\sup_{z} |\\langle \\phi(z),\\theta \\rangle|\\le 1$, by our previous note, a good choice is $P=d M$, where \\(M = \\sum_{z\\in \\mathrm{supp}(\\rho)} \\rho(z) \\phi(z)\\phi(z)^\\top\\) where \\(\\rho\\) is a $G$-optimal design. Indeed, with this choice, \\(\\|\\theta\\|_P^2 = d \\|\\theta \\|_M^2 \\le d\\). Note also that if we apply the feature-standardization transformation of the previous note, we have . \\[(dM)^{-1/2} (\\sum_i \\phi_i \\phi_i^\\top + \\lambda d M ) (dM)^{-1/2} = \\sum_i \\tilde \\phi_i \\tilde \\phi_i^\\top + \\lambda I\\,,\\] showing that the choice of using the identity matrix is justified when the features are standardized as in the proposition of the previous note. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec8/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec8/#notes"
  },"407": {
    "doc": "8. Approximate Policy Iteration",
    "title": "References",
    "content": "We will only scratch the surface now; expect more references to be added later. The bulk of this lecture is based on . | Tor Lattimore, Csaba Szepesvári, and Gellért Weisz. 2020. “Learning with Good Feature Representations in Bandits and in RL with a Generative Model.” ICML and arXiv:1911.07676, | . who introduced the idea of using \\(G\\)-optimal designs for controlling the extrapolation errors. A very early reference on error bounds in “approximate dynamic programming” is the following: . | Whitt, Ward. 1979. “Approximations of Dynamic Programs, II.” Mathematics of Operations Research 4 (2): 179–85. | . The analysis of the generic form of approximate policy iteration is a refinement of Proposition 6.2 from the book of Bertsekas and Tsitsiklis: . | Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, Belmont, Massachusetts, 1996. | . However, there are some differences between the “API” theorem presented here and Proposition 6.2. In particular, the theorem presented here appears to capture all sources of errors in a general way, while Proposition 6.2 is concerned with value function approximation errors and errors introduced in the “greedification step”. The form adopted here appears, for example, in Theorem 1 of a technical report of Scherrer, who also gives earlier references: . | Scherrer, Bruno. 2013. “On the Performance Bounds of Some Policy Search Dynamic Programming Algorithms.” arxiv. | . The earliest of these references is perhaps . | Munos, R. 2003. “Error Bounds for Approximate Policy Iteration.” ICML. | . Least-squares policy iteration appears in . | Lagoudakis, M. G. and Parr, R. Least-squares policy iteration. The Journal of Machine Learning Re-search, 4:1107–1149, 2003. | . The particular form presented in this work though uses value function approximation based on minimizing the Bellman residuals (using the so-called LSTD method). Two books that advocate the ADP approach: . | Powell, Warren B. 2011. Approximate Dynamic Programming. Solving the Curses of Dimensionality. Hoboken, NJ, USA: John Wiley &amp; Sons, Inc. | Lewis, Frank L., and Derong Liu. 2013. Reinforcement Learning and Approximate Dynamic Programming for Feedback Control. Hoboken, NJ, USA: John Wiley &amp; Sons, Inc. | . And a chapter: . | Bertsekas, Dimitri P. 2009. “Chapter 6: Approximate Dynamic Programming,” January, 1–118. | . A paper that is concerned with API and least-squares methods, but uses concentrability is: . Antos, Andras, Csaba Szepesvári, and Rémi Munos. 2007. “Learning near-Optimal Policies with Bellman-Residual Minimization Based Fitted Policy Iteration and a Single Sample Path.” Machine Learning 71 (1): 89–129. Optimal experimental design has a large literature. A nice book concerned with computation is this: . | M. J. Todd. Minimum-volume ellipsoids: Theory and algorithms. SIAM, 2016. | . The Kiefer-Wolfowitz theorem is from: . | J. Kiefer and J. Wolfowitz. The equivalence of two extremum problems. Canadian Journal of Mathematics, 12(5):363–365, 1960. | . More on computation here: . | E. Hazan, Z. Karnin, and R. Meka. Volumetric spanners: an efficient exploration basis for learning. Journal of Machine Learning Research, 17(119):1–34, 2016 | M. Grötschel, L. Lovász, and A. Schrijver. Geometric algorithms and combinatorial optimization, volume 2. Springer Science &amp; Business Media, 2012. | . The latter book is a very good general starting point for convex optimization. That the features are standardized as shown in the notes is assumed (and discussed), e.g., in . | Wang, Ruosong, Dean P. Foster, and Sham M. Kakade. 2020. “What Are the Statistical Limits of Offline RL with Linear Function Approximation?” arXiv [cs.LG]. arXiv | . which we will meet later. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec8/#references",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec8/#references"
  },"408": {
    "doc": "8. Approximate Policy Iteration",
    "title": "8. Approximate Policy Iteration",
    "content": ". Note: On March 13, these notes were updated as follows: . | Tighter bounds are derived; the old analysis was based on bounding \\(\\| q^*-q^{\\pi_k} \\|_\\infty\\); the new analysis directly bounds \\(\\| v^* - v^{\\pi_k} \\|_\\infty\\), which leads to a better dependence on the approximation error; | Unbiased return estimates are introduced that use rollouts of random length. | . One simple idea to use function approximation in MDP planning is to take a planning method that uses internally value functions and add a constraint that restrict the value functions to have a compressed representation. As usual, two questions arise: . | Does this lead to an efficient planner? That is, can the computation be carried out in time polynomial in the relevant quantities, but not the size of the state space? In case of linear function the question is whether we can calculate the coefficients efficiently. | Does this lead to an effective planner? In particular, how good a policy can be arrive at with a limited compute effort? | . In this lecture, as a start into exploring the use of value function approximation in planning, we look at modifying policy iteration in the above described way. The resulting algorithm belongs to the family of approximate policy iteration algorithms, which consists of all algorithms derived from policy iteration by adding approximation to it. We will work with linear function approximation. In particular, we will assume that the planner is given as a hint a feature-map $\\varphi: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$. In this setting, since policy iteration hinges upon evaluating the policies obtained, the hint given to the planner is considered to be “good” if the (action-)value functions of all policies are well-represented with the features. This means, that we will work under assumption B2$_\\varepsilon$ from the previous lecture, which we copy here for convenience. In what follows we fix $\\varepsilon&gt;0$. Assumption B2$\\mbox{}_{\\varepsilon}$ (approximate universal value function realizibility) The MDP $M$ and the featuremap $\\varphi$ are such that for any memoryless policy $\\pi$ of the MDP, \\(q^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\varphi\\). Recall that here the notation $q^\\pi \\in_{\\varepsilon} \\mathcal{F}_\\varphi$ means that $q^\\pi$ can be approximated up to a uniform error of $\\varepsilon$ using linear combinations of the basis functions underlying the feature-map $\\varphi$: . For any policy $\\pi$, . \\[\\begin{align*} \\inf_{\\theta\\in \\mathbb{R}^d} \\max_{(s,a)} | q^\\pi(s,a) - \\langle \\theta, \\varphi(s,a) \\rangle | \\left(= \\inf_{\\theta\\in \\mathbb{R}^d} \\| q^\\pi - \\Phi\\theta \\|_\\infty\\right) \\le \\varepsilon\\,. \\end{align*}\\] One may question whether it is reasonable to expect that the value functions of all policies can be compressed. We will come back to this question later. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec8/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec8/"
  },"409": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Query lower bound for MDPs with large action sets",
    "content": "For the statement of our results, the following definitions will be useful: . Definition (soundness): A local planner is $(\\delta,\\varepsilon)$-sound if for any finite discounted MDP $M=(\\mathcal{S},\\mathcal{A},P,r,\\gamma)$ and feature-map $\\varphi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ such that $\\varepsilon^*(M,\\Phi)\\le \\varepsilon$, when interacting with $(M,\\varphi)$, the planner induces a $\\delta$-suboptimal policy of $M$. Definition (memoryless planner): Call a planner memoryless if it does not retain any information between its calls. The announced result is as follows: . Theorem (Query lower bound: large action sets): For any $\\varepsilon&gt;0$, $0&lt;\\delta\\le 1/2$, positive integer $d$ and for any $(\\delta,\\varepsilon)$-sound local planner $\\mathcal{P}$ there exists a “featurized-MDP” $(M,\\varphi)$ with rewards in $[0,1]$ with $\\varepsilon^*(M,\\Phi)\\le \\varepsilon$ such that when interacting with a simulator of $(M,\\varphi)$, the expected number of queries used by $\\mathcal{P}$ is at least . \\[\\begin{align*} \\Omega\\left( \\exp\\left( \\frac{1}{32} \\left(\\frac{\\sqrt{d}\\varepsilon}{\\delta}\\right)^2 \\right) \\right)\\,. \\end{align*}\\] . Note that if \\(\\delta=\\varepsilon\\) or smaller, the number of queries is exponential in \\(d\\). For the proof we need a result that shows that one can pack the \\(d\\)-dimensional unit sphere with exponential in \\(d\\) many vectors that are nearly orthogonal. The precise result, which is stated without proof, is as follows: . Lemma (Johnson-Lindenstrauss (JL) Lemma) For every \\(\\tau &gt; 0\\) and integers \\(d,k\\) such that . \\[\\left\\lceil\\frac{8 \\ln k}{\\tau^2}\\right\\rceil \\leq d \\leq k\\] then there exists \\(v_1,...,v_k\\) vectors of the \\(d\\)-dimensional unit sphere such that for all \\(1\\le i&lt;j\\le k\\), . \\[\\lvert \\langle v_i,v_j \\rangle | \\leq \\tau\\,.\\] . Note that for a fixed dimension \\(d\\), the valid range for \\(k\\) is . \\(\\begin{align} d\\le k \\le \\exp\\left(\\frac{d\\tau^2}{8}\\right)\\,. \\label{eq:krange} \\end{align}\\) In particular, \\(k\\) can be “exponentially large” in \\(d\\) when \\(\\tau\\) is a constant. We can directly relate this lemma to our feature matrices. In particular, the lemma is equivalent to the following result: . Proposition (JL feature matrix): For any $\\tau,d,k$ as in the JL lemma there exists a matrix $\\Phi \\in \\mathbb{R}^{k\\times d}$ such that for any $i\\in[k]$, . \\[\\begin{align} \\max_{i\\in [k]} \\inf_{\\theta\\in \\mathbb{R}^d} \\|\\Phi \\theta - e_i \\|_\\infty \\le \\tau\\,, \\label{eq:featjl} \\end{align}\\] where \\(e_i\\) is the \\(i\\)th basis vector of standard Euclidean basis of \\(\\mathbb{R}^k\\), and in particular if \\(\\varphi_i^\\top\\) is the \\(i\\)th row of \\(\\Phi\\), \\(\\|\\Phi \\varphi_i - e_i\\|_\\infty \\le \\tau\\) holds. Proof: Choose $v_1,\\dots,v_k$ from the JL lemma as the rows of $\\Phi$. Fix $i\\in [k]$. Then, \\(\\begin{align*} \\Phi v_i - e_i = (v_1^\\top v_i,\\dots,v_i^\\top v_i,\\dots, v_k^\\top v_i)^\\top - e_i = (v_1^\\top v_i,\\dots,0,\\dots, v_k^\\top v_i)^\\top\\,. \\end{align*}\\) Since by construction $|v_j^\\top v_i|\\leq \\tau$ for $j\\ne i$, the statement follows. \\(\\qquad \\blacksquare\\) . Finally, we need a variation of the result of Question 6 of Assignment 0. This question asked for proving that any algorithm that identifies the single nonzero entry in a binary array of length \\(k\\) requires to look at at least \\((k+1)/2-1/k\\) entries of the array on expectation. A similar lower bound applies if we require the algorithm to be correct with, say, probability \\(1/2\\): . Lemma (High-probability needle lemma): Let $p&gt;0$. Any algorithm that correctly identifies the single nonzero entry in any binary array of length \\(k\\) with probability at least $p$ has the property that the expected number of queries that the algorithm uses is at least \\(\\Omega(p k)\\). In fact, if $q_k$ is the worst-case expected number of queries used by an algorithm that is correct with probability $p$ then one can show that for $k\\ge 2$, $q_k \\ge p( \\frac{k+1}{2}-\\frac{1}{k})$. Proof: Left as an exercise. \\(\\qquad \\blacksquare\\) . With this we are ready to give the proof of the theorem: . Proof (of the theorem): We only give a sketch. Fix the planner $\\mathcal{P}$ with the said properties. Let $k$ be a positive integer to be chosen later. We construct a feature map $\\varphi:\\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}^d$ and $k$ MDPs $M_1,\\dots,M_k$ that share $\\mathcal{S}={s,s_{\\text{end}}}$ and $\\mathcal{A}=[k]$ as state- and action-spaces, respectively. Here $s$ will be chosen as the initial state where the planners will be tested from and $s_{\\text{end}}$ will be an absorbing state with zero reward. The MDPs share the same deterministic transition dynamics: All actions in $s$ end up in $s_{\\text{end}}$ with probability one and all actions taken in $s_{\\text{end}}$ end up in $s_{\\text{end}}$ with probability one. The rewards for actions taken in $s_{\\text{end}}$ are all zero. Finally, we choose the reward of MDP $M_i$ in state $s$ to be . \\[\\begin{align*} r_a^{(i)}(s)=\\mathbb{I}(a=i) r^*\\,, \\end{align*}\\] where the value of $r^*\\in [0,1]$ is left to be chosen later. Then, denoting by $A$ the action returned by the planner when called with state $s$, one can see that the value of the policy induced at $s$ in MDP $M_i$ is \\(r^*\\mathbb{P}_i(A=i)\\), where $\\mathbb{P}_i$ is the distribution induced by the interconnection of the planner and MDP $M_i$. Thus, for \\(r^*=2\\delta\\), the planner needs to return $A$ so that $\\mathbb{P}_i(A=i)\\ge 1/2$. Hence, it needs at least $\\Omega(k)$ calls by the high-probability needle lemma. Finally, the JL feature matrix construction allows us to construct a feature-map for this MDP as the action-value functions take the form \\(q^\\pi(s,a)=\\mathbb{I}(a=i)r^*\\), $q^\\pi(s_{\\text{end}},a)=0$ in this MDP. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec9/#query-lower-bound-for-mdps-with-large-action-sets",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec9/#query-lower-bound-for-mdps-with-large-action-sets"
  },"410": {
    "doc": "9. Limits of query-efficient planning",
    "title": "A lower bound when the number of actions is constant",
    "content": "The previous result leaves open whether query-efficient planners exist with a fixed number of actions. Our next result shows that the problem does not get much easier in this setting either. The result is stated for fixed-horizon MDPs. Given an MDP \\(M=(\\mathcal{S},\\mathcal{A},P,r)\\), a policy \\(\\pi\\), a positive integer \\(h&gt;0\\) and state \\(s\\in \\mathcal{S}\\) of the MDP, let . \\[\\begin{align*} v_h^\\pi(s) = \\mathbb{E}_s^{\\pi}[ \\sum_{t=0}^{h-1} r_{A_t}(S_t)] \\end{align*}\\] be the total reward collected by \\(\\pi\\) when it is used for \\(h\\) steps. The action-value functions \\(q_h^\\pi: \\mathcal{S}\\times \\mathcal{A}\\to \\mathbb{R}\\) are defined similarly. The optimal \\(h\\)-step value function is . \\[\\begin{align*} v_h^*(s) = \\sup_{\\pi} v_h^\\pi(s)\\,, \\qquad s\\in \\mathcal{S}\\,. \\end{align*}\\] The Bellman optimality operator \\(T: \\mathbb{R}^{\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}}\\) is defined via . \\[\\begin{align*} T v(s) = \\max_{a\\in \\mathcal{A}} r_a(s) + \\langle P_a(s), v \\rangle\\,. \\end{align*}\\] The policy evaluation operator \\(T_\\pi: \\mathbb{R}^{\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}}\\) of a memoryless policy \\(\\pi\\) is . \\[\\begin{align*} T_\\pi v(s) = \\sum_{a\\in \\mathcal{A}} \\pi(a|s) \\left( r_a(s) + \\langle P_a(s), v \\rangle \\right)\\,. \\end{align*}\\] A policy \\(\\pi\\) is \\(h\\)-step optimal if \\(v_h^\\pi = v_h^*\\). Also, \\(\\pi\\) is greedy with respect to \\(v:\\mathcal{S}\\to \\mathbb{R}\\) if \\(T_\\pi v = T v\\). The analogue of the fundamental theorem looks as follows: . Theorem (fixed-horizon fundamental theorem): We have \\(v_0^*\\equiv \\boldsymbol{0}\\) and for any \\(h\\ge 0\\), \\(v_{h+1}^* = T v_h^*\\). Furthermore, for any \\(\\pi_0^*,\\dots,\\pi_h^*, \\dots\\) such that for \\(i\\ge 0\\), \\(\\pi_i^*\\) is greedy with respect to \\(v_i^*\\), for any \\(h&gt;0\\) it holds that \\(\\pi=(\\pi_{h-1}^*,\\dots,\\pi_0^*,\\dots)\\) (i.e., the policy which in step \\(1\\) uses \\(\\pi_{h-1}^*\\), in step \\(2\\) uses \\(\\pi_{h-2}^*\\), \\(\\dots\\), in step \\(h\\) uses \\(\\pi_0^*\\), after which it continues arbitrarily) is \\(h\\)-step optimal: . \\[\\begin{align*} v_h^{\\pi} = v_h^*\\,. \\end{align*}\\] . Proof: Left as an exercise. Hint: Use induction. \\(\\qquad \\blacksquare\\) . In the theorem our earlier notion of policies is slightly abused: \\(\\pi\\) is only specified for $h$ steps. In any case, according to this result for a fixed horizon \\(H&gt;0\\), the natural analogue for memoryless policies are these $H$-step nonstationary memoryless policies. Let us denote the set these by \\(\\Pi_H\\). In the next result, we will only care about optimality with respect to a fixed initial state \\(s_0\\in \\mathcal{S}\\). Then, without loss of generality, we also assume that the set of states \\(\\mathcal{S}_h\\) reachable from \\(s_0\\) in \\(h\\ge 0\\) steps are disjoint: \\(\\mathcal{S}_h\\cap \\mathcal{S}_{h'}=\\emptyset\\) for \\(h\\ne h'\\) (why?). It follows that we can also find a memoryless policy \\(\\pi\\) that is optimal at \\(s_0\\): \\(v^{\\pi}_H(s_0)=v_H^*(s_0)\\). In fact, one can even find a memoryless policy that also satisfies . \\[\\begin{align} v^{\\pi}_{H-i}(s)=v_{H-i}^*(s), \\qquad s\\in \\mathcal{S}_i \\end{align}\\] simultaneously for all \\(0\\le i \\le H-1\\). Furthermore, the same holds for the action-value functions: . \\[\\begin{align} q^{\\pi}_{H-i}(s,a)=q_{H-i}^*(s,a), \\qquad s\\in \\mathcal{S}_i, a\\in \\mathcal{A}, 0\\le i \\le H-1\\,. \\end{align}\\] Thus, the natural analogue that all action-value functions are well-approximated with some feature-map is that there are feature-maps \\((\\varphi_h)_{0\\le h \\le H-1}\\) such that for \\(0\\le h \\le H-1\\), \\(\\varphi_h: \\mathcal{S}_h \\times \\mathcal{A} \\to \\mathbb{R}^d\\) and for any memoryless policy \\(\\pi\\), the \\(H-h\\)-step action value function of \\(\\pi\\), when restricted to \\(\\mathcal{S}_h\\), is well-approximated by the linear combination of the basis functions induced by \\(\\varphi_h\\). Since we will not need \\(q^{\\pi}_{H-h}\\) outside of \\(\\mathcal{S}_h\\), in what follows, we assume that these are restricted to \\(\\mathcal{S}_h\\). Writing \\(\\Phi_h\\) for the feature matrix induced by \\(\\varphi_h\\) (the rows of \\(\\Phi_h\\) are the feature vectors under \\(\\varphi_h\\) for some ordering of the state-action pairs from \\(\\mathcal{S}_{h}\\times \\mathcal{A}\\)), we redefine \\(\\varepsilon^*(M,\\Phi)\\) as follows: . \\(\\begin{align} \\varepsilon^*(M,\\Phi) : = \\sup_{\\pi \\text{ memoryless}} \\max_{0\\le h \\le H-1}\\inf_{\\theta\\in \\mathbb{R}^d} \\| \\Phi_h \\theta - q^{\\pi}_{H-h} \\|_\\infty\\,. \\end{align}\\) . Since we changed the objective, we also need to change the definition of $(\\delta,\\varepsilon)$-sound local planners: These planners now need to induce policies that are $\\delta$-suboptimal or better when evaluated with the $H$-horizon undiscounted total reward criterion from the designated start-state \\(s_0\\) provided that the MDP satisfies $\\varepsilon^*(M,\\Phi)\\le \\varepsilon$. In what follows, we call these planners $(\\delta,\\varepsilon)$-sound for the $H$-step criterion. With this, we are ready to state the main result of this section: . Theorem (Query lower bound: small action sets, fixed-horizon objective): For $\\varepsilon&gt;0$, $0&lt;\\delta\\le 1/2$ and positive integer $d$, let . \\[\\begin{align*} u(d,\\varepsilon,\\delta) = \\left\\lfloor\\exp\\left(\\frac{d (\\frac{\\varepsilon}{2\\delta})^2}{8}\\right)\\right\\rfloor\\,. \\end{align*}\\] Then, for any $\\varepsilon&gt;0$, $0&lt;\\delta\\le 1/2$, positive integers $\\mathrm{A},H,d$ such that \\(d\\le\\mathrm{A}^H\\) and for any local planner $\\mathcal{P}$ that is $(\\delta,\\varepsilon)$-sound for MDPs with at most $\\mathrm{A}$ actions and the $H$-step criterion, there exists a “featurized-MDP” $(M,\\varphi)$ with \\(\\mathrm{A}\\) actions and rewards in $[0,1]$ such that when interacting with a simulator of $(M,\\varphi)$, the expected number of queries used by $\\mathcal{P}$ is at least . \\[\\begin{align*} \\tilde\\Omega\\left( \\frac{u(d,\\varepsilon,\\delta)}{d(\\varepsilon/\\delta)^2}\\right)\\, \\end{align*}\\] provided that \\(\\mathrm{A}^H&gt;u(d,\\varepsilon,\\delta)\\) (“large horizons”), while it is . \\[\\begin{align*} \\tilde\\Omega\\left( \\frac{\\mathrm{A}^H}{ H }\\right)\\, \\end{align*}\\] otherwise (“small horizon”). In words, if the horizon is large enough, the previous exponential-in-\\(d\\) lower bound continues to hold, while for horizons that are smaller, a lower bound that is exponential in the horizon holds. Note that above \\(\\tilde\\Omega(\\cdot)\\) hides logarithmic terms. Note that the condition \\(d\\le\\mathrm{A}^H\\) is reasonable: We do not expect the feature-space dimension to be comparable to \\(\\mathrm{A}^H\\). Proof: Fix a planner $\\mathcal{P}$ with the required properties. We consider $k=\\mathrm{A}^H$ MDPs $M_1,\\dots,M_k$ that share the state space \\(\\mathcal{S} = \\cup_{0\\le h \\le H} \\mathcal{A}^h\\) and action space \\(\\mathcal{A}\\). Here, by convention, \\(\\mathcal{A}^0\\) is a singleton with the single element \\(\\perp\\), which will play the role of the start state \\(s_0\\). The transition dynamics is also shared by these MDPs: When in state \\(s\\in \\mathcal{S}\\) and action \\(a\\in \\mathcal{A}\\) is taken, the next state is \\(s'=(a)\\) when \\(s=\\perp\\), while if \\(s=(a_1,\\dots,a_h)\\) with some \\(1\\le h \\le H-1\\) then \\(s'=(a_1,\\dots,a_h,a)\\) and when \\(h=H\\) then the next state is \\(s\\) (ever state in \\(\\mathcal{A}^H\\) is absorbing). The MDPs differ in their reward functions. To describe the rewards let \\(f\\) be a bijection from \\([k]\\) to \\(\\mathcal{A}^H\\). Now, fix \\(1\\le i \\le k\\) and define \\((a_0^*,\\dots,a_{H-1}^*)\\) by \\(f(i)=(a_0^*,\\dots,a_{H-1}^*)\\). Let \\(s_0^*=s_0\\), \\(s_1^*=(a_0^*)\\), \\(s_2^*=(a_0^*,a_1^*)\\), \\(\\dots\\), \\(s_H^*=(a_0^*,\\dots,a_{H-1}^*)\\). Then, in MDP \\(M_i\\), \\(r_{a_{H-1}^*}(s_{H-1}^*)=2\\delta\\) while \\(r_a(s)=0\\) for any other state-action pair. Note that the optimal reward in \\(H\\) steps from \\(\\perp\\) is \\(2\\delta\\) and the only policy that achieves this reward is the one that goes through the states in \\(s_0^*,s_1^*,\\dots,s_{H-1}^*\\). We can visualize MDP \\(M_i\\) as a tree, as seen on the figure on the right. The green nodes on the figure correspond to the states \\(s_0^*,s_1^*,\\dots,s_{H-1}^*,s_H^*\\). Note also that \\(\\mathcal{S}_h = \\mathcal{A}^h\\) for \\(0\\le h \\le H\\). We will now describe the action-value functions of the memoryless policies in \\(M_i\\) as this will be useful later. Fix \\(0\\le h \\le H-1\\). Then, \\(q^{\\pi}_{H-h}\\), by our convention, is defined over \\(\\mathcal{S}_h\\). Then, for any \\(s\\in \\mathcal{S}_h (=\\mathcal{A}^h)\\) and \\(a\\in \\mathcal{A}\\), . \\[\\begin{align} q^\\pi_{H-h}(s,a) = \\begin{cases} 2\\delta\\,, &amp; \\text{if } h=H-1, s=s_{H-1}^*, a=a_{H-1}^*\\,;\\\\ v^\\pi_{H-h-1}(s_{h+1}^*)\\,, &amp; \\text{if } h&lt;H-1, s=s_h^*, a=a_h^*\\,;\\\\ 0\\,, &amp; \\text{otherwise}\\,. \\end{cases} \\label{eq:qpiinfh} \\end{align}\\] Note that here \\(0\\le v^\\pi_{H-h-1}(g(s,a))\\le 2\\delta\\). We see that for each stage \\(0\\le h \\le H-1\\), there is only one state-action pair such that the value of \\(q^\\pi_{H-h}\\) is nonzero, and in this case the value is in the \\([0,2\\delta]\\) interval. Now, since the planner induces a policy with suboptimality \\(\\delta\\), for the action \\(A\\) it returns it holds that \\(\\mathbb{P}(A\\ne a_0^*)\\le 1/2\\) (any other action than \\(a_0^*\\) incurs zero total expected reward in our construction). Then with \\(b\\ge \\log(2H)/\\log(2)=\\log_2(2H)\\) fresh calls, by taking the action \\(A_0\\) that is returned most often in these calls, we get \\(\\mathbb{P}(A_0\\ne a_0^*)\\le 1/(2H)\\). Repeating this process in state \\(S_1=g(s_0,A_0)\\) we get action \\(A_1\\) so that . \\[\\begin{align*} \\mathbb{P}(A_0\\ne a_0^* \\text{ or } A_1\\ne a_1^*) &amp; = \\mathbb{P}(A_0\\ne a_0^*)+ \\mathbb{P}(A_0= a_0^*,A_1\\ne a_1^*)\\\\ &amp; \\le \\mathbb{P}(A_0\\ne a_0^*)+ \\mathbb{P}(A_1\\ne a_1^*|A_0= a_0^*) \\le \\frac{1}{2H}+\\frac{1}{2H}\\,. \\end{align*}\\] Now, repeating again the process in state \\(S_2 = g(S_1,A_1)\\) gives \\(A_2\\), etc. Eventually, we get a sequence of actions \\(A_0,\\dots,A_{H-1}\\) such that \\(\\mathbb{P}(A_0\\ne a_0^* \\text{ or } \\dots \\text{ or } A_{H-1}\\ne a_{H-1}^*)\\le 1/2\\). By our previous argument (reduction to the “needle” problem), this whole process needs \\(\\Omega(k)\\) queries. If the expected number of queries issued by \\(\\mathcal{P}\\) is \\(q\\), the expected number of queries issues here is \\(H \\log_2(2H) q\\). Hence, . \\[q = \\Omega\\left( \\frac{k}{\\log_2(2H)H} \\right)\\,.\\] Let us now consider a choice for \\(\\Phi = (\\Phi_h)_{0\\le h \\le H-1}\\) such that \\(\\varepsilon^*(M,\\Phi)\\le \\varepsilon\\). For \\(\\Phi_h\\) choose first a “JL feature matrix” \\(\\tilde \\Phi_h\\in \\mathbb{R}^{|\\mathcal{S}_h| \\times d}\\) such that Eq. \\eqref{eq:featjl} holds. Then let \\(\\Phi_h = \\sqrt{2\\delta} \\tilde \\Phi_h\\). Choose \\(\\theta_h = v^\\pi_{H-h-1}( s_{h+1}^* ) \\varphi_h( s_h^*,a_{h}^* )/(2\\delta)\\) if \\(h&lt;H-1\\) and choose \\(\\theta_h = \\varphi_h( s_h^*,a_{h}^* )\\), otherwise. Then, by Eq. \\eqref{eq:qpiinfh}, for \\((s,a)\\ne (s_h^*,a_{h}^*)\\), \\(|\\varphi_h(s,a)^\\top \\theta_h-q_{H-h}(s,a)| \\le |v^\\pi( s_{h+1}^* )| \\, |\\tilde \\varphi_h(s,a)^\\top \\tilde \\varphi_h(s_h^*,a_{h}^*)|\\le 2\\delta \\tau\\) and for \\((s,a)=(s_h^*,a_{h}^*)\\), \\(\\varphi_h(s,a)^\\top \\theta_h=q_{H-h}(s,a)\\). Hence, \\(\\varepsilon^*(M,\\Phi)\\le \\varepsilon\\) holds if we set \\(\\tau=\\varepsilon/(2\\delta)\\). From Eq. \\eqref{eq:krange}, \\(\\tilde \\Phi_h\\) exists if \\(d\\le k\\) and . \\[\\begin{align*} k \\le u:=\\left\\lfloor\\exp\\left(\\frac{d (\\frac{\\varepsilon}{2\\delta})^2}{8}\\right)\\right\\rfloor\\,. \\end{align*}\\] Recall that \\(k = \\mathrm{A}^H\\). Thus, the required claim holds for the case when \\(\\mathrm{A}^H\\le u\\) (“small horizon case”). In the opposite case (“large horizon”), let \\(\\tilde H\\) be the largest positive number such that \\(\\mathrm{A}^{\\tilde H}\\le u\\) holds. Repeating the above argument with horizon \\(\\tilde H\\) gives the lower bound \\(q = \\Omega\\left( \\frac{\\mathrm{A}^{\\tilde H}}{\\log_2(2\\tilde H)\\tilde H} \\right) = \\Omega\\left( \\frac{u}{\\log_2(2\\tilde H)\\tilde H} \\right)\\,,\\) which finishes the proof. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec9/#a-lower-bound-when-the-number-of-actions-is-constant",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec9/#a-lower-bound-when-the-number-of-actions-is-constant"
  },"411": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Proof of the JL lemma",
    "content": "For completeness, we include a proof of the JL lemma. The proof uses the so-called probabilistic method The idea of this is that sometimes it is easier to establish the existence of some “good configuration” (like the nearly orthogonal vectors on the unit sphere in the JL lemma) by establishing that such a configuration has positive probability under some probability distribution over possible configurations. In our case, this works as follows: Let $V_1,\\dots,V_k$ be random vectors, each uniformly distributed on the $d$-dimensional unit sphere and so that the distinct vectors in this sequence are pairwise independent of each other. Take $i\\ne j$. If we show that $|\\langle V_i, V_j \\rangle| \\le \\tau$ holds with probability at least $1-1/k^2$, by a union bound over the $k(k-1)/2$ pairs $1\\le i &lt;j \\le k$, it follows that $\\max_{i\\ne j} |\\langle V_i,V_j | \\le \\tau$ holds with probability at least $1/2$, from which, the lemma follows. Thus, it remains to show that the angle between the random vectors $V_i$ and $V_j$ is “small” with the claimed probability. Since the uniform distribution is rotation invariant and $V_i$ and $V_j$ are independent of each other, $\\langle V_i, V_j \\rangle$ has the same distribution as $\\langle e_1, V_1 \\rangle = V_{11}\\in [-1,1]$. To see this take a rotation $R$ that rotates $V_i$ to $e_1$; then $\\langle V_i, V_j \\rangle = \\langle R V_i, R^{-1} V_j \\rangle = \\langle e_1, R^{-1} V_j \\rangle$. Now, since $R$ and $V_j$ are independent of each other, $R^{-1} V_j$ is still uniformly distributed on the sphere, hence, $\\langle e_1, R^{-1} V_j \\rangle$ and $\\langle e_1, V_1 \\rangle$ share the same distribution. A tedious calculation shows that for any $x\\ge 6$, . \\[\\begin{align} \\mathbb{P}( V_{11}^2 &gt; x/d) \\le \\exp(-x/4)\\,. \\label{eq:dgtail} \\end{align}\\] (The idea of proving this is to notice that if $X$ is $d$-dimensional standard normal variable then \\(V=X/\\|X\\|_2\\) is uniformly distributed on the sphere. Then, one proceeds using Chernoff’s method.) The result now follows from \\eqref{eq:dgtail} by choosing $x$ so that $\\tau^2 = x/d$ holds. \\(\\qquad \\blacksquare\\) . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec9/#proof-of-the-jl-lemma",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec9/#proof-of-the-jl-lemma"
  },"412": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Notes",
    "content": ". | The lower bound for the discounted case is missing the planning horizon. In the fixed-horizon setting, the lower bound is again missing the horizon when the horizon is large. It remains to be seen whether the extra “horizon terms” in Eq. \\eqref{eq:limit} are necessary. | In any case, the main conclusion is that even when we require “strong features”, high-accuracy planning is intractable. | The reader familiar with the TCS literature may recognize a close resemblance to questions studied there which are concerned with the existence of “fully polynomial time approximation schemes” (FPTAS). | There are many open questions. For one, is there a counterpart of the second theorem for the discounted setting? . | . ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec9/#notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec9/#notes"
  },"413": {
    "doc": "9. Limits of query-efficient planning",
    "title": "Bibliographical notes",
    "content": "The idea of using the Johnson-Lindenstrauss lemma in this context is due to Du, Kakade, Wang and Yang (DKWY, for short). The first theorem is a variant of a result from this paper. The second theorem is a variation of Theorem 4.1 from the paper of Du et al. mentioned above who prove the analoge result for global planners. The proof of the lemma also follows the proof given in this paper. The proof of inequality \\eqref{eq:dgtail} is given in a paper of Dasgupta and Gupta, which also gives the “full version” of the Johnson-Lindenstrauss lemma which states that logarithmically many dimensions are sufficient to keep pairwise distances between a finite set of points. | Dasgupta, Sanjoy; Gupta, Anupam (2003), “An elementary proof of a theorem of Johnson and Lindenstrauss” link, Random Structures &amp; Algorithms, 22 (1): 60–65 | . The presentation of the first result which is for “bandits” (fixed horizon problems with $H=1$) follows closely that of a paper by Lattimore, Weisz and yours truly. This, and a paper by van Roy and Dong were both prompted by the DKWY paper, whose initial version focused on the case when $\\delta \\ll \\sqrt{d} \\varepsilon$, which made the outlook for designing robust RL methods quite bleak. While it is true that in this high-precision regime nothing much can be done (unless further restricting the features), both papers emphasized that the hardness result disappears when the algorithm can deliver $\\delta$ optimal policies with $\\delta \\gtrsim \\sqrt{d} \\varepsilon$. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec9/#bibliographical-notes",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec9/#bibliographical-notes"
  },"414": {
    "doc": "9. Limits of query-efficient planning",
    "title": "9. Limits of query-efficient planning",
    "content": "In the last lecture we have seen that given a discounted MDP $M = (\\mathcal{S},\\mathcal{A},P,r,\\gamma)$, a feature-map $\\varphi: \\mathcal{S}\\times\\mathcal{A}\\to \\mathbb{R}^d$ and a precomputed, suitably small core set, for any $\\varepsilon’&gt;0$ target and any confidence parameter $0\\le \\zeta \\le 1$, interacting with a simulator of $M$, with at most $\\text{poly}(\\frac{1}{1-\\gamma},d,\\mathrm{A},\\frac{1}{(\\varepsilon’)^2},\\log(1/\\zeta))$, compute time, LSPI returns some weight vector $\\theta\\in \\mathbb{R}^d$ such that with probability $1-\\zeta$, the policy that is greedy with respect to $q = \\Phi \\theta$ is $\\delta$-suboptimal with . \\[\\begin{align} \\delta \\le \\frac{4(1 + \\sqrt{d})}{(1-\\gamma)^3}\\, \\varepsilon + \\varepsilon'\\,, \\label{eq:suboptapi} \\end{align}\\] where $\\varepsilon$ is the error with which the features can approximate the action-value functions of the policies of the MDP: . \\[\\begin{align} \\varepsilon = \\varepsilon^*(M,\\Phi) : = \\sup_{\\pi \\text{ memoryless}} \\inf_{\\theta\\in \\mathbb{R}^d} \\| \\Phi \\theta - q^\\pi \\|_\\infty\\,. \\label{eq:polerr} \\end{align}\\] Here, following our earlier convention, $\\Phi$ refers to the \\(| \\mathcal{S}\\times\\mathcal{A} | \\times d\\) matrix that is obtained by stacking the feature vectors $\\varphi^\\top(s,a)$ of all possible state-action pairs on the top of each other in some fixed order. Setting $\\varepsilon’$ to match the first term in Eq. \\eqref{eq:suboptapi}, we can keep the effort polynomial in the relevant quantities (including $1/\\varepsilon$), but even in the limit of infinite computation, the best bound we can obtain is . \\[\\begin{align} \\delta \\le \\frac{4(1 + \\sqrt{d})}{(1-\\gamma)^3}\\, \\varepsilon\\,. \\label{eq:limit} \\end{align}\\] While it makes sense that with a reasonable compute effort $\\delta$ cannot be better than $\\varepsilon$ or a constant multiple of $\\varepsilon$, it is unclear whether the extra $\\sqrt{d}/(1-\\gamma)^3$ factor is an artifact of the proof. We may suspect that some power of $1/(1-\\gamma)$ may be necessary, because even if we knew the parameter vector that gives the best approximation to \\(q^*\\), the error incurred by acting greedily with respect to $q^*$ could be as large as . \\[\\frac{\\varepsilon}{1-\\gamma}\\,.\\] However, at this point, it is completely unclear whether the extra \\(\\sqrt{d}\\) factor is necessary. The main question asked in this lecture: Are the “extra” factors truly necessary in the above bound? Or are there some other polynomial runtime algorithms that are able to produce policies with smaller suboptimality? . In this lecture we will give a partial answer to this questions: We will justify the presence of $\\sqrt{d}$. We start with a lower bound that shows that when there is no limit on the number of actions, efficient algorithms are limited to $\\delta = \\Omega( \\varepsilon\\sqrt{d})$. ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps/lec9/",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps/lec9/"
  },"415": {
    "doc": "Planning in MDPs",
    "title": "Planning in MDPs",
    "content": " ",
    "url": "/2024/w2021-lecture-notes/planning-in-mdps",
    
    "relUrl": "/w2021-lecture-notes/planning-in-mdps"
  }
}
