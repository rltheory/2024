<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/2024/assets/css/just-the-docs-default.css"> <script src="/2024/assets/js/vendor/lunr.min.js"></script> <script src="/2024/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="icon" href="/2024/favicon.ico" type="image/x-icon"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Politex | RL Theory</title> <meta name="generator" content="Jekyll v4.2.2" /> <meta property="og:title" content="Politex" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="PDF Version" /> <meta property="og:description" content="PDF Version" /> <link rel="canonical" href="http://localhost:4000/2024/lecture-notes/planning-in-mdps/lec14/" /> <meta property="og:url" content="http://localhost:4000/2024/lecture-notes/planning-in-mdps/lec14/" /> <meta property="og:site_name" content="RL Theory" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2024-09-21T14:43:24-06:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Politex" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-09-21T14:43:24-06:00","datePublished":"2024-09-21T14:43:24-06:00","description":"PDF Version","headline":"Politex","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2024/lecture-notes/planning-in-mdps/lec14/"},"url":"http://localhost:4000/2024/lecture-notes/planning-in-mdps/lec14/"}</script> <!-- End Jekyll SEO tag --> <!-- MathJax --> <!-- http://docs.mathjax.org/en/latest/web/start.html --> <!-- http://docs.mathjax.org/en/latest/web/configuration.html#web-configuration --> <!-- http://docs.mathjax.org/en/latest/options/input/tex.html --> <!-- http://docs.mathjax.org/en/latest/input/tex/eqnumbers.html --> <script> MathJax = { tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], processEscapes: true, tags: 'ams' } }; </script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <!-- Google fonts --> <!-- https://fonts.google.com/specimen/Merriweather?sidebar.open=true&selection.family=Merriweather:wght@400;900 --> <style> @import url('https://fonts.googleapis.com/css2?family=Merriweather:wght@400;900&display=swap'); </style> </head> <body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header"> <a href="http://localhost:4000/2024/" class="site-title lh-tight"> RL Theory </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav"> <div class="nav-category">Pages</div> <ul class="nav-list"><li class="nav-list-item"><a href="/2024/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="/2024/pages/about/" class="nav-list-link">About CMPUT 605</a></li><li class="nav-list-item"><a href="/2024/pages/about_cmput653/" class="nav-list-link">About CMPUT 653 (OLD)</a></li><li class="nav-list-item"><a href="/2024/pages/lectures/" class="nav-list-link">Lectures</a></li><li class="nav-list-item"><a href="/2024/pages/assignments/" class="nav-list-link">The work you do</a></li></ul> <div class="nav-category">Lecture Notes</div> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Planning in MDPs category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/2024/lecture-notes/planning-in-mdps" class="nav-list-link">Planning in MDPs</a><ul class="nav-list"><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec1/" class="nav-list-link">1. Introductions</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec2/" class="nav-list-link">2. The Fundamental Theorem</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec3/" class="nav-list-link">3. Value Iteration and Our First Lower Bound</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec4/" class="nav-list-link">4. Policy Iteration</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec5/" class="nav-list-link">5. Online Planning - Part I.</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec6/" class="nav-list-link">6. online planning - Part II.</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec7/" class="nav-list-link">7. Function Approximation</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec8/" class="nav-list-link">8. Approximate Policy Iteration</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec9/" class="nav-list-link">9. Limits of query-efficient planning</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec10/" class="nav-list-link">10. Planning under $q^*$ realizability</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec11/" class="nav-list-link">11. Planning under $v^*$ realizability (TensorPlan I.)</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec12/" class="nav-list-link">12. TensorPlan and eluder sequences</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec13/" class="nav-list-link">13. From API to Politex</a></li><li class="nav-list-item active"><a href="/2024/lecture-notes/planning-in-mdps/lec14/" class="nav-list-link active">14. Politex</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec15/" class="nav-list-link">15. From policy search to policy gradients</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/planning-in-mdps/lec16/" class="nav-list-link">16. Policy gradients</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Online RL category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/2024/lecture-notes/online-rl" class="nav-list-link">Online RL</a><ul class="nav-list"><li class="nav-list-item "><a href="/2024/lecture-notes/online-rl/lec22/" class="nav-list-link">22. Introduction</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/online-rl/lec23/" class="nav-list-link">23. Tabular MDPs</a></li><li class="nav-list-item "><a href="/2024/lecture-notes/online-rl/lec24/" class="nav-list-link">24. Featurized MDPs</a></li></ul></li></ul> <div class="nav-category">Winter 2022 Lecture Notes</div> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Planning in MDPs category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/2024/w2022-lecture-notes/planning-in-mdps" class="nav-list-link">Planning in MDPs</a><ul class="nav-list"><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec1/" class="nav-list-link">1. Introductions</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec2/" class="nav-list-link">2. The Fundamental Theorem</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec3/" class="nav-list-link">3. Value Iteration and Our First Lower Bound</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec4/" class="nav-list-link">4. Policy Iteration</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec5/" class="nav-list-link">5. Online Planning - Part I.</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec6/" class="nav-list-link">6. online planning - Part II.</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec7/" class="nav-list-link">7. Function Approximation</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec8/" class="nav-list-link">8. Approximate Policy Iteration</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec9/" class="nav-list-link">9. Limits of query-efficient planning</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec10/" class="nav-list-link">10. Planning under $q^*$ realizability</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec11/" class="nav-list-link">11. Planning under $v^*$ realizability (TensorPlan I.)</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec12/" class="nav-list-link">12. TensorPlan and eluder sequences</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec13/" class="nav-list-link">13. From API to Politex</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec14/" class="nav-list-link">14. Politex</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec15/" class="nav-list-link">15. From policy search to policy gradients</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/planning-in-mdps/lec16/" class="nav-list-link">16. Policy gradients</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Batch RL category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/2024/w2022-lecture-notes/batch-rl" class="nav-list-link">Batch RL</a><ul class="nav-list"><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/batch-rl/lec17/" class="nav-list-link">17. Introduction</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/batch-rl/lec18/" class="nav-list-link">18. Sample complexity in finite MDPs</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/batch-rl/lec19/" class="nav-list-link">19. Scaling with value function approximation</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Online RL category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/2024/w2022-lecture-notes/online-rl" class="nav-list-link">Online RL</a><ul class="nav-list"><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/online-rl/lec22/" class="nav-list-link">22. Introduction</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/online-rl/lec23/" class="nav-list-link">23. Tabular MDPs</a></li><li class="nav-list-item "><a href="/2024/w2022-lecture-notes/online-rl/lec24/" class="nav-list-link">24. Featurized MDPs</a></li></ul></li></ul> <div class="nav-category">Winter 2021 Lecture Notes</div> <ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Planning in MDPs category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/2024/w2021-lecture-notes/planning-in-mdps" class="nav-list-link">Planning in MDPs</a><ul class="nav-list"><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec1/" class="nav-list-link">1. Introductions</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec2/" class="nav-list-link">2. The Fundamental Theorem</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec3/" class="nav-list-link">3. Value Iteration and Our First Lower Bound</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec4/" class="nav-list-link">4. Policy Iteration</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec5/" class="nav-list-link">5. Local Planning - Part I.</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec6/" class="nav-list-link">6. Local Planning - Part II.</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec7/" class="nav-list-link">7. Function Approximation</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec8/" class="nav-list-link">8. Approximate Policy Iteration</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec9/" class="nav-list-link">9. Limits of query-efficient planning</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec10/" class="nav-list-link">10. Planning under $q^*$ realizability</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec11/" class="nav-list-link">11. Planning under $v^*$ realizability (TensorPlan I.)</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec12/" class="nav-list-link">12. TensorPlan and eluder sequences</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec13/" class="nav-list-link">13. From API to Politex</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec14/" class="nav-list-link">14. Politex</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec15/" class="nav-list-link">15. From policy search to policy gradients</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/planning-in-mdps/lec16/" class="nav-list-link">16. Policy gradients</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Batch RL category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/2024/w2021-lecture-notes/batch-rl" class="nav-list-link">Batch RL</a><ul class="nav-list"><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/batch-rl/lec17/" class="nav-list-link">17. Introduction</a></li><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/batch-rl/lec18/" class="nav-list-link">18. Sample complexity in finite MDPs</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Online RL category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/2024/w2021-lecture-notes/online-rl" class="nav-list-link">Online RL</a><ul class="nav-list"><li class="nav-list-item "><a href="/2024/w2021-lecture-notes/online-rl/blank/" class="nav-list-link">Blank</a></li></ul></li></ul> </nav> <footer class="site-footer"> Website of the course CMPUT 653: Theoretical Foundations of Reinforcement Learning. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search RL Theory" aria-label="Search RL Theory" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="">Planning in MDPs</a></li> <li class="breadcrumb-nav-list-item"><span>14. Politex</span></li> </ol> </nav> <div id="main-content" class="main-content" role="main"> <!-- Taken from https://github.com/jekyll/minima/blob/master/_layouts/post.html --> <header class="post-header"> <h1 class="post-title p-name" itemprop="name headline">14. Politex</h1> <p class="text-small text-grey-dk-000 mb-0 mr-2"><time class="dt-published" datetime="2024-09-21T14:43:24-06:00" itemprop="datePublished"> Sep 21, 2024 </time></p> </header><p><a href="../../../documents/lectures/winter_2023/website_notes/planning_in_mdps/lec14.pdf">PDF Version</a></p> <p>The following lemma can be extracted from the calculations found at the<br /> end of <a href="/lecture-notes/planning-in-mdps/lec13/">the last lecture</a>:</p> <hr /> <p><strong>Lemma (Mixture policy suboptimality):</strong> Fix an MDP $M$. For any sequence \(\pi_0,\dots,\pi_{k-1}\) of policies, any sequence $\hat q_0,\dots,\hat q_{k-1}: \mathcal{S}\times \mathcal{A} \to \mathbb{R}$ of functions, and any policy \(\pi^*\), the mixture policy $\bar \pi_k = 1/k(\pi_0+\dots+\pi_{k-1})$ satisfies</p> \[\begin{align} v^{\pi^*} - v^{\bar \pi_k} &amp; \le \frac1k (I-\gamma P_{\pi^*})^{-1} \underbrace{ \sum_{j=0}^{k-1} M_{\pi^*} \hat q_j - M_{\pi_j} \hat q_j}_{T_1} + \frac{2 \max_{0\le j \le k-1}\| q^{\pi_j}-\hat q_j\|_\infty }{1-\gamma} \,. \label{eq:polsubgapgen} \end{align}\] <hr /> <p>In particular, the only restriction is on policy $\pi^*$ so far and that is that it has to be a memoryless policy. To control the suboptimality of the mixture policy, one just needs to control the action-value approximation errors \(\| q^{\pi_j}-\hat q_j\|_\infty\) and the term $T_1$ and for this we are free to choose the policies \(\pi_0,\dots,\pi_{k-1}\) in any way we want them to be chosen. To help with this choice, let us now inspect \(T_1(s)\) for a fixed state $s$:</p> \[\begin{align} T_1(s) = \sum_{j=0}^{k-1} \langle \pi^*(s,\cdot),\hat q_j(s,\cdot)\rangle - \langle \pi_j(s,\cdot),\hat q_j(s,\cdot)\rangle \,, \label{eq:t1s} \end{align}\] <p>where, abusing notation, we use \(\pi(s,a)\) for \(\pi(a|s)\). Now, recall that \(\hat q_j\) will be computed based on \(\pi_j\) while \(\pi^*\) is unknown. One must thus wonder whether it is possible to control this term?</p> <h2 id="online-linear-optimization">Online linear optimization</h2> <p>As it happens, the problem of controlling terms of this type is the central problem studied in a subfield of learning theory, <strong>online learning</strong>. In particular, in <strong>online linear optimization</strong>, the following problem is studied:</p> <p>An adversary and a learner are playing a zero-sum minimax game in $k$ discrete rounds, taking actions in an alternating manner. In round $j$ ($0\le j \le k-1$), first, the learner needs to choose a vector \(x_j\in \mathcal{X}\subset \mathbb{R}^d\). Then, the adversary chooses a vector, \(y_j \in \mathcal{Y}\subset \mathbb{R}^d\). Before its choice, the adversary learns about all previous choices of the learner, and the learner also learns about all previous choices of the adversary. They also remember their own choices. For simplicity, let us constraint the adversary and the learner to be deterministic. The payoff to the adversary at the end of the $k$ rounds is</p> \[\begin{align} R_k = \max_{x\in \mathcal{X}}\sum_{j=0}^{k-1} \langle x, y_j \rangle - \langle x_j,y_j \rangle\,. \label{eq:regretdefolo} \end{align}\] <p>In particular, the adversary’s goal is maximize this, while the learner’s goal is to minimize this (the game is zero-sum). Both the adversary and the learner are given $k$ and the sets $\mathcal{X},\mathcal{Y}$. Letting $L$ to denote the learner’s strategy (a sequence of maps of histories to $\mathcal{X}$) and $A$ to denote the adversary’s strategy (a sequence of maps of histories to $\mathcal{Y}$), the above quantity depends on $L$ and $A$: $R_k = R_K(A,L)$.</p> <p>Taking the perspective of the learner, the quantity defined in \eqref{eq:regretdefolo} is called the learner’s <strong>regret</strong>. Denote the minimax value of the game by \(R_k^*\): \(R_k^* = \inf_L \sup_A R_k(A,L)\).</p> <p>Thus, this only depends on \(k\), \(\mathcal{X}\) and \(\mathcal{Y}\). The dependence is suppressed when it is clear from the context. The central question then is how $R_k^*$ depends on $k$ and also on $\mathcal{X}$ and $\mathcal{Y}$. In online linear optimization both sets $\mathcal{X}$ and $\mathcal{Y}$ are convex.</p> <p>Connecting these games to our problem, we can see that \(T_1(s)\) in \eqref{eq:t1s} matches the regret definition in \eqref{eq:regretdefolo} if we let \(d=\mathrm{A}\), \(\mathcal{X} = \mathcal{M}_1(\mathrm{A}) = \{ p\in [0,1]^{\mathrm{A}} \,:\, \sum_a p_a = 1 \}\) be the \(\mathrm{A}-1\) simplex of \(\mathbb{R}^{\mathrm{A}}\) and \(\mathcal{Y} = [0,1/(1-\gamma)]^{\mathrm{A}}\). Furthermore, \(\pi_j(s,\cdot)\) needs to be chosen first, which is followed by the choice of \(\hat q_j(s,\cdot)\). While \(\hat q_j(s,\cdot)\) will not be chosen in an adversarial fashion, a bound $B$ on the regret against arbitrary choices will also serve as a bound for the specific choice we will need to make for \(\hat q_j(s,\cdot)\). <!-- strength of the lemma: separation of concerns--></p> <h2 id="mirror-descent">Mirror descent</h2> <p>Mirror descent (MD) is an algorithm that originates in optimization theory. In the context of online linear optimization, MD is a strategy for the learner which is known to guarantee near minimax regret for the learner under a wide range of circumstances.</p> <p>To align with the large body of literature on online linear optimization, it will be beneficial to switch signs. Thus, in what follows we assume that the learner will aim at minimizing \(\langle x,y \rangle\) by its choice \(x\in \mathcal{X}\) and the adversary will aim at maximizing the same expression over its choice \(y\in \mathcal{Y}\). This means that we also redefine the regret to</p> \[\begin{align} R_k &amp; = \max_{x\in \mathcal{X}}\sum_{j=0}^{k-1} \langle x_j, y_j \rangle - \langle x,y_j \rangle \nonumber \\ &amp; = \sum_{j=0}^{k-1} \langle x_j, y_j \rangle - \min_{x\in \mathcal{X}} \sum_{j=0}^{k-1}\langle x,y_j \rangle\,. \label{eq:regretdefololosses} \end{align}\] <p>Everything else remains the same: The game is zero-sum, minimax, the regret is the payoff for the adversary and the negative regret is the payoff of the learner. This version is called a loss-game. The reason to prefer the loss game is because most of optimization theory is written for minimizing convex functions rather than for maximizing concave functions. However, clearly, this is an arbitrary choice. The second form of the regret shows that the player’s goal is to compete with the best single decision from \(\mathcal{X}\) but chosen given the hindsight of knowing all the choices of the adversary. That is, the learner’s goal is to keep its cumulative loss \(\sum_{j=0}^{k-1} \langle x_j, y_j \rangle\) close to, or even below the best cumulative loss in hindsight, \(\min_{x\in \mathcal{X}} \sum_{j=0}^{k-1}\langle x,y_j \rangle\). (With this, $T_1(s)$ matches \(R_k\) when we change \(\mathcal{Y} = [-1/(1-\gamma),0]^{\mathrm{A}}\).)</p> <p>MD is recursively defined and in its simplest form it has two design parameters. The first is an extended real-valued convex function \(F: \mathbb{R}^d \to \bar {\mathbb{R}}\), called the “regularizer”, while the second is a stepsize, or learning rate parameter $\eta&gt;0$. (The extended reals is just \(\mathbb{R}\) together with \(+\infty,-\infty\) and an appropriate extension of basic arithmetic. By allowing convex functions to take the value \(+\infty\) allows to merge “constraints” with objectives in a seamless fashion. The value $-\infty$ is added because sometimes we have to work with negated extended real-valued convex functions.)</p> <p>The specification of MD is as follows: In round $0$, $x_0\in \mathcal{X}$ is picked to minimize \(F\):</p> \[x_0 = \arg\min_{x\in \mathcal{X}} F(x)\,.\] <p>In what follows, we assume that all the minimizers that we need in the definition of MD do exist. In the specific case that we need, \(\mathcal{X}\) is the \(d-1\) simplex, which is a closed convex set, and since convex functions are also continuous, the minimizers that we will need are guaranteed to exist.</p> <p>Then, in round \(j&gt;0\), MD chooses \(x_j\) as follows:</p> \[\begin{equation} \begin{split} x_j &amp; = \arg\min_{x\in \mathcal{X}}\,\,\eta \langle x, y_{j-1} \rangle + D_F(x,x_{j-1}) \\ \end{split} \label{eq:mddef} \end{equation}\] <p><img src="/documents/images/lec14-uunegbregman.png" alt="drawing" width="200" align="right" hspace="20" vspace="20" /> Here,</p> \[D_F(x,x') = F(x)-(F(x')+\langle \nabla F(x'), x-x'\rangle)\] <p>is the remainder term in the first-order Taylor-series expansion of the value of $F$ at $x$ when the expansion is carried out at \(x'\) and, for simplicity, we assume that \(F\) is differentiable on the interior of its domain \(\text{dom}(F) = \{ x\in \mathbb{R}\,:\, F(x)&lt;+\infty \}\). Since for any convex function and any linear approximation of it stays below the graph of the convex function, we immediately get that \(D_F\) is nonnegative valued. For an illustration see the figure on the right, which shows a convex function, the first-order Taylor approximation of the function at some point.</p> <p>One should think of \(F\) is a “nonlinear distance inducing function”; above \(D_F(x,x')\) can be thought of penalty imposed on deviating from \(x'\). However, \(D_F\) is more often than not is not a distance, i.e., often it is not even symmetric. Because of this, we can’t really call $D_F$ a distance. Hence, it is called a divergence. In particular, \(D_F(x,x')\) is called the <strong>Bregman divergence</strong> of \(x\) from \(x'\).</p> <p>In the definition of the MD update rule, we tacitly assumed that \(D_F(x,x_{j-1})\) is well-defined. This requires that \(F\) should be differentiable at \(x_{j-1}\), which one needs to check when applying MD. In our specific case, this will hold, again. <!-- homework --></p> <p>The idea of the MD update rule is to (1) allow the learner to react to the last loss \(y_{j-1}\) vector chosen by the adversary, while also (2) limiting how much \(x_j\) can depart from \(x_{j-1}\), thus, effectively stabilizing the algorithm, the tradeoff governed by the choice of $\eta&gt;0$. (Separating \(\eta\) from \(F\) only makes sense because there are some standard choices for \(F\), but \(\eta\) is really just a scale parameter for \(F\)). In particular, the larger the value of \(\eta\) is, the less “data-sensitive” MD will be (here, \(y_0,\dots,y_{k-1}\) constitute the data), and vice versa, the smaller \(\eta\) is, the more data-sensitive MD will be.</p> <h3 id="where-is-the-mirror">Where is the mirror?</h3> <p>Under some technical conditions on \(F\), the update rule \eqref{eq:mddef} has a two step-implementation:</p> \[\begin{align} \tilde x_j &amp; = (\nabla F)^{-1} ( \nabla F (x_{j-1}) - \eta y_{j-1} )\,,\label{eq:mds1}\\ x_j &amp;= \arg\min_{x\in \mathcal{X}} D_F(x,\tilde x_j)\,.\label{eq:mds2} \end{align}\] <p>The first equation above explains the name: To obtain \(\tilde x_j\), one first transforms \(x_{j-1}\) using \(\nabla F: \text{dom}(\nabla F ) \to \mathbb{R}^d\) to the “mirror” (dual) space where “gradients”/”slopes live”, where one then adds to the result \(-\eta y_{j-1}\), which can be seen as a “gradient step” (interpreting \(y_{j-1}\) as the gradient of some loss). Finally, the result is then mapped back to the original (primal) space using the inverse of \(\nabla F\). <!-- gradient descent is MD --> <!-- OCO and linearization --></p> <p>The second step of the update takes the resulting point \(\tilde x_j\) and “projects” it to \(\mathcal{X}\) in a way that respects the “geometry induced by \(F\)” on the space \(\mathbb{R}^d\).</p> <p>The use of complex terminology, like “primal” and “dual” spaces, which happen to be the same old Euclidean space, \(\mathbb{R}^d\), probably sounds like an overkill. Indeed, in the simple case we consider when these spaces are identical it is. The distinction would become important when working with infinite dimensional spaces, which we leave to others for now.</p> <p>Besides helping with understanding the terminology, the two-step update shown can also be useful for computation. In fact, this will be the case in the special case that we need.</p> <h2 id="mirror-descent-on-the-simplex">Mirror descent on the simplex</h2> <p>We have seen that in the special case we need,</p> \[\begin{align*} \mathcal{X} &amp;= \mathcal{P}_{d-1}:=\{ p\in [0,1]^{d}\,:\, \sum_a p_a = 1 \}\,, \\ \mathcal{Y} &amp;= [-1/(1-\gamma),0]^d\,, \text{and} \\ d &amp;= \mathrm{A}\,. \end{align*}\] <p><img src="/documents/images/lec14-unnegentropy.png" alt="drawing" width="200" align="right" hspace="20" vspace="20" /> To use MD we need to specify the regularizer \(F\) and the learning rate. For the former, we choose</p> \[F(x) = \sum_i x_i \log(x_i) - x_i\,,\] <p>which is known as the <strong>unnormalized negentropy</strong> function. Note that \(F\) takes on finite values when \(x\in [0,\infty]^d\) (since \(\lim_{x\to 0+} x \log(x)=0\), we set \(x_i \log(x_i)=0\) whenever $x_i=0$). Outside of this quadrant, we define the value of \(F\) to be \(+\infty\). The plot of \(x\log(x)-x\) for \(x\ge 0\) is shown on the right.</p> <p>It is not hard to verify that \(F\) is convex: First, \(\text{dom}(F) = [0,\infty]^d\) is convex. Taking the first derivative, we find that for any \(x\in (0,\infty)^d\),</p> \[\nabla F(x) = \log(x)\,,\] <p>where \(\log\) is applied componentwise. Taking the derivative again, we find that for \(x\in (0,\infty)^d\),</p> \[\nabla^2 F(x) = \text{diag}(1/x)\,,\] <p>i.e., the matrix whose $(i,i)$th diagonal entry is \(1/x_i\). Clearly, this is a positive definite matrix, which suffices to verify that \(F\) is a convex function.</p> <p>The Bregman divergence induced by \(F\) is</p> \[\begin{align*} D_F(x,x') &amp; = \langle \boldsymbol{1}, x \log(x) - x - x' \log(x')+x'\rangle - \langle \log(x'), x-x'\rangle \\ &amp; = \langle \boldsymbol{1}, x \log(x/x') - x +x'\rangle \,, \end{align*}\] <p>where again we use an “intuitive” notation when operations are first applied componentwise (i.e., $x \log(x)$ denotes a vector whose $i$th component is $x_i \log(x_i)$). Note that the domain of $D_F$ is \([0,\infty)^d \times (0,\infty)^d\). <!-- note: can be extended to $x'=0$..--> If both $x$ and $x’$ lie in the $d-1$-simplex, $D_F$ becomes the well-known <strong>relative entropy</strong>, or <strong>Kullback-Leibler (KL) divergence</strong>.</p> <p>It is not hard to verify that $x_j$ can be obtained as shown in \eqref{eq:mds1}-\eqref{eq:mds2} and in particular this two-step update takes the form</p> \[\begin{align*} \tilde x_{j,i} &amp;= x_{j-1,i} \exp(-\eta y_{j-1,i})\,, \qquad x_{j,i} = \frac{\tilde x_{j,i}}{ \sum_{i'} \tilde x_{j,i'}}\,, \quad i\in [d]\,. \end{align*}\] <!-- homework: politex with approximate policy update step--> <p>Unrolling the recursion, we can also that this is the same as</p> \[\begin{equation} \tilde x_{j,i} = \exp(-\eta (y_{0,i}+\dots + y_{j-1,i}))\,, \qquad x_{j,i} = \frac{\tilde x_{j,i}}{ \sum_{i'} \tilde x_{j,i'}}\,, \quad i\in [d]\,. \label{eq:mdunrolled} \end{equation}\] <p>Based on this, it is obvious that MD can be efficiently implemented with this choice of $F$. As far as the regret is concerned, the following theorem holds:</p> <hr /> <p><strong>Theorem (MD with negentropy on the simplex):</strong> Let \(\mathcal{X}= \mathcal{P}_{d-1}\) amd \(\mathcal{Y} = [0,1]^d\). Then, no matter the adversary, a learner using MD with</p> \[\eta = \sqrt{ \frac{2\log(d)}{k}}\] <p>is guaranteed that its regret \(R_k\) in \(k\) rounds is at most</p> \[R_k \le \sqrt{2k \log(d)}\,.\] <hr /> <p>When the adversary plays in \(\mathcal{Y} = [a,b]^d\) with \(a&lt;b\), we can use MD on the transformed sequence \(\tilde y_j = (y_j-a \boldsymbol{1})/(b-a) \in [0,1]^d\). Then, for any $x\in \mathcal{X}$,</p> \[\begin{align*} R_k(x) &amp; := \sum_{j=0}^{k-1} \langle x_j-x, y_j \rangle \\ &amp; = \sum_{j=0}^{k-1} \langle x_j-x, (b-a)\tilde y_j+a \boldsymbol{1} \rangle \\ &amp; = (b-a)\sum_{j=0}^{k-1} \langle x_j-x, \tilde y_j \rangle \\ &amp; \le (b-a) \sqrt{2k \log(d)}\,, \end{align*}\] <p>where the third equality used that $\langle x_j,\boldsymbol{1}\rangle = \langle x, \boldsymbol{1} \rangle = 1$. Taking the maximum over $x\in \mathcal{X}$ gives that</p> \[\begin{align} R_k \le (b-a) \sqrt{2k \log(d)}\,. \label{eq:mdrbscaled} \end{align}\] <p>By the update rule in \eqref{eq:mdunrolled},</p> \[\begin{align*} \tilde x_{j,i} = \exp(-\eta (\tilde y_{0,i}+\dots + \tilde y_{j-1,i})) = \exp(-\eta/(b-a) (y_{0,i}+\dots + y_{j-1,i}-j b) )\,, \qquad i\in [d]\,. \end{align*}\] <p>Note that the “shift” by $-jb$ cancels out in the normalization step. Hence, MD in this case takes the form</p> \[\begin{equation} \begin{split} \tilde x_{j,i} &amp;= \exp(-\eta/(b-a) (y_{0,i}+\dots + y_{j-1,i}))\,, \qquad x_{j,i} = \frac{\tilde x_{j,i}}{ \sum_{i'} \tilde x_{j,i'}}\,, \quad i\in [d]\,, \label{eq:mdunrolledscaled} \end{split} \end{equation}\] <p>which is the same as before, except that the learning rate is scaled by $1/(b-a)$. In particular, in this case one can set</p> \[\begin{align} \eta = \frac{1}{b-a} \sqrt{\frac{2\log(d)}{k}}\,. \label{eq:etascaled} \end{align}\] <p>and use update rule \eqref{eq:mdunrolled}.</p> <h2 id="md-applied-to-mdp-planning">MD applied to MDP planning</h2> <p>As agreed, \(T_1(s)\) from \eqref{eq:t1s} takes the form of a $k$-round regret against \(\pi^*(s,\cdot)\) in online linear optimization on the simplex with losses in \([-1/(1-\gamma),0]^{\mathrm{A}}\). This suggest to use MD in a state-by-state manner to control \(T_1(s)\). Using \eqref{eq:mdunrolled} and \eqref{eq:etascaled} gives</p> \[E_j(s,a) = \exp(\eta (\hat q_0(s,a) +\dots + \hat q_{j-1}(s,a)))\,, \qquad \pi_j(a|s) = \frac{E_j(s,a)}{ \sum_{a'} E_j(s,a')}\,, \quad a\in \mathcal{A}\] <p>to be used with</p> \[\eta = (1-\gamma) \sqrt{\frac{2\log(\mathrm{A})}{k}}\,.\] <p>Note that this is the update used by Politex. Then, \eqref{eq:mdrbscaled} gives that simultaneously for all $s\in \mathcal{S}$,</p> \[\begin{align} |T_1(s)| \le \frac{1}{1-\gamma} \sqrt{2k \log(\mathrm{A})}\,. \label{eq:t1sbound} \end{align}\] <p>Putting things together, we get the following result:</p> <hr /> <p><strong>Theorem (Politex suboptimality gap bound):</strong> Pick a featurized MDP $(M,\phi)$ with a full rank feature-map $\varphi: \mathcal{S}\times \mathcal{A} \to \mathbb{R}^d$ and let $K,m,H\ge 1$. Assume that <a href="/lecture-notes/planning-in-mdps/lec8#ass:b2e">B2\(_{\varepsilon}\)</a> holds for $(M,\phi)$ and the rewards in $M$ are in the $[0,1]$ interval. For $0\le \zeta&lt;1$, define</p> \[\kappa(\zeta) = \varepsilon (1 + \sqrt{d}) + \sqrt{d} \left(\frac{\gamma^H}{1 - \gamma} + \frac{1}{1 - \gamma} \sqrt{\frac{\log( d(d+1)K / \zeta)}{2m}}\right)\,,\] <p>Then, in $K$ iterations, Politex produces a mixed policy $\bar \pi_K$ such that with probability $1-\zeta$, the suboptimality gap $\delta$ of $\bar \pi_K$ satisfies</p> \[\begin{align*} \delta \le \frac{1}{(1-\gamma)^2}\sqrt{\frac{2\log(\mathrm{A})}{K}}+\frac{2 \kappa(\zeta) }{1-\gamma} \,. \end{align*}\] <p>In particular, for any $\varepsilon’&gt;0$, choosing $K,H,m$ so that</p> \[\begin{align*} K &amp; \ge \frac{32 \log(A)}{ (1-\gamma)^4 (\varepsilon')^2}\,, \\ H &amp; \ge H_{\gamma,(1-\gamma)\varepsilon'/(8\sqrt{d})} \qquad \text{and} \\ m &amp; \ge \frac{32 d}{(1-\gamma)^4 (\varepsilon')^2} \log( (d+1)^2 K /\zeta )\,, \end{align*}\] <p>policy $\pi_K$ is $\delta$-optimal with</p> \[\begin{align*} \delta \le \frac{2(1 + \sqrt{d})}{1-\gamma}\, \varepsilon + \varepsilon'\,, \end{align*}\] <p>while the total computation cost is $\text{poly}(\frac{1}{1-\gamma},d,\mathrm{A},\frac{1}{(\varepsilon’)^2},\log(1/\zeta))$.</p> <hr /> <p>Note that as compared to the result of LSPI with G-optimal design, the amplification of the approximation error $\varepsilon$ is reduced by a factor of $1/(1-\gamma)$, as it was promised. The price is that now the number of iterations $K$, is a polynomial of $\frac{1}{(1-\gamma)\varepsilon’}$, whereas before it was logarithmic. This suggest that perhaps a higher learning rate can help initially to speed up convergence to get the best of both words. <!-- run simulation on Russo's example! --></p> <p><strong>Proof:</strong> As <a href="/lecture-notes/planning-in-mdps/lec8#sec:lspiproof">in the proof</a> of the suboptimality gap for LSPI, we get that for any $0\le \zeta \le 1$, with probability at least $1-\zeta$, for any $0 \le k \le K-1$,</p> \[\begin{align*} \| q^{\pi_k} - \hat q_k \|_\infty =\| q^{\pi_k} - \Pi \Phi \hat \theta_k \|_\infty \le \| q^{\pi_k} - \Phi \hat \theta_k \|_\infty &amp;\leq \kappa(\zeta)\,, \end{align*}\] <p>where the first inequality uses that \(q_{\pi_k}\) takes values in $[0,1]$. On the event when the above inequalities hold, by \eqref{eq:polsubgapgen} and \eqref{eq:t1sbound},</p> \[\begin{align*} \delta \le \frac{1}{(1-\gamma)^2}\sqrt{\frac{2\log(\mathrm{A})}{K}}+\frac{2 \kappa(\zeta) }{1-\gamma} \,. \end{align*}\] <p>The details of this calculation are left to the reader. \(\qquad \blacksquare\)</p> <h2 id="notes">Notes</h2> <h3 id="optimality-of-the-final-policy">Optimality of the Final Policy</h3> <p>Notice that we said the policy returned by Politex after $k$ iterations should be a mixture policy $\bar \pi_k = \frac{1}{k} (\pi_0 + \dots + \pi_{k-1})$. A more natural policy to return is the final policy \(\pi_{k-1}\). The question then is: can one ensure similar optimality gaurantees for the final policy $\pi_{k-1}$ as we have seen for $\bar \pi_k$? The answer turns out to be yes, if we use the unnormalized negentropy regularizer for mirror descent (as we have already been using in this lecture note). To see this, we aim to bound \(\| v^{\pi^*} - v^{\pi_{k-1}} \|_\infty\). We begin by writing.</p> \[\begin{align*} v^{\pi^*} - v^{\pi_{k-1}} &amp; = v^{\pi^*} - v^{\bar \pi_k} + v^{\bar \pi_k} - v^{\pi_{k-1}} \\ &amp; = \frac1k (I-\gamma P_{\pi^*})^{-1} \sum_{j=0}^{k-1} M_{\pi^*} q^{\pi_j} - M_{\pi_j} q^{\pi_j} \\ &amp; \quad + \frac1k (I-\gamma P_{\pi_{k-1}})^{-1} \sum_{j=0}^{k-1} M_{\pi_j} q^{\pi_j} - M_{\pi_{k-1}} q^{\pi_j}\\ &amp; = \frac1k(I-\gamma P_{\pi^*})^{-1} \underbrace{ \sum_{j=0}^{k-1} M_{\pi^*} \hat q_j - M_{\pi_j} \hat q_j}_{T_1} \\ &amp; \quad + \underbrace{\frac1k (I-\gamma P_{\pi^*})^{-1} \sum_{j=0}^{k-1} ( M_{\pi^*} - M_{\pi_j} )( q^{\pi_j}-\hat q_j)}_{T_2} \\ &amp; \quad + \frac1k(I-\gamma P_{\pi_{k-1}})^{-1} \underbrace{ \sum_{j=0}^{k-1} M_{\pi_j} \hat q_j - M_{\pi_{k-1}} \hat q_j}_{T_3} \\ &amp; \quad + \underbrace{\frac1k (I-\gamma P_{\pi_{k-1}})^{-1} \sum_{j=0}^{k-1} ( M_{\pi_j} - M_{\pi_{k-1}} )( q^{\pi_j}-\hat q_j)}_{T_4} \,. \end{align*}\] <p>Notice how $T_1$ and $T_2$ are defined as before, and we already have bounds for both of them. It is also easy to see that that $T_4$ takes a very similar form to $T_2$ and can also be bounded in the same way as $T_2$. If we can show that $T_3(s) \le 0$ for all $s \in \mathcal{S}$ then we would get the result that</p> \[\begin{align*} \|v^{\pi^*} - v^{\pi_{k-1}}\|_\infty \le \delta \le \frac{1}{(1-\gamma)^2}\sqrt{\frac{2\log(\mathrm{A})}{K}}+\frac{\textcolor{blue}{4} \kappa(\zeta) }{1-\gamma} \,. \end{align*}\] <p>Which is identical to the result of the main theorem in the lecture note above, except with the constant $2$ scaling replaced with a constant $\textcolor{blue}{4}$ scaling infront of the approximation error (since $T_4$ used the same bound as $T_2$).</p> <p>We are left to show that indeed $T_3(s) \le 0$. To do this we first write out $T_3(s)$ in vector notation to help us aline with the math syntax to come. Fix a state $s \in \mathcal{S}$, then</p> \[T_3(s) = \sum_{j=0}^{k-1} \langle \pi_j(\cdot|s), \hat q_j(s, \cdot) \rangle - \langle \pi_{k-1}(\cdot|s), \hat q_j(s, \cdot) \rangle\] <p>Since we will hold $s$ fixed for all the following steps we slightly abuse notation in favor of avoiding clutter and write the above equation as follows where it is assumed that all functions were first evaluated at $s$.</p> \[T_3 = \sum_{j=0}^{k-1} \langle \pi_j, \hat q_j \rangle - \langle \pi_{k-1}, \hat q_j \rangle\] <p>Next recall that the policy selected by MD at iteration $k$ is defined as</p> \[\begin{align*} \pi_k &amp; = \arg\min_{\pi \in \mathcal{M}_1(A)}\,\,\eta \langle \pi, -\hat q_{k-1} \rangle + D_F(\pi,\pi_{k-1}) \\ &amp; = \arg\max_{\pi \in \mathcal{M}_1(A)}\,\,\eta \langle \pi, \hat q_{k-1} \rangle - D_F(\pi,\pi_{k-1}) \end{align*}\] <p>where we have negated $\hat q_{k-1}$ to formulate our problem as a minimization problem as was needed for the MD analysis. If we set $F$ to the unnormalized negentropy regularizer (as was done in the notes above)</p> \[F(x) = \sum_i x_i \log(x_i) - x_i\,,\] <p>we have that</p> \[\begin{align*} \pi_k &amp; = \arg\max_{\pi \in \mathcal{M}_1(A)}\,\,\eta \langle \pi, \hat q_{k-1} \rangle - KL(\pi || \pi_{k-1}) \end{align*}\] <p>which turns out to be equivilant to</p> \[\begin{align} \pi_k &amp; = \arg\max_{\pi \in \mathcal{M}_1(A)}\,\,\eta \langle \pi, \sum_{j=0}^{k-1} \hat q_{j} \rangle - F(\pi). \label{eq:ftrlpolicy} \end{align}\] <p>The above equation is the policy selection made by the Follow The Regularized Leader (FTRL) algorithm. For further details of the equavilance between MD and FTRL when $F$ is the unnormalized negentropy one can refer to chapter 28 of the <a href="https://tor-lattimore.com/downloads/book/book.pdf">Bandit Book</a>. Importantly, the above equation will be useful for our proof.</p> <p>We will now show that $T_3 \le 0$ by showing that</p> \[\sum_{j=0}^{k-1} \langle \pi_{k-1}, \hat q_j \rangle \ge \sum_{j=0}^{k-1} \langle \pi_{j}, \hat q_j \rangle\] <p>To do this notice that</p> \[\begin{align*} \eta \sum_{j=0}^{k-1} \langle \pi_{k-1}, \hat q_j \rangle &amp; = \eta \langle \pi_{k-1}, \hat q_{k-1} \rangle + \eta \langle \pi_{k-1}, \sum_{j=0}^{k-2} \hat q_j \rangle - F(\pi_{k-1}) + F(\pi_{k-1}) \\ &amp; \ge \eta \langle \pi_{k-1}, \hat q_{k-1} \rangle + \eta \langle \pi_{k-2}, \sum_{j=0}^{k-2} \hat q_j \rangle - F(\pi_{k-2}) + F(\pi_{k-1}) \\ &amp; \ge \eta \sum_{j=0}^{k-1} \langle \pi_{j}, \hat q_j \rangle - F(\pi_0) + F(\pi_{k-1}) \\ &amp; \ge \eta \sum_{j=0}^{k-1} \langle \pi_{j}, \hat q_j \rangle \\ \end{align*}\] <p>where the first inequality holds since by \eqref{eq:ftrlpolicy} we know that</p> \[\pi_{k-1} = \arg\max_{\pi \in \mathcal{M}_1(A)}\,\,\eta \langle \pi, \sum_{j=0}^{k-2} \hat q_{j} \rangle - F(\pi).\] <p>The second inequality holds by repeatadly apply the first two steps.</p> <p>The third inequality holds since $\pi_0$ was initialized as \(\pi_0 = \arg\min_{\pi \in \mathcal{M}_1(A)} F(\pi)\) so we have that $F(\pi_{k-1}) - F(\pi_0) \ge 0$. Which concludes the argument.</p> <h3 id="online-convex-optimization-online-learning">Online convex optimization, online learning</h3> <p>Online linear optimization is a special case of <strong>online convex/concave optimization</strong>, where the learner chooses elements of some nonempty convex set \(\mathcal{X}\subset \mathbb{R}^d\) and the adversary needs to choose an element of a nonempty set \(\mathcal{Y}\) of concave functions over \(\mathcal{X}\): \(\mathcal{Y} \subset \{ f: \mathcal{X} \to \mathbb{R}\,:\, f \text{ is concave} \}\). Then, the definition of regret is changed to</p> \[\begin{align} R_k = \max_{x\in \mathcal{X}}\sum_{j=0}^{k-1} y_j(x) - y_j(x_j) \,, \label{eq:regretdefoco1} \end{align}\] <p>where as before \(x_j\in \mathcal{X}\) is the choice of the learner for round \(j\) and \(y_j\in \mathcal{Y}\) is the choice of the adversary for the same round. Identifying any vector \(u\) of \(\mathbb{R}^d\) with the linear map \(x \mapsto \langle x, u \rangle\), we see that online linear optimization is a special case of this problem.</p> <p>Of course, by negating all functions in \(\mathcal{Y}\) (i.e., letting \(\tilde {\mathcal{Y}} = \{ - y \,:\, y\in \mathcal{Y} \}\)) and redefining the regret to</p> \[\begin{align} R_k = \max_{x\in \mathcal{X}}\sum_{j=0}^{k-1} \tilde y_j(x_j)- \tilde y_j(x) \, \label{eq:regretdefoco} \end{align}\] <p>we get a definition that is used in the literature, which prefers the convex case to the concave. Here, the interpretation is that \(\tilde y_j\in \tilde {\mathcal{Y}}\) is a “loss function” chosen by the adversary in round \(j\).</p> <p>The standard function notation (\(y_j\) is applied to \(x\)) injects unwarranted asymmetry in the notation. After all, from the perspective of the learner, they need to choose a value in \(\mathcal{X}\) that works for the various functions in \(\mathcal{Y}\). Thus, we can consider any element of \(\mathcal{X}\) as a function that maps elements of \(\mathcal{Y}\) to reals through \(y \mapsto y(x)\). Whether \(\mathcal{Y}\) has functions in them or \(\mathcal{X}\) has functions in them does not matter that much; it is the interconnection between \(\mathcal{X}\) and \(\mathcal{Y}\) that matters more. For this reason, one can study online learning when \(y(x)\) above is replaced by \(b(x,y)\), where \(b: \mathcal{X}\times \mathcal{Y} \to \mathbb{R}\) is a specific map that assigns payoffs to every pair of points in \(\mathcal{X}\) and \(\mathcal{Y}\). When the map is fixed, one can spare an extra symbol by just using \([x,y]\) in place of \(b(x,y)\), which makes things almost a full circle given that we started with the linear case when \([x,y] = \langle x,y \rangle\).</p> <h3 id="truncation-or-no-truncation">Truncation or no truncation?</h3> <p>We introduced truncation to simplify the analysis. The proof can be made to go through even without it, with a mild increase of the suboptimality gap (or runtime). The advantage of removing the projection is that without projection, \(\hat q_0 + \dots + \hat q_{j-1} = \Phi (\hat \theta_0 + \dots + \hat \theta_{j-1})\), which leads to a practically significant reduction of the runtime.</p> <h2 id="references">References</h2> <p>The optimality of the final policy presented in the Notes was shown by <a href="https://scholar.google.com/citations?user=4VJmx8QAAAAJ&amp;hl=en">Tadashi Kozuno</a> when he taugh this lecture in Winter 2022.</p> <!-- notes: weak adversaries --> <!-- MD and info geometry? --> <!-- unbounded range? --> <!-- connection to gradient descent --> <!-- optimality of mirror descent --> <!-- implementation of mirror descent --> <!-- references to online learning; theorem here Prop 28.7 of bandit book --> <!-- Bruno's paper: nonstationary API does it as well? --> <!-- Levarage the average; AVI focused, but interpolates between API and AVI --> <hr> <footer> <p><a href="#top" id="back-to-top">Back to top</a></p> <p class="text-small text-grey-dk-100 mb-0">Copyright &copy; 2024 RL Theory.</p> <div class="d-flex mt-2"> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
