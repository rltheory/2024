I"Ž<p>In the last lecture we have seen that given a discounted MDP $M = (\mathcal{S},\mathcal{A},P,r,\gamma)$,
a feature-map $\varphi: \mathcal{S}\times\mathcal{A}\to \mathbb{R}^d$
and a precomputed, suitably small core set,
for any $\varepsilonâ€™&gt;0$ target and any confidence parameter $0\le \zeta \le 1$,
interacting with a simulator of $M$, with at most
 $\text{poly}(\frac{1}{1-\gamma},d,\mathrm{A},\frac{1}{(\varepsilonâ€™)^2},\log(1/\zeta))$,
 compute time,
 LSPI returns some weight vector $\theta\in \mathbb{R}^d$ such that
 with probability $1-\zeta$, the policy that is greedy with respect to $q = \Phi \theta$
is $\delta$-suboptimal with</p>
:ET